Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(200, 150),b=(150,)
w=(150, 100),b=(100,)
w=(100, 50),b=(50,)
decomposing tensor W of shape (3, 200, 150)...
decomposing tensor B of shape (3, 150)...
Starting training...
Epoch 1 of 2000 took 0.109s
  training loss:		2.990697
  validation loss:		2.979624
  validation accuracy:		12.93 %
Epoch 2 of 2000 took 0.104s
  training loss:		2.967161
  validation loss:		2.952358
  validation accuracy:		12.93 %
Epoch 3 of 2000 took 0.104s
  training loss:		2.937513
  validation loss:		2.923633
  validation accuracy:		12.93 %
Epoch 4 of 2000 took 0.107s
  training loss:		2.907507
  validation loss:		2.894832
  validation accuracy:		12.93 %
Epoch 5 of 2000 took 0.109s
  training loss:		2.877942
  validation loss:		2.866180
  validation accuracy:		12.93 %
Epoch 6 of 2000 took 0.105s
  training loss:		2.848608
  validation loss:		2.836915
  validation accuracy:		5.43 %
Epoch 7 of 2000 took 0.106s
  training loss:		2.818663
  validation loss:		2.806300
  validation accuracy:		0.00 %
Epoch 8 of 2000 took 0.105s
  training loss:		2.789060
  validation loss:		2.774577
  validation accuracy:		0.00 %
Epoch 9 of 2000 took 0.105s
  training loss:		2.758532
  validation loss:		2.740812
  validation accuracy:		0.00 %
Epoch 10 of 2000 took 0.105s
  training loss:		2.725471
  validation loss:		2.704834
  validation accuracy:		0.00 %
Epoch 11 of 2000 took 0.105s
  training loss:		2.688505
  validation loss:		2.666285
  validation accuracy:		12.83 %
Epoch 12 of 2000 took 0.105s
  training loss:		2.655948
  validation loss:		2.625594
  validation accuracy:		12.83 %
Epoch 13 of 2000 took 0.105s
  training loss:		2.618412
  validation loss:		2.583166
  validation accuracy:		12.83 %
Epoch 14 of 2000 took 0.105s
  training loss:		2.580818
  validation loss:		2.539128
  validation accuracy:		12.83 %
Epoch 15 of 2000 took 0.105s
  training loss:		2.541731
  validation loss:		2.493620
  validation accuracy:		12.83 %
Epoch 16 of 2000 took 0.105s
  training loss:		2.503582
  validation loss:		2.449500
  validation accuracy:		12.83 %
Epoch 17 of 2000 took 0.111s
  training loss:		2.469021
  validation loss:		2.408039
  validation accuracy:		12.83 %
Epoch 18 of 2000 took 0.105s
  training loss:		2.438343
  validation loss:		2.371342
  validation accuracy:		12.83 %
Epoch 19 of 2000 took 0.105s
  training loss:		2.407483
  validation loss:		2.340482
  validation accuracy:		12.83 %
Epoch 20 of 2000 took 0.105s
  training loss:		2.381893
  validation loss:		2.315214
  validation accuracy:		12.83 %
Epoch 21 of 2000 took 0.105s
  training loss:		2.363434
  validation loss:		2.296285
  validation accuracy:		12.83 %
Epoch 22 of 2000 took 0.105s
  training loss:		2.347924
  validation loss:		2.282904
  validation accuracy:		12.83 %
Epoch 23 of 2000 took 0.105s
  training loss:		2.336589
  validation loss:		2.273015
  validation accuracy:		22.39 %
Epoch 24 of 2000 took 0.105s
  training loss:		2.328524
  validation loss:		2.267898
  validation accuracy:		15.98 %
Epoch 25 of 2000 took 0.105s
  training loss:		2.321486
  validation loss:		2.264138
  validation accuracy:		27.61 %
Epoch 26 of 2000 took 0.105s
  training loss:		2.317665
  validation loss:		2.261363
  validation accuracy:		20.65 %
Epoch 27 of 2000 took 0.111s
  training loss:		2.313071
  validation loss:		2.258839
  validation accuracy:		12.93 %
Epoch 28 of 2000 took 0.105s
  training loss:		2.310692
  validation loss:		2.258141
  validation accuracy:		23.26 %
Epoch 29 of 2000 took 0.105s
  training loss:		2.306751
  validation loss:		2.254486
  validation accuracy:		22.61 %
Epoch 30 of 2000 took 0.105s
  training loss:		2.306224
  validation loss:		2.253416
  validation accuracy:		16.96 %
Epoch 31 of 2000 took 0.105s
  training loss:		2.304613
  validation loss:		2.250119
  validation accuracy:		21.09 %
Epoch 32 of 2000 took 0.105s
  training loss:		2.302911
  validation loss:		2.247129
  validation accuracy:		14.67 %
Epoch 33 of 2000 took 0.105s
  training loss:		2.302472
  validation loss:		2.247391
  validation accuracy:		22.83 %
Epoch 34 of 2000 took 0.105s
  training loss:		2.300262
  validation loss:		2.250807
  validation accuracy:		12.93 %
Epoch 35 of 2000 took 0.106s
  training loss:		2.299075
  validation loss:		2.248897
  validation accuracy:		15.54 %
Epoch 36 of 2000 took 0.111s
  training loss:		2.299052
  validation loss:		2.246785
  validation accuracy:		18.15 %
Epoch 37 of 2000 took 0.105s
  training loss:		2.298019
  validation loss:		2.244149
  validation accuracy:		16.52 %
Epoch 38 of 2000 took 0.105s
  training loss:		2.297891
  validation loss:		2.247423
  validation accuracy:		25.11 %
Epoch 39 of 2000 took 0.105s
  training loss:		2.296986
  validation loss:		2.245083
  validation accuracy:		13.80 %
Epoch 40 of 2000 took 0.105s
  training loss:		2.297377
  validation loss:		2.245089
  validation accuracy:		16.85 %
Epoch 41 of 2000 took 0.105s
  training loss:		2.295011
  validation loss:		2.243054
  validation accuracy:		18.48 %
Epoch 42 of 2000 took 0.105s
  training loss:		2.295783
  validation loss:		2.243365
  validation accuracy:		14.13 %
Epoch 43 of 2000 took 0.105s
  training loss:		2.295206
  validation loss:		2.242845
  validation accuracy:		26.74 %
Epoch 44 of 2000 took 0.105s
  training loss:		2.294829
  validation loss:		2.243440
  validation accuracy:		20.54 %
Epoch 45 of 2000 took 0.105s
  training loss:		2.294102
  validation loss:		2.242417
  validation accuracy:		23.48 %
Epoch 46 of 2000 took 0.111s
  training loss:		2.294235
  validation loss:		2.242688
  validation accuracy:		18.26 %
Epoch 47 of 2000 took 0.105s
  training loss:		2.292510
  validation loss:		2.238982
  validation accuracy:		20.33 %
Epoch 48 of 2000 took 0.105s
  training loss:		2.292299
  validation loss:		2.237593
  validation accuracy:		21.41 %
Epoch 49 of 2000 took 0.105s
  training loss:		2.292169
  validation loss:		2.237757
  validation accuracy:		21.20 %
Epoch 50 of 2000 took 0.105s
  training loss:		2.292742
  validation loss:		2.238826
  validation accuracy:		23.59 %
Epoch 51 of 2000 took 0.105s
  training loss:		2.292539
  validation loss:		2.240500
  validation accuracy:		22.50 %
Epoch 52 of 2000 took 0.105s
  training loss:		2.291528
  validation loss:		2.240191
  validation accuracy:		26.63 %
Epoch 53 of 2000 took 0.105s
  training loss:		2.291655
  validation loss:		2.239128
  validation accuracy:		24.46 %
Epoch 54 of 2000 took 0.105s
  training loss:		2.291739
  validation loss:		2.236951
  validation accuracy:		23.70 %
Epoch 55 of 2000 took 0.110s
  training loss:		2.290771
  validation loss:		2.238206
  validation accuracy:		15.22 %
Epoch 56 of 2000 took 0.105s
  training loss:		2.291469
  validation loss:		2.241760
  validation accuracy:		24.13 %
Epoch 57 of 2000 took 0.105s
  training loss:		2.290438
  validation loss:		2.239590
  validation accuracy:		28.70 %
Epoch 58 of 2000 took 0.105s
  training loss:		2.291038
  validation loss:		2.234979
  validation accuracy:		24.89 %
Epoch 59 of 2000 took 0.105s
  training loss:		2.290178
  validation loss:		2.240913
  validation accuracy:		20.98 %
Epoch 60 of 2000 took 0.105s
  training loss:		2.289639
  validation loss:		2.241287
  validation accuracy:		25.98 %
Epoch 61 of 2000 took 0.105s
  training loss:		2.290313
  validation loss:		2.237627
  validation accuracy:		27.72 %
Epoch 62 of 2000 took 0.105s
  training loss:		2.289868
  validation loss:		2.237939
  validation accuracy:		27.17 %
Epoch 63 of 2000 took 0.105s
  training loss:		2.288965
  validation loss:		2.237967
  validation accuracy:		23.48 %
Epoch 64 of 2000 took 0.108s
  training loss:		2.288246
  validation loss:		2.235117
  validation accuracy:		29.35 %
Epoch 65 of 2000 took 0.109s
  training loss:		2.288608
  validation loss:		2.236060
  validation accuracy:		22.28 %
Epoch 66 of 2000 took 0.105s
  training loss:		2.288174
  validation loss:		2.232520
  validation accuracy:		22.72 %
Epoch 67 of 2000 took 0.105s
  training loss:		2.287111
  validation loss:		2.234827
  validation accuracy:		22.50 %
Epoch 68 of 2000 took 0.109s
  training loss:		2.286855
  validation loss:		2.232777
  validation accuracy:		26.20 %
Epoch 69 of 2000 took 0.106s
  training loss:		2.286553
  validation loss:		2.233224
  validation accuracy:		24.35 %
Epoch 70 of 2000 took 0.105s
  training loss:		2.287035
  validation loss:		2.230534
  validation accuracy:		25.43 %
Epoch 71 of 2000 took 0.105s
  training loss:		2.285817
  validation loss:		2.231700
  validation accuracy:		25.87 %
Epoch 72 of 2000 took 0.105s
  training loss:		2.285395
  validation loss:		2.234235
  validation accuracy:		22.39 %
Epoch 73 of 2000 took 0.105s
  training loss:		2.285253
  validation loss:		2.231662
  validation accuracy:		26.30 %
Epoch 74 of 2000 took 0.111s
  training loss:		2.286660
  validation loss:		2.233583
  validation accuracy:		30.65 %
Epoch 75 of 2000 took 0.105s
  training loss:		2.285143
  validation loss:		2.232078
  validation accuracy:		23.80 %
Epoch 76 of 2000 took 0.105s
  training loss:		2.285774
  validation loss:		2.235296
  validation accuracy:		26.85 %
Epoch 77 of 2000 took 0.105s
  training loss:		2.283965
  validation loss:		2.232240
  validation accuracy:		29.89 %
Epoch 78 of 2000 took 0.105s
  training loss:		2.283799
  validation loss:		2.229111
  validation accuracy:		22.93 %
Epoch 79 of 2000 took 0.105s
  training loss:		2.282670
  validation loss:		2.228597
  validation accuracy:		23.26 %
Epoch 80 of 2000 took 0.105s
  training loss:		2.282475
  validation loss:		2.227846
  validation accuracy:		25.11 %
Epoch 81 of 2000 took 0.105s
  training loss:		2.282846
  validation loss:		2.231224
  validation accuracy:		27.72 %
Epoch 82 of 2000 took 0.105s
  training loss:		2.281819
  validation loss:		2.228496
  validation accuracy:		24.67 %
Epoch 83 of 2000 took 0.110s
  training loss:		2.281362
  validation loss:		2.225642
  validation accuracy:		30.00 %
Epoch 84 of 2000 took 0.107s
  training loss:		2.280743
  validation loss:		2.225799
  validation accuracy:		26.96 %
Epoch 85 of 2000 took 0.105s
  training loss:		2.280018
  validation loss:		2.228255
  validation accuracy:		26.52 %
Epoch 86 of 2000 took 0.105s
  training loss:		2.279746
  validation loss:		2.221993
  validation accuracy:		26.09 %
Epoch 87 of 2000 took 0.105s
  training loss:		2.278281
  validation loss:		2.223249
  validation accuracy:		24.89 %
Epoch 88 of 2000 took 0.105s
  training loss:		2.278301
  validation loss:		2.223153
  validation accuracy:		23.91 %
Epoch 89 of 2000 took 0.105s
  training loss:		2.278039
  validation loss:		2.225798
  validation accuracy:		30.00 %
Epoch 90 of 2000 took 0.105s
  training loss:		2.276393
  validation loss:		2.221456
  validation accuracy:		24.57 %
Epoch 91 of 2000 took 0.105s
  training loss:		2.276235
  validation loss:		2.222701
  validation accuracy:		23.48 %
Epoch 92 of 2000 took 0.106s
  training loss:		2.275318
  validation loss:		2.214589
  validation accuracy:		28.37 %
Epoch 93 of 2000 took 0.110s
  training loss:		2.275024
  validation loss:		2.222258
  validation accuracy:		19.57 %
Epoch 94 of 2000 took 0.105s
  training loss:		2.274020
  validation loss:		2.216666
  validation accuracy:		27.07 %
Epoch 95 of 2000 took 0.105s
  training loss:		2.271743
  validation loss:		2.217459
  validation accuracy:		27.83 %
Epoch 96 of 2000 took 0.105s
  training loss:		2.271196
  validation loss:		2.213872
  validation accuracy:		25.43 %
Epoch 97 of 2000 took 0.105s
  training loss:		2.269197
  validation loss:		2.214932
  validation accuracy:		28.91 %
Epoch 98 of 2000 took 0.105s
  training loss:		2.269254
  validation loss:		2.209982
  validation accuracy:		23.91 %
Epoch 99 of 2000 took 0.105s
  training loss:		2.267000
  validation loss:		2.208771
  validation accuracy:		29.78 %
Epoch 100 of 2000 took 0.105s
  training loss:		2.266235
  validation loss:		2.210909
  validation accuracy:		26.09 %
Epoch 101 of 2000 took 0.105s
  training loss:		2.265386
  validation loss:		2.207692
  validation accuracy:		27.17 %
Epoch 102 of 2000 took 0.110s
  training loss:		2.263118
  validation loss:		2.205996
  validation accuracy:		30.22 %
Epoch 103 of 2000 took 0.106s
  training loss:		2.261453
  validation loss:		2.204490
  validation accuracy:		24.67 %
Epoch 104 of 2000 took 0.105s
  training loss:		2.259105
  validation loss:		2.196607
  validation accuracy:		23.91 %
Epoch 105 of 2000 took 0.106s
  training loss:		2.257912
  validation loss:		2.197867
  validation accuracy:		34.57 %
Epoch 106 of 2000 took 0.105s
  training loss:		2.255383
  validation loss:		2.192880
  validation accuracy:		28.48 %
Epoch 107 of 2000 took 0.105s
  training loss:		2.253178
  validation loss:		2.193421
  validation accuracy:		26.41 %
Epoch 108 of 2000 took 0.105s
  training loss:		2.250747
  validation loss:		2.194303
  validation accuracy:		26.85 %
Epoch 109 of 2000 took 0.105s
  training loss:		2.248223
  validation loss:		2.187508
  validation accuracy:		27.39 %
Epoch 110 of 2000 took 0.105s
  training loss:		2.243492
  validation loss:		2.179929
  validation accuracy:		29.02 %
Epoch 111 of 2000 took 0.105s
  training loss:		2.240541
  validation loss:		2.174736
  validation accuracy:		28.48 %
Epoch 112 of 2000 took 0.111s
  training loss:		2.236866
  validation loss:		2.174095
  validation accuracy:		27.07 %
Epoch 113 of 2000 took 0.105s
  training loss:		2.233844
  validation loss:		2.166570
  validation accuracy:		28.04 %
Epoch 114 of 2000 took 0.105s
  training loss:		2.229353
  validation loss:		2.164009
  validation accuracy:		28.80 %
Epoch 115 of 2000 took 0.105s
  training loss:		2.222929
  validation loss:		2.157739
  validation accuracy:		28.80 %
Epoch 116 of 2000 took 0.105s
  training loss:		2.219053
  validation loss:		2.149528
  validation accuracy:		28.80 %
Epoch 117 of 2000 took 0.105s
  training loss:		2.213889
  validation loss:		2.144099
  validation accuracy:		29.35 %
Epoch 118 of 2000 took 0.105s
  training loss:		2.204929
  validation loss:		2.135989
  validation accuracy:		27.39 %
Epoch 119 of 2000 took 0.105s
  training loss:		2.196734
  validation loss:		2.123189
  validation accuracy:		27.61 %
Epoch 120 of 2000 took 0.105s
  training loss:		2.188935
  validation loss:		2.112969
  validation accuracy:		28.70 %
Epoch 121 of 2000 took 0.111s
  training loss:		2.180181
  validation loss:		2.104420
  validation accuracy:		28.04 %
Epoch 122 of 2000 took 0.106s
  training loss:		2.168593
  validation loss:		2.087167
  validation accuracy:		28.80 %
Epoch 123 of 2000 took 0.105s
  training loss:		2.158083
  validation loss:		2.077865
  validation accuracy:		32.39 %
Epoch 124 of 2000 took 0.105s
  training loss:		2.144159
  validation loss:		2.056118
  validation accuracy:		32.17 %
Epoch 125 of 2000 took 0.105s
  training loss:		2.126358
  validation loss:		2.034084
  validation accuracy:		29.89 %
Epoch 126 of 2000 took 0.105s
  training loss:		2.109848
  validation loss:		2.017366
  validation accuracy:		28.70 %
Epoch 127 of 2000 took 0.105s
  training loss:		2.093902
  validation loss:		1.996312
  validation accuracy:		31.63 %
Epoch 128 of 2000 took 0.105s
  training loss:		2.075603
  validation loss:		1.975938
  validation accuracy:		31.41 %
Epoch 129 of 2000 took 0.105s
  training loss:		2.051959
  validation loss:		1.945146
  validation accuracy:		30.43 %
Epoch 130 of 2000 took 0.105s
  training loss:		2.031727
  validation loss:		1.924937
  validation accuracy:		30.00 %
Epoch 131 of 2000 took 0.110s
  training loss:		2.010401
  validation loss:		1.899165
  validation accuracy:		32.93 %
Epoch 132 of 2000 took 0.105s
  training loss:		1.983858
  validation loss:		1.873198
  validation accuracy:		35.76 %
Epoch 133 of 2000 took 0.105s
  training loss:		1.961177
  validation loss:		1.842670
  validation accuracy:		34.67 %
Epoch 134 of 2000 took 0.105s
  training loss:		1.934010
  validation loss:		1.818348
  validation accuracy:		36.20 %
Epoch 135 of 2000 took 0.105s
  training loss:		1.914265
  validation loss:		1.786856
  validation accuracy:		34.78 %
Epoch 136 of 2000 took 0.106s
  training loss:		1.888278
  validation loss:		1.768579
  validation accuracy:		35.33 %
Epoch 137 of 2000 took 0.105s
  training loss:		1.862694
  validation loss:		1.745153
  validation accuracy:		36.09 %
Epoch 138 of 2000 took 0.105s
  training loss:		1.843013
  validation loss:		1.725753
  validation accuracy:		36.96 %
Epoch 139 of 2000 took 0.106s
  training loss:		1.825237
  validation loss:		1.699809
  validation accuracy:		38.59 %
Epoch 140 of 2000 took 0.111s
  training loss:		1.805022
  validation loss:		1.688184
  validation accuracy:		39.35 %
Epoch 141 of 2000 took 0.105s
  training loss:		1.786581
  validation loss:		1.666321
  validation accuracy:		38.48 %
Epoch 142 of 2000 took 0.105s
  training loss:		1.764499
  validation loss:		1.643572
  validation accuracy:		38.91 %
Epoch 143 of 2000 took 0.105s
  training loss:		1.754416
  validation loss:		1.632972
  validation accuracy:		40.43 %
Epoch 144 of 2000 took 0.105s
  training loss:		1.733959
  validation loss:		1.616250
  validation accuracy:		40.11 %
Epoch 145 of 2000 took 0.105s
  training loss:		1.719264
  validation loss:		1.592617
  validation accuracy:		40.43 %
Epoch 146 of 2000 took 0.105s
  training loss:		1.702234
  validation loss:		1.576978
  validation accuracy:		40.87 %
Epoch 147 of 2000 took 0.105s
  training loss:		1.684050
  validation loss:		1.569508
  validation accuracy:		42.50 %
Epoch 148 of 2000 took 0.105s
  training loss:		1.676335
  validation loss:		1.558457
  validation accuracy:		42.39 %
Epoch 149 of 2000 took 0.106s
  training loss:		1.658310
  validation loss:		1.544024
  validation accuracy:		41.52 %
Epoch 150 of 2000 took 0.111s
  training loss:		1.647382
  validation loss:		1.530253
  validation accuracy:		42.83 %
Epoch 151 of 2000 took 0.105s
  training loss:		1.634342
  validation loss:		1.520183
  validation accuracy:		43.70 %
Epoch 152 of 2000 took 0.105s
  training loss:		1.616113
  validation loss:		1.509660
  validation accuracy:		44.78 %
Epoch 153 of 2000 took 0.105s
  training loss:		1.609658
  validation loss:		1.498820
  validation accuracy:		44.78 %
Epoch 154 of 2000 took 0.105s
  training loss:		1.592209
  validation loss:		1.486807
  validation accuracy:		45.98 %
Epoch 155 of 2000 took 0.105s
  training loss:		1.584798
  validation loss:		1.470721
  validation accuracy:		46.96 %
Epoch 156 of 2000 took 0.105s
  training loss:		1.571566
  validation loss:		1.465578
  validation accuracy:		45.54 %
Epoch 157 of 2000 took 0.105s
  training loss:		1.565537
  validation loss:		1.459672
  validation accuracy:		46.41 %
Epoch 158 of 2000 took 0.105s
  training loss:		1.554501
  validation loss:		1.450915
  validation accuracy:		47.28 %
Epoch 159 of 2000 took 0.111s
  training loss:		1.547207
  validation loss:		1.434733
  validation accuracy:		48.59 %
Epoch 160 of 2000 took 0.106s
  training loss:		1.532810
  validation loss:		1.428117
  validation accuracy:		49.13 %
Epoch 161 of 2000 took 0.105s
  training loss:		1.527406
  validation loss:		1.418411
  validation accuracy:		48.70 %
Epoch 162 of 2000 took 0.105s
  training loss:		1.519683
  validation loss:		1.418354
  validation accuracy:		49.89 %
Epoch 163 of 2000 took 0.105s
  training loss:		1.510478
  validation loss:		1.408506
  validation accuracy:		49.13 %
Epoch 164 of 2000 took 0.105s
  training loss:		1.507758
  validation loss:		1.401561
  validation accuracy:		49.67 %
Epoch 165 of 2000 took 0.105s
  training loss:		1.501305
  validation loss:		1.399089
  validation accuracy:		50.22 %
Epoch 166 of 2000 took 0.105s
  training loss:		1.496334
  validation loss:		1.384715
  validation accuracy:		50.54 %
Epoch 167 of 2000 took 0.105s
  training loss:		1.484280
  validation loss:		1.379479
  validation accuracy:		50.43 %
Epoch 168 of 2000 took 0.107s
  training loss:		1.476153
  validation loss:		1.374658
  validation accuracy:		50.33 %
Epoch 169 of 2000 took 0.109s
  training loss:		1.476990
  validation loss:		1.375595
  validation accuracy:		51.30 %
Epoch 170 of 2000 took 0.105s
  training loss:		1.469115
  validation loss:		1.365323
  validation accuracy:		51.52 %
Epoch 171 of 2000 took 0.105s
  training loss:		1.461959
  validation loss:		1.362065
  validation accuracy:		51.96 %
Epoch 172 of 2000 took 0.105s
  training loss:		1.467507
  validation loss:		1.355263
  validation accuracy:		51.41 %
Epoch 173 of 2000 took 0.105s
  training loss:		1.449151
  validation loss:		1.350640
  validation accuracy:		52.39 %
Epoch 174 of 2000 took 0.102s
  training loss:		1.449599
  validation loss:		1.355591
  validation accuracy:		52.39 %
Epoch 175 of 2000 took 0.109s
  training loss:		1.449442
  validation loss:		1.346330
  validation accuracy:		52.50 %
Epoch 176 of 2000 took 0.129s
  training loss:		1.446494
  validation loss:		1.338835
  validation accuracy:		53.70 %
Epoch 177 of 2000 took 0.129s
  training loss:		1.444519
  validation loss:		1.341572
  validation accuracy:		53.04 %
Epoch 178 of 2000 took 0.110s
  training loss:		1.440868
  validation loss:		1.342566
  validation accuracy:		54.57 %
Epoch 179 of 2000 took 0.103s
  training loss:		1.435519
  validation loss:		1.329024
  validation accuracy:		53.80 %
Epoch 180 of 2000 took 0.098s
  training loss:		1.424585
  validation loss:		1.332596
  validation accuracy:		53.48 %
Epoch 181 of 2000 took 0.103s
  training loss:		1.435745
  validation loss:		1.328593
  validation accuracy:		53.80 %
Epoch 182 of 2000 took 0.103s
  training loss:		1.420244
  validation loss:		1.322083
  validation accuracy:		54.46 %
Epoch 183 of 2000 took 0.100s
  training loss:		1.417743
  validation loss:		1.326797
  validation accuracy:		53.91 %
Epoch 184 of 2000 took 0.100s
  training loss:		1.420570
  validation loss:		1.315302
  validation accuracy:		53.91 %
Epoch 185 of 2000 took 0.100s
  training loss:		1.421112
  validation loss:		1.306381
  validation accuracy:		54.89 %
Epoch 186 of 2000 took 0.100s
  training loss:		1.417472
  validation loss:		1.316624
  validation accuracy:		54.46 %
Epoch 187 of 2000 took 0.103s
  training loss:		1.408952
  validation loss:		1.323525
  validation accuracy:		55.22 %
Epoch 188 of 2000 took 0.101s
  training loss:		1.406045
  validation loss:		1.311173
  validation accuracy:		54.89 %
Epoch 189 of 2000 took 0.100s
  training loss:		1.415981
  validation loss:		1.301289
  validation accuracy:		54.78 %
Epoch 190 of 2000 took 0.100s
  training loss:		1.401276
  validation loss:		1.306013
  validation accuracy:		54.89 %
Epoch 191 of 2000 took 0.100s
  training loss:		1.407601
  validation loss:		1.302842
  validation accuracy:		53.15 %
Epoch 192 of 2000 took 0.101s
  training loss:		1.397820
  validation loss:		1.294235
  validation accuracy:		54.89 %
Epoch 193 of 2000 took 0.100s
  training loss:		1.387797
  validation loss:		1.290532
  validation accuracy:		54.89 %
Epoch 194 of 2000 took 0.100s
  training loss:		1.393192
  validation loss:		1.294014
  validation accuracy:		55.11 %
Epoch 195 of 2000 took 0.100s
  training loss:		1.389165
  validation loss:		1.308644
  validation accuracy:		56.63 %
Epoch 196 of 2000 took 0.100s
  training loss:		1.394809
  validation loss:		1.291576
  validation accuracy:		55.11 %
Epoch 197 of 2000 took 0.103s
  training loss:		1.387194
  validation loss:		1.287335
  validation accuracy:		54.78 %
Epoch 198 of 2000 took 0.101s
  training loss:		1.381501
  validation loss:		1.288038
  validation accuracy:		56.52 %
Epoch 199 of 2000 took 0.101s
  training loss:		1.383382
  validation loss:		1.282135
  validation accuracy:		56.41 %
Epoch 200 of 2000 took 0.100s
  training loss:		1.384332
  validation loss:		1.323470
  validation accuracy:		56.20 %
Epoch 201 of 2000 took 0.101s
  training loss:		1.382091
  validation loss:		1.279214
  validation accuracy:		55.76 %
Epoch 202 of 2000 took 0.100s
  training loss:		1.374142
  validation loss:		1.273718
  validation accuracy:		57.07 %
Epoch 203 of 2000 took 0.101s
  training loss:		1.382621
  validation loss:		1.276947
  validation accuracy:		56.41 %
Epoch 204 of 2000 took 0.100s
  training loss:		1.376592
  validation loss:		1.272013
  validation accuracy:		57.07 %
Epoch 205 of 2000 took 0.100s
  training loss:		1.367545
  validation loss:		1.270009
  validation accuracy:		56.52 %
Epoch 206 of 2000 took 0.101s
  training loss:		1.370288
  validation loss:		1.267793
  validation accuracy:		56.41 %
Epoch 207 of 2000 took 0.103s
  training loss:		1.370590
  validation loss:		1.272623
  validation accuracy:		57.50 %
Epoch 208 of 2000 took 0.100s
  training loss:		1.364245
  validation loss:		1.263548
  validation accuracy:		59.24 %
Epoch 209 of 2000 took 0.101s
  training loss:		1.362770
  validation loss:		1.264732
  validation accuracy:		58.59 %
Epoch 210 of 2000 took 0.100s
  training loss:		1.356068
  validation loss:		1.262078
  validation accuracy:		58.15 %
Epoch 211 of 2000 took 0.100s
  training loss:		1.365052
  validation loss:		1.265040
  validation accuracy:		58.48 %
Epoch 212 of 2000 took 0.101s
  training loss:		1.364913
  validation loss:		1.257554
  validation accuracy:		60.11 %
Epoch 213 of 2000 took 0.101s
  training loss:		1.364501
  validation loss:		1.272198
  validation accuracy:		55.87 %
Epoch 214 of 2000 took 0.100s
  training loss:		1.357873
  validation loss:		1.267110
  validation accuracy:		57.17 %
Epoch 215 of 2000 took 0.102s
  training loss:		1.362124
  validation loss:		1.264962
  validation accuracy:		60.22 %
Epoch 216 of 2000 took 0.129s
  training loss:		1.348143
  validation loss:		1.254331
  validation accuracy:		60.33 %
Epoch 217 of 2000 took 0.141s
  training loss:		1.341992
  validation loss:		1.264070
  validation accuracy:		60.22 %
Epoch 218 of 2000 took 0.151s
  training loss:		1.355734
  validation loss:		1.277313
  validation accuracy:		56.52 %
Epoch 219 of 2000 took 0.149s
  training loss:		1.350364
  validation loss:		1.248942
  validation accuracy:		59.89 %
Epoch 220 of 2000 took 0.128s
  training loss:		1.341932
  validation loss:		1.251119
  validation accuracy:		58.26 %
Epoch 221 of 2000 took 0.107s
  training loss:		1.341322
  validation loss:		1.261085
  validation accuracy:		56.63 %
Epoch 222 of 2000 took 0.107s
  training loss:		1.336272
  validation loss:		1.230579
  validation accuracy:		60.43 %
Epoch 223 of 2000 took 0.106s
  training loss:		1.352200
  validation loss:		1.259178
  validation accuracy:		60.33 %
Epoch 224 of 2000 took 0.105s
  training loss:		1.345235
  validation loss:		1.229795
  validation accuracy:		58.37 %
Epoch 225 of 2000 took 0.105s
  training loss:		1.328765
  validation loss:		1.219245
  validation accuracy:		59.35 %
Epoch 226 of 2000 took 0.105s
  training loss:		1.330064
  validation loss:		1.221312
  validation accuracy:		59.89 %
Epoch 227 of 2000 took 0.105s
  training loss:		1.315859
  validation loss:		1.220972
  validation accuracy:		59.89 %
Epoch 228 of 2000 took 0.105s
  training loss:		1.324942
  validation loss:		1.213216
  validation accuracy:		60.11 %
Epoch 229 of 2000 took 0.105s
  training loss:		1.313947
  validation loss:		1.213925
  validation accuracy:		60.87 %
Epoch 230 of 2000 took 0.105s
  training loss:		1.306805
  validation loss:		1.253220
  validation accuracy:		61.09 %
Epoch 231 of 2000 took 0.108s
  training loss:		1.316972
  validation loss:		1.255686
  validation accuracy:		60.33 %
Epoch 232 of 2000 took 0.105s
  training loss:		1.301868
  validation loss:		1.212623
  validation accuracy:		60.98 %
Epoch 233 of 2000 took 0.105s
  training loss:		1.287748
  validation loss:		1.192334
  validation accuracy:		61.63 %
Epoch 234 of 2000 took 0.106s
  training loss:		1.281927
  validation loss:		1.181170
  validation accuracy:		61.74 %
Epoch 235 of 2000 took 0.105s
  training loss:		1.282879
  validation loss:		1.173053
  validation accuracy:		62.17 %
Epoch 236 of 2000 took 0.105s
  training loss:		1.278271
  validation loss:		1.174098
  validation accuracy:		62.50 %
Epoch 237 of 2000 took 0.105s
  training loss:		1.272513
  validation loss:		1.167878
  validation accuracy:		62.83 %
Epoch 238 of 2000 took 0.105s
  training loss:		1.268693
  validation loss:		1.190757
  validation accuracy:		63.80 %
Epoch 239 of 2000 took 0.105s
  training loss:		1.269838
  validation loss:		1.164210
  validation accuracy:		62.93 %
Epoch 240 of 2000 took 0.105s
  training loss:		1.255230
  validation loss:		1.155133
  validation accuracy:		62.83 %
Epoch 241 of 2000 took 0.127s
  training loss:		1.245566
  validation loss:		1.138965
  validation accuracy:		64.02 %
Epoch 242 of 2000 took 0.169s
  training loss:		1.239751
  validation loss:		1.153111
  validation accuracy:		61.63 %
Epoch 243 of 2000 took 0.172s
  training loss:		1.244491
  validation loss:		1.127557
  validation accuracy:		65.11 %
Epoch 244 of 2000 took 0.169s
  training loss:		1.215371
  validation loss:		1.113802
  validation accuracy:		65.00 %
Epoch 245 of 2000 took 0.175s
  training loss:		1.225386
  validation loss:		1.115737
  validation accuracy:		64.46 %
Epoch 246 of 2000 took 0.172s
  training loss:		1.218271
  validation loss:		1.122348
  validation accuracy:		65.87 %
Epoch 247 of 2000 took 0.171s
  training loss:		1.206564
  validation loss:		1.088040
  validation accuracy:		67.50 %
Epoch 248 of 2000 took 0.170s
  training loss:		1.189833
  validation loss:		1.074032
  validation accuracy:		67.28 %
Epoch 249 of 2000 took 0.169s
  training loss:		1.190693
  validation loss:		1.078516
  validation accuracy:		66.52 %
Epoch 250 of 2000 took 0.172s
  training loss:		1.165184
  validation loss:		1.089524
  validation accuracy:		67.17 %
Epoch 251 of 2000 took 0.170s
  training loss:		1.158669
  validation loss:		1.048421
  validation accuracy:		68.91 %
Epoch 252 of 2000 took 0.174s
  training loss:		1.168548
  validation loss:		1.032863
  validation accuracy:		69.24 %
Epoch 253 of 2000 took 0.167s
  training loss:		1.142328
  validation loss:		1.022505
  validation accuracy:		69.02 %
Epoch 254 of 2000 took 0.175s
  training loss:		1.132702
  validation loss:		1.013385
  validation accuracy:		68.91 %
Epoch 255 of 2000 took 0.175s
  training loss:		1.121140
  validation loss:		1.031636
  validation accuracy:		68.59 %
Epoch 256 of 2000 took 0.174s
  training loss:		1.103515
  validation loss:		1.024691
  validation accuracy:		69.13 %
Epoch 257 of 2000 took 0.175s
  training loss:		1.091991
  validation loss:		0.991567
  validation accuracy:		70.11 %
Epoch 258 of 2000 took 0.172s
  training loss:		1.077268
  validation loss:		0.961577
  validation accuracy:		70.43 %
Epoch 259 of 2000 took 0.175s
  training loss:		1.084772
  validation loss:		0.954731
  validation accuracy:		70.43 %
Epoch 260 of 2000 took 0.174s
  training loss:		1.055877
  validation loss:		0.951789
  validation accuracy:		69.78 %
Epoch 261 of 2000 took 0.174s
  training loss:		1.065471
  validation loss:		0.938079
  validation accuracy:		71.09 %
Epoch 262 of 2000 took 0.170s
  training loss:		1.039380
  validation loss:		0.916402
  validation accuracy:		70.76 %
Epoch 263 of 2000 took 0.178s
  training loss:		1.027757
  validation loss:		0.914937
  validation accuracy:		70.98 %
Epoch 264 of 2000 took 0.175s
  training loss:		1.018908
  validation loss:		0.916217
  validation accuracy:		69.78 %
Epoch 265 of 2000 took 0.175s
  training loss:		1.007200
  validation loss:		0.895122
  validation accuracy:		70.98 %
Epoch 266 of 2000 took 0.173s
  training loss:		0.993837
  validation loss:		0.893858
  validation accuracy:		71.09 %
Epoch 267 of 2000 took 0.169s
  training loss:		0.987980
  validation loss:		0.871887
  validation accuracy:		71.63 %
Epoch 268 of 2000 took 0.175s
  training loss:		0.976664
  validation loss:		0.880438
  validation accuracy:		70.54 %
Epoch 269 of 2000 took 0.179s
  training loss:		0.968418
  validation loss:		0.879253
  validation accuracy:		70.33 %
Epoch 270 of 2000 took 0.175s
  training loss:		0.957424
  validation loss:		0.857325
  validation accuracy:		71.20 %
Epoch 271 of 2000 took 0.172s
  training loss:		0.958770
  validation loss:		0.843616
  validation accuracy:		72.39 %
Epoch 272 of 2000 took 0.171s
  training loss:		0.948178
  validation loss:		0.844698
  validation accuracy:		72.07 %
Epoch 273 of 2000 took 0.175s
  training loss:		0.927176
  validation loss:		0.824300
  validation accuracy:		72.17 %
Epoch 274 of 2000 took 0.175s
  training loss:		0.921028
  validation loss:		0.823745
  validation accuracy:		72.50 %
Epoch 275 of 2000 took 0.175s
  training loss:		0.908635
  validation loss:		0.839887
  validation accuracy:		71.41 %
Epoch 276 of 2000 took 0.171s
  training loss:		0.907890
  validation loss:		0.806411
  validation accuracy:		73.04 %
Epoch 277 of 2000 took 0.176s
  training loss:		0.899251
  validation loss:		0.806813
  validation accuracy:		72.83 %
Epoch 278 of 2000 took 0.175s
  training loss:		0.897724
  validation loss:		0.797371
  validation accuracy:		73.37 %
Epoch 279 of 2000 took 0.175s
  training loss:		0.884799
  validation loss:		0.808022
  validation accuracy:		72.50 %
Epoch 280 of 2000 took 0.175s
  training loss:		0.872797
  validation loss:		0.802980
  validation accuracy:		73.15 %
Epoch 281 of 2000 took 0.171s
  training loss:		0.863627
  validation loss:		0.777528
  validation accuracy:		73.15 %
Epoch 282 of 2000 took 0.172s
  training loss:		0.858024
  validation loss:		0.766094
  validation accuracy:		73.80 %
Epoch 283 of 2000 took 0.175s
  training loss:		0.859777
  validation loss:		0.776792
  validation accuracy:		73.48 %
Epoch 284 of 2000 took 0.175s
  training loss:		0.845544
  validation loss:		0.757982
  validation accuracy:		74.02 %
Epoch 285 of 2000 took 0.174s
  training loss:		0.848030
  validation loss:		0.748142
  validation accuracy:		73.70 %
Epoch 286 of 2000 took 0.175s
  training loss:		0.847768
  validation loss:		0.764164
  validation accuracy:		73.26 %
Epoch 287 of 2000 took 0.173s
  training loss:		0.833689
  validation loss:		0.743933
  validation accuracy:		74.89 %
Epoch 288 of 2000 took 0.175s
  training loss:		0.818209
  validation loss:		0.736984
  validation accuracy:		75.11 %
Epoch 289 of 2000 took 0.175s
  training loss:		0.818288
  validation loss:		0.730625
  validation accuracy:		76.09 %
Epoch 290 of 2000 took 0.174s
  training loss:		0.804258
  validation loss:		0.721519
  validation accuracy:		75.22 %
Epoch 291 of 2000 took 0.170s
  training loss:		0.804839
  validation loss:		0.715841
  validation accuracy:		76.20 %
Epoch 292 of 2000 took 0.174s
  training loss:		0.797467
  validation loss:		0.724486
  validation accuracy:		74.78 %
Epoch 293 of 2000 took 0.175s
  training loss:		0.801352
  validation loss:		0.708044
  validation accuracy:		75.98 %
Epoch 294 of 2000 took 0.174s
  training loss:		0.787012
  validation loss:		0.704947
  validation accuracy:		76.20 %
Epoch 295 of 2000 took 0.172s
  training loss:		0.782821
  validation loss:		0.693124
  validation accuracy:		76.74 %
Epoch 296 of 2000 took 0.172s
  training loss:		0.777711
  validation loss:		0.711902
  validation accuracy:		75.43 %
Epoch 297 of 2000 took 0.179s
  training loss:		0.808666
  validation loss:		0.684320
  validation accuracy:		76.96 %
Epoch 298 of 2000 took 0.175s
  training loss:		0.763467
  validation loss:		0.686071
  validation accuracy:		76.74 %
Epoch 299 of 2000 took 0.174s
  training loss:		0.757502
  validation loss:		0.690424
  validation accuracy:		76.41 %
Epoch 300 of 2000 took 0.172s
  training loss:		0.769858
  validation loss:		0.739587
  validation accuracy:		74.57 %
Epoch 301 of 2000 took 0.172s
  training loss:		0.758819
  validation loss:		0.692990
  validation accuracy:		76.85 %
Epoch 302 of 2000 took 0.174s
  training loss:		0.749952
  validation loss:		0.670075
  validation accuracy:		76.52 %
Epoch 303 of 2000 took 0.175s
  training loss:		0.742980
  validation loss:		0.663928
  validation accuracy:		77.72 %
Epoch 304 of 2000 took 0.175s
  training loss:		0.746717
  validation loss:		0.674748
  validation accuracy:		76.96 %
Epoch 305 of 2000 took 0.171s
  training loss:		0.740630
  validation loss:		0.664207
  validation accuracy:		77.50 %
Epoch 306 of 2000 took 0.172s
  training loss:		0.742205
  validation loss:		0.646486
  validation accuracy:		77.39 %
Epoch 307 of 2000 took 0.175s
  training loss:		0.730148
  validation loss:		0.649857
  validation accuracy:		77.61 %
Epoch 308 of 2000 took 0.175s
  training loss:		0.744199
  validation loss:		0.664338
  validation accuracy:		77.28 %
Epoch 309 of 2000 took 0.174s
  training loss:		0.729728
  validation loss:		0.642044
  validation accuracy:		78.37 %
Epoch 310 of 2000 took 0.175s
  training loss:		0.722920
  validation loss:		0.637142
  validation accuracy:		78.04 %
Epoch 311 of 2000 took 0.172s
  training loss:		0.712441
  validation loss:		0.647717
  validation accuracy:		78.04 %
Epoch 312 of 2000 took 0.175s
  training loss:		0.713124
  validation loss:		0.658242
  validation accuracy:		76.74 %
Epoch 313 of 2000 took 0.174s
  training loss:		0.706858
  validation loss:		0.634680
  validation accuracy:		77.93 %
Epoch 314 of 2000 took 0.174s
  training loss:		0.709660
  validation loss:		0.633094
  validation accuracy:		77.28 %
Epoch 315 of 2000 took 0.170s
  training loss:		0.703416
  validation loss:		0.628616
  validation accuracy:		77.93 %
Epoch 316 of 2000 took 0.174s
  training loss:		0.705362
  validation loss:		0.622347
  validation accuracy:		77.72 %
Epoch 317 of 2000 took 0.175s
  training loss:		0.708835
  validation loss:		0.644440
  validation accuracy:		77.61 %
Epoch 318 of 2000 took 0.174s
  training loss:		0.688525
  validation loss:		0.623518
  validation accuracy:		77.83 %
Epoch 319 of 2000 took 0.172s
  training loss:		0.696036
  validation loss:		0.635690
  validation accuracy:		77.93 %
Epoch 320 of 2000 took 0.172s
  training loss:		0.690361
  validation loss:		0.608186
  validation accuracy:		77.93 %
Epoch 321 of 2000 took 0.174s
  training loss:		0.698160
  validation loss:		0.637858
  validation accuracy:		78.04 %
Epoch 322 of 2000 took 0.175s
  training loss:		0.688506
  validation loss:		0.609485
  validation accuracy:		78.37 %
Epoch 323 of 2000 took 0.174s
  training loss:		0.694789
  validation loss:		0.603252
  validation accuracy:		78.91 %
Epoch 324 of 2000 took 0.170s
  training loss:		0.693545
  validation loss:		0.677037
  validation accuracy:		76.20 %
Epoch 325 of 2000 took 0.173s
  training loss:		0.685317
  validation loss:		0.628842
  validation accuracy:		77.72 %
Epoch 326 of 2000 took 0.179s
  training loss:		0.699264
  validation loss:		0.613908
  validation accuracy:		77.83 %
Epoch 327 of 2000 took 0.175s
  training loss:		0.686956
  validation loss:		0.601677
  validation accuracy:		78.59 %
Epoch 328 of 2000 took 0.174s
  training loss:		0.675655
  validation loss:		0.648244
  validation accuracy:		77.28 %
Epoch 329 of 2000 took 0.170s
  training loss:		0.684655
  validation loss:		0.635847
  validation accuracy:		78.15 %
Epoch 330 of 2000 took 0.173s
  training loss:		0.685181
  validation loss:		0.598845
  validation accuracy:		78.59 %
Epoch 331 of 2000 took 0.175s
  training loss:		0.674072
  validation loss:		0.596773
  validation accuracy:		78.59 %
Epoch 332 of 2000 took 0.175s
  training loss:		0.675010
  validation loss:		0.640550
  validation accuracy:		77.39 %
Epoch 333 of 2000 took 0.173s
  training loss:		0.671853
  validation loss:		0.606845
  validation accuracy:		78.80 %
Epoch 334 of 2000 took 0.169s
  training loss:		0.672684
  validation loss:		0.636098
  validation accuracy:		77.93 %
Epoch 335 of 2000 took 0.175s
  training loss:		0.666575
  validation loss:		0.636807
  validation accuracy:		78.48 %
Epoch 336 of 2000 took 0.175s
  training loss:		0.673981
  validation loss:		0.609093
  validation accuracy:		78.91 %
Epoch 337 of 2000 took 0.174s
  training loss:		0.664167
  validation loss:		0.585000
  validation accuracy:		78.80 %
Epoch 338 of 2000 took 0.172s
  training loss:		0.659626
  validation loss:		0.581321
  validation accuracy:		78.91 %
Epoch 339 of 2000 took 0.172s
  training loss:		0.658080
  validation loss:		0.594632
  validation accuracy:		79.02 %
Epoch 340 of 2000 took 0.174s
  training loss:		0.654186
  validation loss:		0.609624
  validation accuracy:		78.70 %
Epoch 341 of 2000 took 0.175s
  training loss:		0.663034
  validation loss:		0.578798
  validation accuracy:		79.57 %
Epoch 342 of 2000 took 0.175s
  training loss:		0.679482
  validation loss:		0.618436
  validation accuracy:		78.48 %
Epoch 343 of 2000 took 0.171s
  training loss:		0.669221
  validation loss:		0.589676
  validation accuracy:		79.24 %
Epoch 344 of 2000 took 0.172s
  training loss:		0.653726
  validation loss:		0.650706
  validation accuracy:		77.07 %
Epoch 345 of 2000 took 0.178s
  training loss:		0.663748
  validation loss:		0.584451
  validation accuracy:		79.02 %
Epoch 346 of 2000 took 0.175s
  training loss:		0.652322
  validation loss:		0.601866
  validation accuracy:		79.57 %
Epoch 347 of 2000 took 0.175s
  training loss:		0.646873
  validation loss:		0.599641
  validation accuracy:		79.24 %
Epoch 348 of 2000 took 0.170s
  training loss:		0.649190
  validation loss:		0.572786
  validation accuracy:		79.78 %
Epoch 349 of 2000 took 0.173s
  training loss:		0.672392
  validation loss:		0.625716
  validation accuracy:		77.83 %
Epoch 350 of 2000 took 0.175s
  training loss:		0.659091
  validation loss:		0.576079
  validation accuracy:		79.78 %
Epoch 351 of 2000 took 0.174s
  training loss:		0.654069
  validation loss:		0.589510
  validation accuracy:		78.70 %
Epoch 352 of 2000 took 0.174s
  training loss:		0.639118
  validation loss:		0.592549
  validation accuracy:		79.89 %
Epoch 353 of 2000 took 0.170s
  training loss:		0.645935
  validation loss:		0.646906
  validation accuracy:		77.17 %
Epoch 354 of 2000 took 0.175s
  training loss:		0.645286
  validation loss:		0.573877
  validation accuracy:		78.91 %
Epoch 355 of 2000 took 0.175s
  training loss:		0.675539
  validation loss:		0.582150
  validation accuracy:		79.67 %
Epoch 356 of 2000 took 0.175s
  training loss:		0.640550
  validation loss:		0.582093
  validation accuracy:		80.11 %
Epoch 357 of 2000 took 0.172s
  training loss:		0.636837
  validation loss:		0.581127
  validation accuracy:		80.00 %
Epoch 358 of 2000 took 0.172s
  training loss:		0.634643
  validation loss:		0.591642
  validation accuracy:		80.00 %
Epoch 359 of 2000 took 0.175s
  training loss:		0.635353
  validation loss:		0.594257
  validation accuracy:		79.78 %
Epoch 360 of 2000 took 0.175s
  training loss:		0.641973
  validation loss:		0.585984
  validation accuracy:		79.78 %
Epoch 361 of 2000 took 0.175s
  training loss:		0.640395
  validation loss:		0.574976
  validation accuracy:		79.78 %
Epoch 362 of 2000 took 0.171s
  training loss:		0.644842
  validation loss:		0.572178
  validation accuracy:		80.11 %
Epoch 363 of 2000 took 0.172s
  training loss:		0.644726
  validation loss:		0.574365
  validation accuracy:		80.22 %
Epoch 364 of 2000 took 0.175s
  training loss:		0.639006
  validation loss:		0.569150
  validation accuracy:		80.22 %
Epoch 365 of 2000 took 0.174s
  training loss:		0.642613
  validation loss:		0.564881
  validation accuracy:		80.22 %
Epoch 366 of 2000 took 0.174s
  training loss:		0.668129
  validation loss:		0.659450
  validation accuracy:		77.72 %
Epoch 367 of 2000 took 0.175s
  training loss:		0.632966
  validation loss:		0.589190
  validation accuracy:		79.78 %
Epoch 368 of 2000 took 0.173s
  training loss:		0.650119
  validation loss:		0.577325
  validation accuracy:		80.65 %
Epoch 369 of 2000 took 0.175s
  training loss:		0.666981
  validation loss:		0.568106
  validation accuracy:		80.43 %
Epoch 370 of 2000 took 0.174s
  training loss:		0.635967
  validation loss:		0.563703
  validation accuracy:		80.43 %
Epoch 371 of 2000 took 0.174s
  training loss:		0.637836
  validation loss:		0.572276
  validation accuracy:		80.11 %
Epoch 372 of 2000 took 0.170s
  training loss:		0.636989
  validation loss:		0.573937
  validation accuracy:		80.22 %
Epoch 373 of 2000 took 0.174s
  training loss:		0.630988
  validation loss:		0.569981
  validation accuracy:		81.20 %
Epoch 374 of 2000 took 0.175s
  training loss:		0.637224
  validation loss:		0.584251
  validation accuracy:		79.78 %
Epoch 375 of 2000 took 0.174s
  training loss:		0.619579
  validation loss:		0.583952
  validation accuracy:		80.00 %
Epoch 376 of 2000 took 0.172s
  training loss:		0.626015
  validation loss:		0.561625
  validation accuracy:		80.22 %
Epoch 377 of 2000 took 0.172s
  training loss:		0.619574
  validation loss:		0.588655
  validation accuracy:		79.78 %
Epoch 378 of 2000 took 0.175s
  training loss:		0.623888
  validation loss:		0.575474
  validation accuracy:		80.22 %
Epoch 379 of 2000 took 0.175s
  training loss:		0.628298
  validation loss:		0.582615
  validation accuracy:		80.33 %
Epoch 380 of 2000 took 0.175s
  training loss:		0.621847
  validation loss:		0.568788
  validation accuracy:		80.98 %
Epoch 381 of 2000 took 0.171s
  training loss:		0.630881
  validation loss:		0.606567
  validation accuracy:		79.24 %
Epoch 382 of 2000 took 0.172s
  training loss:		0.626034
  validation loss:		0.612496
  validation accuracy:		79.24 %
Epoch 383 of 2000 took 0.175s
  training loss:		0.622563
  validation loss:		0.626662
  validation accuracy:		78.80 %
Epoch 384 of 2000 took 0.174s
  training loss:		0.638898
  validation loss:		0.627846
  validation accuracy:		78.59 %
Epoch 385 of 2000 took 0.174s
  training loss:		0.629253
  validation loss:		0.585692
  validation accuracy:		80.11 %
Epoch 386 of 2000 took 0.170s
  training loss:		0.626776
  validation loss:		0.567172
  validation accuracy:		80.22 %
Epoch 387 of 2000 took 0.173s
  training loss:		0.628923
  validation loss:		0.560694
  validation accuracy:		80.76 %
Epoch 388 of 2000 took 0.175s
  training loss:		0.626938
  validation loss:		0.573214
  validation accuracy:		80.65 %
Epoch 389 of 2000 took 0.175s
  training loss:		0.626870
  validation loss:		0.582612
  validation accuracy:		80.00 %
Epoch 390 of 2000 took 0.172s
  training loss:		0.636543
  validation loss:		0.621013
  validation accuracy:		78.59 %
Epoch 391 of 2000 took 0.172s
  training loss:		0.631383
  validation loss:		0.578248
  validation accuracy:		80.22 %
Epoch 392 of 2000 took 0.175s
  training loss:		0.619658
  validation loss:		0.557957
  validation accuracy:		81.20 %
Epoch 393 of 2000 took 0.174s
  training loss:		0.611600
  validation loss:		0.613168
  validation accuracy:		79.02 %
Epoch 394 of 2000 took 0.175s
  training loss:		0.612367
  validation loss:		0.582560
  validation accuracy:		79.89 %
Epoch 395 of 2000 took 0.177s
  training loss:		0.625727
  validation loss:		0.598984
  validation accuracy:		79.57 %
Epoch 396 of 2000 took 0.171s
  training loss:		0.626534
  validation loss:		0.624996
  validation accuracy:		79.02 %
Epoch 397 of 2000 took 0.175s
  training loss:		0.626555
  validation loss:		0.676717
  validation accuracy:		77.17 %
Epoch 398 of 2000 took 0.174s
  training loss:		0.637584
  validation loss:		0.571405
  validation accuracy:		80.54 %
Epoch 399 of 2000 took 0.175s
  training loss:		0.634026
  validation loss:		0.597818
  validation accuracy:		79.78 %
Epoch 400 of 2000 took 0.171s
  training loss:		0.603492
  validation loss:		0.606856
  validation accuracy:		79.57 %
Epoch 401 of 2000 took 0.171s
  training loss:		0.623133
  validation loss:		0.559826
  validation accuracy:		80.98 %
Epoch 402 of 2000 took 0.175s
  training loss:		0.628300
  validation loss:		0.615554
  validation accuracy:		79.13 %
Epoch 403 of 2000 took 0.175s
  training loss:		0.625801
  validation loss:		0.633971
  validation accuracy:		78.26 %
Epoch 404 of 2000 took 0.174s
  training loss:		0.615231
  validation loss:		0.555193
  validation accuracy:		81.20 %
Epoch 405 of 2000 took 0.170s
  training loss:		0.619432
  validation loss:		0.576154
  validation accuracy:		80.87 %
Epoch 406 of 2000 took 0.173s
  training loss:		0.620357
  validation loss:		0.604661
  validation accuracy:		79.46 %
Epoch 407 of 2000 took 0.175s
  training loss:		0.618239
  validation loss:		0.574320
  validation accuracy:		80.11 %
Epoch 408 of 2000 took 0.175s
  training loss:		0.616333
  validation loss:		0.558379
  validation accuracy:		81.30 %
Epoch 409 of 2000 took 0.172s
  training loss:		0.610485
  validation loss:		0.554893
  validation accuracy:		81.63 %
Epoch 410 of 2000 took 0.171s
  training loss:		0.616235
  validation loss:		0.590797
  validation accuracy:		79.89 %
Epoch 411 of 2000 took 0.175s
  training loss:		0.615018
  validation loss:		0.561712
  validation accuracy:		81.20 %
Epoch 412 of 2000 took 0.175s
  training loss:		0.644301
  validation loss:		0.651012
  validation accuracy:		77.72 %
Epoch 413 of 2000 took 0.175s
  training loss:		0.626469
  validation loss:		0.587436
  validation accuracy:		80.00 %
Epoch 414 of 2000 took 0.171s
  training loss:		0.632309
  validation loss:		0.575444
  validation accuracy:		79.89 %
Epoch 415 of 2000 took 0.172s
  training loss:		0.618050
  validation loss:		0.586234
  validation accuracy:		79.89 %
Epoch 416 of 2000 took 0.175s
  training loss:		0.616175
  validation loss:		0.575834
  validation accuracy:		80.22 %
Epoch 417 of 2000 took 0.175s
  training loss:		0.610026
  validation loss:		0.582899
  validation accuracy:		80.33 %
Epoch 418 of 2000 took 0.174s
  training loss:		0.615575
  validation loss:		0.567424
  validation accuracy:		80.22 %
Epoch 419 of 2000 took 0.170s
  training loss:		0.618035
  validation loss:		0.559064
  validation accuracy:		81.52 %
Epoch 420 of 2000 took 0.146s
  training loss:		0.631874
  validation loss:		0.579225
  validation accuracy:		80.33 %
Epoch 421 of 2000 took 0.096s
  training loss:		0.611719
  validation loss:		0.554470
  validation accuracy:		80.22 %
Epoch 422 of 2000 took 0.096s
  training loss:		0.628085
  validation loss:		0.558760
  validation accuracy:		82.07 %
Epoch 423 of 2000 took 0.100s
  training loss:		0.636047
  validation loss:		0.567045
  validation accuracy:		80.87 %
Epoch 424 of 2000 took 0.103s
  training loss:		0.604223
  validation loss:		0.566781
  validation accuracy:		80.65 %
Epoch 425 of 2000 took 0.105s
  training loss:		0.602740
  validation loss:		0.556919
  validation accuracy:		81.09 %
Epoch 426 of 2000 took 0.104s
  training loss:		0.610814
  validation loss:		0.622473
  validation accuracy:		78.15 %
Epoch 427 of 2000 took 0.096s
  training loss:		0.604167
  validation loss:		0.560040
  validation accuracy:		81.41 %
Epoch 428 of 2000 took 0.096s
  training loss:		0.605074
  validation loss:		0.587316
  validation accuracy:		79.78 %
Epoch 429 of 2000 took 0.097s
  training loss:		0.614289
  validation loss:		0.553216
  validation accuracy:		80.98 %
Epoch 430 of 2000 took 0.096s
  training loss:		0.618514
  validation loss:		0.556897
  validation accuracy:		81.30 %
Epoch 431 of 2000 took 0.098s
  training loss:		0.603527
  validation loss:		0.575561
  validation accuracy:		80.43 %
Epoch 432 of 2000 took 0.097s
  training loss:		0.606613
  validation loss:		0.563732
  validation accuracy:		80.54 %
Epoch 433 of 2000 took 0.099s
  training loss:		0.617872
  validation loss:		0.555143
  validation accuracy:		81.41 %
Epoch 434 of 2000 took 0.099s
  training loss:		0.619171
  validation loss:		0.561035
  validation accuracy:		81.30 %
Epoch 435 of 2000 took 0.096s
  training loss:		0.608822
  validation loss:		0.559670
  validation accuracy:		81.63 %
Epoch 436 of 2000 took 0.096s
  training loss:		0.620404
  validation loss:		0.591618
  validation accuracy:		80.00 %
Epoch 437 of 2000 took 0.100s
  training loss:		0.600781
  validation loss:		0.632708
  validation accuracy:		78.48 %
Epoch 438 of 2000 took 0.098s
  training loss:		0.618804
  validation loss:		0.609998
  validation accuracy:		79.57 %
Epoch 439 of 2000 took 0.096s
  training loss:		0.615572
  validation loss:		0.579316
  validation accuracy:		79.89 %
Epoch 440 of 2000 took 0.097s
  training loss:		0.610365
  validation loss:		0.591479
  validation accuracy:		79.67 %
Epoch 441 of 2000 took 0.096s
  training loss:		0.608302
  validation loss:		0.567365
  validation accuracy:		80.98 %
Epoch 442 of 2000 took 0.097s
  training loss:		0.606088
  validation loss:		0.553591
  validation accuracy:		81.63 %
Epoch 443 of 2000 took 0.096s
  training loss:		0.604517
  validation loss:		0.561222
  validation accuracy:		81.09 %
Epoch 444 of 2000 took 0.097s
  training loss:		0.628072
  validation loss:		0.665612
  validation accuracy:		78.26 %
Epoch 445 of 2000 took 0.096s
  training loss:		0.649581
  validation loss:		0.597240
  validation accuracy:		79.78 %
Epoch 446 of 2000 took 0.097s
  training loss:		0.612220
  validation loss:		0.615435
  validation accuracy:		79.67 %
Epoch 447 of 2000 took 0.097s
  training loss:		0.611277
  validation loss:		0.560536
  validation accuracy:		80.76 %
Epoch 448 of 2000 took 0.097s
  training loss:		0.618233
  validation loss:		0.582919
  validation accuracy:		80.00 %
Epoch 449 of 2000 took 0.097s
  training loss:		0.605821
  validation loss:		0.626802
  validation accuracy:		79.24 %
Epoch 450 of 2000 took 0.097s
  training loss:		0.608208
  validation loss:		0.587647
  validation accuracy:		80.11 %
Epoch 451 of 2000 took 0.097s
  training loss:		0.613307
  validation loss:		0.561543
  validation accuracy:		80.54 %
Epoch 452 of 2000 took 0.097s
  training loss:		0.609949
  validation loss:		0.558593
  validation accuracy:		81.20 %
Epoch 453 of 2000 took 0.096s
  training loss:		0.608313
  validation loss:		0.550146
  validation accuracy:		80.98 %
Epoch 454 of 2000 took 0.097s
  training loss:		0.634468
  validation loss:		0.580143
  validation accuracy:		80.54 %
Epoch 455 of 2000 took 0.096s
  training loss:		0.618197
  validation loss:		0.551414
  validation accuracy:		81.30 %
Epoch 456 of 2000 took 0.097s
  training loss:		0.603179
  validation loss:		0.552225
  validation accuracy:		81.30 %
Epoch 457 of 2000 took 0.097s
  training loss:		0.619214
  validation loss:		0.597939
  validation accuracy:		79.67 %
Epoch 458 of 2000 took 0.097s
  training loss:		0.622940
  validation loss:		0.556347
  validation accuracy:		80.98 %
Epoch 459 of 2000 took 0.097s
  training loss:		0.618684
  validation loss:		0.574471
  validation accuracy:		80.43 %
Epoch 460 of 2000 took 0.097s
  training loss:		0.593187
  validation loss:		0.557533
  validation accuracy:		81.41 %
Epoch 461 of 2000 took 0.096s
  training loss:		0.609880
  validation loss:		0.574781
  validation accuracy:		79.67 %
Epoch 462 of 2000 took 0.097s
  training loss:		0.605615
  validation loss:		0.571919
  validation accuracy:		81.09 %
Epoch 463 of 2000 took 0.096s
  training loss:		0.598090
  validation loss:		0.588440
  validation accuracy:		79.67 %
Epoch 464 of 2000 took 0.097s
  training loss:		0.627999
  validation loss:		0.577339
  validation accuracy:		80.11 %
Epoch 465 of 2000 took 0.097s
  training loss:		0.606405
  validation loss:		0.557014
  validation accuracy:		80.87 %
Epoch 466 of 2000 took 0.097s
  training loss:		0.600286
  validation loss:		0.577386
  validation accuracy:		80.76 %
Epoch 467 of 2000 took 0.097s
  training loss:		0.607626
  validation loss:		0.625833
  validation accuracy:		79.24 %
Epoch 468 of 2000 took 0.097s
  training loss:		0.602835
  validation loss:		0.575694
  validation accuracy:		80.22 %
Epoch 469 of 2000 took 0.096s
  training loss:		0.627019
  validation loss:		0.608500
  validation accuracy:		80.22 %
Epoch 470 of 2000 took 0.097s
  training loss:		0.599090
  validation loss:		0.566263
  validation accuracy:		80.22 %
Epoch 471 of 2000 took 0.097s
  training loss:		0.600883
  validation loss:		0.570531
  validation accuracy:		80.76 %
Epoch 472 of 2000 took 0.096s
  training loss:		0.606041
  validation loss:		0.550251
  validation accuracy:		81.52 %
Epoch 473 of 2000 took 0.097s
  training loss:		0.610187
  validation loss:		0.592566
  validation accuracy:		80.43 %
Epoch 474 of 2000 took 0.096s
  training loss:		0.592185
  validation loss:		0.584883
  validation accuracy:		80.00 %
Epoch 475 of 2000 took 0.097s
  training loss:		0.607068
  validation loss:		0.577443
  validation accuracy:		80.00 %
Epoch 476 of 2000 took 0.096s
  training loss:		0.613145
  validation loss:		0.563181
  validation accuracy:		80.43 %
Epoch 477 of 2000 took 0.097s
  training loss:		0.599099
  validation loss:		0.574848
  validation accuracy:		80.54 %
Epoch 478 of 2000 took 0.097s
  training loss:		0.601874
  validation loss:		0.568396
  validation accuracy:		80.22 %
Epoch 479 of 2000 took 0.097s
  training loss:		0.626563
  validation loss:		0.550512
  validation accuracy:		81.52 %
Epoch 480 of 2000 took 0.096s
  training loss:		0.613404
  validation loss:		0.603835
  validation accuracy:		79.67 %
Epoch 481 of 2000 took 0.097s
  training loss:		0.617540
  validation loss:		0.557552
  validation accuracy:		80.76 %
Epoch 482 of 2000 took 0.096s
  training loss:		0.629305
  validation loss:		0.579628
  validation accuracy:		80.11 %
Epoch 483 of 2000 took 0.097s
  training loss:		0.607122
  validation loss:		0.567901
  validation accuracy:		80.43 %
Epoch 484 of 2000 took 0.096s
  training loss:		0.602618
  validation loss:		0.557058
  validation accuracy:		81.20 %
Epoch 485 of 2000 took 0.097s
  training loss:		0.629590
  validation loss:		0.553815
  validation accuracy:		81.63 %
Epoch 486 of 2000 took 0.096s
  training loss:		0.615606
  validation loss:		0.569559
  validation accuracy:		80.33 %
Epoch 487 of 2000 took 0.097s
  training loss:		0.592926
  validation loss:		0.595192
  validation accuracy:		80.11 %
Epoch 488 of 2000 took 0.097s
  training loss:		0.607473
  validation loss:		0.554807
  validation accuracy:		81.63 %
Epoch 489 of 2000 took 0.097s
  training loss:		0.605997
  validation loss:		0.561536
  validation accuracy:		80.76 %
Epoch 490 of 2000 took 0.097s
  training loss:		0.604506
  validation loss:		0.579137
  validation accuracy:		80.22 %
Epoch 491 of 2000 took 0.097s
  training loss:		0.608478
  validation loss:		0.603039
  validation accuracy:		80.33 %
Epoch 492 of 2000 took 0.096s
  training loss:		0.616516
  validation loss:		0.550477
  validation accuracy:		81.30 %
Epoch 493 of 2000 took 0.097s
  training loss:		0.605072
  validation loss:		0.583457
  validation accuracy:		80.54 %
Epoch 494 of 2000 took 0.096s
  training loss:		0.599114
  validation loss:		0.553908
  validation accuracy:		81.30 %
Epoch 495 of 2000 took 0.097s
  training loss:		0.598865
  validation loss:		0.636711
  validation accuracy:		79.02 %
Epoch 496 of 2000 took 0.096s
  training loss:		0.593985
  validation loss:		0.575043
  validation accuracy:		80.87 %
Epoch 497 of 2000 took 0.097s
  training loss:		0.600407
  validation loss:		0.556800
  validation accuracy:		81.20 %
Epoch 498 of 2000 took 0.096s
  training loss:		0.603183
  validation loss:		0.588734
  validation accuracy:		80.33 %
Epoch 499 of 2000 took 0.096s
  training loss:		0.609762
  validation loss:		0.574661
  validation accuracy:		80.76 %
Epoch 500 of 2000 took 0.097s
  training loss:		0.592080
  validation loss:		0.577676
  validation accuracy:		80.22 %
Epoch 501 of 2000 took 0.096s
  training loss:		0.599389
  validation loss:		0.552886
  validation accuracy:		81.74 %
Epoch 502 of 2000 took 0.097s
  training loss:		0.621198
  validation loss:		0.574690
  validation accuracy:		81.30 %
Epoch 503 of 2000 took 0.097s
  training loss:		0.612587
  validation loss:		0.551105
  validation accuracy:		81.52 %
Epoch 504 of 2000 took 0.100s
  training loss:		0.590309
  validation loss:		0.583176
  validation accuracy:		80.22 %
Epoch 505 of 2000 took 0.097s
  training loss:		0.598832
  validation loss:		0.559492
  validation accuracy:		80.98 %
Epoch 506 of 2000 took 0.097s
  training loss:		0.613242
  validation loss:		0.594624
  validation accuracy:		80.54 %
Epoch 507 of 2000 took 0.096s
  training loss:		0.602992
  validation loss:		0.576643
  validation accuracy:		80.76 %
Epoch 508 of 2000 took 0.097s
  training loss:		0.617083
  validation loss:		0.563312
  validation accuracy:		81.52 %
Epoch 509 of 2000 took 0.097s
  training loss:		0.602055
  validation loss:		0.604240
  validation accuracy:		80.22 %
Epoch 510 of 2000 took 0.097s
  training loss:		0.604470
  validation loss:		0.556157
  validation accuracy:		81.30 %
Epoch 511 of 2000 took 0.097s
  training loss:		0.606240
  validation loss:		0.564438
  validation accuracy:		80.87 %
Epoch 512 of 2000 took 0.096s
  training loss:		0.590976
  validation loss:		0.551075
  validation accuracy:		81.85 %
Epoch 513 of 2000 took 0.096s
  training loss:		0.594859
  validation loss:		0.640607
  validation accuracy:		78.48 %
Epoch 514 of 2000 took 0.097s
  training loss:		0.613405
  validation loss:		0.598183
  validation accuracy:		80.76 %
Epoch 515 of 2000 took 0.096s
  training loss:		0.590725
  validation loss:		0.549502
  validation accuracy:		81.63 %
Epoch 516 of 2000 took 0.097s
  training loss:		0.598749
  validation loss:		0.563653
  validation accuracy:		81.30 %
Epoch 517 of 2000 took 0.097s
  training loss:		0.592527
  validation loss:		0.569193
  validation accuracy:		81.09 %
Epoch 518 of 2000 took 0.097s
  training loss:		0.603936
  validation loss:		0.572822
  validation accuracy:		80.43 %
Epoch 519 of 2000 took 0.097s
  training loss:		0.619614
  validation loss:		0.600841
  validation accuracy:		80.00 %
Epoch 520 of 2000 took 0.096s
  training loss:		0.602955
  validation loss:		0.612427
  validation accuracy:		79.67 %
Epoch 521 of 2000 took 0.097s
  training loss:		0.596853
  validation loss:		0.573830
  validation accuracy:		80.76 %
Epoch 522 of 2000 took 0.096s
  training loss:		0.599421
  validation loss:		0.593377
  validation accuracy:		80.65 %
Epoch 523 of 2000 took 0.096s
  training loss:		0.597141
  validation loss:		0.578879
  validation accuracy:		81.30 %
Epoch 524 of 2000 took 0.097s
  training loss:		0.600765
  validation loss:		0.569162
  validation accuracy:		81.09 %
Epoch 525 of 2000 took 0.096s
  training loss:		0.609391
  validation loss:		0.556198
  validation accuracy:		81.30 %
Epoch 526 of 2000 took 0.096s
  training loss:		0.603173
  validation loss:		0.566169
  validation accuracy:		80.65 %
Epoch 527 of 2000 took 0.096s
  training loss:		0.593095
  validation loss:		0.576444
  validation accuracy:		80.76 %
Epoch 528 of 2000 took 0.099s
  training loss:		0.596411
  validation loss:		0.578484
  validation accuracy:		80.76 %
Epoch 529 of 2000 took 0.097s
  training loss:		0.588085
  validation loss:		0.549942
  validation accuracy:		81.52 %
Epoch 530 of 2000 took 0.096s
  training loss:		0.588861
  validation loss:		0.549136
  validation accuracy:		81.96 %
Epoch 531 of 2000 took 0.097s
  training loss:		0.600611
  validation loss:		0.553565
  validation accuracy:		81.63 %
Epoch 532 of 2000 took 0.096s
  training loss:		0.596633
  validation loss:		0.580409
  validation accuracy:		80.76 %
Epoch 533 of 2000 took 0.096s
  training loss:		0.589395
  validation loss:		0.564078
  validation accuracy:		80.98 %
Epoch 534 of 2000 took 0.096s
  training loss:		0.599014
  validation loss:		0.560973
  validation accuracy:		81.09 %
Epoch 535 of 2000 took 0.097s
  training loss:		0.614629
  validation loss:		0.733203
  validation accuracy:		76.20 %
Epoch 536 of 2000 took 0.096s
  training loss:		0.653771
  validation loss:		0.548436
  validation accuracy:		82.07 %
Epoch 537 of 2000 took 0.097s
  training loss:		0.601019
  validation loss:		0.600191
  validation accuracy:		80.00 %
Epoch 538 of 2000 took 0.096s
  training loss:		0.628652
  validation loss:		0.554618
  validation accuracy:		80.87 %
Epoch 539 of 2000 took 0.096s
  training loss:		0.611484
  validation loss:		0.616837
  validation accuracy:		79.89 %
Epoch 540 of 2000 took 0.097s
  training loss:		0.613656
  validation loss:		0.562332
  validation accuracy:		80.87 %
Epoch 541 of 2000 took 0.097s
  training loss:		0.599189
  validation loss:		0.567405
  validation accuracy:		81.30 %
Epoch 542 of 2000 took 0.097s
  training loss:		0.589897
  validation loss:		0.602214
  validation accuracy:		80.33 %
Epoch 543 of 2000 took 0.097s
  training loss:		0.600924
  validation loss:		0.559812
  validation accuracy:		80.98 %
Epoch 544 of 2000 took 0.096s
  training loss:		0.595920
  validation loss:		0.568406
  validation accuracy:		80.76 %
Epoch 545 of 2000 took 0.097s
  training loss:		0.587988
  validation loss:		0.572608
  validation accuracy:		80.87 %
Epoch 546 of 2000 took 0.097s
  training loss:		0.603673
  validation loss:		0.566404
  validation accuracy:		80.87 %
Epoch 547 of 2000 took 0.097s
  training loss:		0.597581
  validation loss:		0.579186
  validation accuracy:		80.76 %
Epoch 548 of 2000 took 0.096s
  training loss:		0.601849
  validation loss:		0.554673
  validation accuracy:		81.20 %
Epoch 549 of 2000 took 0.097s
  training loss:		0.601907
  validation loss:		0.563126
  validation accuracy:		81.20 %
Epoch 550 of 2000 took 0.097s
  training loss:		0.588269
  validation loss:		0.561266
  validation accuracy:		81.20 %
Epoch 551 of 2000 took 0.097s
  training loss:		0.605038
  validation loss:		0.552830
  validation accuracy:		81.74 %
Epoch 552 of 2000 took 0.096s
  training loss:		0.613764
  validation loss:		0.548170
  validation accuracy:		82.07 %
Epoch 553 of 2000 took 0.097s
  training loss:		0.598811
  validation loss:		0.574626
  validation accuracy:		80.98 %
Epoch 554 of 2000 took 0.096s
  training loss:		0.622439
  validation loss:		0.597310
  validation accuracy:		80.00 %
Epoch 555 of 2000 took 0.097s
  training loss:		0.599485
  validation loss:		0.549138
  validation accuracy:		81.74 %
Epoch 556 of 2000 took 0.096s
  training loss:		0.607374
  validation loss:		0.614125
  validation accuracy:		79.13 %
Epoch 557 of 2000 took 0.097s
  training loss:		0.588504
  validation loss:		0.558940
  validation accuracy:		81.41 %
Epoch 558 of 2000 took 0.096s
  training loss:		0.589828
  validation loss:		0.559889
  validation accuracy:		81.30 %
Epoch 559 of 2000 took 0.097s
  training loss:		0.576227
  validation loss:		0.556025
  validation accuracy:		81.74 %
Epoch 560 of 2000 took 0.097s
  training loss:		0.592675
  validation loss:		0.552463
  validation accuracy:		81.85 %
Epoch 561 of 2000 took 0.096s
  training loss:		0.605201
  validation loss:		0.546052
  validation accuracy:		82.07 %
Epoch 562 of 2000 took 0.097s
  training loss:		0.599660
  validation loss:		0.554082
  validation accuracy:		81.20 %
Epoch 563 of 2000 took 0.096s
  training loss:		0.602015
  validation loss:		0.597369
  validation accuracy:		79.89 %
Epoch 564 of 2000 took 0.097s
  training loss:		0.611170
  validation loss:		0.595528
  validation accuracy:		80.65 %
Epoch 565 of 2000 took 0.096s
  training loss:		0.593973
  validation loss:		0.552234
  validation accuracy:		81.74 %
Epoch 566 of 2000 took 0.097s
  training loss:		0.606846
  validation loss:		0.550863
  validation accuracy:		81.52 %
Epoch 567 of 2000 took 0.096s
  training loss:		0.599407
  validation loss:		0.575934
  validation accuracy:		80.87 %
Epoch 568 of 2000 took 0.096s
  training loss:		0.593664
  validation loss:		0.563604
  validation accuracy:		81.20 %
Epoch 569 of 2000 took 0.096s
  training loss:		0.591447
  validation loss:		0.550885
  validation accuracy:		81.30 %
Epoch 570 of 2000 took 0.097s
  training loss:		0.594584
  validation loss:		0.567452
  validation accuracy:		81.41 %
Epoch 571 of 2000 took 0.098s
  training loss:		0.592878
  validation loss:		0.548055
  validation accuracy:		81.74 %
Epoch 572 of 2000 took 0.097s
  training loss:		0.593785
  validation loss:		0.584009
  validation accuracy:		80.76 %
Epoch 573 of 2000 took 0.096s
  training loss:		0.586049
  validation loss:		0.574866
  validation accuracy:		81.20 %
Epoch 574 of 2000 took 0.097s
  training loss:		0.594563
  validation loss:		0.573737
  validation accuracy:		81.20 %
Epoch 575 of 2000 took 0.096s
  training loss:		0.587772
  validation loss:		0.555852
  validation accuracy:		81.20 %
Epoch 576 of 2000 took 0.097s
  training loss:		0.593967
  validation loss:		0.551047
  validation accuracy:		81.96 %
Epoch 577 of 2000 took 0.096s
  training loss:		0.617084
  validation loss:		0.574170
  validation accuracy:		80.87 %
Epoch 578 of 2000 took 0.097s
  training loss:		0.582123
  validation loss:		0.592564
  validation accuracy:		80.87 %
Epoch 579 of 2000 took 0.096s
  training loss:		0.593516
  validation loss:		0.577116
  validation accuracy:		81.30 %
Epoch 580 of 2000 took 0.097s
  training loss:		0.596567
  validation loss:		0.577116
  validation accuracy:		80.43 %
Epoch 581 of 2000 took 0.097s
  training loss:		0.594944
  validation loss:		0.595512
  validation accuracy:		80.22 %
Epoch 582 of 2000 took 0.096s
  training loss:		0.598035
  validation loss:		0.588149
  validation accuracy:		80.76 %
Epoch 583 of 2000 took 0.096s
  training loss:		0.603272
  validation loss:		0.550893
  validation accuracy:		82.07 %
Epoch 584 of 2000 took 0.097s
  training loss:		0.596066
  validation loss:		0.564978
  validation accuracy:		80.76 %
Epoch 585 of 2000 took 0.096s
  training loss:		0.597233
  validation loss:		0.565477
  validation accuracy:		81.52 %
Epoch 586 of 2000 took 0.097s
  training loss:		0.597936
  validation loss:		0.555907
  validation accuracy:		81.74 %
Epoch 587 of 2000 took 0.096s
  training loss:		0.585427
  validation loss:		0.564006
  validation accuracy:		81.09 %
Epoch 588 of 2000 took 0.097s
  training loss:		0.594253
  validation loss:		0.571457
  validation accuracy:		80.76 %
Epoch 589 of 2000 took 0.100s
  training loss:		0.589802
  validation loss:		0.554475
  validation accuracy:		81.30 %
Epoch 590 of 2000 took 0.098s
  training loss:		0.599024
  validation loss:		0.555574
  validation accuracy:		81.63 %
Epoch 591 of 2000 took 0.097s
  training loss:		0.591612
  validation loss:		0.558495
  validation accuracy:		81.52 %
Epoch 592 of 2000 took 0.097s
  training loss:		0.604008
  validation loss:		0.555163
  validation accuracy:		81.41 %
Epoch 593 of 2000 took 0.097s
  training loss:		0.600157
  validation loss:		0.550064
  validation accuracy:		81.85 %
Epoch 594 of 2000 took 0.097s
  training loss:		0.601274
  validation loss:		0.549326
  validation accuracy:		81.41 %
Epoch 595 of 2000 took 0.097s
  training loss:		0.607928
  validation loss:		0.580444
  validation accuracy:		81.09 %
Epoch 596 of 2000 took 0.096s
  training loss:		0.597032
  validation loss:		0.585108
  validation accuracy:		80.76 %
Epoch 597 of 2000 took 0.097s
  training loss:		0.598768
  validation loss:		0.615452
  validation accuracy:		80.11 %
Epoch 598 of 2000 took 0.097s
  training loss:		0.607137
  validation loss:		0.599389
  validation accuracy:		80.65 %
Epoch 599 of 2000 took 0.097s
  training loss:		0.607469
  validation loss:		0.571786
  validation accuracy:		79.35 %
Epoch 600 of 2000 took 0.096s
  training loss:		0.608817
  validation loss:		0.547411
  validation accuracy:		82.28 %
Epoch 601 of 2000 took 0.096s
  training loss:		0.591881
  validation loss:		0.585361
  validation accuracy:		80.43 %
Epoch 602 of 2000 took 0.096s
  training loss:		0.589629
  validation loss:		0.575973
  validation accuracy:		81.20 %
Epoch 603 of 2000 took 0.097s
  training loss:		0.597481
  validation loss:		0.613825
  validation accuracy:		80.00 %
Epoch 604 of 2000 took 0.096s
  training loss:		0.598087
  validation loss:		0.587175
  validation accuracy:		80.76 %
Epoch 605 of 2000 took 0.096s
  training loss:		0.602319
  validation loss:		0.570619
  validation accuracy:		80.87 %
Epoch 606 of 2000 took 0.096s
  training loss:		0.600501
  validation loss:		0.583933
  validation accuracy:		80.65 %
Epoch 607 of 2000 took 0.097s
  training loss:		0.587615
  validation loss:		0.553556
  validation accuracy:		81.52 %
Epoch 608 of 2000 took 0.096s
  training loss:		0.599037
  validation loss:		0.544053
  validation accuracy:		81.96 %
Epoch 609 of 2000 took 0.096s
  training loss:		0.603879
  validation loss:		0.551920
  validation accuracy:		82.07 %
Epoch 610 of 2000 took 0.096s
  training loss:		0.598293
  validation loss:		0.551308
  validation accuracy:		81.63 %
Epoch 611 of 2000 took 0.096s
  training loss:		0.600973
  validation loss:		0.558126
  validation accuracy:		81.20 %
Epoch 612 of 2000 took 0.097s
  training loss:		0.584476
  validation loss:		0.558188
  validation accuracy:		81.30 %
Epoch 613 of 2000 took 0.097s
  training loss:		0.587194
  validation loss:		0.571060
  validation accuracy:		81.41 %
Epoch 614 of 2000 took 0.096s
  training loss:		0.595027
  validation loss:		0.576788
  validation accuracy:		80.98 %
Epoch 615 of 2000 took 0.097s
  training loss:		0.586685
  validation loss:		0.594737
  validation accuracy:		80.11 %
Epoch 616 of 2000 took 0.096s
  training loss:		0.588254
  validation loss:		0.576165
  validation accuracy:		81.30 %
Epoch 617 of 2000 took 0.097s
  training loss:		0.604264
  validation loss:		0.551021
  validation accuracy:		81.63 %
Epoch 618 of 2000 took 0.096s
  training loss:		0.592934
  validation loss:		0.565454
  validation accuracy:		80.98 %
Epoch 619 of 2000 took 0.096s
  training loss:		0.590075
  validation loss:		0.582267
  validation accuracy:		80.87 %
Epoch 620 of 2000 took 0.097s
  training loss:		0.587493
  validation loss:		0.581842
  validation accuracy:		80.65 %
Epoch 621 of 2000 took 0.097s
  training loss:		0.600082
  validation loss:		0.547220
  validation accuracy:		82.17 %
Epoch 622 of 2000 took 0.097s
  training loss:		0.588216
  validation loss:		0.578298
  validation accuracy:		81.20 %
Epoch 623 of 2000 took 0.097s
  training loss:		0.585672
  validation loss:		0.580912
  validation accuracy:		81.09 %
Epoch 624 of 2000 took 0.097s
  training loss:		0.594174
  validation loss:		0.625728
  validation accuracy:		79.02 %
Epoch 625 of 2000 took 0.096s
  training loss:		0.593655
  validation loss:		0.542609
  validation accuracy:		82.39 %
Epoch 626 of 2000 took 0.097s
  training loss:		0.592709
  validation loss:		0.569479
  validation accuracy:		81.63 %
Epoch 627 of 2000 took 0.096s
  training loss:		0.585268
  validation loss:		0.614924
  validation accuracy:		79.67 %
Epoch 628 of 2000 took 0.097s
  training loss:		0.586207
  validation loss:		0.550974
  validation accuracy:		81.30 %
Epoch 629 of 2000 took 0.097s
  training loss:		0.592171
  validation loss:		0.555890
  validation accuracy:		81.20 %
Epoch 630 of 2000 took 0.097s
  training loss:		0.593851
  validation loss:		0.563252
  validation accuracy:		81.52 %
Epoch 631 of 2000 took 0.096s
  training loss:		0.593066
  validation loss:		0.549457
  validation accuracy:		81.96 %
Epoch 632 of 2000 took 0.097s
  training loss:		0.597284
  validation loss:		0.557452
  validation accuracy:		81.63 %
Epoch 633 of 2000 took 0.097s
  training loss:		0.594097
  validation loss:		0.548809
  validation accuracy:		81.63 %
Epoch 634 of 2000 took 0.097s
  training loss:		0.589095
  validation loss:		0.556450
  validation accuracy:		80.98 %
Epoch 635 of 2000 took 0.096s
  training loss:		0.579415
  validation loss:		0.554783
  validation accuracy:		80.98 %
Epoch 636 of 2000 took 0.097s
  training loss:		0.593618
  validation loss:		0.569920
  validation accuracy:		81.09 %
Epoch 637 of 2000 took 0.096s
  training loss:		0.602938
  validation loss:		0.569444
  validation accuracy:		81.52 %
Epoch 638 of 2000 took 0.097s
  training loss:		0.595170
  validation loss:		0.599512
  validation accuracy:		79.89 %
Epoch 639 of 2000 took 0.097s
  training loss:		0.596649
  validation loss:		0.567656
  validation accuracy:		81.41 %
Epoch 640 of 2000 took 0.097s
  training loss:		0.600410
  validation loss:		0.590390
  validation accuracy:		80.54 %
Epoch 641 of 2000 took 0.096s
  training loss:		0.588904
  validation loss:		0.561785
  validation accuracy:		80.76 %
Epoch 642 of 2000 took 0.097s
  training loss:		0.590870
  validation loss:		0.544852
  validation accuracy:		82.39 %
Epoch 643 of 2000 took 0.097s
  training loss:		0.594520
  validation loss:		0.607850
  validation accuracy:		79.89 %
Epoch 644 of 2000 took 0.097s
  training loss:		0.591845
  validation loss:		0.548603
  validation accuracy:		81.52 %
Epoch 645 of 2000 took 0.097s
  training loss:		0.595155
  validation loss:		0.573423
  validation accuracy:		81.52 %
Epoch 646 of 2000 took 0.097s
  training loss:		0.590828
  validation loss:		0.589803
  validation accuracy:		81.41 %
Epoch 647 of 2000 took 0.096s
  training loss:		0.596469
  validation loss:		0.549456
  validation accuracy:		81.74 %
Epoch 648 of 2000 took 0.096s
  training loss:		0.591840
  validation loss:		0.568777
  validation accuracy:		81.30 %
Epoch 649 of 2000 took 0.097s
  training loss:		0.595631
  validation loss:		0.570815
  validation accuracy:		80.65 %
Epoch 650 of 2000 took 0.096s
  training loss:		0.596518
  validation loss:		0.565338
  validation accuracy:		81.09 %
Epoch 651 of 2000 took 0.097s
  training loss:		0.593301
  validation loss:		0.581697
  validation accuracy:		80.87 %
Epoch 652 of 2000 took 0.096s
  training loss:		0.589543
  validation loss:		0.553202
  validation accuracy:		81.85 %
Epoch 653 of 2000 took 0.097s
  training loss:		0.595718
  validation loss:		0.545887
  validation accuracy:		81.85 %
Epoch 654 of 2000 took 0.097s
  training loss:		0.601379
  validation loss:		0.570568
  validation accuracy:		81.52 %
Epoch 655 of 2000 took 0.097s
  training loss:		0.581226
  validation loss:		0.574199
  validation accuracy:		81.41 %
Epoch 656 of 2000 took 0.096s
  training loss:		0.597881
  validation loss:		0.577626
  validation accuracy:		81.09 %
Epoch 657 of 2000 took 0.097s
  training loss:		0.601447
  validation loss:		0.551490
  validation accuracy:		80.98 %
Epoch 658 of 2000 took 0.096s
  training loss:		0.602724
  validation loss:		0.575658
  validation accuracy:		81.09 %
Epoch 659 of 2000 took 0.097s
  training loss:		0.588723
  validation loss:		0.550733
  validation accuracy:		81.85 %
Epoch 660 of 2000 took 0.096s
  training loss:		0.587830
  validation loss:		0.579004
  validation accuracy:		81.30 %
Epoch 661 of 2000 took 0.097s
  training loss:		0.594857
  validation loss:		0.622895
  validation accuracy:		79.46 %
Epoch 662 of 2000 took 0.096s
  training loss:		0.584966
  validation loss:		0.558611
  validation accuracy:		81.20 %
Epoch 663 of 2000 took 0.097s
  training loss:		0.588857
  validation loss:		0.576411
  validation accuracy:		81.30 %
Epoch 664 of 2000 took 0.096s
  training loss:		0.593510
  validation loss:		0.588650
  validation accuracy:		80.87 %
Epoch 665 of 2000 took 0.097s
  training loss:		0.582039
  validation loss:		0.566572
  validation accuracy:		81.41 %
Epoch 666 of 2000 took 0.096s
  training loss:		0.597512
  validation loss:		0.581516
  validation accuracy:		81.30 %
Epoch 667 of 2000 took 0.097s
  training loss:		0.589150
  validation loss:		0.554861
  validation accuracy:		81.20 %
Epoch 668 of 2000 took 0.096s
  training loss:		0.597923
  validation loss:		0.566178
  validation accuracy:		81.63 %
Epoch 669 of 2000 took 0.096s
  training loss:		0.579486
  validation loss:		0.576669
  validation accuracy:		81.20 %
Epoch 670 of 2000 took 0.096s
  training loss:		0.602555
  validation loss:		0.583128
  validation accuracy:		81.09 %
Epoch 671 of 2000 took 0.097s
  training loss:		0.590922
  validation loss:		0.548526
  validation accuracy:		81.63 %
Epoch 672 of 2000 took 0.096s
  training loss:		0.589782
  validation loss:		0.552107
  validation accuracy:		81.96 %
Epoch 673 of 2000 took 0.096s
  training loss:		0.588134
  validation loss:		0.560234
  validation accuracy:		81.96 %
Epoch 674 of 2000 took 0.099s
  training loss:		0.597161
  validation loss:		0.567588
  validation accuracy:		81.74 %
Epoch 675 of 2000 took 0.100s
  training loss:		0.581532
  validation loss:		0.552242
  validation accuracy:		81.96 %
Epoch 676 of 2000 took 0.100s
  training loss:		0.603982
  validation loss:		0.611178
  validation accuracy:		80.22 %
Epoch 677 of 2000 took 0.100s
  training loss:		0.601835
  validation loss:		0.583336
  validation accuracy:		81.20 %
Epoch 678 of 2000 took 0.099s
  training loss:		0.591899
  validation loss:		0.563464
  validation accuracy:		81.52 %
Epoch 679 of 2000 took 0.100s
  training loss:		0.583421
  validation loss:		0.552779
  validation accuracy:		81.63 %
Epoch 680 of 2000 took 0.099s
  training loss:		0.596166
  validation loss:		0.546781
  validation accuracy:		81.63 %
Epoch 681 of 2000 took 0.100s
  training loss:		0.586593
  validation loss:		0.564528
  validation accuracy:		81.74 %
Epoch 682 of 2000 took 0.100s
  training loss:		0.586061
  validation loss:		0.562567
  validation accuracy:		81.09 %
Epoch 683 of 2000 took 0.100s
  training loss:		0.587001
  validation loss:		0.575518
  validation accuracy:		81.52 %
Epoch 684 of 2000 took 0.100s
  training loss:		0.589516
  validation loss:		0.590206
  validation accuracy:		81.74 %
Epoch 685 of 2000 took 0.100s
  training loss:		0.590530
  validation loss:		0.605176
  validation accuracy:		80.22 %
Epoch 686 of 2000 took 0.100s
  training loss:		0.587079
  validation loss:		0.569909
  validation accuracy:		80.54 %
Epoch 687 of 2000 took 0.100s
  training loss:		0.595130
  validation loss:		0.546697
  validation accuracy:		81.74 %
Epoch 688 of 2000 took 0.099s
  training loss:		0.604197
  validation loss:		0.557824
  validation accuracy:		80.98 %
Epoch 689 of 2000 took 0.100s
  training loss:		0.587255
  validation loss:		0.550331
  validation accuracy:		81.09 %
Epoch 690 of 2000 took 0.102s
  training loss:		0.604934
  validation loss:		0.578283
  validation accuracy:		80.43 %
Epoch 691 of 2000 took 0.100s
  training loss:		0.589310
  validation loss:		0.552391
  validation accuracy:		81.41 %
Epoch 692 of 2000 took 0.099s
  training loss:		0.596484
  validation loss:		0.564002
  validation accuracy:		81.30 %
Epoch 693 of 2000 took 0.100s
  training loss:		0.584033
  validation loss:		0.553037
  validation accuracy:		81.74 %
Epoch 694 of 2000 took 0.100s
  training loss:		0.592631
  validation loss:		0.623992
  validation accuracy:		79.46 %
Epoch 695 of 2000 took 0.100s
  training loss:		0.595927
  validation loss:		0.594809
  validation accuracy:		80.33 %
Epoch 696 of 2000 took 0.100s
  training loss:		0.582251
  validation loss:		0.612372
  validation accuracy:		80.22 %
Epoch 697 of 2000 took 0.100s
  training loss:		0.593731
  validation loss:		0.554406
  validation accuracy:		81.41 %
Epoch 698 of 2000 took 0.099s
  training loss:		0.587637
  validation loss:		0.565020
  validation accuracy:		80.87 %
Epoch 699 of 2000 took 0.100s
  training loss:		0.588544
  validation loss:		0.557366
  validation accuracy:		81.30 %
Epoch 700 of 2000 took 0.100s
  training loss:		0.584834
  validation loss:		0.559671
  validation accuracy:		81.30 %
Epoch 701 of 2000 took 0.100s
  training loss:		0.593374
  validation loss:		0.551085
  validation accuracy:		81.85 %
Epoch 702 of 2000 took 0.099s
  training loss:		0.587320
  validation loss:		0.546609
  validation accuracy:		81.41 %
Epoch 703 of 2000 took 0.099s
  training loss:		0.577825
  validation loss:		0.565049
  validation accuracy:		81.41 %
Epoch 704 of 2000 took 0.100s
  training loss:		0.593838
  validation loss:		0.592292
  validation accuracy:		80.98 %
Epoch 705 of 2000 took 0.099s
  training loss:		0.586274
  validation loss:		0.553502
  validation accuracy:		81.52 %
Epoch 706 of 2000 took 0.100s
  training loss:		0.590454
  validation loss:		0.560759
  validation accuracy:		81.30 %
Epoch 707 of 2000 took 0.099s
  training loss:		0.594507
  validation loss:		0.552520
  validation accuracy:		80.65 %
Epoch 708 of 2000 took 0.100s
  training loss:		0.598530
  validation loss:		0.576211
  validation accuracy:		81.52 %
Epoch 709 of 2000 took 0.100s
  training loss:		0.587708
  validation loss:		0.587471
  validation accuracy:		80.22 %
Epoch 710 of 2000 took 0.100s
  training loss:		0.584388
  validation loss:		0.584541
  validation accuracy:		81.63 %
Epoch 711 of 2000 took 0.100s
  training loss:		0.582103
  validation loss:		0.598774
  validation accuracy:		80.22 %
Epoch 712 of 2000 took 0.100s
  training loss:		0.589813
  validation loss:		0.567421
  validation accuracy:		81.96 %
Epoch 713 of 2000 took 0.099s
  training loss:		0.594677
  validation loss:		0.558912
  validation accuracy:		80.76 %
Epoch 714 of 2000 took 0.101s
  training loss:		0.589764
  validation loss:		0.549626
  validation accuracy:		81.96 %
Epoch 715 of 2000 took 0.100s
  training loss:		0.583536
  validation loss:		0.571891
  validation accuracy:		81.30 %
Epoch 716 of 2000 took 0.100s
  training loss:		0.592539
  validation loss:		0.556466
  validation accuracy:		81.41 %
Epoch 717 of 2000 took 0.099s
  training loss:		0.584787
  validation loss:		0.593808
  validation accuracy:		81.20 %
Epoch 718 of 2000 took 0.100s
  training loss:		0.598152
  validation loss:		0.634368
  validation accuracy:		79.02 %
Epoch 719 of 2000 took 0.099s
  training loss:		0.583340
  validation loss:		0.551863
  validation accuracy:		81.63 %
Epoch 720 of 2000 took 0.100s
  training loss:		0.603296
  validation loss:		0.603909
  validation accuracy:		79.78 %
Epoch 721 of 2000 took 0.099s
  training loss:		0.593336
  validation loss:		0.580695
  validation accuracy:		79.89 %
Epoch 722 of 2000 took 0.100s
  training loss:		0.590054
  validation loss:		0.564625
  validation accuracy:		81.74 %
Epoch 723 of 2000 took 0.099s
  training loss:		0.585782
  validation loss:		0.544695
  validation accuracy:		82.28 %
Epoch 724 of 2000 took 0.100s
  training loss:		0.589952
  validation loss:		0.605174
  validation accuracy:		80.54 %
Epoch 725 of 2000 took 0.099s
  training loss:		0.587817
  validation loss:		0.548765
  validation accuracy:		81.96 %
Epoch 726 of 2000 took 0.100s
  training loss:		0.585958
  validation loss:		0.621141
  validation accuracy:		79.13 %
Epoch 727 of 2000 took 0.100s
  training loss:		0.595974
  validation loss:		0.556844
  validation accuracy:		81.41 %
Epoch 728 of 2000 took 0.100s
  training loss:		0.595999
  validation loss:		0.549666
  validation accuracy:		82.07 %
Epoch 729 of 2000 took 0.099s
  training loss:		0.579932
  validation loss:		0.576210
  validation accuracy:		81.09 %
Epoch 730 of 2000 took 0.100s
  training loss:		0.583961
  validation loss:		0.596136
  validation accuracy:		80.54 %
Epoch 731 of 2000 took 0.099s
  training loss:		0.583956
  validation loss:		0.557168
  validation accuracy:		81.30 %
Epoch 732 of 2000 took 0.100s
  training loss:		0.600402
  validation loss:		0.654245
  validation accuracy:		78.26 %
Epoch 733 of 2000 took 0.099s
  training loss:		0.594626
  validation loss:		0.584118
  validation accuracy:		81.30 %
Epoch 734 of 2000 took 0.100s
  training loss:		0.584747
  validation loss:		0.561503
  validation accuracy:		81.74 %
Epoch 735 of 2000 took 0.100s
  training loss:		0.585106
  validation loss:		0.557477
  validation accuracy:		81.30 %
Epoch 736 of 2000 took 0.100s
  training loss:		0.582802
  validation loss:		0.556192
  validation accuracy:		81.20 %
Epoch 737 of 2000 took 0.099s
  training loss:		0.594186
  validation loss:		0.571091
  validation accuracy:		81.09 %
Epoch 738 of 2000 took 0.100s
  training loss:		0.599774
  validation loss:		0.575445
  validation accuracy:		81.09 %
Epoch 739 of 2000 took 0.099s
  training loss:		0.586693
  validation loss:		0.572027
  validation accuracy:		81.20 %
Epoch 740 of 2000 took 0.100s
  training loss:		0.594769
  validation loss:		0.548111
  validation accuracy:		81.63 %
Epoch 741 of 2000 took 0.099s
  training loss:		0.591601
  validation loss:		0.563590
  validation accuracy:		81.63 %
Epoch 742 of 2000 took 0.100s
  training loss:		0.590059
  validation loss:		0.550530
  validation accuracy:		81.20 %
Epoch 743 of 2000 took 0.099s
  training loss:		0.584894
  validation loss:		0.586363
  validation accuracy:		80.65 %
Epoch 744 of 2000 took 0.100s
  training loss:		0.580026
  validation loss:		0.547660
  validation accuracy:		82.07 %
Epoch 745 of 2000 took 0.100s
  training loss:		0.586567
  validation loss:		0.618421
  validation accuracy:		79.89 %
Epoch 746 of 2000 took 0.100s
  training loss:		0.590375
  validation loss:		0.557963
  validation accuracy:		81.63 %
Epoch 747 of 2000 took 0.099s
  training loss:		0.570917
  validation loss:		0.567717
  validation accuracy:		80.22 %
Epoch 748 of 2000 took 0.100s
  training loss:		0.592616
  validation loss:		0.592289
  validation accuracy:		81.20 %
Epoch 749 of 2000 took 0.099s
  training loss:		0.587264
  validation loss:		0.543463
  validation accuracy:		81.85 %
Epoch 750 of 2000 took 0.100s
  training loss:		0.583972
  validation loss:		0.548494
  validation accuracy:		81.30 %
Epoch 751 of 2000 took 0.100s
  training loss:		0.580681
  validation loss:		0.591152
  validation accuracy:		80.76 %
Epoch 752 of 2000 took 0.100s
  training loss:		0.584428
  validation loss:		0.576118
  validation accuracy:		81.20 %
Epoch 753 of 2000 took 0.099s
  training loss:		0.584706
  validation loss:		0.559434
  validation accuracy:		80.98 %
Epoch 754 of 2000 took 0.098s
  training loss:		0.581357
  validation loss:		0.551490
  validation accuracy:		81.41 %
Epoch 755 of 2000 took 0.097s
  training loss:		0.604369
  validation loss:		0.603773
  validation accuracy:		80.43 %
Epoch 756 of 2000 took 0.097s
  training loss:		0.591126
  validation loss:		0.589216
  validation accuracy:		81.09 %
Epoch 757 of 2000 took 0.096s
  training loss:		0.582858
  validation loss:		0.546688
  validation accuracy:		82.07 %
Epoch 758 of 2000 took 0.097s
  training loss:		0.598587
  validation loss:		0.627985
  validation accuracy:		79.02 %
Epoch 759 of 2000 took 0.096s
  training loss:		0.582673
  validation loss:		0.563016
  validation accuracy:		81.30 %
Epoch 760 of 2000 took 0.097s
  training loss:		0.577765
  validation loss:		0.563913
  validation accuracy:		81.30 %
Epoch 761 of 2000 took 0.096s
  training loss:		0.581213
  validation loss:		0.569256
  validation accuracy:		81.30 %
Epoch 762 of 2000 took 0.096s
  training loss:		0.583647
  validation loss:		0.546774
  validation accuracy:		82.07 %
Epoch 763 of 2000 took 0.096s
  training loss:		0.591099
  validation loss:		0.570825
  validation accuracy:		81.30 %
Epoch 764 of 2000 took 0.097s
  training loss:		0.582052
  validation loss:		0.556517
  validation accuracy:		81.09 %
Epoch 765 of 2000 took 0.098s
  training loss:		0.587064
  validation loss:		0.573415
  validation accuracy:		81.85 %
Epoch 766 of 2000 took 0.100s
  training loss:		0.587811
  validation loss:		0.593323
  validation accuracy:		80.65 %
Epoch 767 of 2000 took 0.099s
  training loss:		0.571682
  validation loss:		0.560491
  validation accuracy:		81.09 %
Epoch 768 of 2000 took 0.100s
  training loss:		0.576891
  validation loss:		0.574117
  validation accuracy:		80.87 %
Epoch 769 of 2000 took 0.099s
  training loss:		0.582597
  validation loss:		0.553462
  validation accuracy:		81.41 %
Epoch 770 of 2000 took 0.100s
  training loss:		0.574578
  validation loss:		0.562392
  validation accuracy:		81.63 %
Epoch 771 of 2000 took 0.099s
  training loss:		0.588316
  validation loss:		0.575061
  validation accuracy:		81.20 %
Epoch 772 of 2000 took 0.100s
  training loss:		0.594975
  validation loss:		0.595549
  validation accuracy:		80.43 %
Epoch 773 of 2000 took 0.099s
  training loss:		0.581220
  validation loss:		0.579126
  validation accuracy:		81.30 %
Epoch 774 of 2000 took 0.100s
  training loss:		0.596373
  validation loss:		0.544444
  validation accuracy:		81.30 %
Epoch 775 of 2000 took 0.100s
  training loss:		0.588732
  validation loss:		0.553384
  validation accuracy:		81.63 %
Epoch 776 of 2000 took 0.100s
  training loss:		0.578173
  validation loss:		0.574057
  validation accuracy:		81.20 %
Epoch 777 of 2000 took 0.099s
  training loss:		0.586418
  validation loss:		0.581946
  validation accuracy:		81.09 %
Epoch 778 of 2000 took 0.100s
  training loss:		0.583407
  validation loss:		0.560243
  validation accuracy:		81.41 %
Epoch 779 of 2000 took 0.099s
  training loss:		0.589678
  validation loss:		0.555453
  validation accuracy:		81.41 %
Epoch 780 of 2000 took 0.100s
  training loss:		0.582598
  validation loss:		0.567636
  validation accuracy:		81.30 %
Epoch 781 of 2000 took 0.100s
  training loss:		0.578542
  validation loss:		0.569015
  validation accuracy:		81.20 %
Epoch 782 of 2000 took 0.100s
  training loss:		0.584788
  validation loss:		0.561776
  validation accuracy:		81.52 %
Epoch 783 of 2000 took 0.100s
  training loss:		0.582128
  validation loss:		0.586948
  validation accuracy:		80.87 %
Epoch 784 of 2000 took 0.100s
  training loss:		0.581869
  validation loss:		0.584700
  validation accuracy:		80.87 %
Epoch 785 of 2000 took 0.100s
  training loss:		0.575545
  validation loss:		0.597996
  validation accuracy:		80.33 %
Epoch 786 of 2000 took 0.100s
  training loss:		0.576532
  validation loss:		0.578896
  validation accuracy:		81.41 %
Epoch 787 of 2000 took 0.099s
  training loss:		0.584454
  validation loss:		0.574164
  validation accuracy:		81.30 %
Epoch 788 of 2000 took 0.100s
  training loss:		0.593258
  validation loss:		0.567667
  validation accuracy:		81.30 %
Epoch 789 of 2000 took 0.099s
  training loss:		0.572063
  validation loss:		0.543356
  validation accuracy:		82.17 %
Epoch 790 of 2000 took 0.100s
  training loss:		0.585586
  validation loss:		0.542577
  validation accuracy:		82.28 %
Epoch 791 of 2000 took 0.100s
  training loss:		0.584836
  validation loss:		0.541940
  validation accuracy:		82.39 %
Epoch 792 of 2000 took 0.100s
  training loss:		0.576286
  validation loss:		0.543005
  validation accuracy:		81.30 %
Epoch 793 of 2000 took 0.099s
  training loss:		0.585157
  validation loss:		0.600413
  validation accuracy:		80.87 %
Epoch 794 of 2000 took 0.099s
  training loss:		0.591887
  validation loss:		0.547418
  validation accuracy:		81.85 %
Epoch 795 of 2000 took 0.101s
  training loss:		0.580726
  validation loss:		0.584149
  validation accuracy:		80.87 %
Epoch 796 of 2000 took 0.100s
  training loss:		0.579825
  validation loss:		0.546335
  validation accuracy:		81.85 %
Epoch 797 of 2000 took 0.100s
  training loss:		0.573696
  validation loss:		0.548027
  validation accuracy:		81.52 %
Epoch 798 of 2000 took 0.100s
  training loss:		0.582760
  validation loss:		0.550851
  validation accuracy:		81.52 %
Epoch 799 of 2000 took 0.099s
  training loss:		0.582212
  validation loss:		0.547358
  validation accuracy:		81.63 %
Epoch 800 of 2000 took 0.100s
  training loss:		0.577532
  validation loss:		0.566848
  validation accuracy:		81.30 %
Epoch 801 of 2000 took 0.099s
  training loss:		0.576662
  validation loss:		0.537760
  validation accuracy:		82.28 %
Epoch 802 of 2000 took 0.100s
  training loss:		0.578864
  validation loss:		0.543816
  validation accuracy:		81.63 %
Epoch 803 of 2000 took 0.099s
  training loss:		0.579035
  validation loss:		0.599006
  validation accuracy:		79.78 %
Epoch 804 of 2000 took 0.100s
  training loss:		0.584134
  validation loss:		0.546143
  validation accuracy:		82.07 %
Epoch 805 of 2000 took 0.100s
  training loss:		0.568188
  validation loss:		0.551891
  validation accuracy:		81.74 %
Epoch 806 of 2000 took 0.100s
  training loss:		0.574116
  validation loss:		0.555777
  validation accuracy:		81.85 %
Epoch 807 of 2000 took 0.100s
  training loss:		0.574904
  validation loss:		0.558089
  validation accuracy:		81.41 %
Epoch 808 of 2000 took 0.100s
  training loss:		0.586742
  validation loss:		0.560771
  validation accuracy:		81.63 %
Epoch 809 of 2000 took 0.102s
  training loss:		0.572838
  validation loss:		0.581742
  validation accuracy:		80.98 %
Epoch 810 of 2000 took 0.100s
  training loss:		0.600249
  validation loss:		0.559188
  validation accuracy:		81.20 %
Epoch 811 of 2000 took 0.100s
  training loss:		0.575159
  validation loss:		0.558049
  validation accuracy:		81.52 %
Epoch 812 of 2000 took 0.099s
  training loss:		0.581845
  validation loss:		0.543042
  validation accuracy:		81.96 %
Epoch 813 of 2000 took 0.099s
  training loss:		0.575555
  validation loss:		0.540754
  validation accuracy:		82.17 %
Epoch 814 of 2000 took 0.100s
  training loss:		0.567438
  validation loss:		0.549758
  validation accuracy:		81.85 %
Epoch 815 of 2000 took 0.100s
  training loss:		0.572710
  validation loss:		0.576829
  validation accuracy:		81.09 %
Epoch 816 of 2000 took 0.100s
  training loss:		0.570926
  validation loss:		0.557892
  validation accuracy:		81.30 %
Epoch 817 of 2000 took 0.099s
  training loss:		0.575829
  validation loss:		0.549688
  validation accuracy:		81.41 %
Epoch 818 of 2000 took 0.100s
  training loss:		0.566010
  validation loss:		0.542423
  validation accuracy:		81.96 %
Epoch 819 of 2000 took 0.099s
  training loss:		0.571341
  validation loss:		0.573917
  validation accuracy:		81.09 %
Epoch 820 of 2000 took 0.100s
  training loss:		0.579338
  validation loss:		0.549400
  validation accuracy:		81.52 %
Epoch 821 of 2000 took 0.100s
  training loss:		0.578155
  validation loss:		0.555847
  validation accuracy:		81.63 %
Epoch 822 of 2000 took 0.100s
  training loss:		0.556408
  validation loss:		0.545351
  validation accuracy:		81.85 %
Epoch 823 of 2000 took 0.099s
  training loss:		0.566916
  validation loss:		0.538882
  validation accuracy:		81.85 %
Epoch 824 of 2000 took 0.100s
  training loss:		0.574771
  validation loss:		0.544972
  validation accuracy:		81.52 %
Epoch 825 of 2000 took 0.100s
  training loss:		0.565296
  validation loss:		0.578265
  validation accuracy:		80.98 %
Epoch 826 of 2000 took 0.099s
  training loss:		0.573123
  validation loss:		0.555476
  validation accuracy:		81.20 %
Epoch 827 of 2000 took 0.099s
  training loss:		0.557291
  validation loss:		0.557438
  validation accuracy:		81.52 %
Epoch 828 of 2000 took 0.100s
  training loss:		0.564543
  validation loss:		0.547399
  validation accuracy:		81.52 %
Epoch 829 of 2000 took 0.099s
  training loss:		0.567641
  validation loss:		0.612148
  validation accuracy:		80.11 %
Epoch 830 of 2000 took 0.100s
  training loss:		0.574898
  validation loss:		0.532815
  validation accuracy:		82.17 %
Epoch 831 of 2000 took 0.099s
  training loss:		0.561110
  validation loss:		0.544764
  validation accuracy:		81.74 %
Epoch 832 of 2000 took 0.100s
  training loss:		0.559109
  validation loss:		0.532223
  validation accuracy:		82.07 %
Epoch 833 of 2000 took 0.100s
  training loss:		0.557120
  validation loss:		0.535420
  validation accuracy:		82.28 %
Epoch 834 of 2000 took 0.100s
  training loss:		0.564695
  validation loss:		0.540787
  validation accuracy:		81.96 %
Epoch 835 of 2000 took 0.100s
  training loss:		0.561799
  validation loss:		0.572982
  validation accuracy:		81.09 %
Epoch 836 of 2000 took 0.097s
  training loss:		0.557526
  validation loss:		0.538271
  validation accuracy:		82.17 %
Epoch 837 of 2000 took 0.096s
  training loss:		0.558711
  validation loss:		0.541318
  validation accuracy:		81.96 %
Epoch 838 of 2000 took 0.096s
  training loss:		0.549790
  validation loss:		0.533516
  validation accuracy:		81.96 %
Epoch 839 of 2000 took 0.096s
  training loss:		0.567965
  validation loss:		0.565406
  validation accuracy:		81.41 %
Epoch 840 of 2000 took 0.096s
  training loss:		0.562436
  validation loss:		0.560464
  validation accuracy:		80.98 %
Epoch 841 of 2000 took 0.097s
  training loss:		0.556775
  validation loss:		0.568581
  validation accuracy:		81.41 %
Epoch 842 of 2000 took 0.096s
  training loss:		0.552262
  validation loss:		0.540178
  validation accuracy:		81.63 %
Epoch 843 of 2000 took 0.096s
  training loss:		0.554386
  validation loss:		0.573449
  validation accuracy:		81.63 %
Epoch 844 of 2000 took 0.096s
  training loss:		0.559386
  validation loss:		0.548837
  validation accuracy:		81.30 %
Epoch 845 of 2000 took 0.096s
  training loss:		0.552092
  validation loss:		0.537563
  validation accuracy:		81.74 %
Epoch 846 of 2000 took 0.097s
  training loss:		0.546774
  validation loss:		0.537984
  validation accuracy:		81.74 %
Epoch 847 of 2000 took 0.097s
  training loss:		0.548881
  validation loss:		0.537327
  validation accuracy:		81.96 %
Epoch 848 of 2000 took 0.096s
  training loss:		0.547043
  validation loss:		0.536277
  validation accuracy:		81.41 %
Epoch 849 of 2000 took 0.096s
  training loss:		0.550287
  validation loss:		0.529391
  validation accuracy:		82.39 %
Epoch 850 of 2000 took 0.096s
  training loss:		0.546359
  validation loss:		0.529245
  validation accuracy:		82.39 %
Epoch 851 of 2000 took 0.096s
  training loss:		0.538014
  validation loss:		0.522106
  validation accuracy:		82.83 %
Epoch 852 of 2000 took 0.096s
  training loss:		0.541081
  validation loss:		0.550480
  validation accuracy:		81.30 %
Epoch 853 of 2000 took 0.096s
  training loss:		0.538073
  validation loss:		0.530015
  validation accuracy:		82.17 %
Epoch 854 of 2000 took 0.096s
  training loss:		0.536747
  validation loss:		0.535531
  validation accuracy:		81.63 %
Epoch 855 of 2000 took 0.096s
  training loss:		0.540932
  validation loss:		0.536535
  validation accuracy:		80.98 %
Epoch 856 of 2000 took 0.097s
  training loss:		0.535713
  validation loss:		0.584352
  validation accuracy:		81.20 %
Epoch 857 of 2000 took 0.097s
  training loss:		0.535307
  validation loss:		0.524414
  validation accuracy:		81.96 %
Epoch 858 of 2000 took 0.096s
  training loss:		0.533756
  validation loss:		0.556996
  validation accuracy:		81.52 %
Epoch 859 of 2000 took 0.096s
  training loss:		0.537858
  validation loss:		0.522182
  validation accuracy:		82.17 %
Epoch 860 of 2000 took 0.097s
  training loss:		0.537800
  validation loss:		0.515282
  validation accuracy:		82.93 %
Epoch 861 of 2000 took 0.097s
  training loss:		0.533463
  validation loss:		0.530300
  validation accuracy:		81.74 %
Epoch 862 of 2000 took 0.096s
  training loss:		0.530591
  validation loss:		0.520825
  validation accuracy:		82.07 %
Epoch 863 of 2000 took 0.097s
  training loss:		0.522234
  validation loss:		0.529582
  validation accuracy:		81.85 %
Epoch 864 of 2000 took 0.096s
  training loss:		0.524299
  validation loss:		0.525764
  validation accuracy:		81.85 %
Epoch 865 of 2000 took 0.096s
  training loss:		0.523857
  validation loss:		0.550463
  validation accuracy:		81.52 %
Epoch 866 of 2000 took 0.097s
  training loss:		0.513529
  validation loss:		0.515216
  validation accuracy:		82.39 %
Epoch 867 of 2000 took 0.097s
  training loss:		0.511922
  validation loss:		0.525672
  validation accuracy:		82.28 %
Epoch 868 of 2000 took 0.096s
  training loss:		0.519101
  validation loss:		0.512304
  validation accuracy:		82.72 %
Epoch 869 of 2000 took 0.096s
  training loss:		0.518459
  validation loss:		0.556862
  validation accuracy:		81.30 %
Epoch 870 of 2000 took 0.096s
  training loss:		0.521469
  validation loss:		0.508442
  validation accuracy:		82.72 %
Epoch 871 of 2000 took 0.096s
  training loss:		0.515413
  validation loss:		0.510853
  validation accuracy:		82.28 %
Epoch 872 of 2000 took 0.097s
  training loss:		0.504308
  validation loss:		0.511630
  validation accuracy:		82.50 %
Epoch 873 of 2000 took 0.097s
  training loss:		0.503020
  validation loss:		0.526782
  validation accuracy:		82.07 %
Epoch 874 of 2000 took 0.097s
  training loss:		0.516924
  validation loss:		0.497989
  validation accuracy:		83.04 %
Epoch 875 of 2000 took 0.096s
  training loss:		0.495049
  validation loss:		0.522755
  validation accuracy:		82.07 %
Epoch 876 of 2000 took 0.097s
  training loss:		0.511334
  validation loss:		0.513273
  validation accuracy:		82.93 %
Epoch 877 of 2000 took 0.097s
  training loss:		0.495379
  validation loss:		0.495999
  validation accuracy:		82.93 %
Epoch 878 of 2000 took 0.097s
  training loss:		0.501592
  validation loss:		0.495569
  validation accuracy:		83.04 %
Epoch 879 of 2000 took 0.096s
  training loss:		0.499843
  validation loss:		0.505788
  validation accuracy:		82.39 %
Epoch 880 of 2000 took 0.097s
  training loss:		0.489763
  validation loss:		0.507349
  validation accuracy:		82.83 %
Epoch 881 of 2000 took 0.096s
  training loss:		0.491714
  validation loss:		0.504125
  validation accuracy:		82.39 %
Epoch 882 of 2000 took 0.097s
  training loss:		0.480456
  validation loss:		0.495743
  validation accuracy:		83.59 %
Epoch 883 of 2000 took 0.096s
  training loss:		0.483536
  validation loss:		0.527792
  validation accuracy:		81.96 %
Epoch 884 of 2000 took 0.097s
  training loss:		0.483438
  validation loss:		0.507541
  validation accuracy:		82.50 %
Epoch 885 of 2000 took 0.096s
  training loss:		0.489372
  validation loss:		0.519937
  validation accuracy:		82.39 %
Epoch 886 of 2000 took 0.096s
  training loss:		0.484326
  validation loss:		0.508554
  validation accuracy:		82.72 %
Epoch 887 of 2000 took 0.096s
  training loss:		0.485813
  validation loss:		0.521193
  validation accuracy:		83.15 %
Epoch 888 of 2000 took 0.096s
  training loss:		0.483783
  validation loss:		0.535354
  validation accuracy:		82.39 %
Epoch 889 of 2000 took 0.096s
  training loss:		0.477324
  validation loss:		0.510438
  validation accuracy:		82.61 %
Epoch 890 of 2000 took 0.097s
  training loss:		0.475259
  validation loss:		0.492198
  validation accuracy:		82.93 %
Epoch 891 of 2000 took 0.096s
  training loss:		0.472598
  validation loss:		0.498184
  validation accuracy:		82.72 %
Epoch 892 of 2000 took 0.097s
  training loss:		0.477315
  validation loss:		0.497742
  validation accuracy:		83.15 %
Epoch 893 of 2000 took 0.096s
  training loss:		0.470945
  validation loss:		0.497762
  validation accuracy:		82.72 %
Epoch 894 of 2000 took 0.096s
  training loss:		0.474133
  validation loss:		0.488404
  validation accuracy:		83.37 %
Epoch 895 of 2000 took 0.096s
  training loss:		0.471011
  validation loss:		0.492139
  validation accuracy:		82.83 %
Epoch 896 of 2000 took 0.096s
  training loss:		0.467904
  validation loss:		0.474814
  validation accuracy:		83.04 %
Epoch 897 of 2000 took 0.097s
  training loss:		0.469141
  validation loss:		0.473009
  validation accuracy:		84.13 %
Epoch 898 of 2000 took 0.096s
  training loss:		0.455969
  validation loss:		0.472281
  validation accuracy:		84.13 %
Epoch 899 of 2000 took 0.097s
  training loss:		0.462562
  validation loss:		0.469682
  validation accuracy:		84.35 %
Epoch 900 of 2000 took 0.096s
  training loss:		0.459715
  validation loss:		0.478345
  validation accuracy:		83.48 %
Epoch 901 of 2000 took 0.096s
  training loss:		0.454770
  validation loss:		0.478238
  validation accuracy:		83.37 %
Epoch 902 of 2000 took 0.096s
  training loss:		0.462322
  validation loss:		0.467501
  validation accuracy:		83.70 %
Epoch 903 of 2000 took 0.097s
  training loss:		0.460344
  validation loss:		0.472814
  validation accuracy:		84.02 %
Epoch 904 of 2000 took 0.096s
  training loss:		0.445936
  validation loss:		0.467009
  validation accuracy:		84.24 %
Epoch 905 of 2000 took 0.096s
  training loss:		0.459858
  validation loss:		0.477460
  validation accuracy:		84.02 %
Epoch 906 of 2000 took 0.096s
  training loss:		0.452562
  validation loss:		0.470557
  validation accuracy:		84.46 %
Epoch 907 of 2000 took 0.098s
  training loss:		0.449401
  validation loss:		0.470878
  validation accuracy:		83.80 %
Epoch 908 of 2000 took 0.103s
  training loss:		0.448466
  validation loss:		0.472282
  validation accuracy:		83.80 %
Epoch 909 of 2000 took 0.103s
  training loss:		0.446032
  validation loss:		0.474946
  validation accuracy:		83.70 %
Epoch 910 of 2000 took 0.100s
  training loss:		0.446443
  validation loss:		0.458504
  validation accuracy:		84.13 %
Epoch 911 of 2000 took 0.100s
  training loss:		0.443485
  validation loss:		0.482540
  validation accuracy:		83.91 %
Epoch 912 of 2000 took 0.100s
  training loss:		0.451356
  validation loss:		0.460744
  validation accuracy:		84.46 %
Epoch 913 of 2000 took 0.100s
  training loss:		0.443734
  validation loss:		0.455002
  validation accuracy:		84.57 %
Epoch 914 of 2000 took 0.099s
  training loss:		0.446256
  validation loss:		0.462467
  validation accuracy:		84.13 %
Epoch 915 of 2000 took 0.097s
  training loss:		0.433955
  validation loss:		0.452813
  validation accuracy:		84.89 %
Epoch 916 of 2000 took 0.096s
  training loss:		0.432789
  validation loss:		0.442293
  validation accuracy:		85.54 %
Epoch 917 of 2000 took 0.097s
  training loss:		0.422763
  validation loss:		0.462829
  validation accuracy:		84.35 %
Epoch 918 of 2000 took 0.097s
  training loss:		0.434204
  validation loss:		0.451133
  validation accuracy:		85.00 %
Epoch 919 of 2000 took 0.097s
  training loss:		0.434836
  validation loss:		0.446099
  validation accuracy:		85.22 %
Epoch 920 of 2000 took 0.097s
  training loss:		0.439701
  validation loss:		0.460696
  validation accuracy:		84.35 %
Epoch 921 of 2000 took 0.096s
  training loss:		0.440865
  validation loss:		0.458500
  validation accuracy:		84.57 %
Epoch 922 of 2000 took 0.096s
  training loss:		0.436912
  validation loss:		0.448638
  validation accuracy:		85.22 %
Epoch 923 of 2000 took 0.097s
  training loss:		0.427572
  validation loss:		0.458015
  validation accuracy:		83.91 %
Epoch 924 of 2000 took 0.096s
  training loss:		0.416602
  validation loss:		0.443565
  validation accuracy:		85.54 %
Epoch 925 of 2000 took 0.096s
  training loss:		0.425319
  validation loss:		0.466294
  validation accuracy:		84.35 %
Epoch 926 of 2000 took 0.096s
  training loss:		0.423943
  validation loss:		0.444687
  validation accuracy:		85.00 %
Epoch 927 of 2000 took 0.097s
  training loss:		0.428402
  validation loss:		0.431598
  validation accuracy:		85.11 %
Epoch 928 of 2000 took 0.097s
  training loss:		0.422072
  validation loss:		0.426630
  validation accuracy:		85.11 %
Epoch 929 of 2000 took 0.097s
  training loss:		0.426163
  validation loss:		0.430878
  validation accuracy:		85.43 %
Epoch 930 of 2000 took 0.097s
  training loss:		0.422404
  validation loss:		0.435918
  validation accuracy:		85.11 %
Epoch 931 of 2000 took 0.096s
  training loss:		0.422811
  validation loss:		0.428018
  validation accuracy:		85.33 %
Epoch 932 of 2000 took 0.096s
  training loss:		0.418907
  validation loss:		0.428790
  validation accuracy:		85.54 %
Epoch 933 of 2000 took 0.096s
  training loss:		0.423856
  validation loss:		0.432419
  validation accuracy:		85.22 %
Epoch 934 of 2000 took 0.096s
  training loss:		0.407646
  validation loss:		0.434813
  validation accuracy:		85.65 %
Epoch 935 of 2000 took 0.096s
  training loss:		0.421273
  validation loss:		0.443848
  validation accuracy:		85.11 %
Epoch 936 of 2000 took 0.096s
  training loss:		0.419680
  validation loss:		0.439368
  validation accuracy:		85.22 %
Epoch 937 of 2000 took 0.096s
  training loss:		0.412846
  validation loss:		0.427846
  validation accuracy:		85.22 %
Epoch 938 of 2000 took 0.096s
  training loss:		0.415547
  validation loss:		0.437123
  validation accuracy:		84.89 %
Epoch 939 of 2000 took 0.096s
  training loss:		0.410286
  validation loss:		0.431983
  validation accuracy:		85.11 %
Epoch 940 of 2000 took 0.097s
  training loss:		0.409457
  validation loss:		0.421352
  validation accuracy:		85.43 %
Epoch 941 of 2000 took 0.096s
  training loss:		0.408653
  validation loss:		0.417791
  validation accuracy:		85.11 %
Epoch 942 of 2000 took 0.097s
  training loss:		0.411324
  validation loss:		0.417293
  validation accuracy:		85.76 %
Epoch 943 of 2000 took 0.096s
  training loss:		0.406724
  validation loss:		0.418098
  validation accuracy:		85.87 %
Epoch 944 of 2000 took 0.097s
  training loss:		0.409223
  validation loss:		0.420259
  validation accuracy:		85.43 %
Epoch 945 of 2000 took 0.096s
  training loss:		0.403547
  validation loss:		0.437684
  validation accuracy:		84.78 %
Epoch 946 of 2000 took 0.096s
  training loss:		0.412004
  validation loss:		0.409573
  validation accuracy:		85.87 %
Epoch 947 of 2000 took 0.096s
  training loss:		0.402043
  validation loss:		0.412685
  validation accuracy:		85.54 %
Epoch 948 of 2000 took 0.096s
  training loss:		0.402997
  validation loss:		0.421168
  validation accuracy:		85.65 %
Epoch 949 of 2000 took 0.096s
  training loss:		0.402044
  validation loss:		0.435048
  validation accuracy:		85.76 %
Epoch 950 of 2000 took 0.096s
  training loss:		0.402539
  validation loss:		0.415966
  validation accuracy:		85.76 %
Epoch 951 of 2000 took 0.096s
  training loss:		0.397051
  validation loss:		0.411753
  validation accuracy:		85.54 %
Epoch 952 of 2000 took 0.097s
  training loss:		0.404906
  validation loss:		0.410512
  validation accuracy:		85.98 %
Epoch 953 of 2000 took 0.096s
  training loss:		0.403656
  validation loss:		0.404925
  validation accuracy:		85.54 %
Epoch 954 of 2000 took 0.096s
  training loss:		0.392342
  validation loss:		0.431365
  validation accuracy:		84.67 %
Epoch 955 of 2000 took 0.099s
  training loss:		0.400940
  validation loss:		0.423251
  validation accuracy:		85.33 %
Epoch 956 of 2000 took 0.097s
  training loss:		0.397989
  validation loss:		0.418724
  validation accuracy:		85.87 %
Epoch 957 of 2000 took 0.096s
  training loss:		0.387257
  validation loss:		0.428726
  validation accuracy:		85.65 %
Epoch 958 of 2000 took 0.097s
  training loss:		0.394333
  validation loss:		0.407178
  validation accuracy:		85.65 %
Epoch 959 of 2000 took 0.097s
  training loss:		0.389993
  validation loss:		0.407860
  validation accuracy:		86.20 %
Epoch 960 of 2000 took 0.097s
  training loss:		0.391489
  validation loss:		0.410135
  validation accuracy:		85.54 %
Epoch 961 of 2000 took 0.096s
  training loss:		0.393680
  validation loss:		0.408421
  validation accuracy:		85.76 %
Epoch 962 of 2000 took 0.096s
  training loss:		0.383289
  validation loss:		0.416859
  validation accuracy:		85.76 %
Epoch 963 of 2000 took 0.096s
  training loss:		0.384714
  validation loss:		0.403765
  validation accuracy:		85.87 %
Epoch 964 of 2000 took 0.096s
  training loss:		0.381821
  validation loss:		0.405781
  validation accuracy:		86.20 %
Epoch 965 of 2000 took 0.097s
  training loss:		0.387833
  validation loss:		0.417218
  validation accuracy:		85.43 %
Epoch 966 of 2000 took 0.096s
  training loss:		0.385885
  validation loss:		0.427633
  validation accuracy:		84.78 %
Epoch 967 of 2000 took 0.096s
  training loss:		0.384671
  validation loss:		0.407181
  validation accuracy:		86.20 %
Epoch 968 of 2000 took 0.096s
  training loss:		0.387898
  validation loss:		0.420153
  validation accuracy:		86.20 %
Epoch 969 of 2000 took 0.096s
  training loss:		0.376952
  validation loss:		0.411139
  validation accuracy:		85.43 %
Epoch 970 of 2000 took 0.096s
  training loss:		0.384189
  validation loss:		0.401323
  validation accuracy:		85.76 %
Epoch 971 of 2000 took 0.096s
  training loss:		0.379327
  validation loss:		0.393927
  validation accuracy:		85.98 %
Epoch 972 of 2000 took 0.096s
  training loss:		0.380191
  validation loss:		0.393061
  validation accuracy:		85.98 %
Epoch 973 of 2000 took 0.096s
  training loss:		0.369365
  validation loss:		0.425171
  validation accuracy:		85.43 %
Epoch 974 of 2000 took 0.096s
  training loss:		0.376097
  validation loss:		0.429719
  validation accuracy:		85.22 %
Epoch 975 of 2000 took 0.096s
  training loss:		0.372080
  validation loss:		0.405006
  validation accuracy:		86.09 %
Epoch 976 of 2000 took 0.096s
  training loss:		0.370811
  validation loss:		0.412701
  validation accuracy:		85.98 %
Epoch 977 of 2000 took 0.096s
  training loss:		0.377959
  validation loss:		0.404801
  validation accuracy:		86.41 %
Epoch 978 of 2000 took 0.096s
  training loss:		0.375091
  validation loss:		0.401021
  validation accuracy:		86.41 %
Epoch 979 of 2000 took 0.096s
  training loss:		0.368993
  validation loss:		0.407640
  validation accuracy:		86.41 %
Epoch 980 of 2000 took 0.097s
  training loss:		0.370622
  validation loss:		0.388766
  validation accuracy:		85.87 %
Epoch 981 of 2000 took 0.097s
  training loss:		0.370326
  validation loss:		0.389033
  validation accuracy:		86.20 %
Epoch 982 of 2000 took 0.096s
  training loss:		0.369748
  validation loss:		0.404435
  validation accuracy:		86.09 %
Epoch 983 of 2000 took 0.096s
  training loss:		0.372431
  validation loss:		0.383924
  validation accuracy:		86.63 %
Epoch 984 of 2000 took 0.096s
  training loss:		0.367985
  validation loss:		0.389528
  validation accuracy:		86.09 %
Epoch 985 of 2000 took 0.097s
  training loss:		0.367159
  validation loss:		0.390147
  validation accuracy:		86.85 %
Epoch 986 of 2000 took 0.096s
  training loss:		0.368776
  validation loss:		0.399031
  validation accuracy:		85.87 %
Epoch 987 of 2000 took 0.096s
  training loss:		0.372068
  validation loss:		0.406203
  validation accuracy:		86.30 %
Epoch 988 of 2000 took 0.096s
  training loss:		0.370521
  validation loss:		0.393391
  validation accuracy:		86.20 %
Epoch 989 of 2000 took 0.097s
  training loss:		0.368499
  validation loss:		0.387507
  validation accuracy:		86.09 %
Epoch 990 of 2000 took 0.096s
  training loss:		0.370166
  validation loss:		0.408447
  validation accuracy:		86.41 %
Epoch 991 of 2000 took 0.097s
  training loss:		0.367897
  validation loss:		0.383270
  validation accuracy:		86.74 %
Epoch 992 of 2000 took 0.097s
  training loss:		0.369835
  validation loss:		0.379656
  validation accuracy:		86.85 %
Epoch 993 of 2000 took 0.096s
  training loss:		0.366422
  validation loss:		0.378732
  validation accuracy:		87.39 %
Epoch 994 of 2000 took 0.096s
  training loss:		0.364346
  validation loss:		0.386150
  validation accuracy:		85.98 %
Epoch 995 of 2000 took 0.096s
  training loss:		0.363946
  validation loss:		0.397654
  validation accuracy:		86.20 %
Epoch 996 of 2000 took 0.096s
  training loss:		0.358027
  validation loss:		0.407517
  validation accuracy:		86.20 %
Epoch 997 of 2000 took 0.097s
  training loss:		0.350322
  validation loss:		0.394402
  validation accuracy:		85.98 %
Epoch 998 of 2000 took 0.097s
  training loss:		0.366248
  validation loss:		0.380736
  validation accuracy:		86.85 %
Epoch 999 of 2000 took 0.096s
  training loss:		0.363305
  validation loss:		0.382049
  validation accuracy:		86.63 %
Epoch 1000 of 2000 took 0.097s
  training loss:		0.361815
  validation loss:		0.385830
  validation accuracy:		85.98 %
Epoch 1001 of 2000 took 0.097s
  training loss:		0.357511
  validation loss:		0.402658
  validation accuracy:		86.30 %
Epoch 1002 of 2000 took 0.097s
  training loss:		0.361894
  validation loss:		0.385302
  validation accuracy:		86.30 %
Epoch 1003 of 2000 took 0.096s
  training loss:		0.362025
  validation loss:		0.378840
  validation accuracy:		86.85 %
Epoch 1004 of 2000 took 0.096s
  training loss:		0.356633
  validation loss:		0.382968
  validation accuracy:		86.30 %
Epoch 1005 of 2000 took 0.096s
  training loss:		0.354894
  validation loss:		0.393259
  validation accuracy:		86.20 %
Epoch 1006 of 2000 took 0.097s
  training loss:		0.348382
  validation loss:		0.387102
  validation accuracy:		86.09 %
Epoch 1007 of 2000 took 0.096s
  training loss:		0.356311
  validation loss:		0.383560
  validation accuracy:		87.17 %
Epoch 1008 of 2000 took 0.096s
  training loss:		0.358939
  validation loss:		0.374061
  validation accuracy:		87.07 %
Epoch 1009 of 2000 took 0.096s
  training loss:		0.355660
  validation loss:		0.399792
  validation accuracy:		86.52 %
Epoch 1010 of 2000 took 0.096s
  training loss:		0.352078
  validation loss:		0.373754
  validation accuracy:		87.07 %
Epoch 1011 of 2000 took 0.096s
  training loss:		0.356401
  validation loss:		0.389840
  validation accuracy:		86.85 %
Epoch 1012 of 2000 took 0.096s
  training loss:		0.360177
  validation loss:		0.375233
  validation accuracy:		87.17 %
Epoch 1013 of 2000 took 0.096s
  training loss:		0.358291
  validation loss:		0.374900
  validation accuracy:		87.39 %
Epoch 1014 of 2000 took 0.096s
  training loss:		0.359459
  validation loss:		0.382080
  validation accuracy:		86.96 %
Epoch 1015 of 2000 took 0.096s
  training loss:		0.361299
  validation loss:		0.381476
  validation accuracy:		86.85 %
Epoch 1016 of 2000 took 0.097s
  training loss:		0.343183
  validation loss:		0.383881
  validation accuracy:		86.63 %
Epoch 1017 of 2000 took 0.096s
  training loss:		0.354767
  validation loss:		0.395908
  validation accuracy:		86.85 %
Epoch 1018 of 2000 took 0.096s
  training loss:		0.356145
  validation loss:		0.385855
  validation accuracy:		86.41 %
Epoch 1019 of 2000 took 0.096s
  training loss:		0.351674
  validation loss:		0.378899
  validation accuracy:		86.63 %
Epoch 1020 of 2000 took 0.096s
  training loss:		0.351155
  validation loss:		0.394178
  validation accuracy:		86.52 %
Epoch 1021 of 2000 took 0.097s
  training loss:		0.353159
  validation loss:		0.390012
  validation accuracy:		86.41 %
Epoch 1022 of 2000 took 0.097s
  training loss:		0.352986
  validation loss:		0.386383
  validation accuracy:		86.52 %
Epoch 1023 of 2000 took 0.097s
  training loss:		0.356849
  validation loss:		0.371999
  validation accuracy:		87.50 %
Epoch 1024 of 2000 took 0.096s
  training loss:		0.343904
  validation loss:		0.368590
  validation accuracy:		87.61 %
Epoch 1025 of 2000 took 0.096s
  training loss:		0.351195
  validation loss:		0.369897
  validation accuracy:		87.17 %
Epoch 1026 of 2000 took 0.096s
  training loss:		0.348176
  validation loss:		0.383885
  validation accuracy:		86.74 %
Epoch 1027 of 2000 took 0.097s
  training loss:		0.352033
  validation loss:		0.386711
  validation accuracy:		86.85 %
Epoch 1028 of 2000 took 0.096s
  training loss:		0.352014
  validation loss:		0.366205
  validation accuracy:		87.72 %
Epoch 1029 of 2000 took 0.096s
  training loss:		0.350294
  validation loss:		0.379523
  validation accuracy:		86.41 %
Epoch 1030 of 2000 took 0.096s
  training loss:		0.345293
  validation loss:		0.371523
  validation accuracy:		87.17 %
Epoch 1031 of 2000 took 0.096s
  training loss:		0.343825
  validation loss:		0.374600
  validation accuracy:		87.07 %
Epoch 1032 of 2000 took 0.096s
  training loss:		0.344397
  validation loss:		0.385501
  validation accuracy:		86.52 %
Epoch 1033 of 2000 took 0.096s
  training loss:		0.348427
  validation loss:		0.382997
  validation accuracy:		87.39 %
Epoch 1034 of 2000 took 0.096s
  training loss:		0.336008
  validation loss:		0.375147
  validation accuracy:		87.39 %
Epoch 1035 of 2000 took 0.096s
  training loss:		0.340336
  validation loss:		0.390108
  validation accuracy:		86.85 %
Epoch 1036 of 2000 took 0.096s
  training loss:		0.345081
  validation loss:		0.397154
  validation accuracy:		85.98 %
Epoch 1037 of 2000 took 0.096s
  training loss:		0.337543
  validation loss:		0.375368
  validation accuracy:		86.96 %
Epoch 1038 of 2000 took 0.096s
  training loss:		0.345011
  validation loss:		0.390395
  validation accuracy:		86.52 %
Epoch 1039 of 2000 took 0.097s
  training loss:		0.342370
  validation loss:		0.379709
  validation accuracy:		86.52 %
Epoch 1040 of 2000 took 0.096s
  training loss:		0.344832
  validation loss:		0.371298
  validation accuracy:		87.39 %
Epoch 1041 of 2000 took 0.097s
  training loss:		0.340323
  validation loss:		0.377970
  validation accuracy:		86.85 %
Epoch 1042 of 2000 took 0.099s
  training loss:		0.341751
  validation loss:		0.385335
  validation accuracy:		86.41 %
Epoch 1043 of 2000 took 0.100s
  training loss:		0.341826
  validation loss:		0.374716
  validation accuracy:		87.28 %
Epoch 1044 of 2000 took 0.099s
  training loss:		0.340513
  validation loss:		0.406978
  validation accuracy:		86.52 %
Epoch 1045 of 2000 took 0.099s
  training loss:		0.351980
  validation loss:		0.375087
  validation accuracy:		87.07 %
Epoch 1046 of 2000 took 0.099s
  training loss:		0.341054
  validation loss:		0.379248
  validation accuracy:		86.96 %
Epoch 1047 of 2000 took 0.100s
  training loss:		0.344636
  validation loss:		0.387031
  validation accuracy:		86.74 %
Epoch 1048 of 2000 took 0.099s
  training loss:		0.338964
  validation loss:		0.373892
  validation accuracy:		87.28 %
Epoch 1049 of 2000 took 0.099s
  training loss:		0.341611
  validation loss:		0.365822
  validation accuracy:		87.50 %
Epoch 1050 of 2000 took 0.099s
  training loss:		0.345245
  validation loss:		0.375765
  validation accuracy:		87.28 %
Epoch 1051 of 2000 took 0.100s
  training loss:		0.340457
  validation loss:		0.391745
  validation accuracy:		86.20 %
Epoch 1052 of 2000 took 0.100s
  training loss:		0.336495
  validation loss:		0.371659
  validation accuracy:		87.17 %
Epoch 1053 of 2000 took 0.100s
  training loss:		0.333215
  validation loss:		0.374032
  validation accuracy:		86.74 %
Epoch 1054 of 2000 took 0.100s
  training loss:		0.339374
  validation loss:		0.373524
  validation accuracy:		87.28 %
Epoch 1055 of 2000 took 0.099s
  training loss:		0.340843
  validation loss:		0.366767
  validation accuracy:		87.61 %
Epoch 1056 of 2000 took 0.099s
  training loss:		0.336188
  validation loss:		0.373971
  validation accuracy:		87.61 %
Epoch 1057 of 2000 took 0.100s
  training loss:		0.341561
  validation loss:		0.369738
  validation accuracy:		87.93 %
Epoch 1058 of 2000 took 0.099s
  training loss:		0.333994
  validation loss:		0.378847
  validation accuracy:		87.39 %
Epoch 1059 of 2000 took 0.100s
  training loss:		0.335730
  validation loss:		0.387198
  validation accuracy:		86.63 %
Epoch 1060 of 2000 took 0.099s
  training loss:		0.337320
  validation loss:		0.375903
  validation accuracy:		87.28 %
Epoch 1061 of 2000 took 0.100s
  training loss:		0.338058
  validation loss:		0.381943
  validation accuracy:		86.96 %
Epoch 1062 of 2000 took 0.100s
  training loss:		0.334640
  validation loss:		0.385310
  validation accuracy:		86.41 %
Epoch 1063 of 2000 took 0.100s
  training loss:		0.338291
  validation loss:		0.380026
  validation accuracy:		87.39 %
Epoch 1064 of 2000 took 0.099s
  training loss:		0.337225
  validation loss:		0.377818
  validation accuracy:		86.85 %
Epoch 1065 of 2000 took 0.100s
  training loss:		0.334375
  validation loss:		0.370324
  validation accuracy:		87.50 %
Epoch 1066 of 2000 took 0.099s
  training loss:		0.342657
  validation loss:		0.375593
  validation accuracy:		87.61 %
Epoch 1067 of 2000 took 0.099s
  training loss:		0.335366
  validation loss:		0.369881
  validation accuracy:		87.61 %
Epoch 1068 of 2000 took 0.099s
  training loss:		0.334953
  validation loss:		0.372569
  validation accuracy:		87.17 %
Epoch 1069 of 2000 took 0.100s
  training loss:		0.336329
  validation loss:		0.381607
  validation accuracy:		86.41 %
Epoch 1070 of 2000 took 0.099s
  training loss:		0.328943
  validation loss:		0.380616
  validation accuracy:		86.85 %
Epoch 1071 of 2000 took 0.099s
  training loss:		0.332154
  validation loss:		0.370892
  validation accuracy:		87.50 %
Epoch 1072 of 2000 took 0.099s
  training loss:		0.323184
  validation loss:		0.382902
  validation accuracy:		86.96 %
Epoch 1073 of 2000 took 0.099s
  training loss:		0.327946
  validation loss:		0.365723
  validation accuracy:		87.93 %
Epoch 1074 of 2000 took 0.096s
  training loss:		0.329082
  validation loss:		0.366926
  validation accuracy:		87.83 %
Epoch 1075 of 2000 took 0.096s
  training loss:		0.326689
  validation loss:		0.378009
  validation accuracy:		87.50 %
Epoch 1076 of 2000 took 0.096s
  training loss:		0.331934
  validation loss:		0.377046
  validation accuracy:		87.61 %
Epoch 1077 of 2000 took 0.096s
  training loss:		0.332502
  validation loss:		0.367773
  validation accuracy:		87.50 %
Epoch 1078 of 2000 took 0.096s
  training loss:		0.332381
  validation loss:		0.386136
  validation accuracy:		86.63 %
Epoch 1079 of 2000 took 0.097s
  training loss:		0.333056
  validation loss:		0.377850
  validation accuracy:		87.28 %
Epoch 1080 of 2000 took 0.096s
  training loss:		0.323770
  validation loss:		0.365416
  validation accuracy:		87.83 %
Epoch 1081 of 2000 took 0.096s
  training loss:		0.331549
  validation loss:		0.363537
  validation accuracy:		87.72 %
Epoch 1082 of 2000 took 0.097s
  training loss:		0.324330
  validation loss:		0.366327
  validation accuracy:		87.39 %
Epoch 1083 of 2000 took 0.098s
  training loss:		0.329399
  validation loss:		0.373688
  validation accuracy:		87.39 %
Epoch 1084 of 2000 took 0.097s
  training loss:		0.341730
  validation loss:		0.373394
  validation accuracy:		87.50 %
Epoch 1085 of 2000 took 0.096s
  training loss:		0.331175
  validation loss:		0.372433
  validation accuracy:		87.39 %
Epoch 1086 of 2000 took 0.097s
  training loss:		0.330685
  validation loss:		0.388870
  validation accuracy:		86.52 %
Epoch 1087 of 2000 took 0.096s
  training loss:		0.330124
  validation loss:		0.378864
  validation accuracy:		87.17 %
Epoch 1088 of 2000 took 0.096s
  training loss:		0.325041
  validation loss:		0.380152
  validation accuracy:		86.85 %
Epoch 1089 of 2000 took 0.096s
  training loss:		0.328691
  validation loss:		0.383560
  validation accuracy:		86.96 %
Epoch 1090 of 2000 took 0.096s
  training loss:		0.326772
  validation loss:		0.383692
  validation accuracy:		87.07 %
Epoch 1091 of 2000 took 0.096s
  training loss:		0.322534
  validation loss:		0.369733
  validation accuracy:		87.61 %
Epoch 1092 of 2000 took 0.096s
  training loss:		0.325088
  validation loss:		0.374476
  validation accuracy:		87.61 %
Epoch 1093 of 2000 took 0.096s
  training loss:		0.327878
  validation loss:		0.391433
  validation accuracy:		86.74 %
Epoch 1094 of 2000 took 0.096s
  training loss:		0.325519
  validation loss:		0.372025
  validation accuracy:		87.07 %
Epoch 1095 of 2000 took 0.096s
  training loss:		0.329844
  validation loss:		0.368187
  validation accuracy:		87.50 %
Epoch 1096 of 2000 took 0.096s
  training loss:		0.324814
  validation loss:		0.374684
  validation accuracy:		87.61 %
Epoch 1097 of 2000 took 0.096s
  training loss:		0.326009
  validation loss:		0.358454
  validation accuracy:		87.83 %
Epoch 1098 of 2000 took 0.097s
  training loss:		0.327139
  validation loss:		0.364167
  validation accuracy:		87.93 %
Epoch 1099 of 2000 took 0.096s
  training loss:		0.326542
  validation loss:		0.369586
  validation accuracy:		86.96 %
Epoch 1100 of 2000 took 0.097s
  training loss:		0.327496
  validation loss:		0.382154
  validation accuracy:		87.17 %
Epoch 1101 of 2000 took 0.096s
  training loss:		0.328120
  validation loss:		0.381243
  validation accuracy:		87.28 %
Epoch 1102 of 2000 took 0.096s
  training loss:		0.328738
  validation loss:		0.373450
  validation accuracy:		87.50 %
Epoch 1103 of 2000 took 0.096s
  training loss:		0.330181
  validation loss:		0.380819
  validation accuracy:		87.07 %
Epoch 1104 of 2000 took 0.096s
  training loss:		0.323875
  validation loss:		0.377107
  validation accuracy:		87.39 %
Epoch 1105 of 2000 took 0.097s
  training loss:		0.329928
  validation loss:		0.371889
  validation accuracy:		87.83 %
Epoch 1106 of 2000 took 0.096s
  training loss:		0.325440
  validation loss:		0.387155
  validation accuracy:		87.17 %
Epoch 1107 of 2000 took 0.096s
  training loss:		0.329117
  validation loss:		0.387185
  validation accuracy:		86.63 %
Epoch 1108 of 2000 took 0.096s
  training loss:		0.326928
  validation loss:		0.368624
  validation accuracy:		87.61 %
Epoch 1109 of 2000 took 0.097s
  training loss:		0.331338
  validation loss:		0.383385
  validation accuracy:		86.96 %
Epoch 1110 of 2000 took 0.096s
  training loss:		0.325858
  validation loss:		0.370867
  validation accuracy:		87.72 %
Epoch 1111 of 2000 took 0.096s
  training loss:		0.317470
  validation loss:		0.372355
  validation accuracy:		87.07 %
Epoch 1112 of 2000 took 0.096s
  training loss:		0.323387
  validation loss:		0.378085
  validation accuracy:		87.28 %
Epoch 1113 of 2000 took 0.097s
  training loss:		0.323612
  validation loss:		0.373829
  validation accuracy:		87.61 %
Epoch 1114 of 2000 took 0.096s
  training loss:		0.325759
  validation loss:		0.368862
  validation accuracy:		87.61 %
Epoch 1115 of 2000 took 0.097s
  training loss:		0.322799
  validation loss:		0.373238
  validation accuracy:		87.61 %
Epoch 1116 of 2000 took 0.096s
  training loss:		0.322232
  validation loss:		0.368516
  validation accuracy:		87.28 %
Epoch 1117 of 2000 took 0.096s
  training loss:		0.321539
  validation loss:		0.358702
  validation accuracy:		88.26 %
Epoch 1118 of 2000 took 0.096s
  training loss:		0.321632
  validation loss:		0.368061
  validation accuracy:		87.28 %
Epoch 1119 of 2000 took 0.097s
  training loss:		0.318132
  validation loss:		0.368098
  validation accuracy:		87.17 %
Epoch 1120 of 2000 took 0.097s
  training loss:		0.327288
  validation loss:		0.374163
  validation accuracy:		87.72 %
Epoch 1121 of 2000 took 0.097s
  training loss:		0.322915
  validation loss:		0.368377
  validation accuracy:		87.39 %
Epoch 1122 of 2000 took 0.096s
  training loss:		0.322794
  validation loss:		0.374904
  validation accuracy:		87.61 %
Epoch 1123 of 2000 took 0.097s
  training loss:		0.326298
  validation loss:		0.367826
  validation accuracy:		87.39 %
Epoch 1124 of 2000 took 0.097s
  training loss:		0.329636
  validation loss:		0.369340
  validation accuracy:		87.72 %
Epoch 1125 of 2000 took 0.097s
  training loss:		0.323425
  validation loss:		0.370176
  validation accuracy:		87.83 %
Epoch 1126 of 2000 took 0.096s
  training loss:		0.324268
  validation loss:		0.379621
  validation accuracy:		87.17 %
Epoch 1127 of 2000 took 0.097s
  training loss:		0.321497
  validation loss:		0.369795
  validation accuracy:		87.83 %
Epoch 1128 of 2000 took 0.096s
  training loss:		0.321752
  validation loss:		0.355716
  validation accuracy:		87.83 %
Epoch 1129 of 2000 took 0.096s
  training loss:		0.323145
  validation loss:		0.359000
  validation accuracy:		87.39 %
Epoch 1130 of 2000 took 0.099s
  training loss:		0.329197
  validation loss:		0.366943
  validation accuracy:		87.72 %
Epoch 1131 of 2000 took 0.097s
  training loss:		0.326897
  validation loss:		0.358938
  validation accuracy:		87.83 %
Epoch 1132 of 2000 took 0.096s
  training loss:		0.323996
  validation loss:		0.391429
  validation accuracy:		86.74 %
Epoch 1133 of 2000 took 0.097s
  training loss:		0.326860
  validation loss:		0.371144
  validation accuracy:		87.39 %
Epoch 1134 of 2000 took 0.097s
  training loss:		0.318311
  validation loss:		0.377527
  validation accuracy:		87.61 %
Epoch 1135 of 2000 took 0.096s
  training loss:		0.322717
  validation loss:		0.373734
  validation accuracy:		88.04 %
Epoch 1136 of 2000 took 0.097s
  training loss:		0.319728
  validation loss:		0.383406
  validation accuracy:		87.17 %
Epoch 1137 of 2000 took 0.096s
  training loss:		0.325503
  validation loss:		0.360365
  validation accuracy:		87.72 %
Epoch 1138 of 2000 took 0.096s
  training loss:		0.322048
  validation loss:		0.371227
  validation accuracy:		87.17 %
Epoch 1139 of 2000 took 0.096s
  training loss:		0.322536
  validation loss:		0.372022
  validation accuracy:		88.04 %
Epoch 1140 of 2000 took 0.096s
  training loss:		0.314946
  validation loss:		0.375990
  validation accuracy:		87.07 %
Epoch 1141 of 2000 took 0.096s
  training loss:		0.327375
  validation loss:		0.368412
  validation accuracy:		87.50 %
Epoch 1142 of 2000 took 0.096s
  training loss:		0.325236
  validation loss:		0.370452
  validation accuracy:		87.61 %
Epoch 1143 of 2000 took 0.096s
  training loss:		0.317783
  validation loss:		0.383985
  validation accuracy:		86.20 %
Epoch 1144 of 2000 took 0.096s
  training loss:		0.329148
  validation loss:		0.372796
  validation accuracy:		87.17 %
Epoch 1145 of 2000 took 0.097s
  training loss:		0.322972
  validation loss:		0.386618
  validation accuracy:		85.87 %
Epoch 1146 of 2000 took 0.097s
  training loss:		0.326202
  validation loss:		0.357177
  validation accuracy:		88.04 %
Epoch 1147 of 2000 took 0.096s
  training loss:		0.324430
  validation loss:		0.378237
  validation accuracy:		87.39 %
Epoch 1148 of 2000 took 0.096s
  training loss:		0.324364
  validation loss:		0.372158
  validation accuracy:		88.04 %
Epoch 1149 of 2000 took 0.096s
  training loss:		0.327431
  validation loss:		0.364105
  validation accuracy:		87.83 %
Epoch 1150 of 2000 took 0.097s
  training loss:		0.329010
  validation loss:		0.372970
  validation accuracy:		87.61 %
Epoch 1151 of 2000 took 0.096s
  training loss:		0.322658
  validation loss:		0.361398
  validation accuracy:		87.93 %
Epoch 1152 of 2000 took 0.097s
  training loss:		0.317728
  validation loss:		0.383751
  validation accuracy:		87.39 %
Epoch 1153 of 2000 took 0.096s
  training loss:		0.322085
  validation loss:		0.379139
  validation accuracy:		86.96 %
Epoch 1154 of 2000 took 0.096s
  training loss:		0.323400
  validation loss:		0.385004
  validation accuracy:		86.96 %
Epoch 1155 of 2000 took 0.096s
  training loss:		0.329778
  validation loss:		0.380929
  validation accuracy:		87.39 %
Epoch 1156 of 2000 took 0.096s
  training loss:		0.316130
  validation loss:		0.375318
  validation accuracy:		87.39 %
Epoch 1157 of 2000 took 0.096s
  training loss:		0.319591
  validation loss:		0.359852
  validation accuracy:		87.72 %
Epoch 1158 of 2000 took 0.096s
  training loss:		0.319446
  validation loss:		0.371564
  validation accuracy:		87.50 %
Epoch 1159 of 2000 took 0.096s
  training loss:		0.314551
  validation loss:		0.384113
  validation accuracy:		86.96 %
Epoch 1160 of 2000 took 0.096s
  training loss:		0.323880
  validation loss:		0.368534
  validation accuracy:		87.93 %
Epoch 1161 of 2000 took 0.097s
  training loss:		0.315510
  validation loss:		0.369381
  validation accuracy:		87.17 %
Epoch 1162 of 2000 took 0.097s
  training loss:		0.315596
  validation loss:		0.368791
  validation accuracy:		87.83 %
Epoch 1163 of 2000 took 0.097s
  training loss:		0.322078
  validation loss:		0.367019
  validation accuracy:		87.39 %
Epoch 1164 of 2000 took 0.096s
  training loss:		0.323946
  validation loss:		0.368900
  validation accuracy:		87.50 %
Epoch 1165 of 2000 took 0.097s
  training loss:		0.307189
  validation loss:		0.365742
  validation accuracy:		87.61 %
Epoch 1166 of 2000 took 0.096s
  training loss:		0.313385
  validation loss:		0.368836
  validation accuracy:		87.50 %
Epoch 1167 of 2000 took 0.097s
  training loss:		0.321670
  validation loss:		0.364924
  validation accuracy:		88.26 %
Epoch 1168 of 2000 took 0.096s
  training loss:		0.316444
  validation loss:		0.365177
  validation accuracy:		87.61 %
Epoch 1169 of 2000 took 0.096s
  training loss:		0.318002
  validation loss:		0.389727
  validation accuracy:		86.96 %
Epoch 1170 of 2000 took 0.096s
  training loss:		0.320189
  validation loss:		0.358857
  validation accuracy:		88.04 %
Epoch 1171 of 2000 took 0.097s
  training loss:		0.318653
  validation loss:		0.394838
  validation accuracy:		86.52 %
Epoch 1172 of 2000 took 0.096s
  training loss:		0.318338
  validation loss:		0.374313
  validation accuracy:		87.39 %
Epoch 1173 of 2000 took 0.096s
  training loss:		0.320072
  validation loss:		0.373503
  validation accuracy:		87.61 %
Epoch 1174 of 2000 took 0.096s
  training loss:		0.320045
  validation loss:		0.367222
  validation accuracy:		87.50 %
Epoch 1175 of 2000 took 0.096s
  training loss:		0.316984
  validation loss:		0.368821
  validation accuracy:		87.72 %
Epoch 1176 of 2000 took 0.096s
  training loss:		0.321494
  validation loss:		0.372577
  validation accuracy:		87.61 %
Epoch 1177 of 2000 took 0.097s
  training loss:		0.315606
  validation loss:		0.373319
  validation accuracy:		87.17 %
Epoch 1178 of 2000 took 0.096s
  training loss:		0.318917
  validation loss:		0.368745
  validation accuracy:		87.07 %
Epoch 1179 of 2000 took 0.096s
  training loss:		0.316084
  validation loss:		0.367635
  validation accuracy:		87.72 %
Epoch 1180 of 2000 took 0.096s
  training loss:		0.317095
  validation loss:		0.382227
  validation accuracy:		87.17 %
Epoch 1181 of 2000 took 0.096s
  training loss:		0.320555
  validation loss:		0.365544
  validation accuracy:		88.15 %
Epoch 1182 of 2000 took 0.096s
  training loss:		0.312640
  validation loss:		0.372538
  validation accuracy:		87.17 %
Epoch 1183 of 2000 took 0.096s
  training loss:		0.317496
  validation loss:		0.360793
  validation accuracy:		87.93 %
Epoch 1184 of 2000 took 0.096s
  training loss:		0.319133
  validation loss:		0.365508
  validation accuracy:		87.93 %
Epoch 1185 of 2000 took 0.096s
  training loss:		0.323412
  validation loss:		0.366985
  validation accuracy:		87.93 %
Epoch 1186 of 2000 took 0.097s
  training loss:		0.312743
  validation loss:		0.383517
  validation accuracy:		87.61 %
Epoch 1187 of 2000 took 0.096s
  training loss:		0.319274
  validation loss:		0.373562
  validation accuracy:		87.72 %
Epoch 1188 of 2000 took 0.096s
  training loss:		0.315920
  validation loss:		0.374048
  validation accuracy:		87.17 %
Epoch 1189 of 2000 took 0.096s
  training loss:		0.317938
  validation loss:		0.369252
  validation accuracy:		87.28 %
Epoch 1190 of 2000 took 0.096s
  training loss:		0.321259
  validation loss:		0.380272
  validation accuracy:		86.52 %
Epoch 1191 of 2000 took 0.096s
  training loss:		0.318799
  validation loss:		0.361915
  validation accuracy:		87.93 %
Epoch 1192 of 2000 took 0.097s
  training loss:		0.323567
  validation loss:		0.389437
  validation accuracy:		86.96 %
Epoch 1193 of 2000 took 0.096s
  training loss:		0.313033
  validation loss:		0.360107
  validation accuracy:		87.93 %
Epoch 1194 of 2000 took 0.097s
  training loss:		0.316906
  validation loss:		0.378331
  validation accuracy:		87.17 %
Epoch 1195 of 2000 took 0.096s
  training loss:		0.313659
  validation loss:		0.374795
  validation accuracy:		87.72 %
Epoch 1196 of 2000 took 0.096s
  training loss:		0.315147
  validation loss:		0.376579
  validation accuracy:		87.72 %
Epoch 1197 of 2000 took 0.096s
  training loss:		0.318398
  validation loss:		0.367141
  validation accuracy:		87.72 %
Epoch 1198 of 2000 took 0.097s
  training loss:		0.323283
  validation loss:		0.362326
  validation accuracy:		87.72 %
Epoch 1199 of 2000 took 0.096s
  training loss:		0.311313
  validation loss:		0.369223
  validation accuracy:		87.50 %
Epoch 1200 of 2000 took 0.096s
  training loss:		0.317187
  validation loss:		0.374715
  validation accuracy:		87.50 %
Epoch 1201 of 2000 took 0.096s
  training loss:		0.320966
  validation loss:		0.376304
  validation accuracy:		87.50 %
Epoch 1202 of 2000 took 0.096s
  training loss:		0.311435
  validation loss:		0.386795
  validation accuracy:		87.61 %
Epoch 1203 of 2000 took 0.097s
  training loss:		0.310277
  validation loss:		0.379733
  validation accuracy:		87.39 %
Epoch 1204 of 2000 took 0.097s
  training loss:		0.313931
  validation loss:		0.374933
  validation accuracy:		87.61 %
Epoch 1205 of 2000 took 0.096s
  training loss:		0.312377
  validation loss:		0.397588
  validation accuracy:		86.96 %
Epoch 1206 of 2000 took 0.097s
  training loss:		0.311009
  validation loss:		0.366541
  validation accuracy:		88.04 %
Epoch 1207 of 2000 took 0.097s
  training loss:		0.316559
  validation loss:		0.365244
  validation accuracy:		87.72 %
Epoch 1208 of 2000 took 0.097s
  training loss:		0.310027
  validation loss:		0.375225
  validation accuracy:		87.50 %
Epoch 1209 of 2000 took 0.097s
  training loss:		0.309402
  validation loss:		0.370904
  validation accuracy:		87.61 %
Epoch 1210 of 2000 took 0.096s
  training loss:		0.321840
  validation loss:		0.371820
  validation accuracy:		87.83 %
Epoch 1211 of 2000 took 0.096s
  training loss:		0.317770
  validation loss:		0.370141
  validation accuracy:		87.93 %
Epoch 1212 of 2000 took 0.098s
  training loss:		0.309193
  validation loss:		0.375318
  validation accuracy:		87.61 %
Epoch 1213 of 2000 took 0.099s
  training loss:		0.314831
  validation loss:		0.367653
  validation accuracy:		88.04 %
Epoch 1214 of 2000 took 0.100s
  training loss:		0.307978
  validation loss:		0.370194
  validation accuracy:		88.15 %
Epoch 1215 of 2000 took 0.099s
  training loss:		0.313771
  validation loss:		0.368985
  validation accuracy:		87.93 %
Epoch 1216 of 2000 took 0.099s
  training loss:		0.314911
  validation loss:		0.367984
  validation accuracy:		87.72 %
Epoch 1217 of 2000 took 0.099s
  training loss:		0.315702
  validation loss:		0.383084
  validation accuracy:		87.17 %
Epoch 1218 of 2000 took 0.099s
  training loss:		0.310254
  validation loss:		0.372492
  validation accuracy:		87.93 %
Epoch 1219 of 2000 took 0.100s
  training loss:		0.315781
  validation loss:		0.366099
  validation accuracy:		88.04 %
Epoch 1220 of 2000 took 0.100s
  training loss:		0.317080
  validation loss:		0.370355
  validation accuracy:		87.07 %
Epoch 1221 of 2000 took 0.099s
  training loss:		0.321418
  validation loss:		0.375655
  validation accuracy:		87.07 %
Epoch 1222 of 2000 took 0.100s
  training loss:		0.315298
  validation loss:		0.361757
  validation accuracy:		87.61 %
Epoch 1223 of 2000 took 0.099s
  training loss:		0.306224
  validation loss:		0.379266
  validation accuracy:		87.39 %
Epoch 1224 of 2000 took 0.100s
  training loss:		0.314395
  validation loss:		0.365316
  validation accuracy:		88.15 %
Epoch 1225 of 2000 took 0.099s
  training loss:		0.311148
  validation loss:		0.364749
  validation accuracy:		87.93 %
Epoch 1226 of 2000 took 0.100s
  training loss:		0.320661
  validation loss:		0.378917
  validation accuracy:		87.17 %
Epoch 1227 of 2000 took 0.100s
  training loss:		0.323255
  validation loss:		0.363364
  validation accuracy:		87.72 %
Epoch 1228 of 2000 took 0.100s
  training loss:		0.312534
  validation loss:		0.360521
  validation accuracy:		88.15 %
Epoch 1229 of 2000 took 0.099s
  training loss:		0.315725
  validation loss:		0.369444
  validation accuracy:		87.93 %
Epoch 1230 of 2000 took 0.100s
  training loss:		0.313469
  validation loss:		0.373431
  validation accuracy:		87.50 %
Epoch 1231 of 2000 took 0.099s
  training loss:		0.313296
  validation loss:		0.364169
  validation accuracy:		88.04 %
Epoch 1232 of 2000 took 0.099s
  training loss:		0.314446
  validation loss:		0.365279
  validation accuracy:		88.04 %
Epoch 1233 of 2000 took 0.099s
  training loss:		0.309655
  validation loss:		0.356519
  validation accuracy:		88.04 %
Epoch 1234 of 2000 took 0.100s
  training loss:		0.319976
  validation loss:		0.361321
  validation accuracy:		87.61 %
Epoch 1235 of 2000 took 0.099s
  training loss:		0.314198
  validation loss:		0.368414
  validation accuracy:		87.93 %
Epoch 1236 of 2000 took 0.100s
  training loss:		0.312923
  validation loss:		0.365721
  validation accuracy:		87.83 %
Epoch 1237 of 2000 took 0.100s
  training loss:		0.310386
  validation loss:		0.365699
  validation accuracy:		87.83 %
Epoch 1238 of 2000 took 0.100s
  training loss:		0.319499
  validation loss:		0.384042
  validation accuracy:		87.61 %
Epoch 1239 of 2000 took 0.097s
  training loss:		0.308649
  validation loss:		0.363940
  validation accuracy:		88.15 %
Epoch 1240 of 2000 took 0.096s
  training loss:		0.312605
  validation loss:		0.363270
  validation accuracy:		88.15 %
Epoch 1241 of 2000 took 0.096s
  training loss:		0.311057
  validation loss:		0.363904
  validation accuracy:		87.83 %
Epoch 1242 of 2000 took 0.096s
  training loss:		0.315154
  validation loss:		0.367815
  validation accuracy:		87.93 %
Epoch 1243 of 2000 took 0.097s
  training loss:		0.313286
  validation loss:		0.365340
  validation accuracy:		87.93 %
Epoch 1244 of 2000 took 0.097s
  training loss:		0.316430
  validation loss:		0.391160
  validation accuracy:		87.28 %
Epoch 1245 of 2000 took 0.097s
  training loss:		0.322889
  validation loss:		0.374282
  validation accuracy:		87.50 %
Epoch 1246 of 2000 took 0.096s
  training loss:		0.319028
  validation loss:		0.372396
  validation accuracy:		87.83 %
Epoch 1247 of 2000 took 0.100s
  training loss:		0.313455
  validation loss:		0.382781
  validation accuracy:		87.28 %
Epoch 1248 of 2000 took 0.103s
  training loss:		0.310524
  validation loss:		0.377090
  validation accuracy:		87.83 %
Epoch 1249 of 2000 took 0.103s
  training loss:		0.319152
  validation loss:		0.370743
  validation accuracy:		87.83 %
Epoch 1250 of 2000 took 0.103s
  training loss:		0.312102
  validation loss:		0.361299
  validation accuracy:		88.15 %
Epoch 1251 of 2000 took 0.103s
  training loss:		0.313667
  validation loss:		0.363680
  validation accuracy:		88.15 %
Epoch 1252 of 2000 took 0.103s
  training loss:		0.310674
  validation loss:		0.382826
  validation accuracy:		87.83 %
Epoch 1253 of 2000 took 0.103s
  training loss:		0.320442
  validation loss:		0.375633
  validation accuracy:		87.72 %
Epoch 1254 of 2000 took 0.103s
  training loss:		0.311297
  validation loss:		0.358939
  validation accuracy:		88.15 %
Epoch 1255 of 2000 took 0.103s
  training loss:		0.315517
  validation loss:		0.365691
  validation accuracy:		87.93 %
Epoch 1256 of 2000 took 0.103s
  training loss:		0.326725
  validation loss:		0.396091
  validation accuracy:		86.52 %
Epoch 1257 of 2000 took 0.103s
  training loss:		0.315579
  validation loss:		0.362670
  validation accuracy:		87.61 %
Epoch 1258 of 2000 took 0.103s
  training loss:		0.305251
  validation loss:		0.373051
  validation accuracy:		87.72 %
Epoch 1259 of 2000 took 0.103s
  training loss:		0.309584
  validation loss:		0.370190
  validation accuracy:		87.83 %
Epoch 1260 of 2000 took 0.103s
  training loss:		0.312830
  validation loss:		0.374675
  validation accuracy:		87.61 %
Epoch 1261 of 2000 took 0.103s
  training loss:		0.301786
  validation loss:		0.352308
  validation accuracy:		88.59 %
Epoch 1262 of 2000 took 0.103s
  training loss:		0.313324
  validation loss:		0.387882
  validation accuracy:		87.72 %
Epoch 1263 of 2000 took 0.103s
  training loss:		0.311940
  validation loss:		0.370166
  validation accuracy:		88.04 %
Epoch 1264 of 2000 took 0.103s
  training loss:		0.315765
  validation loss:		0.376823
  validation accuracy:		87.72 %
Epoch 1265 of 2000 took 0.103s
  training loss:		0.313215
  validation loss:		0.370300
  validation accuracy:		87.61 %
Epoch 1266 of 2000 took 0.103s
  training loss:		0.306353
  validation loss:		0.361910
  validation accuracy:		88.37 %
Epoch 1267 of 2000 took 0.103s
  training loss:		0.303640
  validation loss:		0.359016
  validation accuracy:		88.04 %
Epoch 1268 of 2000 took 0.103s
  training loss:		0.329373
  validation loss:		0.370327
  validation accuracy:		87.93 %
Epoch 1269 of 2000 took 0.103s
  training loss:		0.312764
  validation loss:		0.367595
  validation accuracy:		87.72 %
Epoch 1270 of 2000 took 0.103s
  training loss:		0.310439
  validation loss:		0.371234
  validation accuracy:		87.93 %
Epoch 1271 of 2000 took 0.103s
  training loss:		0.316197
  validation loss:		0.368862
  validation accuracy:		88.15 %
Epoch 1272 of 2000 took 0.103s
  training loss:		0.312372
  validation loss:		0.385003
  validation accuracy:		87.39 %
Epoch 1273 of 2000 took 0.103s
  training loss:		0.315423
  validation loss:		0.367549
  validation accuracy:		87.83 %
Epoch 1274 of 2000 took 0.103s
  training loss:		0.309788
  validation loss:		0.375700
  validation accuracy:		87.93 %
Epoch 1275 of 2000 took 0.103s
  training loss:		0.310334
  validation loss:		0.364085
  validation accuracy:		87.93 %
Epoch 1276 of 2000 took 0.103s
  training loss:		0.314665
  validation loss:		0.362016
  validation accuracy:		88.04 %
Epoch 1277 of 2000 took 0.103s
  training loss:		0.311310
  validation loss:		0.378012
  validation accuracy:		87.93 %
Epoch 1278 of 2000 took 0.103s
  training loss:		0.308502
  validation loss:		0.375194
  validation accuracy:		87.72 %
Epoch 1279 of 2000 took 0.103s
  training loss:		0.312603
  validation loss:		0.373665
  validation accuracy:		87.93 %
Epoch 1280 of 2000 took 0.103s
  training loss:		0.303671
  validation loss:		0.364764
  validation accuracy:		88.15 %
Epoch 1281 of 2000 took 0.103s
  training loss:		0.318733
  validation loss:		0.364186
  validation accuracy:		88.15 %
Epoch 1282 of 2000 took 0.103s
  training loss:		0.311028
  validation loss:		0.377472
  validation accuracy:		87.61 %
Epoch 1283 of 2000 took 0.103s
  training loss:		0.321629
  validation loss:		0.362097
  validation accuracy:		88.37 %
Epoch 1284 of 2000 took 0.103s
  training loss:		0.309109
  validation loss:		0.365825
  validation accuracy:		88.15 %
Epoch 1285 of 2000 took 0.103s
  training loss:		0.305878
  validation loss:		0.357451
  validation accuracy:		88.80 %
Epoch 1286 of 2000 took 0.101s
  training loss:		0.313301
  validation loss:		0.367811
  validation accuracy:		88.26 %
Epoch 1287 of 2000 took 0.100s
  training loss:		0.306968
  validation loss:		0.361653
  validation accuracy:		87.93 %
Epoch 1288 of 2000 took 0.100s
  training loss:		0.303210
  validation loss:		0.368208
  validation accuracy:		87.72 %
Epoch 1289 of 2000 took 0.099s
  training loss:		0.312323
  validation loss:		0.380046
  validation accuracy:		87.83 %
Epoch 1290 of 2000 took 0.099s
  training loss:		0.307347
  validation loss:		0.365589
  validation accuracy:		87.83 %
Epoch 1291 of 2000 took 0.099s
  training loss:		0.303460
  validation loss:		0.363574
  validation accuracy:		88.15 %
Epoch 1292 of 2000 took 0.100s
  training loss:		0.314095
  validation loss:		0.362318
  validation accuracy:		88.37 %
Epoch 1293 of 2000 took 0.099s
  training loss:		0.315562
  validation loss:		0.358478
  validation accuracy:		88.80 %
Epoch 1294 of 2000 took 0.099s
  training loss:		0.303336
  validation loss:		0.368889
  validation accuracy:		87.83 %
Epoch 1295 of 2000 took 0.099s
  training loss:		0.313748
  validation loss:		0.357661
  validation accuracy:		88.59 %
Epoch 1296 of 2000 took 0.100s
  training loss:		0.309461
  validation loss:		0.365181
  validation accuracy:		88.15 %
Epoch 1297 of 2000 took 0.099s
  training loss:		0.310534
  validation loss:		0.356410
  validation accuracy:		88.48 %
Epoch 1298 of 2000 took 0.100s
  training loss:		0.305278
  validation loss:		0.360560
  validation accuracy:		88.26 %
Epoch 1299 of 2000 took 0.099s
  training loss:		0.314331
  validation loss:		0.359017
  validation accuracy:		87.93 %
Epoch 1300 of 2000 took 0.100s
  training loss:		0.307875
  validation loss:		0.387981
  validation accuracy:		87.83 %
Epoch 1301 of 2000 took 0.099s
  training loss:		0.311650
  validation loss:		0.376052
  validation accuracy:		87.93 %
Epoch 1302 of 2000 took 0.100s
  training loss:		0.315458
  validation loss:		0.358750
  validation accuracy:		88.26 %
Epoch 1303 of 2000 took 0.099s
  training loss:		0.311228
  validation loss:		0.358282
  validation accuracy:		88.37 %
Epoch 1304 of 2000 took 0.099s
  training loss:		0.312186
  validation loss:		0.367877
  validation accuracy:		88.04 %
Epoch 1305 of 2000 took 0.099s
  training loss:		0.312410
  validation loss:		0.371829
  validation accuracy:		88.04 %
Epoch 1306 of 2000 took 0.100s
  training loss:		0.308777
  validation loss:		0.368505
  validation accuracy:		87.93 %
Epoch 1307 of 2000 took 0.099s
  training loss:		0.314561
  validation loss:		0.375633
  validation accuracy:		87.83 %
Epoch 1308 of 2000 took 0.097s
  training loss:		0.313959
  validation loss:		0.373457
  validation accuracy:		87.83 %
Epoch 1309 of 2000 took 0.096s
  training loss:		0.302033
  validation loss:		0.394773
  validation accuracy:		87.28 %
Epoch 1310 of 2000 took 0.096s
  training loss:		0.315397
  validation loss:		0.383709
  validation accuracy:		87.50 %
Epoch 1311 of 2000 took 0.096s
  training loss:		0.312816
  validation loss:		0.375408
  validation accuracy:		87.93 %
Epoch 1312 of 2000 took 0.096s
  training loss:		0.310913
  validation loss:		0.376172
  validation accuracy:		87.61 %
Epoch 1313 of 2000 took 0.096s
  training loss:		0.312592
  validation loss:		0.361224
  validation accuracy:		88.48 %
Epoch 1314 of 2000 took 0.096s
  training loss:		0.306894
  validation loss:		0.362978
  validation accuracy:		88.15 %
Epoch 1315 of 2000 took 0.096s
  training loss:		0.302357
  validation loss:		0.372997
  validation accuracy:		88.04 %
Epoch 1316 of 2000 took 0.096s
  training loss:		0.314794
  validation loss:		0.368524
  validation accuracy:		87.93 %
Epoch 1317 of 2000 took 0.096s
  training loss:		0.305579
  validation loss:		0.367124
  validation accuracy:		87.93 %
Epoch 1318 of 2000 took 0.096s
  training loss:		0.302706
  validation loss:		0.380371
  validation accuracy:		87.61 %
Epoch 1319 of 2000 took 0.096s
  training loss:		0.310618
  validation loss:		0.362471
  validation accuracy:		88.37 %
Epoch 1320 of 2000 took 0.096s
  training loss:		0.307945
  validation loss:		0.380790
  validation accuracy:		87.50 %
Epoch 1321 of 2000 took 0.096s
  training loss:		0.317323
  validation loss:		0.364820
  validation accuracy:		88.15 %
Epoch 1322 of 2000 took 0.097s
  training loss:		0.307246
  validation loss:		0.364411
  validation accuracy:		87.39 %
Epoch 1323 of 2000 took 0.097s
  training loss:		0.304531
  validation loss:		0.366580
  validation accuracy:		88.15 %
Epoch 1324 of 2000 took 0.097s
  training loss:		0.308940
  validation loss:		0.366089
  validation accuracy:		87.93 %
Epoch 1325 of 2000 took 0.096s
  training loss:		0.303984
  validation loss:		0.373527
  validation accuracy:		87.61 %
Epoch 1326 of 2000 took 0.096s
  training loss:		0.308376
  validation loss:		0.369635
  validation accuracy:		88.26 %
Epoch 1327 of 2000 took 0.097s
  training loss:		0.308230
  validation loss:		0.377018
  validation accuracy:		87.83 %
Epoch 1328 of 2000 took 0.097s
  training loss:		0.310512
  validation loss:		0.386028
  validation accuracy:		87.93 %
Epoch 1329 of 2000 took 0.097s
  training loss:		0.310924
  validation loss:		0.385826
  validation accuracy:		87.28 %
Epoch 1330 of 2000 took 0.096s
  training loss:		0.313318
  validation loss:		0.370557
  validation accuracy:		87.83 %
Epoch 1331 of 2000 took 0.096s
  training loss:		0.313528
  validation loss:		0.384182
  validation accuracy:		87.50 %
Epoch 1332 of 2000 took 0.096s
  training loss:		0.305773
  validation loss:		0.357643
  validation accuracy:		88.37 %
Epoch 1333 of 2000 took 0.096s
  training loss:		0.318795
  validation loss:		0.373367
  validation accuracy:		87.72 %
Epoch 1334 of 2000 took 0.096s
  training loss:		0.314772
  validation loss:		0.378648
  validation accuracy:		87.61 %
Epoch 1335 of 2000 took 0.096s
  training loss:		0.313338
  validation loss:		0.393588
  validation accuracy:		87.50 %
Epoch 1336 of 2000 took 0.096s
  training loss:		0.311464
  validation loss:		0.364941
  validation accuracy:		88.04 %
Epoch 1337 of 2000 took 0.099s
  training loss:		0.306124
  validation loss:		0.366005
  validation accuracy:		87.93 %
Epoch 1338 of 2000 took 0.097s
  training loss:		0.305704
  validation loss:		0.382080
  validation accuracy:		87.72 %
Epoch 1339 of 2000 took 0.096s
  training loss:		0.310448
  validation loss:		0.373655
  validation accuracy:		87.93 %
Epoch 1340 of 2000 took 0.096s
  training loss:		0.304756
  validation loss:		0.362365
  validation accuracy:		88.37 %
Epoch 1341 of 2000 took 0.096s
  training loss:		0.300178
  validation loss:		0.361231
  validation accuracy:		88.26 %
Epoch 1342 of 2000 took 0.096s
  training loss:		0.312125
  validation loss:		0.389881
  validation accuracy:		87.50 %
Epoch 1343 of 2000 took 0.096s
  training loss:		0.311992
  validation loss:		0.367802
  validation accuracy:		87.93 %
Epoch 1344 of 2000 took 0.096s
  training loss:		0.313153
  validation loss:		0.392494
  validation accuracy:		87.72 %
Epoch 1345 of 2000 took 0.097s
  training loss:		0.308605
  validation loss:		0.358694
  validation accuracy:		88.04 %
Epoch 1346 of 2000 took 0.096s
  training loss:		0.309928
  validation loss:		0.382134
  validation accuracy:		87.50 %
Epoch 1347 of 2000 took 0.096s
  training loss:		0.311588
  validation loss:		0.381396
  validation accuracy:		87.39 %
Epoch 1348 of 2000 took 0.096s
  training loss:		0.310197
  validation loss:		0.378525
  validation accuracy:		87.93 %
Epoch 1349 of 2000 took 0.097s
  training loss:		0.319500
  validation loss:		0.376809
  validation accuracy:		87.72 %
Epoch 1350 of 2000 took 0.096s
  training loss:		0.306279
  validation loss:		0.373076
  validation accuracy:		87.83 %
Epoch 1351 of 2000 took 0.096s
  training loss:		0.308312
  validation loss:		0.373956
  validation accuracy:		87.61 %
Epoch 1352 of 2000 took 0.096s
  training loss:		0.314605
  validation loss:		0.362779
  validation accuracy:		88.04 %
Epoch 1353 of 2000 took 0.096s
  training loss:		0.301322
  validation loss:		0.377770
  validation accuracy:		87.72 %
Epoch 1354 of 2000 took 0.096s
  training loss:		0.314985
  validation loss:		0.378580
  validation accuracy:		87.61 %
Epoch 1355 of 2000 took 0.096s
  training loss:		0.307630
  validation loss:		0.391482
  validation accuracy:		87.28 %
Epoch 1356 of 2000 took 0.096s
  training loss:		0.308702
  validation loss:		0.379721
  validation accuracy:		87.61 %
Epoch 1357 of 2000 took 0.096s
  training loss:		0.308288
  validation loss:		0.382940
  validation accuracy:		87.17 %
Epoch 1358 of 2000 took 0.096s
  training loss:		0.310948
  validation loss:		0.383251
  validation accuracy:		87.61 %
Epoch 1359 of 2000 took 0.096s
  training loss:		0.310909
  validation loss:		0.368807
  validation accuracy:		88.15 %
Epoch 1360 of 2000 took 0.098s
  training loss:		0.317110
  validation loss:		0.371834
  validation accuracy:		88.15 %
Epoch 1361 of 2000 took 0.096s
  training loss:		0.310157
  validation loss:		0.372472
  validation accuracy:		87.83 %
Epoch 1362 of 2000 took 0.096s
  training loss:		0.305994
  validation loss:		0.387050
  validation accuracy:		87.83 %
Epoch 1363 of 2000 took 0.096s
  training loss:		0.310125
  validation loss:		0.380735
  validation accuracy:		87.93 %
Epoch 1364 of 2000 took 0.097s
  training loss:		0.312814
  validation loss:		0.382995
  validation accuracy:		87.39 %
Epoch 1365 of 2000 took 0.097s
  training loss:		0.310574
  validation loss:		0.395550
  validation accuracy:		87.17 %
Epoch 1366 of 2000 took 0.097s
  training loss:		0.302074
  validation loss:		0.365644
  validation accuracy:		88.15 %
Epoch 1367 of 2000 took 0.096s
  training loss:		0.313530
  validation loss:		0.369255
  validation accuracy:		88.15 %
Epoch 1368 of 2000 took 0.097s
  training loss:		0.309816
  validation loss:		0.365280
  validation accuracy:		88.26 %
Epoch 1369 of 2000 took 0.097s
  training loss:		0.311062
  validation loss:		0.388823
  validation accuracy:		87.28 %
Epoch 1370 of 2000 took 0.097s
  training loss:		0.304705
  validation loss:		0.377438
  validation accuracy:		87.72 %
Epoch 1371 of 2000 took 0.096s
  training loss:		0.306770
  validation loss:		0.397306
  validation accuracy:		87.72 %
Epoch 1372 of 2000 took 0.096s
  training loss:		0.305552
  validation loss:		0.383790
  validation accuracy:		87.72 %
Epoch 1373 of 2000 took 0.096s
  training loss:		0.311830
  validation loss:		0.373735
  validation accuracy:		87.61 %
Epoch 1374 of 2000 took 0.096s
  training loss:		0.310874
  validation loss:		0.361256
  validation accuracy:		88.48 %
Epoch 1375 of 2000 took 0.096s
  training loss:		0.307481
  validation loss:		0.372216
  validation accuracy:		87.72 %
Epoch 1376 of 2000 took 0.097s
  training loss:		0.321754
  validation loss:		0.362427
  validation accuracy:		87.93 %
Epoch 1377 of 2000 took 0.096s
  training loss:		0.301823
  validation loss:		0.370389
  validation accuracy:		88.04 %
Epoch 1378 of 2000 took 0.097s
  training loss:		0.306298
  validation loss:		0.382750
  validation accuracy:		87.50 %
Epoch 1379 of 2000 took 0.097s
  training loss:		0.305570
  validation loss:		0.372267
  validation accuracy:		87.83 %
Epoch 1380 of 2000 took 0.097s
  training loss:		0.300477
  validation loss:		0.364287
  validation accuracy:		88.26 %
Epoch 1381 of 2000 took 0.097s
  training loss:		0.307081
  validation loss:		0.386333
  validation accuracy:		86.96 %
Epoch 1382 of 2000 took 0.096s
  training loss:		0.310266
  validation loss:		0.380681
  validation accuracy:		87.50 %
Epoch 1383 of 2000 took 0.096s
  training loss:		0.303794
  validation loss:		0.371341
  validation accuracy:		87.93 %
Epoch 1384 of 2000 took 0.097s
  training loss:		0.311188
  validation loss:		0.371822
  validation accuracy:		87.83 %
Epoch 1385 of 2000 took 0.096s
  training loss:		0.304641
  validation loss:		0.367452
  validation accuracy:		87.50 %
Epoch 1386 of 2000 took 0.097s
  training loss:		0.304331
  validation loss:		0.369696
  validation accuracy:		87.28 %
Epoch 1387 of 2000 took 0.096s
  training loss:		0.312587
  validation loss:		0.372693
  validation accuracy:		87.61 %
Epoch 1388 of 2000 took 0.096s
  training loss:		0.309534
  validation loss:		0.375119
  validation accuracy:		87.61 %
Epoch 1389 of 2000 took 0.097s
  training loss:		0.311283
  validation loss:		0.367352
  validation accuracy:		88.04 %
Epoch 1390 of 2000 took 0.097s
  training loss:		0.313026
  validation loss:		0.364093
  validation accuracy:		87.93 %
Epoch 1391 of 2000 took 0.098s
  training loss:		0.308899
  validation loss:		0.369647
  validation accuracy:		87.93 %
Epoch 1392 of 2000 took 0.097s
  training loss:		0.303497
  validation loss:		0.362450
  validation accuracy:		87.72 %
Epoch 1393 of 2000 took 0.097s
  training loss:		0.315918
  validation loss:		0.360420
  validation accuracy:		88.26 %
Epoch 1394 of 2000 took 0.096s
  training loss:		0.308063
  validation loss:		0.381361
  validation accuracy:		87.72 %
Epoch 1395 of 2000 took 0.097s
  training loss:		0.305698
  validation loss:		0.366262
  validation accuracy:		88.15 %
Epoch 1396 of 2000 took 0.096s
  training loss:		0.306445
  validation loss:		0.377568
  validation accuracy:		87.50 %
Epoch 1397 of 2000 took 0.097s
  training loss:		0.303369
  validation loss:		0.371648
  validation accuracy:		87.83 %
Epoch 1398 of 2000 took 0.096s
  training loss:		0.300956
  validation loss:		0.374724
  validation accuracy:		88.04 %
Epoch 1399 of 2000 took 0.097s
  training loss:		0.305696
  validation loss:		0.370067
  validation accuracy:		88.15 %
Epoch 1400 of 2000 took 0.097s
  training loss:		0.299891
  validation loss:		0.381149
  validation accuracy:		87.61 %
Epoch 1401 of 2000 took 0.097s
  training loss:		0.307894
  validation loss:		0.371430
  validation accuracy:		87.83 %
Epoch 1402 of 2000 took 0.096s
  training loss:		0.305172
  validation loss:		0.363448
  validation accuracy:		88.26 %
Epoch 1403 of 2000 took 0.099s
  training loss:		0.310154
  validation loss:		0.379847
  validation accuracy:		87.50 %
Epoch 1404 of 2000 took 0.108s
  training loss:		0.313403
  validation loss:		0.387354
  validation accuracy:		87.28 %
Epoch 1405 of 2000 took 0.111s
  training loss:		0.307568
  validation loss:		0.384860
  validation accuracy:		87.72 %
Epoch 1406 of 2000 took 0.143s
  training loss:		0.302467
  validation loss:		0.364004
  validation accuracy:		87.61 %
Epoch 1407 of 2000 took 0.103s
  training loss:		0.303001
  validation loss:		0.386924
  validation accuracy:		87.50 %
Epoch 1408 of 2000 took 0.105s
  training loss:		0.314502
  validation loss:		0.359795
  validation accuracy:		87.83 %
Epoch 1409 of 2000 took 0.105s
  training loss:		0.307532
  validation loss:		0.377550
  validation accuracy:		87.61 %
Epoch 1410 of 2000 took 0.105s
  training loss:		0.305166
  validation loss:		0.369821
  validation accuracy:		88.04 %
Epoch 1411 of 2000 took 0.104s
  training loss:		0.303415
  validation loss:		0.381387
  validation accuracy:		87.83 %
Epoch 1412 of 2000 took 0.106s
  training loss:		0.306486
  validation loss:		0.362807
  validation accuracy:		88.04 %
Epoch 1413 of 2000 took 0.103s
  training loss:		0.302081
  validation loss:		0.369159
  validation accuracy:		87.83 %
Epoch 1414 of 2000 took 0.101s
  training loss:		0.312575
  validation loss:		0.370796
  validation accuracy:		87.83 %
Epoch 1415 of 2000 took 0.100s
  training loss:		0.303404
  validation loss:		0.363292
  validation accuracy:		88.37 %
Epoch 1416 of 2000 took 0.100s
  training loss:		0.305791
  validation loss:		0.365347
  validation accuracy:		88.04 %
Epoch 1417 of 2000 took 0.101s
  training loss:		0.307851
  validation loss:		0.386322
  validation accuracy:		87.28 %
Epoch 1418 of 2000 took 0.101s
  training loss:		0.308435
  validation loss:		0.369205
  validation accuracy:		88.26 %
Epoch 1419 of 2000 took 0.100s
  training loss:		0.313142
  validation loss:		0.379593
  validation accuracy:		87.50 %
Epoch 1420 of 2000 took 0.101s
  training loss:		0.309712
  validation loss:		0.373908
  validation accuracy:		87.61 %
Epoch 1421 of 2000 took 0.101s
  training loss:		0.309598
  validation loss:		0.388051
  validation accuracy:		87.39 %
Epoch 1422 of 2000 took 0.101s
  training loss:		0.308654
  validation loss:		0.371951
  validation accuracy:		87.72 %
Epoch 1423 of 2000 took 0.100s
  training loss:		0.291495
  validation loss:		0.383402
  validation accuracy:		87.61 %
Epoch 1424 of 2000 took 0.101s
  training loss:		0.304011
  validation loss:		0.367995
  validation accuracy:		87.72 %
Epoch 1425 of 2000 took 0.101s
  training loss:		0.315847
  validation loss:		0.378541
  validation accuracy:		87.72 %
Epoch 1426 of 2000 took 0.101s
  training loss:		0.309986
  validation loss:		0.367416
  validation accuracy:		87.93 %
Epoch 1427 of 2000 took 0.100s
  training loss:		0.307083
  validation loss:		0.370115
  validation accuracy:		87.39 %
Epoch 1428 of 2000 took 0.100s
  training loss:		0.301746
  validation loss:		0.361594
  validation accuracy:		88.04 %
Epoch 1429 of 2000 took 0.101s
  training loss:		0.306277
  validation loss:		0.367377
  validation accuracy:		88.04 %
Epoch 1430 of 2000 took 0.100s
  training loss:		0.302023
  validation loss:		0.364632
  validation accuracy:		87.83 %
Epoch 1431 of 2000 took 0.101s
  training loss:		0.306986
  validation loss:		0.400565
  validation accuracy:		86.85 %
Epoch 1432 of 2000 took 0.101s
  training loss:		0.303738
  validation loss:		0.368927
  validation accuracy:		87.61 %
Epoch 1433 of 2000 took 0.100s
  training loss:		0.312850
  validation loss:		0.371660
  validation accuracy:		88.04 %
Epoch 1434 of 2000 took 0.101s
  training loss:		0.310860
  validation loss:		0.379975
  validation accuracy:		87.72 %
Epoch 1435 of 2000 took 0.100s
  training loss:		0.301166
  validation loss:		0.364263
  validation accuracy:		88.15 %
Epoch 1436 of 2000 took 0.101s
  training loss:		0.308816
  validation loss:		0.375527
  validation accuracy:		87.72 %
Epoch 1437 of 2000 took 0.101s
  training loss:		0.306691
  validation loss:		0.385034
  validation accuracy:		87.39 %
Epoch 1438 of 2000 took 0.101s
  training loss:		0.305451
  validation loss:		0.379578
  validation accuracy:		87.61 %
Epoch 1439 of 2000 took 0.101s
  training loss:		0.308538
  validation loss:		0.364364
  validation accuracy:		88.26 %
Epoch 1440 of 2000 took 0.100s
  training loss:		0.306616
  validation loss:		0.375380
  validation accuracy:		87.93 %
Epoch 1441 of 2000 took 0.101s
  training loss:		0.304520
  validation loss:		0.374184
  validation accuracy:		87.72 %
Epoch 1442 of 2000 took 0.102s
  training loss:		0.307842
  validation loss:		0.366354
  validation accuracy:		87.50 %
Epoch 1443 of 2000 took 0.101s
  training loss:		0.302944
  validation loss:		0.375292
  validation accuracy:		87.72 %
Epoch 1444 of 2000 took 0.101s
  training loss:		0.310750
  validation loss:		0.382675
  validation accuracy:		87.39 %
Epoch 1445 of 2000 took 0.103s
  training loss:		0.304413
  validation loss:		0.371607
  validation accuracy:		88.04 %
Epoch 1446 of 2000 took 0.143s
  training loss:		0.312164
  validation loss:		0.370784
  validation accuracy:		87.93 %
Epoch 1447 of 2000 took 0.107s
  training loss:		0.302325
  validation loss:		0.363992
  validation accuracy:		88.26 %
Epoch 1448 of 2000 took 0.107s
  training loss:		0.301596
  validation loss:		0.386852
  validation accuracy:		87.61 %
Epoch 1449 of 2000 took 0.107s
  training loss:		0.312093
  validation loss:		0.395146
  validation accuracy:		87.28 %
Epoch 1450 of 2000 took 0.107s
  training loss:		0.303823
  validation loss:		0.378726
  validation accuracy:		87.50 %
Epoch 1451 of 2000 took 0.107s
  training loss:		0.306033
  validation loss:		0.390425
  validation accuracy:		87.39 %
Epoch 1452 of 2000 took 0.107s
  training loss:		0.301993
  validation loss:		0.402259
  validation accuracy:		87.61 %
Epoch 1453 of 2000 took 0.107s
  training loss:		0.310259
  validation loss:		0.383418
  validation accuracy:		87.72 %
Epoch 1454 of 2000 took 0.107s
  training loss:		0.305298
  validation loss:		0.365130
  validation accuracy:		88.15 %
Epoch 1455 of 2000 took 0.107s
  training loss:		0.297591
  validation loss:		0.363280
  validation accuracy:		88.15 %
Epoch 1456 of 2000 took 0.107s
  training loss:		0.313487
  validation loss:		0.378908
  validation accuracy:		87.83 %
Epoch 1457 of 2000 took 0.107s
  training loss:		0.307924
  validation loss:		0.367885
  validation accuracy:		87.93 %
Epoch 1458 of 2000 took 0.107s
  training loss:		0.309129
  validation loss:		0.378114
  validation accuracy:		87.61 %
Epoch 1459 of 2000 took 0.107s
  training loss:		0.303924
  validation loss:		0.387029
  validation accuracy:		87.61 %
Epoch 1460 of 2000 took 0.107s
  training loss:		0.296920
  validation loss:		0.378842
  validation accuracy:		87.61 %
Epoch 1461 of 2000 took 0.107s
  training loss:		0.304825
  validation loss:		0.372574
  validation accuracy:		87.50 %
Epoch 1462 of 2000 took 0.107s
  training loss:		0.307346
  validation loss:		0.381657
  validation accuracy:		87.50 %
Epoch 1463 of 2000 took 0.107s
  training loss:		0.311686
  validation loss:		0.366395
  validation accuracy:		87.83 %
Epoch 1464 of 2000 took 0.107s
  training loss:		0.305840
  validation loss:		0.391584
  validation accuracy:		87.28 %
Epoch 1465 of 2000 took 0.107s
  training loss:		0.310994
  validation loss:		0.380176
  validation accuracy:		87.61 %
Epoch 1466 of 2000 took 0.107s
  training loss:		0.314065
  validation loss:		0.394634
  validation accuracy:		87.07 %
Epoch 1467 of 2000 took 0.107s
  training loss:		0.308816
  validation loss:		0.376796
  validation accuracy:		87.61 %
Epoch 1468 of 2000 took 0.107s
  training loss:		0.307002
  validation loss:		0.383459
  validation accuracy:		87.61 %
Epoch 1469 of 2000 took 0.107s
  training loss:		0.305377
  validation loss:		0.359949
  validation accuracy:		88.59 %
Epoch 1470 of 2000 took 0.107s
  training loss:		0.302891
  validation loss:		0.382494
  validation accuracy:		87.50 %
Epoch 1471 of 2000 took 0.107s
  training loss:		0.302376
  validation loss:		0.381298
  validation accuracy:		87.72 %
Epoch 1472 of 2000 took 0.107s
  training loss:		0.305414
  validation loss:		0.383390
  validation accuracy:		87.39 %
Epoch 1473 of 2000 took 0.107s
  training loss:		0.306996
  validation loss:		0.382831
  validation accuracy:		87.50 %
Epoch 1474 of 2000 took 0.107s
  training loss:		0.309454
  validation loss:		0.374822
  validation accuracy:		87.83 %
Epoch 1475 of 2000 took 0.107s
  training loss:		0.299424
  validation loss:		0.358638
  validation accuracy:		88.37 %
Epoch 1476 of 2000 took 0.107s
  training loss:		0.316017
  validation loss:		0.373476
  validation accuracy:		87.83 %
Epoch 1477 of 2000 took 0.107s
  training loss:		0.295686
  validation loss:		0.373476
  validation accuracy:		87.50 %
Epoch 1478 of 2000 took 0.107s
  training loss:		0.302685
  validation loss:		0.367130
  validation accuracy:		87.93 %
Epoch 1479 of 2000 took 0.107s
  training loss:		0.301275
  validation loss:		0.376761
  validation accuracy:		87.93 %
Epoch 1480 of 2000 took 0.107s
  training loss:		0.307940
  validation loss:		0.391904
  validation accuracy:		86.74 %
Epoch 1481 of 2000 took 0.107s
  training loss:		0.307604
  validation loss:		0.379179
  validation accuracy:		87.28 %
Epoch 1482 of 2000 took 0.107s
  training loss:		0.311340
  validation loss:		0.378351
  validation accuracy:		87.39 %
Epoch 1483 of 2000 took 0.107s
  training loss:		0.304020
  validation loss:		0.381409
  validation accuracy:		87.61 %
Epoch 1484 of 2000 took 0.106s
  training loss:		0.303523
  validation loss:		0.386414
  validation accuracy:		87.50 %
Epoch 1485 of 2000 took 0.107s
  training loss:		0.309689
  validation loss:		0.386583
  validation accuracy:		87.72 %
Epoch 1486 of 2000 took 0.106s
  training loss:		0.308944
  validation loss:		0.389798
  validation accuracy:		87.07 %
Epoch 1487 of 2000 took 0.106s
  training loss:		0.301578
  validation loss:		0.372010
  validation accuracy:		87.83 %
Epoch 1488 of 2000 took 0.106s
  training loss:		0.306950
  validation loss:		0.374912
  validation accuracy:		87.83 %
Epoch 1489 of 2000 took 0.106s
  training loss:		0.308122
  validation loss:		0.371030
  validation accuracy:		88.04 %
Epoch 1490 of 2000 took 0.106s
  training loss:		0.299441
  validation loss:		0.384794
  validation accuracy:		87.61 %
Epoch 1491 of 2000 took 0.106s
  training loss:		0.313445
  validation loss:		0.369725
  validation accuracy:		87.83 %
Epoch 1492 of 2000 took 0.106s
  training loss:		0.326660
  validation loss:		0.387582
  validation accuracy:		87.39 %
Epoch 1493 of 2000 took 0.106s
  training loss:		0.305808
  validation loss:		0.383654
  validation accuracy:		87.17 %
Epoch 1494 of 2000 took 0.106s
  training loss:		0.305763
  validation loss:		0.389791
  validation accuracy:		87.07 %
Epoch 1495 of 2000 took 0.106s
  training loss:		0.308010
  validation loss:		0.364577
  validation accuracy:		87.61 %
Epoch 1496 of 2000 took 0.106s
  training loss:		0.303877
  validation loss:		0.364646
  validation accuracy:		87.72 %
Epoch 1497 of 2000 took 0.106s
  training loss:		0.304693
  validation loss:		0.377365
  validation accuracy:		87.83 %
Epoch 1498 of 2000 took 0.106s
  training loss:		0.307477
  validation loss:		0.363963
  validation accuracy:		87.72 %
Epoch 1499 of 2000 took 0.106s
  training loss:		0.306284
  validation loss:		0.390258
  validation accuracy:		87.61 %
Epoch 1500 of 2000 took 0.106s
  training loss:		0.307448
  validation loss:		0.396945
  validation accuracy:		87.72 %
Epoch 1501 of 2000 took 0.106s
  training loss:		0.301424
  validation loss:		0.382125
  validation accuracy:		87.61 %
Epoch 1502 of 2000 took 0.106s
  training loss:		0.300537
  validation loss:		0.370088
  validation accuracy:		87.83 %
Epoch 1503 of 2000 took 0.106s
  training loss:		0.298712
  validation loss:		0.377319
  validation accuracy:		87.93 %
Epoch 1504 of 2000 took 0.107s
  training loss:		0.294763
  validation loss:		0.360299
  validation accuracy:		88.70 %
Epoch 1505 of 2000 took 0.106s
  training loss:		0.302057
  validation loss:		0.369702
  validation accuracy:		87.72 %
Epoch 1506 of 2000 took 0.106s
  training loss:		0.303813
  validation loss:		0.370621
  validation accuracy:		88.15 %
Epoch 1507 of 2000 took 0.107s
  training loss:		0.303752
  validation loss:		0.373753
  validation accuracy:		87.93 %
Epoch 1508 of 2000 took 0.106s
  training loss:		0.295420
  validation loss:		0.386745
  validation accuracy:		87.50 %
Epoch 1509 of 2000 took 0.106s
  training loss:		0.299348
  validation loss:		0.371846
  validation accuracy:		87.72 %
Epoch 1510 of 2000 took 0.106s
  training loss:		0.308493
  validation loss:		0.365742
  validation accuracy:		88.37 %
Epoch 1511 of 2000 took 0.110s
  training loss:		0.299293
  validation loss:		0.390375
  validation accuracy:		87.50 %
Epoch 1512 of 2000 took 0.109s
  training loss:		0.302551
  validation loss:		0.366863
  validation accuracy:		88.15 %
Epoch 1513 of 2000 took 0.111s
  training loss:		0.307875
  validation loss:		0.394053
  validation accuracy:		87.39 %
Epoch 1514 of 2000 took 0.116s
  training loss:		0.305714
  validation loss:		0.399551
  validation accuracy:		87.39 %
Epoch 1515 of 2000 took 0.109s
  training loss:		0.306072
  validation loss:		0.383906
  validation accuracy:		87.50 %
Epoch 1516 of 2000 took 0.111s
  training loss:		0.302897
  validation loss:		0.376836
  validation accuracy:		87.50 %
Epoch 1517 of 2000 took 0.117s
  training loss:		0.305098
  validation loss:		0.378210
  validation accuracy:		87.72 %
Epoch 1518 of 2000 took 0.108s
  training loss:		0.305222
  validation loss:		0.385170
  validation accuracy:		87.28 %
Epoch 1519 of 2000 took 0.111s
  training loss:		0.308146
  validation loss:		0.370881
  validation accuracy:		88.26 %
Epoch 1520 of 2000 took 0.108s
  training loss:		0.305660
  validation loss:		0.362102
  validation accuracy:		88.15 %
Epoch 1521 of 2000 took 0.116s
  training loss:		0.307465
  validation loss:		0.372298
  validation accuracy:		88.15 %
Epoch 1522 of 2000 took 0.112s
  training loss:		0.301498
  validation loss:		0.376581
  validation accuracy:		87.72 %
Epoch 1523 of 2000 took 0.108s
  training loss:		0.306423
  validation loss:		0.360968
  validation accuracy:		88.04 %
Epoch 1524 of 2000 took 0.117s
  training loss:		0.300531
  validation loss:		0.383808
  validation accuracy:		87.83 %
Epoch 1525 of 2000 took 0.110s
  training loss:		0.303286
  validation loss:		0.385656
  validation accuracy:		87.61 %
Epoch 1526 of 2000 took 0.111s
  training loss:		0.307818
  validation loss:		0.376231
  validation accuracy:		87.93 %
Epoch 1527 of 2000 took 0.108s
  training loss:		0.301240
  validation loss:		0.369333
  validation accuracy:		87.93 %
Epoch 1528 of 2000 took 0.112s
  training loss:		0.302626
  validation loss:		0.360683
  validation accuracy:		88.26 %
Epoch 1529 of 2000 took 0.116s
  training loss:		0.312199
  validation loss:		0.370199
  validation accuracy:		87.93 %
Epoch 1530 of 2000 took 0.108s
  training loss:		0.306694
  validation loss:		0.379091
  validation accuracy:		87.39 %
Epoch 1531 of 2000 took 0.112s
  training loss:		0.309212
  validation loss:		0.382569
  validation accuracy:		87.72 %
Epoch 1532 of 2000 took 0.115s
  training loss:		0.304573
  validation loss:		0.375682
  validation accuracy:		87.83 %
Epoch 1533 of 2000 took 0.108s
  training loss:		0.300712
  validation loss:		0.364951
  validation accuracy:		88.04 %
Epoch 1534 of 2000 took 0.111s
  training loss:		0.297084
  validation loss:		0.367945
  validation accuracy:		87.83 %
Epoch 1535 of 2000 took 0.108s
  training loss:		0.302880
  validation loss:		0.387019
  validation accuracy:		87.50 %
Epoch 1536 of 2000 took 0.117s
  training loss:		0.302604
  validation loss:		0.360481
  validation accuracy:		87.93 %
Epoch 1537 of 2000 took 0.111s
  training loss:		0.296880
  validation loss:		0.373833
  validation accuracy:		87.72 %
Epoch 1538 of 2000 took 0.109s
  training loss:		0.304553
  validation loss:		0.382569
  validation accuracy:		87.50 %
Epoch 1539 of 2000 took 0.118s
  training loss:		0.300216
  validation loss:		0.402120
  validation accuracy:		86.96 %
Epoch 1540 of 2000 took 0.109s
  training loss:		0.299623
  validation loss:		0.367935
  validation accuracy:		88.15 %
Epoch 1541 of 2000 took 0.111s
  training loss:		0.319808
  validation loss:		0.374251
  validation accuracy:		87.83 %
Epoch 1542 of 2000 took 0.108s
  training loss:		0.297854
  validation loss:		0.374853
  validation accuracy:		88.04 %
Epoch 1543 of 2000 took 0.114s
  training loss:		0.303596
  validation loss:		0.375355
  validation accuracy:		87.83 %
Epoch 1544 of 2000 took 0.114s
  training loss:		0.303389
  validation loss:		0.377297
  validation accuracy:		87.50 %
Epoch 1545 of 2000 took 0.108s
  training loss:		0.304495
  validation loss:		0.376006
  validation accuracy:		87.93 %
Epoch 1546 of 2000 took 0.115s
  training loss:		0.299317
  validation loss:		0.361539
  validation accuracy:		88.37 %
Epoch 1547 of 2000 took 0.112s
  training loss:		0.299730
  validation loss:		0.395876
  validation accuracy:		87.17 %
Epoch 1548 of 2000 took 0.110s
  training loss:		0.302379
  validation loss:		0.385033
  validation accuracy:		87.39 %
Epoch 1549 of 2000 took 0.109s
  training loss:		0.304820
  validation loss:		0.386436
  validation accuracy:		87.39 %
Epoch 1550 of 2000 took 0.110s
  training loss:		0.299458
  validation loss:		0.379143
  validation accuracy:		87.50 %
Epoch 1551 of 2000 took 0.116s
  training loss:		0.299701
  validation loss:		0.379701
  validation accuracy:		87.50 %
Epoch 1552 of 2000 took 0.110s
  training loss:		0.305321
  validation loss:		0.376815
  validation accuracy:		87.83 %
Epoch 1553 of 2000 took 0.110s
  training loss:		0.307653
  validation loss:		0.365798
  validation accuracy:		87.83 %
Epoch 1554 of 2000 took 0.117s
  training loss:		0.300261
  validation loss:		0.369564
  validation accuracy:		87.93 %
Epoch 1555 of 2000 took 0.108s
  training loss:		0.303335
  validation loss:		0.365326
  validation accuracy:		88.15 %
Epoch 1556 of 2000 took 0.111s
  training loss:		0.300117
  validation loss:		0.386159
  validation accuracy:		87.28 %
Epoch 1557 of 2000 took 0.108s
  training loss:		0.298468
  validation loss:		0.373563
  validation accuracy:		87.07 %
Epoch 1558 of 2000 took 0.115s
  training loss:		0.304895
  validation loss:		0.370170
  validation accuracy:		87.83 %
Epoch 1559 of 2000 took 0.112s
  training loss:		0.309786
  validation loss:		0.376254
  validation accuracy:		87.93 %
Epoch 1560 of 2000 took 0.108s
  training loss:		0.297755
  validation loss:		0.380122
  validation accuracy:		88.04 %
Epoch 1561 of 2000 took 0.116s
  training loss:		0.300728
  validation loss:		0.372732
  validation accuracy:		87.72 %
Epoch 1562 of 2000 took 0.110s
  training loss:		0.298144
  validation loss:		0.392084
  validation accuracy:		87.72 %
Epoch 1563 of 2000 took 0.110s
  training loss:		0.301888
  validation loss:		0.377864
  validation accuracy:		87.72 %
Epoch 1564 of 2000 took 0.109s
  training loss:		0.302640
  validation loss:		0.364337
  validation accuracy:		87.72 %
Epoch 1565 of 2000 took 0.111s
  training loss:		0.300615
  validation loss:		0.386401
  validation accuracy:		87.28 %
Epoch 1566 of 2000 took 0.116s
  training loss:		0.309344
  validation loss:		0.368169
  validation accuracy:		88.15 %
Epoch 1567 of 2000 took 0.108s
  training loss:		0.307788
  validation loss:		0.372491
  validation accuracy:		88.04 %
Epoch 1568 of 2000 took 0.112s
  training loss:		0.298307
  validation loss:		0.400746
  validation accuracy:		86.96 %
Epoch 1569 of 2000 took 0.116s
  training loss:		0.305745
  validation loss:		0.372777
  validation accuracy:		87.61 %
Epoch 1570 of 2000 took 0.108s
  training loss:		0.306206
  validation loss:		0.385660
  validation accuracy:		87.61 %
Epoch 1571 of 2000 took 0.111s
  training loss:		0.306194
  validation loss:		0.394500
  validation accuracy:		87.72 %
Epoch 1572 of 2000 took 0.112s
  training loss:		0.302018
  validation loss:		0.374272
  validation accuracy:		88.04 %
Epoch 1573 of 2000 took 0.117s
  training loss:		0.296638
  validation loss:		0.379513
  validation accuracy:		87.72 %
Epoch 1574 of 2000 took 0.111s
  training loss:		0.305280
  validation loss:		0.372006
  validation accuracy:		87.72 %
Epoch 1575 of 2000 took 0.109s
  training loss:		0.302027
  validation loss:		0.392554
  validation accuracy:		87.07 %
Epoch 1576 of 2000 took 0.118s
  training loss:		0.302318
  validation loss:		0.369710
  validation accuracy:		87.50 %
Epoch 1577 of 2000 took 0.108s
  training loss:		0.295531
  validation loss:		0.371462
  validation accuracy:		87.61 %
Epoch 1578 of 2000 took 0.111s
  training loss:		0.303316
  validation loss:		0.405728
  validation accuracy:		87.39 %
Epoch 1579 of 2000 took 0.108s
  training loss:		0.300325
  validation loss:		0.371215
  validation accuracy:		87.72 %
Epoch 1580 of 2000 took 0.114s
  training loss:		0.299615
  validation loss:		0.387952
  validation accuracy:		87.39 %
Epoch 1581 of 2000 took 0.114s
  training loss:		0.297604
  validation loss:		0.372214
  validation accuracy:		87.83 %
Epoch 1582 of 2000 took 0.108s
  training loss:		0.301456
  validation loss:		0.376250
  validation accuracy:		87.93 %
Epoch 1583 of 2000 took 0.114s
  training loss:		0.300014
  validation loss:		0.388940
  validation accuracy:		87.28 %
Epoch 1584 of 2000 took 0.113s
  training loss:		0.308284
  validation loss:		0.369739
  validation accuracy:		87.72 %
Epoch 1585 of 2000 took 0.109s
  training loss:		0.299804
  validation loss:		0.384020
  validation accuracy:		87.72 %
Epoch 1586 of 2000 took 0.110s
  training loss:		0.296313
  validation loss:		0.364371
  validation accuracy:		88.37 %
Epoch 1587 of 2000 took 0.109s
  training loss:		0.309184
  validation loss:		0.400716
  validation accuracy:		86.96 %
Epoch 1588 of 2000 took 0.116s
  training loss:		0.305264
  validation loss:		0.364756
  validation accuracy:		88.26 %
Epoch 1589 of 2000 took 0.110s
  training loss:		0.301877
  validation loss:		0.365623
  validation accuracy:		88.48 %
Epoch 1590 of 2000 took 0.111s
  training loss:		0.300279
  validation loss:		0.386999
  validation accuracy:		87.83 %
Epoch 1591 of 2000 took 0.118s
  training loss:		0.297511
  validation loss:		0.368236
  validation accuracy:		87.93 %
Epoch 1592 of 2000 took 0.109s
  training loss:		0.305204
  validation loss:		0.387230
  validation accuracy:		87.72 %
Epoch 1593 of 2000 took 0.112s
  training loss:		0.298486
  validation loss:		0.395702
  validation accuracy:		87.07 %
Epoch 1594 of 2000 took 0.109s
  training loss:		0.308870
  validation loss:		0.387793
  validation accuracy:		87.72 %
Epoch 1595 of 2000 took 0.116s
  training loss:		0.310549
  validation loss:		0.381721
  validation accuracy:		87.61 %
Epoch 1596 of 2000 took 0.113s
  training loss:		0.302426
  validation loss:		0.366396
  validation accuracy:		88.26 %
Epoch 1597 of 2000 took 0.109s
  training loss:		0.299906
  validation loss:		0.373469
  validation accuracy:		87.83 %
Epoch 1598 of 2000 took 0.117s
  training loss:		0.303713
  validation loss:		0.368491
  validation accuracy:		88.15 %
Epoch 1599 of 2000 took 0.109s
  training loss:		0.309522
  validation loss:		0.384782
  validation accuracy:		87.83 %
Epoch 1600 of 2000 took 0.113s
  training loss:		0.299912
  validation loss:		0.385407
  validation accuracy:		87.39 %
Epoch 1601 of 2000 took 0.112s
  training loss:		0.306476
  validation loss:		0.385960
  validation accuracy:		87.50 %
Epoch 1602 of 2000 took 0.111s
  training loss:		0.301135
  validation loss:		0.368725
  validation accuracy:		87.93 %
Epoch 1603 of 2000 took 0.114s
  training loss:		0.304935
  validation loss:		0.369389
  validation accuracy:		87.93 %
Epoch 1604 of 2000 took 0.109s
  training loss:		0.303371
  validation loss:		0.367050
  validation accuracy:		88.26 %
Epoch 1605 of 2000 took 0.116s
  training loss:		0.306614
  validation loss:		0.373058
  validation accuracy:		87.72 %
Epoch 1606 of 2000 took 0.109s
  training loss:		0.300393
  validation loss:		0.384401
  validation accuracy:		87.83 %
Epoch 1607 of 2000 took 0.115s
  training loss:		0.305755
  validation loss:		0.383434
  validation accuracy:		87.28 %
Epoch 1608 of 2000 took 0.110s
  training loss:		0.307473
  validation loss:		0.375897
  validation accuracy:		88.04 %
Epoch 1609 of 2000 took 0.112s
  training loss:		0.299862
  validation loss:		0.366519
  validation accuracy:		88.37 %
Epoch 1610 of 2000 took 0.113s
  training loss:		0.297095
  validation loss:		0.373403
  validation accuracy:		87.61 %
Epoch 1611 of 2000 took 0.110s
  training loss:		0.305822
  validation loss:		0.377574
  validation accuracy:		87.83 %
Epoch 1612 of 2000 took 0.115s
  training loss:		0.296792
  validation loss:		0.389392
  validation accuracy:		87.50 %
Epoch 1613 of 2000 took 0.109s
  training loss:		0.307629
  validation loss:		0.365651
  validation accuracy:		88.37 %
Epoch 1614 of 2000 took 0.116s
  training loss:		0.305211
  validation loss:		0.373548
  validation accuracy:		87.83 %
Epoch 1615 of 2000 took 0.109s
  training loss:		0.301568
  validation loss:		0.370092
  validation accuracy:		88.04 %
Epoch 1616 of 2000 took 0.113s
  training loss:		0.297847
  validation loss:		0.363017
  validation accuracy:		87.93 %
Epoch 1617 of 2000 took 0.111s
  training loss:		0.301632
  validation loss:		0.362983
  validation accuracy:		87.93 %
Epoch 1618 of 2000 took 0.111s
  training loss:		0.299785
  validation loss:		0.414344
  validation accuracy:		87.39 %
Epoch 1619 of 2000 took 0.114s
  training loss:		0.296363
  validation loss:		0.379747
  validation accuracy:		87.72 %
Epoch 1620 of 2000 took 0.109s
  training loss:		0.311953
  validation loss:		0.366345
  validation accuracy:		88.26 %
Epoch 1621 of 2000 took 0.116s
  training loss:		0.301223
  validation loss:		0.377717
  validation accuracy:		88.15 %
Epoch 1622 of 2000 took 0.109s
  training loss:		0.300011
  validation loss:		0.404589
  validation accuracy:		86.85 %
Epoch 1623 of 2000 took 0.115s
  training loss:		0.298711
  validation loss:		0.383463
  validation accuracy:		87.61 %
Epoch 1624 of 2000 took 0.113s
  training loss:		0.297367
  validation loss:		0.369043
  validation accuracy:		88.04 %
Epoch 1625 of 2000 took 0.110s
  training loss:		0.298880
  validation loss:		0.387913
  validation accuracy:		87.72 %
Epoch 1626 of 2000 took 0.110s
  training loss:		0.302866
  validation loss:		0.375876
  validation accuracy:		87.61 %
Epoch 1627 of 2000 took 0.110s
  training loss:		0.305181
  validation loss:		0.387459
  validation accuracy:		87.72 %
Epoch 1628 of 2000 took 0.117s
  training loss:		0.302764
  validation loss:		0.373069
  validation accuracy:		87.72 %
Epoch 1629 of 2000 took 0.110s
  training loss:		0.302859
  validation loss:		0.370801
  validation accuracy:		87.93 %
Epoch 1630 of 2000 took 0.111s
  training loss:		0.302149
  validation loss:		0.379064
  validation accuracy:		87.83 %
Epoch 1631 of 2000 took 0.118s
  training loss:		0.294067
  validation loss:		0.378465
  validation accuracy:		87.39 %
Epoch 1632 of 2000 took 0.109s
  training loss:		0.298394
  validation loss:		0.390995
  validation accuracy:		87.50 %
Epoch 1633 of 2000 took 0.112s
  training loss:		0.302169
  validation loss:		0.385036
  validation accuracy:		87.72 %
Epoch 1634 of 2000 took 0.109s
  training loss:		0.302536
  validation loss:		0.373109
  validation accuracy:		87.72 %
Epoch 1635 of 2000 took 0.117s
  training loss:		0.302215
  validation loss:		0.388295
  validation accuracy:		87.39 %
Epoch 1636 of 2000 took 0.112s
  training loss:		0.298524
  validation loss:		0.375330
  validation accuracy:		87.93 %
Epoch 1637 of 2000 took 0.109s
  training loss:		0.296561
  validation loss:		0.376489
  validation accuracy:		87.93 %
Epoch 1638 of 2000 took 0.118s
  training loss:		0.299627
  validation loss:		0.381865
  validation accuracy:		87.83 %
Epoch 1639 of 2000 took 0.110s
  training loss:		0.293269
  validation loss:		0.377874
  validation accuracy:		87.61 %
Epoch 1640 of 2000 took 0.112s
  training loss:		0.293167
  validation loss:		0.394873
  validation accuracy:		87.07 %
Epoch 1641 of 2000 took 0.109s
  training loss:		0.303122
  validation loss:		0.384894
  validation accuracy:		87.61 %
Epoch 1642 of 2000 took 0.113s
  training loss:		0.300318
  validation loss:		0.396652
  validation accuracy:		87.17 %
Epoch 1643 of 2000 took 0.117s
  training loss:		0.294645
  validation loss:		0.365881
  validation accuracy:		88.26 %
Epoch 1644 of 2000 took 0.109s
  training loss:		0.303079
  validation loss:		0.381017
  validation accuracy:		87.50 %
Epoch 1645 of 2000 took 0.113s
  training loss:		0.299971
  validation loss:		0.367347
  validation accuracy:		88.59 %
Epoch 1646 of 2000 took 0.115s
  training loss:		0.289501
  validation loss:		0.374403
  validation accuracy:		87.83 %
Epoch 1647 of 2000 took 0.110s
  training loss:		0.298323
  validation loss:		0.374380
  validation accuracy:		88.37 %
Epoch 1648 of 2000 took 0.111s
  training loss:		0.300769
  validation loss:		0.369877
  validation accuracy:		87.72 %
Epoch 1649 of 2000 took 0.110s
  training loss:		0.298304
  validation loss:		0.369690
  validation accuracy:		87.83 %
Epoch 1650 of 2000 took 0.117s
  training loss:		0.303932
  validation loss:		0.367361
  validation accuracy:		88.04 %
Epoch 1651 of 2000 took 0.111s
  training loss:		0.303339
  validation loss:		0.418278
  validation accuracy:		87.07 %
Epoch 1652 of 2000 took 0.111s
  training loss:		0.302264
  validation loss:		0.377802
  validation accuracy:		87.72 %
Epoch 1653 of 2000 took 0.118s
  training loss:		0.307745
  validation loss:		0.369289
  validation accuracy:		87.83 %
Epoch 1654 of 2000 took 0.109s
  training loss:		0.306176
  validation loss:		0.376101
  validation accuracy:		87.93 %
Epoch 1655 of 2000 took 0.112s
  training loss:		0.306080
  validation loss:		0.379721
  validation accuracy:		87.93 %
Epoch 1656 of 2000 took 0.109s
  training loss:		0.299960
  validation loss:		0.383052
  validation accuracy:		87.72 %
Epoch 1657 of 2000 took 0.116s
  training loss:		0.299430
  validation loss:		0.386714
  validation accuracy:		87.72 %
Epoch 1658 of 2000 took 0.113s
  training loss:		0.296783
  validation loss:		0.372518
  validation accuracy:		88.26 %
Epoch 1659 of 2000 took 0.109s
  training loss:		0.295435
  validation loss:		0.396729
  validation accuracy:		87.50 %
Epoch 1660 of 2000 took 0.117s
  training loss:		0.297356
  validation loss:		0.373840
  validation accuracy:		88.37 %
Epoch 1661 of 2000 took 0.111s
  training loss:		0.300996
  validation loss:		0.385718
  validation accuracy:		87.61 %
Epoch 1662 of 2000 took 0.111s
  training loss:		0.295675
  validation loss:		0.372788
  validation accuracy:		88.04 %
Epoch 1663 of 2000 took 0.110s
  training loss:		0.295455
  validation loss:		0.383582
  validation accuracy:		87.28 %
Epoch 1664 of 2000 took 0.112s
  training loss:		0.300856
  validation loss:		0.375722
  validation accuracy:		88.15 %
Epoch 1665 of 2000 took 0.117s
  training loss:		0.303788
  validation loss:		0.392028
  validation accuracy:		87.61 %
Epoch 1666 of 2000 took 0.109s
  training loss:		0.300790
  validation loss:		0.379335
  validation accuracy:		87.83 %
Epoch 1667 of 2000 took 0.112s
  training loss:		0.305765
  validation loss:		0.375689
  validation accuracy:		88.26 %
Epoch 1668 of 2000 took 0.116s
  training loss:		0.297468
  validation loss:		0.366641
  validation accuracy:		87.72 %
Epoch 1669 of 2000 took 0.109s
  training loss:		0.296459
  validation loss:		0.369089
  validation accuracy:		88.37 %
Epoch 1670 of 2000 took 0.111s
  training loss:		0.300083
  validation loss:		0.375251
  validation accuracy:		87.83 %
Epoch 1671 of 2000 took 0.109s
  training loss:		0.301536
  validation loss:		0.381795
  validation accuracy:		87.39 %
Epoch 1672 of 2000 took 0.118s
  training loss:		0.302834
  validation loss:		0.383086
  validation accuracy:		87.50 %
Epoch 1673 of 2000 took 0.111s
  training loss:		0.305273
  validation loss:		0.433702
  validation accuracy:		85.33 %
Epoch 1674 of 2000 took 0.110s
  training loss:		0.299451
  validation loss:		0.395655
  validation accuracy:		87.17 %
Epoch 1675 of 2000 took 0.118s
  training loss:		0.303726
  validation loss:		0.374300
  validation accuracy:		88.37 %
Epoch 1676 of 2000 took 0.109s
  training loss:		0.294376
  validation loss:		0.377575
  validation accuracy:		87.72 %
Epoch 1677 of 2000 took 0.112s
  training loss:		0.302283
  validation loss:		0.390878
  validation accuracy:		87.61 %
Epoch 1678 of 2000 took 0.109s
  training loss:		0.285487
  validation loss:		0.377194
  validation accuracy:		87.93 %
Epoch 1679 of 2000 took 0.114s
  training loss:		0.296642
  validation loss:		0.366108
  validation accuracy:		88.26 %
Epoch 1680 of 2000 took 0.115s
  training loss:		0.295605
  validation loss:		0.383166
  validation accuracy:		87.83 %
Epoch 1681 of 2000 took 0.109s
  training loss:		0.303536
  validation loss:		0.386644
  validation accuracy:		87.93 %
Epoch 1682 of 2000 took 0.116s
  training loss:		0.291762
  validation loss:		0.391729
  validation accuracy:		87.83 %
Epoch 1683 of 2000 took 0.113s
  training loss:		0.301427
  validation loss:		0.374428
  validation accuracy:		87.83 %
Epoch 1684 of 2000 took 0.110s
  training loss:		0.303129
  validation loss:		0.409058
  validation accuracy:		86.96 %
Epoch 1685 of 2000 took 0.110s
  training loss:		0.301758
  validation loss:		0.372636
  validation accuracy:		88.37 %
Epoch 1686 of 2000 took 0.111s
  training loss:		0.295442
  validation loss:		0.376335
  validation accuracy:		87.61 %
Epoch 1687 of 2000 took 0.117s
  training loss:		0.297707
  validation loss:		0.382188
  validation accuracy:		87.72 %
Epoch 1688 of 2000 took 0.110s
  training loss:		0.306285
  validation loss:		0.389445
  validation accuracy:		87.17 %
Epoch 1689 of 2000 took 0.112s
  training loss:		0.303552
  validation loss:		0.377056
  validation accuracy:		88.04 %
Epoch 1690 of 2000 took 0.117s
  training loss:		0.302558
  validation loss:		0.384350
  validation accuracy:		87.39 %
Epoch 1691 of 2000 took 0.109s
  training loss:		0.305650
  validation loss:		0.373585
  validation accuracy:		87.72 %
Epoch 1692 of 2000 took 0.112s
  training loss:		0.298248
  validation loss:		0.367836
  validation accuracy:		88.15 %
Epoch 1693 of 2000 took 0.109s
  training loss:		0.305824
  validation loss:		0.378112
  validation accuracy:		88.04 %
Epoch 1694 of 2000 took 0.117s
  training loss:		0.298034
  validation loss:		0.367959
  validation accuracy:		88.15 %
Epoch 1695 of 2000 took 0.113s
  training loss:		0.297196
  validation loss:		0.367249
  validation accuracy:		88.37 %
Epoch 1696 of 2000 took 0.109s
  training loss:		0.297187
  validation loss:		0.363862
  validation accuracy:		88.37 %
Epoch 1697 of 2000 took 0.118s
  training loss:		0.302954
  validation loss:		0.393024
  validation accuracy:		87.17 %
Epoch 1698 of 2000 took 0.110s
  training loss:		0.298702
  validation loss:		0.370479
  validation accuracy:		88.04 %
Epoch 1699 of 2000 took 0.111s
  training loss:		0.295618
  validation loss:		0.375709
  validation accuracy:		88.15 %
Epoch 1700 of 2000 took 0.109s
  training loss:		0.290748
  validation loss:		0.372049
  validation accuracy:		87.61 %
Epoch 1701 of 2000 took 0.113s
  training loss:		0.295636
  validation loss:		0.392907
  validation accuracy:		87.28 %
Epoch 1702 of 2000 took 0.116s
  training loss:		0.296104
  validation loss:		0.386938
  validation accuracy:		87.61 %
Epoch 1703 of 2000 took 0.109s
  training loss:		0.294037
  validation loss:		0.396959
  validation accuracy:		87.39 %
Epoch 1704 of 2000 took 0.114s
  training loss:		0.298170
  validation loss:		0.380688
  validation accuracy:		88.04 %
Epoch 1705 of 2000 took 0.115s
  training loss:		0.298377
  validation loss:		0.367474
  validation accuracy:		88.15 %
Epoch 1706 of 2000 took 0.110s
  training loss:		0.297930
  validation loss:		0.392442
  validation accuracy:		87.72 %
Epoch 1707 of 2000 took 0.111s
  training loss:		0.296001
  validation loss:		0.395709
  validation accuracy:		87.50 %
Epoch 1708 of 2000 took 0.110s
  training loss:		0.296983
  validation loss:		0.377886
  validation accuracy:		87.61 %
Epoch 1709 of 2000 took 0.117s
  training loss:		0.294041
  validation loss:		0.388278
  validation accuracy:		87.83 %
Epoch 1710 of 2000 took 0.111s
  training loss:		0.295510
  validation loss:		0.378729
  validation accuracy:		87.83 %
Epoch 1711 of 2000 took 0.111s
  training loss:		0.295446
  validation loss:		0.367982
  validation accuracy:		88.37 %
Epoch 1712 of 2000 took 0.118s
  training loss:		0.296832
  validation loss:		0.381845
  validation accuracy:		87.93 %
Epoch 1713 of 2000 took 0.109s
  training loss:		0.305415
  validation loss:		0.373736
  validation accuracy:		87.93 %
Epoch 1714 of 2000 took 0.112s
  training loss:		0.300309
  validation loss:		0.373222
  validation accuracy:		87.83 %
Epoch 1715 of 2000 took 0.109s
  training loss:		0.300430
  validation loss:		0.371443
  validation accuracy:		88.04 %
Epoch 1716 of 2000 took 0.115s
  training loss:		0.298186
  validation loss:		0.371675
  validation accuracy:		88.15 %
Epoch 1717 of 2000 took 0.114s
  training loss:		0.300906
  validation loss:		0.367152
  validation accuracy:		88.15 %
Epoch 1718 of 2000 took 0.109s
  training loss:		0.298951
  validation loss:		0.376581
  validation accuracy:		88.15 %
Epoch 1719 of 2000 took 0.116s
  training loss:		0.294408
  validation loss:		0.366734
  validation accuracy:		88.15 %
Epoch 1720 of 2000 took 0.112s
  training loss:		0.301242
  validation loss:		0.380427
  validation accuracy:		88.15 %
Epoch 1721 of 2000 took 0.111s
  training loss:		0.298661
  validation loss:		0.378263
  validation accuracy:		87.83 %
Epoch 1722 of 2000 took 0.110s
  training loss:		0.300728
  validation loss:		0.385220
  validation accuracy:		87.72 %
Epoch 1723 of 2000 took 0.111s
  training loss:		0.309660
  validation loss:		0.376171
  validation accuracy:		87.83 %
Epoch 1724 of 2000 took 0.117s
  training loss:		0.298245
  validation loss:		0.388575
  validation accuracy:		87.83 %
Epoch 1725 of 2000 took 0.109s
  training loss:		0.293242
  validation loss:		0.376073
  validation accuracy:		87.83 %
Epoch 1726 of 2000 took 0.112s
  training loss:		0.300663
  validation loss:		0.400371
  validation accuracy:		87.39 %
Epoch 1727 of 2000 took 0.117s
  training loss:		0.295589
  validation loss:		0.393295
  validation accuracy:		87.50 %
Epoch 1728 of 2000 took 0.109s
  training loss:		0.308233
  validation loss:		0.381059
  validation accuracy:		87.61 %
Epoch 1729 of 2000 took 0.112s
  training loss:		0.300814
  validation loss:		0.401984
  validation accuracy:		86.63 %
Epoch 1730 of 2000 took 0.109s
  training loss:		0.300200
  validation loss:		0.384287
  validation accuracy:		87.83 %
Epoch 1731 of 2000 took 0.118s
  training loss:		0.294837
  validation loss:		0.380979
  validation accuracy:		87.72 %
Epoch 1732 of 2000 took 0.112s
  training loss:		0.290995
  validation loss:		0.367791
  validation accuracy:		87.83 %
Epoch 1733 of 2000 took 0.109s
  training loss:		0.301976
  validation loss:		0.369289
  validation accuracy:		88.15 %
Epoch 1734 of 2000 took 0.118s
  training loss:		0.297433
  validation loss:		0.370201
  validation accuracy:		88.59 %
Epoch 1735 of 2000 took 0.109s
  training loss:		0.296661
  validation loss:		0.374575
  validation accuracy:		87.72 %
Epoch 1736 of 2000 took 0.112s
  training loss:		0.301621
  validation loss:		0.386046
  validation accuracy:		87.93 %
Epoch 1737 of 2000 took 0.109s
  training loss:		0.311980
  validation loss:		0.385302
  validation accuracy:		87.93 %
Epoch 1738 of 2000 took 0.113s
  training loss:		0.296586
  validation loss:		0.368279
  validation accuracy:		88.48 %
Epoch 1739 of 2000 took 0.116s
  training loss:		0.298394
  validation loss:		0.369750
  validation accuracy:		88.04 %
Epoch 1740 of 2000 took 0.108s
  training loss:		0.301234
  validation loss:		0.375274
  validation accuracy:		88.37 %
Epoch 1741 of 2000 took 0.115s
  training loss:		0.302719
  validation loss:		0.374766
  validation accuracy:		88.15 %
Epoch 1742 of 2000 took 0.114s
  training loss:		0.290915
  validation loss:		0.370646
  validation accuracy:		88.26 %
Epoch 1743 of 2000 took 0.110s
  training loss:		0.295802
  validation loss:		0.377899
  validation accuracy:		88.04 %
Epoch 1744 of 2000 took 0.110s
  training loss:		0.297586
  validation loss:		0.377082
  validation accuracy:		87.72 %
Epoch 1745 of 2000 took 0.110s
  training loss:		0.314258
  validation loss:		0.370011
  validation accuracy:		88.04 %
Epoch 1746 of 2000 took 0.117s
  training loss:		0.295420
  validation loss:		0.396242
  validation accuracy:		87.50 %
Epoch 1747 of 2000 took 0.111s
  training loss:		0.297611
  validation loss:		0.387183
  validation accuracy:		87.39 %
Epoch 1748 of 2000 took 0.111s
  training loss:		0.299674
  validation loss:		0.381530
  validation accuracy:		87.93 %
Epoch 1749 of 2000 took 0.118s
  training loss:		0.295014
  validation loss:		0.391138
  validation accuracy:		87.50 %
Epoch 1750 of 2000 took 0.109s
  training loss:		0.298208
  validation loss:		0.381409
  validation accuracy:		87.83 %
Epoch 1751 of 2000 took 0.112s
  training loss:		0.302674
  validation loss:		0.379544
  validation accuracy:		88.04 %
Epoch 1752 of 2000 took 0.109s
  training loss:		0.298553
  validation loss:		0.375583
  validation accuracy:		87.83 %
Epoch 1753 of 2000 took 0.117s
  training loss:		0.300618
  validation loss:		0.387595
  validation accuracy:		87.83 %
Epoch 1754 of 2000 took 0.112s
  training loss:		0.295911
  validation loss:		0.377607
  validation accuracy:		88.15 %
Epoch 1755 of 2000 took 0.109s
  training loss:		0.293143
  validation loss:		0.391945
  validation accuracy:		87.50 %
Epoch 1756 of 2000 took 0.118s
  training loss:		0.295123
  validation loss:		0.384101
  validation accuracy:		87.83 %
Epoch 1757 of 2000 took 0.115s
  training loss:		0.295940
  validation loss:		0.373613
  validation accuracy:		88.04 %
Epoch 1758 of 2000 took 0.109s
  training loss:		0.299017
  validation loss:		0.380400
  validation accuracy:		87.93 %
Epoch 1759 of 2000 took 0.107s
  training loss:		0.300536
  validation loss:		0.399816
  validation accuracy:		87.39 %
Epoch 1760 of 2000 took 0.109s
  training loss:		0.303199
  validation loss:		0.395521
  validation accuracy:		87.50 %
Epoch 1761 of 2000 took 0.113s
  training loss:		0.298255
  validation loss:		0.382007
  validation accuracy:		87.93 %
Epoch 1762 of 2000 took 0.107s
  training loss:		0.299212
  validation loss:		0.378429
  validation accuracy:		88.26 %
Epoch 1763 of 2000 took 0.109s
  training loss:		0.297136
  validation loss:		0.383805
  validation accuracy:		87.93 %
Epoch 1764 of 2000 took 0.114s
  training loss:		0.298282
  validation loss:		0.386643
  validation accuracy:		87.83 %
Epoch 1765 of 2000 took 0.107s
  training loss:		0.301849
  validation loss:		0.375763
  validation accuracy:		87.93 %
Epoch 1766 of 2000 took 0.109s
  training loss:		0.296099
  validation loss:		0.371138
  validation accuracy:		88.04 %
Epoch 1767 of 2000 took 0.106s
  training loss:		0.292140
  validation loss:		0.366798
  validation accuracy:		88.26 %
Epoch 1768 of 2000 took 0.113s
  training loss:		0.299187
  validation loss:		0.380573
  validation accuracy:		87.83 %
Epoch 1769 of 2000 took 0.110s
  training loss:		0.298567
  validation loss:		0.371119
  validation accuracy:		88.26 %
Epoch 1770 of 2000 took 0.106s
  training loss:		0.305643
  validation loss:		0.392505
  validation accuracy:		87.50 %
Epoch 1771 of 2000 took 0.113s
  training loss:		0.301223
  validation loss:		0.379730
  validation accuracy:		87.93 %
Epoch 1772 of 2000 took 0.110s
  training loss:		0.295101
  validation loss:		0.378302
  validation accuracy:		88.15 %
Epoch 1773 of 2000 took 0.109s
  training loss:		0.300360
  validation loss:		0.390684
  validation accuracy:		87.72 %
Epoch 1774 of 2000 took 0.109s
  training loss:		0.300819
  validation loss:		0.381062
  validation accuracy:		87.83 %
Epoch 1775 of 2000 took 0.109s
  training loss:		0.295019
  validation loss:		0.382720
  validation accuracy:		87.61 %
Epoch 1776 of 2000 took 0.114s
  training loss:		0.298425
  validation loss:		0.406783
  validation accuracy:		87.07 %
Epoch 1777 of 2000 took 0.108s
  training loss:		0.296243
  validation loss:		0.382017
  validation accuracy:		88.04 %
Epoch 1778 of 2000 took 0.110s
  training loss:		0.298243
  validation loss:		0.367220
  validation accuracy:		88.37 %
Epoch 1779 of 2000 took 0.115s
  training loss:		0.305431
  validation loss:		0.384233
  validation accuracy:		87.83 %
Epoch 1780 of 2000 took 0.108s
  training loss:		0.298194
  validation loss:		0.388310
  validation accuracy:		87.50 %
Epoch 1781 of 2000 took 0.110s
  training loss:		0.293690
  validation loss:		0.369104
  validation accuracy:		88.26 %
Epoch 1782 of 2000 took 0.107s
  training loss:		0.296932
  validation loss:		0.379136
  validation accuracy:		88.04 %
Epoch 1783 of 2000 took 0.114s
  training loss:		0.295184
  validation loss:		0.385498
  validation accuracy:		87.72 %
Epoch 1784 of 2000 took 0.111s
  training loss:		0.298636
  validation loss:		0.369804
  validation accuracy:		88.37 %
Epoch 1785 of 2000 took 0.107s
  training loss:		0.298466
  validation loss:		0.371732
  validation accuracy:		88.04 %
Epoch 1786 of 2000 took 0.115s
  training loss:		0.295669
  validation loss:		0.372412
  validation accuracy:		88.26 %
Epoch 1787 of 2000 took 0.110s
  training loss:		0.301741
  validation loss:		0.373942
  validation accuracy:		88.48 %
Epoch 1788 of 2000 took 0.109s
  training loss:		0.299088
  validation loss:		0.372304
  validation accuracy:		88.37 %
Epoch 1789 of 2000 took 0.108s
  training loss:		0.296297
  validation loss:		0.382075
  validation accuracy:		87.83 %
Epoch 1790 of 2000 took 0.110s
  training loss:		0.300072
  validation loss:		0.374761
  validation accuracy:		88.04 %
Epoch 1791 of 2000 took 0.114s
  training loss:		0.292332
  validation loss:		0.381056
  validation accuracy:		87.93 %
Epoch 1792 of 2000 took 0.109s
  training loss:		0.294700
  validation loss:		0.385263
  validation accuracy:		88.04 %
Epoch 1793 of 2000 took 0.110s
  training loss:		0.295089
  validation loss:		0.390232
  validation accuracy:		87.61 %
Epoch 1794 of 2000 took 0.115s
  training loss:		0.298112
  validation loss:		0.380272
  validation accuracy:		88.15 %
Epoch 1795 of 2000 took 0.108s
  training loss:		0.296502
  validation loss:		0.400498
  validation accuracy:		87.50 %
Epoch 1796 of 2000 took 0.110s
  training loss:		0.302562
  validation loss:		0.378026
  validation accuracy:		87.93 %
Epoch 1797 of 2000 took 0.108s
  training loss:		0.298822
  validation loss:		0.394061
  validation accuracy:		87.83 %
Epoch 1798 of 2000 took 0.114s
  training loss:		0.298037
  validation loss:		0.378923
  validation accuracy:		88.26 %
Epoch 1799 of 2000 took 0.111s
  training loss:		0.294042
  validation loss:		0.372776
  validation accuracy:		88.48 %
Epoch 1800 of 2000 took 0.107s
  training loss:		0.297285
  validation loss:		0.384030
  validation accuracy:		88.15 %
Epoch 1801 of 2000 took 0.115s
  training loss:		0.297054
  validation loss:		0.389827
  validation accuracy:		87.93 %
Epoch 1802 of 2000 took 0.110s
  training loss:		0.301497
  validation loss:		0.367667
  validation accuracy:		88.04 %
Epoch 1803 of 2000 took 0.109s
  training loss:		0.302446
  validation loss:		0.372714
  validation accuracy:		88.15 %
Epoch 1804 of 2000 took 0.108s
  training loss:		0.300790
  validation loss:		0.392731
  validation accuracy:		87.61 %
Epoch 1805 of 2000 took 0.110s
  training loss:		0.295218
  validation loss:		0.375444
  validation accuracy:		88.15 %
Epoch 1806 of 2000 took 0.114s
  training loss:		0.292006
  validation loss:		0.391754
  validation accuracy:		87.93 %
Epoch 1807 of 2000 took 0.108s
  training loss:		0.298609
  validation loss:		0.369092
  validation accuracy:		88.59 %
Epoch 1808 of 2000 took 0.110s
  training loss:		0.297988
  validation loss:		0.379927
  validation accuracy:		88.15 %
Epoch 1809 of 2000 took 0.115s
  training loss:		0.300616
  validation loss:		0.388260
  validation accuracy:		87.83 %
Epoch 1810 of 2000 took 0.108s
  training loss:		0.295597
  validation loss:		0.377585
  validation accuracy:		88.26 %
Epoch 1811 of 2000 took 0.110s
  training loss:		0.293045
  validation loss:		0.383917
  validation accuracy:		88.26 %
Epoch 1812 of 2000 took 0.107s
  training loss:		0.305539
  validation loss:		0.383670
  validation accuracy:		87.72 %
Epoch 1813 of 2000 took 0.114s
  training loss:		0.297768
  validation loss:		0.399217
  validation accuracy:		87.50 %
Epoch 1814 of 2000 took 0.111s
  training loss:		0.301973
  validation loss:		0.401631
  validation accuracy:		86.85 %
Epoch 1815 of 2000 took 0.107s
  training loss:		0.294910
  validation loss:		0.370561
  validation accuracy:		88.37 %
Epoch 1816 of 2000 took 0.115s
  training loss:		0.290861
  validation loss:		0.383197
  validation accuracy:		88.15 %
Epoch 1817 of 2000 took 0.110s
  training loss:		0.298209
  validation loss:		0.379977
  validation accuracy:		88.04 %
Epoch 1818 of 2000 took 0.109s
  training loss:		0.295798
  validation loss:		0.392089
  validation accuracy:		87.61 %
Epoch 1819 of 2000 took 0.108s
  training loss:		0.300793
  validation loss:		0.372872
  validation accuracy:		88.70 %
Epoch 1820 of 2000 took 0.110s
  training loss:		0.299774
  validation loss:		0.373172
  validation accuracy:		88.59 %
Epoch 1821 of 2000 took 0.114s
  training loss:		0.301106
  validation loss:		0.382334
  validation accuracy:		87.83 %
Epoch 1822 of 2000 took 0.108s
  training loss:		0.295648
  validation loss:		0.371619
  validation accuracy:		88.26 %
Epoch 1823 of 2000 took 0.110s
  training loss:		0.301158
  validation loss:		0.365665
  validation accuracy:		88.26 %
Epoch 1824 of 2000 took 0.115s
  training loss:		0.299649
  validation loss:		0.397170
  validation accuracy:		87.72 %
Epoch 1825 of 2000 took 0.108s
  training loss:		0.297693
  validation loss:		0.401433
  validation accuracy:		86.85 %
Epoch 1826 of 2000 took 0.110s
  training loss:		0.300461
  validation loss:		0.380129
  validation accuracy:		87.83 %
Epoch 1827 of 2000 took 0.107s
  training loss:		0.300941
  validation loss:		0.371805
  validation accuracy:		88.59 %
Epoch 1828 of 2000 took 0.114s
  training loss:		0.293927
  validation loss:		0.393391
  validation accuracy:		87.39 %
Epoch 1829 of 2000 took 0.111s
  training loss:		0.298859
  validation loss:		0.378662
  validation accuracy:		88.15 %
Epoch 1830 of 2000 took 0.107s
  training loss:		0.298030
  validation loss:		0.392138
  validation accuracy:		87.39 %
Epoch 1831 of 2000 took 0.115s
  training loss:		0.293347
  validation loss:		0.376806
  validation accuracy:		88.26 %
Epoch 1832 of 2000 took 0.110s
  training loss:		0.297982
  validation loss:		0.391523
  validation accuracy:		87.72 %
Epoch 1833 of 2000 took 0.109s
  training loss:		0.290190
  validation loss:		0.378858
  validation accuracy:		87.93 %
Epoch 1834 of 2000 took 0.108s
  training loss:		0.295431
  validation loss:		0.362310
  validation accuracy:		88.91 %
Epoch 1835 of 2000 took 0.110s
  training loss:		0.298803
  validation loss:		0.398793
  validation accuracy:		87.50 %
Epoch 1836 of 2000 took 0.105s
  training loss:		0.302604
  validation loss:		0.391439
  validation accuracy:		87.72 %
Epoch 1837 of 2000 took 0.105s
  training loss:		0.294831
  validation loss:		0.376496
  validation accuracy:		88.15 %
Epoch 1838 of 2000 took 0.105s
  training loss:		0.292198
  validation loss:		0.398625
  validation accuracy:		87.50 %
Epoch 1839 of 2000 took 0.105s
  training loss:		0.292747
  validation loss:		0.376736
  validation accuracy:		88.37 %
Epoch 1840 of 2000 took 0.105s
  training loss:		0.297630
  validation loss:		0.376216
  validation accuracy:		88.15 %
Epoch 1841 of 2000 took 0.105s
  training loss:		0.302571
  validation loss:		0.396310
  validation accuracy:		87.61 %
Epoch 1842 of 2000 took 0.105s
  training loss:		0.291524
  validation loss:		0.363667
  validation accuracy:		88.80 %
Epoch 1843 of 2000 took 0.105s
  training loss:		0.297339
  validation loss:		0.400279
  validation accuracy:		87.50 %
Epoch 1844 of 2000 took 0.105s
  training loss:		0.295157
  validation loss:		0.372311
  validation accuracy:		88.48 %
Epoch 1845 of 2000 took 0.105s
  training loss:		0.294129
  validation loss:		0.374625
  validation accuracy:		88.04 %
Epoch 1846 of 2000 took 0.105s
  training loss:		0.302383
  validation loss:		0.384451
  validation accuracy:		88.15 %
Epoch 1847 of 2000 took 0.105s
  training loss:		0.304396
  validation loss:		0.391070
  validation accuracy:		87.50 %
Epoch 1848 of 2000 took 0.105s
  training loss:		0.290283
  validation loss:		0.402463
  validation accuracy:		86.85 %
Epoch 1849 of 2000 took 0.105s
  training loss:		0.300245
  validation loss:		0.442113
  validation accuracy:		86.30 %
Epoch 1850 of 2000 took 0.105s
  training loss:		0.297541
  validation loss:		0.368849
  validation accuracy:		88.59 %
Epoch 1851 of 2000 took 0.097s
  training loss:		0.303034
  validation loss:		0.380246
  validation accuracy:		88.37 %
Epoch 1852 of 2000 took 0.096s
  training loss:		0.294489
  validation loss:		0.387747
  validation accuracy:		88.04 %
Epoch 1853 of 2000 took 0.100s
  training loss:		0.297357
  validation loss:		0.395142
  validation accuracy:		87.83 %
Epoch 1854 of 2000 took 0.105s
  training loss:		0.287829
  validation loss:		0.389627
  validation accuracy:		87.83 %
Epoch 1855 of 2000 took 0.105s
  training loss:		0.298402
  validation loss:		0.369332
  validation accuracy:		88.26 %
Epoch 1856 of 2000 took 0.100s
  training loss:		0.290253
  validation loss:		0.381883
  validation accuracy:		87.93 %
Epoch 1857 of 2000 took 0.100s
  training loss:		0.302355
  validation loss:		0.377609
  validation accuracy:		88.26 %
Epoch 1858 of 2000 took 0.100s
  training loss:		0.289183
  validation loss:		0.374764
  validation accuracy:		88.15 %
Epoch 1859 of 2000 took 0.100s
  training loss:		0.299071
  validation loss:		0.388141
  validation accuracy:		87.72 %
Epoch 1860 of 2000 took 0.106s
  training loss:		0.289808
  validation loss:		0.380685
  validation accuracy:		88.48 %
Epoch 1861 of 2000 took 0.100s
  training loss:		0.289382
  validation loss:		0.374060
  validation accuracy:		88.70 %
Epoch 1862 of 2000 took 0.100s
  training loss:		0.303322
  validation loss:		0.379444
  validation accuracy:		88.26 %
Epoch 1863 of 2000 took 0.100s
  training loss:		0.292554
  validation loss:		0.390387
  validation accuracy:		87.50 %
Epoch 1864 of 2000 took 0.100s
  training loss:		0.296487
  validation loss:		0.402262
  validation accuracy:		87.61 %
Epoch 1865 of 2000 took 0.100s
  training loss:		0.293999
  validation loss:		0.396661
  validation accuracy:		87.83 %
Epoch 1866 of 2000 took 0.107s
  training loss:		0.296070
  validation loss:		0.402709
  validation accuracy:		86.96 %
Epoch 1867 of 2000 took 0.104s
  training loss:		0.301513
  validation loss:		0.376634
  validation accuracy:		88.15 %
Epoch 1868 of 2000 took 0.100s
  training loss:		0.297173
  validation loss:		0.388706
  validation accuracy:		87.61 %
Epoch 1869 of 2000 took 0.103s
  training loss:		0.292258
  validation loss:		0.378370
  validation accuracy:		88.04 %
Epoch 1870 of 2000 took 0.103s
  training loss:		0.294137
  validation loss:		0.367089
  validation accuracy:		88.80 %
Epoch 1871 of 2000 took 0.103s
  training loss:		0.299540
  validation loss:		0.382718
  validation accuracy:		87.83 %
Epoch 1872 of 2000 took 0.103s
  training loss:		0.290629
  validation loss:		0.380381
  validation accuracy:		88.48 %
Epoch 1873 of 2000 took 0.103s
  training loss:		0.295373
  validation loss:		0.367623
  validation accuracy:		88.48 %
Epoch 1874 of 2000 took 0.103s
  training loss:		0.298285
  validation loss:		0.390001
  validation accuracy:		87.72 %
Epoch 1875 of 2000 took 0.103s
  training loss:		0.286770
  validation loss:		0.373030
  validation accuracy:		88.26 %
Epoch 1876 of 2000 took 0.103s
  training loss:		0.297393
  validation loss:		0.373044
  validation accuracy:		88.59 %
Epoch 1877 of 2000 took 0.103s
  training loss:		0.297116
  validation loss:		0.393053
  validation accuracy:		87.61 %
Epoch 1878 of 2000 took 0.103s
  training loss:		0.295002
  validation loss:		0.374280
  validation accuracy:		88.37 %
Epoch 1879 of 2000 took 0.103s
  training loss:		0.298805
  validation loss:		0.378162
  validation accuracy:		88.04 %
Epoch 1880 of 2000 took 0.104s
  training loss:		0.285212
  validation loss:		0.391264
  validation accuracy:		87.50 %
Epoch 1881 of 2000 took 0.103s
  training loss:		0.295742
  validation loss:		0.398047
  validation accuracy:		87.50 %
Epoch 1882 of 2000 took 0.103s
  training loss:		0.299999
  validation loss:		0.383556
  validation accuracy:		88.15 %
Epoch 1883 of 2000 took 0.103s
  training loss:		0.295672
  validation loss:		0.370343
  validation accuracy:		88.37 %
Epoch 1884 of 2000 took 0.103s
  training loss:		0.298012
  validation loss:		0.387197
  validation accuracy:		88.15 %
Epoch 1885 of 2000 took 0.103s
  training loss:		0.298371
  validation loss:		0.385170
  validation accuracy:		87.93 %
Epoch 1886 of 2000 took 0.103s
  training loss:		0.289612
  validation loss:		0.375905
  validation accuracy:		88.48 %
Epoch 1887 of 2000 took 0.103s
  training loss:		0.289255
  validation loss:		0.381071
  validation accuracy:		87.83 %
Epoch 1888 of 2000 took 0.103s
  training loss:		0.288790
  validation loss:		0.387316
  validation accuracy:		88.26 %
Epoch 1889 of 2000 took 0.110s
  training loss:		0.298108
  validation loss:		0.388479
  validation accuracy:		88.15 %
Epoch 1890 of 2000 took 0.107s
  training loss:		0.301064
  validation loss:		0.375171
  validation accuracy:		88.26 %
Epoch 1891 of 2000 took 0.106s
  training loss:		0.300622
  validation loss:		0.384456
  validation accuracy:		88.26 %
Epoch 1892 of 2000 took 0.106s
  training loss:		0.295433
  validation loss:		0.387413
  validation accuracy:		87.93 %
Epoch 1893 of 2000 took 0.106s
  training loss:		0.295577
  validation loss:		0.382466
  validation accuracy:		87.93 %
Epoch 1894 of 2000 took 0.106s
  training loss:		0.291481
  validation loss:		0.441672
  validation accuracy:		85.76 %
Epoch 1895 of 2000 took 0.106s
  training loss:		0.296213
  validation loss:		0.395212
  validation accuracy:		87.39 %
Epoch 1896 of 2000 took 0.106s
  training loss:		0.296079
  validation loss:		0.389141
  validation accuracy:		87.93 %
Epoch 1897 of 2000 took 0.106s
  training loss:		0.290228
  validation loss:		0.384669
  validation accuracy:		88.04 %
Epoch 1898 of 2000 took 0.106s
  training loss:		0.296841
  validation loss:		0.386022
  validation accuracy:		87.93 %
Epoch 1899 of 2000 took 0.106s
  training loss:		0.296497
  validation loss:		0.370688
  validation accuracy:		88.48 %
Epoch 1900 of 2000 took 0.104s
  training loss:		0.299730
  validation loss:		0.372522
  validation accuracy:		88.37 %
Epoch 1901 of 2000 took 0.103s
  training loss:		0.298529
  validation loss:		0.394219
  validation accuracy:		88.15 %
Epoch 1902 of 2000 took 0.103s
  training loss:		0.296175
  validation loss:		0.389797
  validation accuracy:		87.83 %
Epoch 1903 of 2000 took 0.103s
  training loss:		0.292675
  validation loss:		0.374255
  validation accuracy:		88.26 %
Epoch 1904 of 2000 took 0.103s
  training loss:		0.293599
  validation loss:		0.400030
  validation accuracy:		87.83 %
Epoch 1905 of 2000 took 0.103s
  training loss:		0.289335
  validation loss:		0.388923
  validation accuracy:		88.04 %
Epoch 1906 of 2000 took 0.103s
  training loss:		0.296839
  validation loss:		0.369884
  validation accuracy:		88.59 %
Epoch 1907 of 2000 took 0.103s
  training loss:		0.290970
  validation loss:		0.378759
  validation accuracy:		88.48 %
Epoch 1908 of 2000 took 0.103s
  training loss:		0.291571
  validation loss:		0.377016
  validation accuracy:		88.37 %
Epoch 1909 of 2000 took 0.103s
  training loss:		0.296712
  validation loss:		0.375627
  validation accuracy:		88.15 %
Epoch 1910 of 2000 took 0.103s
  training loss:		0.282758
  validation loss:		0.413175
  validation accuracy:		86.20 %
Epoch 1911 of 2000 took 0.103s
  training loss:		0.296273
  validation loss:		0.390791
  validation accuracy:		87.93 %
Epoch 1912 of 2000 took 0.103s
  training loss:		0.304542
  validation loss:		0.382396
  validation accuracy:		88.15 %
Epoch 1913 of 2000 took 0.103s
  training loss:		0.292747
  validation loss:		0.382999
  validation accuracy:		87.83 %
Epoch 1914 of 2000 took 0.103s
  training loss:		0.294096
  validation loss:		0.387727
  validation accuracy:		88.48 %
Epoch 1915 of 2000 took 0.103s
  training loss:		0.300357
  validation loss:		0.377219
  validation accuracy:		88.15 %
Epoch 1916 of 2000 took 0.103s
  training loss:		0.294996
  validation loss:		0.386745
  validation accuracy:		88.15 %
Epoch 1917 of 2000 took 0.103s
  training loss:		0.292982
  validation loss:		0.388507
  validation accuracy:		88.04 %
Epoch 1918 of 2000 took 0.110s
  training loss:		0.297900
  validation loss:		0.374858
  validation accuracy:		88.04 %
Epoch 1919 of 2000 took 0.103s
  training loss:		0.295830
  validation loss:		0.380667
  validation accuracy:		88.48 %
Epoch 1920 of 2000 took 0.103s
  training loss:		0.296819
  validation loss:		0.383200
  validation accuracy:		88.15 %
Epoch 1921 of 2000 took 0.103s
  training loss:		0.299302
  validation loss:		0.388726
  validation accuracy:		87.72 %
Epoch 1922 of 2000 took 0.103s
  training loss:		0.292041
  validation loss:		0.370768
  validation accuracy:		88.59 %
Epoch 1923 of 2000 took 0.103s
  training loss:		0.302276
  validation loss:		0.379475
  validation accuracy:		88.37 %
Epoch 1924 of 2000 took 0.103s
  training loss:		0.297679
  validation loss:		0.386289
  validation accuracy:		88.48 %
Epoch 1925 of 2000 took 0.103s
  training loss:		0.292420
  validation loss:		0.380913
  validation accuracy:		88.26 %
Epoch 1926 of 2000 took 0.103s
  training loss:		0.291549
  validation loss:		0.386916
  validation accuracy:		88.26 %
Epoch 1927 of 2000 took 0.103s
  training loss:		0.296864
  validation loss:		0.370124
  validation accuracy:		88.70 %
Epoch 1928 of 2000 took 0.103s
  training loss:		0.295565
  validation loss:		0.366800
  validation accuracy:		88.59 %
Epoch 1929 of 2000 took 0.103s
  training loss:		0.304242
  validation loss:		0.387468
  validation accuracy:		88.04 %
Epoch 1930 of 2000 took 0.103s
  training loss:		0.285123
  validation loss:		0.388587
  validation accuracy:		88.15 %
Epoch 1931 of 2000 took 0.103s
  training loss:		0.296450
  validation loss:		0.376153
  validation accuracy:		88.48 %
Epoch 1932 of 2000 took 0.103s
  training loss:		0.304602
  validation loss:		0.379008
  validation accuracy:		88.48 %
Epoch 1933 of 2000 took 0.103s
  training loss:		0.291646
  validation loss:		0.380304
  validation accuracy:		88.59 %
Epoch 1934 of 2000 took 0.103s
  training loss:		0.298670
  validation loss:		0.371077
  validation accuracy:		88.80 %
Epoch 1935 of 2000 took 0.103s
  training loss:		0.292840
  validation loss:		0.387346
  validation accuracy:		88.04 %
Epoch 1936 of 2000 took 0.103s
  training loss:		0.292436
  validation loss:		0.384715
  validation accuracy:		88.15 %
Epoch 1937 of 2000 took 0.103s
  training loss:		0.299792
  validation loss:		0.375519
  validation accuracy:		88.37 %
Epoch 1938 of 2000 took 0.103s
  training loss:		0.293603
  validation loss:		0.372051
  validation accuracy:		88.48 %
Epoch 1939 of 2000 took 0.103s
  training loss:		0.294796
  validation loss:		0.369664
  validation accuracy:		88.70 %
Epoch 1940 of 2000 took 0.103s
  training loss:		0.294813
  validation loss:		0.387356
  validation accuracy:		87.93 %
Epoch 1941 of 2000 took 0.103s
  training loss:		0.290138
  validation loss:		0.395330
  validation accuracy:		87.39 %
Epoch 1942 of 2000 took 0.103s
  training loss:		0.297814
  validation loss:		0.372421
  validation accuracy:		88.59 %
Epoch 1943 of 2000 took 0.103s
  training loss:		0.291672
  validation loss:		0.381402
  validation accuracy:		88.59 %
Epoch 1944 of 2000 took 0.103s
  training loss:		0.288225
  validation loss:		0.385620
  validation accuracy:		88.04 %
Epoch 1945 of 2000 took 0.103s
  training loss:		0.297453
  validation loss:		0.378997
  validation accuracy:		88.59 %
Epoch 1946 of 2000 took 0.103s
  training loss:		0.289248
  validation loss:		0.378560
  validation accuracy:		88.70 %
Epoch 1947 of 2000 took 0.104s
  training loss:		0.293968
  validation loss:		0.376264
  validation accuracy:		88.59 %
Epoch 1948 of 2000 took 0.103s
  training loss:		0.292876
  validation loss:		0.381344
  validation accuracy:		87.93 %
Epoch 1949 of 2000 took 0.103s
  training loss:		0.298148
  validation loss:		0.378061
  validation accuracy:		87.93 %
Epoch 1950 of 2000 took 0.103s
  training loss:		0.291298
  validation loss:		0.372608
  validation accuracy:		88.59 %
Epoch 1951 of 2000 took 0.103s
  training loss:		0.295866
  validation loss:		0.388487
  validation accuracy:		88.04 %
Epoch 1952 of 2000 took 0.103s
  training loss:		0.292371
  validation loss:		0.417559
  validation accuracy:		86.09 %
Epoch 1953 of 2000 took 0.103s
  training loss:		0.292833
  validation loss:		0.380434
  validation accuracy:		88.15 %
Epoch 1954 of 2000 took 0.103s
  training loss:		0.295486
  validation loss:		0.368993
  validation accuracy:		88.91 %
Epoch 1955 of 2000 took 0.103s
  training loss:		0.297413
  validation loss:		0.375851
  validation accuracy:		87.83 %
Epoch 1956 of 2000 took 0.103s
  training loss:		0.296333
  validation loss:		0.401730
  validation accuracy:		87.93 %
Epoch 1957 of 2000 took 0.103s
  training loss:		0.292229
  validation loss:		0.381681
  validation accuracy:		88.04 %
Epoch 1958 of 2000 took 0.103s
  training loss:		0.293117
  validation loss:		0.398520
  validation accuracy:		87.50 %
Epoch 1959 of 2000 took 0.103s
  training loss:		0.297198
  validation loss:		0.398720
  validation accuracy:		87.39 %
Epoch 1960 of 2000 took 0.103s
  training loss:		0.293817
  validation loss:		0.372213
  validation accuracy:		88.91 %
Epoch 1961 of 2000 took 0.103s
  training loss:		0.295630
  validation loss:		0.387172
  validation accuracy:		88.15 %
Epoch 1962 of 2000 took 0.103s
  training loss:		0.292040
  validation loss:		0.399216
  validation accuracy:		87.83 %
Epoch 1963 of 2000 took 0.104s
  training loss:		0.298614
  validation loss:		0.389518
  validation accuracy:		87.93 %
Epoch 1964 of 2000 took 0.103s
  training loss:		0.284694
  validation loss:		0.387188
  validation accuracy:		88.48 %
Epoch 1965 of 2000 took 0.103s
  training loss:		0.292546
  validation loss:		0.394815
  validation accuracy:		87.50 %
Epoch 1966 of 2000 took 0.103s
  training loss:		0.296084
  validation loss:		0.374313
  validation accuracy:		88.59 %
Epoch 1967 of 2000 took 0.103s
  training loss:		0.292987
  validation loss:		0.377870
  validation accuracy:		88.15 %
Epoch 1968 of 2000 took 0.103s
  training loss:		0.284749
  validation loss:		0.387306
  validation accuracy:		88.59 %
Epoch 1969 of 2000 took 0.103s
  training loss:		0.296615
  validation loss:		0.374592
  validation accuracy:		87.83 %
Epoch 1970 of 2000 took 0.103s
  training loss:		0.286993
  validation loss:		0.372226
  validation accuracy:		88.91 %
Epoch 1971 of 2000 took 0.103s
  training loss:		0.294162
  validation loss:		0.372253
  validation accuracy:		88.48 %
Epoch 1972 of 2000 took 0.103s
  training loss:		0.292440
  validation loss:		0.417110
  validation accuracy:		87.07 %
Epoch 1973 of 2000 took 0.103s
  training loss:		0.287172
  validation loss:		0.395974
  validation accuracy:		88.15 %
Epoch 1974 of 2000 took 0.103s
  training loss:		0.293466
  validation loss:		0.385980
  validation accuracy:		88.15 %
Epoch 1975 of 2000 took 0.103s
  training loss:		0.290989
  validation loss:		0.399164
  validation accuracy:		87.39 %
Epoch 1976 of 2000 took 0.104s
  training loss:		0.292871
  validation loss:		0.380302
  validation accuracy:		88.37 %
Epoch 1977 of 2000 took 0.104s
  training loss:		0.297584
  validation loss:		0.376706
  validation accuracy:		88.80 %
Epoch 1978 of 2000 took 0.103s
  training loss:		0.295784
  validation loss:		0.382575
  validation accuracy:		88.48 %
Epoch 1979 of 2000 took 0.103s
  training loss:		0.289499
  validation loss:		0.371005
  validation accuracy:		88.70 %
Epoch 1980 of 2000 took 0.103s
  training loss:		0.287386
  validation loss:		0.383901
  validation accuracy:		88.04 %
Epoch 1981 of 2000 took 0.103s
  training loss:		0.291263
  validation loss:		0.372063
  validation accuracy:		88.59 %
Epoch 1982 of 2000 took 0.103s
  training loss:		0.288631
  validation loss:		0.370793
  validation accuracy:		88.37 %
Epoch 1983 of 2000 took 0.103s
  training loss:		0.298728
  validation loss:		0.385079
  validation accuracy:		87.93 %
Epoch 1984 of 2000 took 0.103s
  training loss:		0.290644
  validation loss:		0.387429
  validation accuracy:		87.72 %
Epoch 1985 of 2000 took 0.103s
  training loss:		0.290296
  validation loss:		0.376162
  validation accuracy:		88.91 %
Epoch 1986 of 2000 took 0.103s
  training loss:		0.294850
  validation loss:		0.390362
  validation accuracy:		87.72 %
Epoch 1987 of 2000 took 0.103s
  training loss:		0.291463
  validation loss:		0.390051
  validation accuracy:		88.37 %
Epoch 1988 of 2000 took 0.103s
  training loss:		0.298175
  validation loss:		0.374332
  validation accuracy:		88.70 %
Epoch 1989 of 2000 took 0.103s
  training loss:		0.287150
  validation loss:		0.393254
  validation accuracy:		87.61 %
Epoch 1990 of 2000 took 0.103s
  training loss:		0.294684
  validation loss:		0.381062
  validation accuracy:		88.37 %
Epoch 1991 of 2000 took 0.103s
  training loss:		0.295596
  validation loss:		0.390475
  validation accuracy:		88.15 %
Epoch 1992 of 2000 took 0.103s
  training loss:		0.292047
  validation loss:		0.412618
  validation accuracy:		87.07 %
Epoch 1993 of 2000 took 0.103s
  training loss:		0.289393
  validation loss:		0.379318
  validation accuracy:		88.70 %
Epoch 1994 of 2000 took 0.103s
  training loss:		0.296184
  validation loss:		0.382738
  validation accuracy:		88.04 %
Epoch 1995 of 2000 took 0.103s
  training loss:		0.292979
  validation loss:		0.384386
  validation accuracy:		88.48 %
Epoch 1996 of 2000 took 0.103s
  training loss:		0.293874
  validation loss:		0.379925
  validation accuracy:		88.80 %
Epoch 1997 of 2000 took 0.103s
  training loss:		0.296773
  validation loss:		0.381999
  validation accuracy:		87.93 %
Epoch 1998 of 2000 took 0.103s
  training loss:		0.296647
  validation loss:		0.386412
  validation accuracy:		88.80 %
Epoch 1999 of 2000 took 0.103s
  training loss:		0.295993
  validation loss:		0.384807
  validation accuracy:		88.37 %
Epoch 2000 of 2000 took 0.103s
  training loss:		0.305212
  validation loss:		0.392028
  validation accuracy:		88.04 %
Final results:
  test loss:			0.757234
  test accuracy:		79.78 %
