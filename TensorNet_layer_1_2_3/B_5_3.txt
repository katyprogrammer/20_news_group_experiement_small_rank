Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(200, 150),b=(150,)
w=(150, 100),b=(100,)
w=(100, 50),b=(50,)
decomposing tensor W of shape (3, 200, 150)...
decomposing tensor B of shape (3, 150)...
Starting training...
Epoch 1 of 2000 took 0.104s
  training loss:		2.988175
  validation loss:		2.973438
  validation accuracy:		13.04 %
Epoch 2 of 2000 took 0.097s
  training loss:		2.954980
  validation loss:		2.934797
  validation accuracy:		11.85 %
Epoch 3 of 2000 took 0.097s
  training loss:		2.914709
  validation loss:		2.892813
  validation accuracy:		11.85 %
Epoch 4 of 2000 took 0.097s
  training loss:		2.871743
  validation loss:		2.849453
  validation accuracy:		11.85 %
Epoch 5 of 2000 took 0.097s
  training loss:		2.828013
  validation loss:		2.804733
  validation accuracy:		11.85 %
Epoch 6 of 2000 took 0.097s
  training loss:		2.784955
  validation loss:		2.756785
  validation accuracy:		11.85 %
Epoch 7 of 2000 took 0.097s
  training loss:		2.738647
  validation loss:		2.705626
  validation accuracy:		11.85 %
Epoch 8 of 2000 took 0.097s
  training loss:		2.690744
  validation loss:		2.650794
  validation accuracy:		11.85 %
Epoch 9 of 2000 took 0.097s
  training loss:		2.641743
  validation loss:		2.594699
  validation accuracy:		11.85 %
Epoch 10 of 2000 took 0.097s
  training loss:		2.594840
  validation loss:		2.538609
  validation accuracy:		11.85 %
Epoch 11 of 2000 took 0.097s
  training loss:		2.546551
  validation loss:		2.483553
  validation accuracy:		12.93 %
Epoch 12 of 2000 took 0.097s
  training loss:		2.502482
  validation loss:		2.430988
  validation accuracy:		12.93 %
Epoch 13 of 2000 took 0.097s
  training loss:		2.461313
  validation loss:		2.384450
  validation accuracy:		12.93 %
Epoch 14 of 2000 took 0.102s
  training loss:		2.422858
  validation loss:		2.345553
  validation accuracy:		12.93 %
Epoch 15 of 2000 took 0.097s
  training loss:		2.393236
  validation loss:		2.314645
  validation accuracy:		12.83 %
Epoch 16 of 2000 took 0.097s
  training loss:		2.367300
  validation loss:		2.295653
  validation accuracy:		10.76 %
Epoch 17 of 2000 took 0.097s
  training loss:		2.351044
  validation loss:		2.279817
  validation accuracy:		12.83 %
Epoch 18 of 2000 took 0.097s
  training loss:		2.337085
  validation loss:		2.272383
  validation accuracy:		13.37 %
Epoch 19 of 2000 took 0.097s
  training loss:		2.327896
  validation loss:		2.266643
  validation accuracy:		17.07 %
Epoch 20 of 2000 took 0.097s
  training loss:		2.321105
  validation loss:		2.265107
  validation accuracy:		20.65 %
Epoch 21 of 2000 took 0.097s
  training loss:		2.315620
  validation loss:		2.259242
  validation accuracy:		15.33 %
Epoch 22 of 2000 took 0.097s
  training loss:		2.312741
  validation loss:		2.256332
  validation accuracy:		13.70 %
Epoch 23 of 2000 took 0.097s
  training loss:		2.309965
  validation loss:		2.257994
  validation accuracy:		12.83 %
Epoch 24 of 2000 took 0.097s
  training loss:		2.307431
  validation loss:		2.254969
  validation accuracy:		14.02 %
Epoch 25 of 2000 took 0.097s
  training loss:		2.306251
  validation loss:		2.254307
  validation accuracy:		13.04 %
Epoch 26 of 2000 took 0.097s
  training loss:		2.304819
  validation loss:		2.254970
  validation accuracy:		16.30 %
Epoch 27 of 2000 took 0.097s
  training loss:		2.302962
  validation loss:		2.248993
  validation accuracy:		12.93 %
Epoch 28 of 2000 took 0.097s
  training loss:		2.302083
  validation loss:		2.250609
  validation accuracy:		13.04 %
Epoch 29 of 2000 took 0.097s
  training loss:		2.301847
  validation loss:		2.249421
  validation accuracy:		13.04 %
Epoch 30 of 2000 took 0.097s
  training loss:		2.301242
  validation loss:		2.247738
  validation accuracy:		19.24 %
Epoch 31 of 2000 took 0.097s
  training loss:		2.300010
  validation loss:		2.250192
  validation accuracy:		20.00 %
Epoch 32 of 2000 took 0.097s
  training loss:		2.299331
  validation loss:		2.246952
  validation accuracy:		12.83 %
Epoch 33 of 2000 took 0.097s
  training loss:		2.299673
  validation loss:		2.244768
  validation accuracy:		12.83 %
Epoch 34 of 2000 took 0.097s
  training loss:		2.298918
  validation loss:		2.252036
  validation accuracy:		12.93 %
Epoch 35 of 2000 took 0.097s
  training loss:		2.298730
  validation loss:		2.249505
  validation accuracy:		13.37 %
Epoch 36 of 2000 took 0.097s
  training loss:		2.297758
  validation loss:		2.242982
  validation accuracy:		12.93 %
Epoch 37 of 2000 took 0.097s
  training loss:		2.298156
  validation loss:		2.247528
  validation accuracy:		13.04 %
Epoch 38 of 2000 took 0.103s
  training loss:		2.297759
  validation loss:		2.247253
  validation accuracy:		20.22 %
Epoch 39 of 2000 took 0.097s
  training loss:		2.297497
  validation loss:		2.247497
  validation accuracy:		18.59 %
Epoch 40 of 2000 took 0.097s
  training loss:		2.297467
  validation loss:		2.246607
  validation accuracy:		13.26 %
Epoch 41 of 2000 took 0.097s
  training loss:		2.296231
  validation loss:		2.244883
  validation accuracy:		16.96 %
Epoch 42 of 2000 took 0.097s
  training loss:		2.297483
  validation loss:		2.244026
  validation accuracy:		15.00 %
Epoch 43 of 2000 took 0.097s
  training loss:		2.296366
  validation loss:		2.245584
  validation accuracy:		12.93 %
Epoch 44 of 2000 took 0.098s
  training loss:		2.296504
  validation loss:		2.245832
  validation accuracy:		23.15 %
Epoch 45 of 2000 took 0.097s
  training loss:		2.296298
  validation loss:		2.246310
  validation accuracy:		13.04 %
Epoch 46 of 2000 took 0.097s
  training loss:		2.295426
  validation loss:		2.243363
  validation accuracy:		12.93 %
Epoch 47 of 2000 took 0.097s
  training loss:		2.296452
  validation loss:		2.242877
  validation accuracy:		13.15 %
Epoch 48 of 2000 took 0.097s
  training loss:		2.295383
  validation loss:		2.242800
  validation accuracy:		12.93 %
Epoch 49 of 2000 took 0.097s
  training loss:		2.295779
  validation loss:		2.247058
  validation accuracy:		12.83 %
Epoch 50 of 2000 took 0.097s
  training loss:		2.295343
  validation loss:		2.243727
  validation accuracy:		12.93 %
Epoch 51 of 2000 took 0.097s
  training loss:		2.296372
  validation loss:		2.239787
  validation accuracy:		12.93 %
Epoch 52 of 2000 took 0.097s
  training loss:		2.296222
  validation loss:		2.249648
  validation accuracy:		19.24 %
Epoch 53 of 2000 took 0.097s
  training loss:		2.295975
  validation loss:		2.243246
  validation accuracy:		18.59 %
Epoch 54 of 2000 took 0.097s
  training loss:		2.295258
  validation loss:		2.242557
  validation accuracy:		17.93 %
Epoch 55 of 2000 took 0.102s
  training loss:		2.295140
  validation loss:		2.243638
  validation accuracy:		14.24 %
Epoch 56 of 2000 took 0.097s
  training loss:		2.295261
  validation loss:		2.246316
  validation accuracy:		12.93 %
Epoch 57 of 2000 took 0.097s
  training loss:		2.296121
  validation loss:		2.244842
  validation accuracy:		14.57 %
Epoch 58 of 2000 took 0.097s
  training loss:		2.294466
  validation loss:		2.243513
  validation accuracy:		13.04 %
Epoch 59 of 2000 took 0.097s
  training loss:		2.293250
  validation loss:		2.241059
  validation accuracy:		15.33 %
Epoch 60 of 2000 took 0.097s
  training loss:		2.294097
  validation loss:		2.239744
  validation accuracy:		13.59 %
Epoch 61 of 2000 took 0.097s
  training loss:		2.293862
  validation loss:		2.240612
  validation accuracy:		15.22 %
Epoch 62 of 2000 took 0.096s
  training loss:		2.295255
  validation loss:		2.242687
  validation accuracy:		12.93 %
Epoch 63 of 2000 took 0.097s
  training loss:		2.294543
  validation loss:		2.238922
  validation accuracy:		15.87 %
Epoch 64 of 2000 took 0.097s
  training loss:		2.294148
  validation loss:		2.245095
  validation accuracy:		13.37 %
Epoch 65 of 2000 took 0.097s
  training loss:		2.295068
  validation loss:		2.245734
  validation accuracy:		17.50 %
Epoch 66 of 2000 took 0.100s
  training loss:		2.294012
  validation loss:		2.239063
  validation accuracy:		21.30 %
Epoch 67 of 2000 took 0.099s
  training loss:		2.294000
  validation loss:		2.240839
  validation accuracy:		12.83 %
Epoch 68 of 2000 took 0.097s
  training loss:		2.293879
  validation loss:		2.246062
  validation accuracy:		16.96 %
Epoch 69 of 2000 took 0.097s
  training loss:		2.293657
  validation loss:		2.237873
  validation accuracy:		12.93 %
Epoch 70 of 2000 took 0.097s
  training loss:		2.293933
  validation loss:		2.242540
  validation accuracy:		13.04 %
Epoch 71 of 2000 took 0.097s
  training loss:		2.293684
  validation loss:		2.243287
  validation accuracy:		12.83 %
Epoch 72 of 2000 took 0.097s
  training loss:		2.294295
  validation loss:		2.242169
  validation accuracy:		14.67 %
Epoch 73 of 2000 took 0.097s
  training loss:		2.293347
  validation loss:		2.240219
  validation accuracy:		20.22 %
Epoch 74 of 2000 took 0.097s
  training loss:		2.293695
  validation loss:		2.239676
  validation accuracy:		17.28 %
Epoch 75 of 2000 took 0.098s
  training loss:		2.294438
  validation loss:		2.242750
  validation accuracy:		13.26 %
Epoch 76 of 2000 took 0.097s
  training loss:		2.293562
  validation loss:		2.242188
  validation accuracy:		13.26 %
Epoch 77 of 2000 took 0.102s
  training loss:		2.293583
  validation loss:		2.243703
  validation accuracy:		13.48 %
Epoch 78 of 2000 took 0.097s
  training loss:		2.292798
  validation loss:		2.237727
  validation accuracy:		13.15 %
Epoch 79 of 2000 took 0.097s
  training loss:		2.293173
  validation loss:		2.245338
  validation accuracy:		17.50 %
Epoch 80 of 2000 took 0.097s
  training loss:		2.293333
  validation loss:		2.243982
  validation accuracy:		18.48 %
Epoch 81 of 2000 took 0.097s
  training loss:		2.293043
  validation loss:		2.239016
  validation accuracy:		13.91 %
Epoch 82 of 2000 took 0.097s
  training loss:		2.292820
  validation loss:		2.240738
  validation accuracy:		14.13 %
Epoch 83 of 2000 took 0.097s
  training loss:		2.293718
  validation loss:		2.242089
  validation accuracy:		19.02 %
Epoch 84 of 2000 took 0.097s
  training loss:		2.292538
  validation loss:		2.244352
  validation accuracy:		13.04 %
Epoch 85 of 2000 took 0.097s
  training loss:		2.293082
  validation loss:		2.241418
  validation accuracy:		21.30 %
Epoch 86 of 2000 took 0.097s
  training loss:		2.292763
  validation loss:		2.240793
  validation accuracy:		18.80 %
Epoch 87 of 2000 took 0.102s
  training loss:		2.293730
  validation loss:		2.239129
  validation accuracy:		13.04 %
Epoch 88 of 2000 took 0.097s
  training loss:		2.293421
  validation loss:		2.243282
  validation accuracy:		14.89 %
Epoch 89 of 2000 took 0.097s
  training loss:		2.292518
  validation loss:		2.239170
  validation accuracy:		15.22 %
Epoch 90 of 2000 took 0.097s
  training loss:		2.292724
  validation loss:		2.239674
  validation accuracy:		18.48 %
Epoch 91 of 2000 took 0.097s
  training loss:		2.291991
  validation loss:		2.239392
  validation accuracy:		13.48 %
Epoch 92 of 2000 took 0.097s
  training loss:		2.291255
  validation loss:		2.238033
  validation accuracy:		17.50 %
Epoch 93 of 2000 took 0.097s
  training loss:		2.292835
  validation loss:		2.235260
  validation accuracy:		18.26 %
Epoch 94 of 2000 took 0.097s
  training loss:		2.292972
  validation loss:		2.248226
  validation accuracy:		21.52 %
Epoch 95 of 2000 took 0.097s
  training loss:		2.292156
  validation loss:		2.242157
  validation accuracy:		22.07 %
Epoch 96 of 2000 took 0.097s
  training loss:		2.291781
  validation loss:		2.238460
  validation accuracy:		14.89 %
Epoch 97 of 2000 took 0.101s
  training loss:		2.291963
  validation loss:		2.238851
  validation accuracy:		17.93 %
Epoch 98 of 2000 took 0.098s
  training loss:		2.291401
  validation loss:		2.237128
  validation accuracy:		14.46 %
Epoch 99 of 2000 took 0.097s
  training loss:		2.291613
  validation loss:		2.238633
  validation accuracy:		13.70 %
Epoch 100 of 2000 took 0.097s
  training loss:		2.291125
  validation loss:		2.237818
  validation accuracy:		20.87 %
Epoch 101 of 2000 took 0.097s
  training loss:		2.290887
  validation loss:		2.241110
  validation accuracy:		13.48 %
Epoch 102 of 2000 took 0.097s
  training loss:		2.291103
  validation loss:		2.238649
  validation accuracy:		23.91 %
Epoch 103 of 2000 took 0.097s
  training loss:		2.292040
  validation loss:		2.242548
  validation accuracy:		17.39 %
Epoch 104 of 2000 took 0.097s
  training loss:		2.291485
  validation loss:		2.239901
  validation accuracy:		16.74 %
Epoch 105 of 2000 took 0.097s
  training loss:		2.291144
  validation loss:		2.239981
  validation accuracy:		19.46 %
Epoch 106 of 2000 took 0.097s
  training loss:		2.290344
  validation loss:		2.237304
  validation accuracy:		13.37 %
Epoch 107 of 2000 took 0.098s
  training loss:		2.290779
  validation loss:		2.237239
  validation accuracy:		19.89 %
Epoch 108 of 2000 took 0.101s
  training loss:		2.290161
  validation loss:		2.235740
  validation accuracy:		12.93 %
Epoch 109 of 2000 took 0.097s
  training loss:		2.290749
  validation loss:		2.242319
  validation accuracy:		19.78 %
Epoch 110 of 2000 took 0.097s
  training loss:		2.291641
  validation loss:		2.241165
  validation accuracy:		18.48 %
Epoch 111 of 2000 took 0.097s
  training loss:		2.291076
  validation loss:		2.238172
  validation accuracy:		17.50 %
Epoch 112 of 2000 took 0.097s
  training loss:		2.290161
  validation loss:		2.242422
  validation accuracy:		18.70 %
Epoch 113 of 2000 took 0.097s
  training loss:		2.290295
  validation loss:		2.234667
  validation accuracy:		14.24 %
Epoch 114 of 2000 took 0.097s
  training loss:		2.288962
  validation loss:		2.240322
  validation accuracy:		16.09 %
Epoch 115 of 2000 took 0.097s
  training loss:		2.290276
  validation loss:		2.234341
  validation accuracy:		16.41 %
Epoch 116 of 2000 took 0.097s
  training loss:		2.290140
  validation loss:		2.234300
  validation accuracy:		13.59 %
Epoch 117 of 2000 took 0.097s
  training loss:		2.290349
  validation loss:		2.241929
  validation accuracy:		17.83 %
Epoch 118 of 2000 took 0.102s
  training loss:		2.289265
  validation loss:		2.235338
  validation accuracy:		12.83 %
Epoch 119 of 2000 took 0.097s
  training loss:		2.289420
  validation loss:		2.238104
  validation accuracy:		18.80 %
Epoch 120 of 2000 took 0.097s
  training loss:		2.288073
  validation loss:		2.234276
  validation accuracy:		14.13 %
Epoch 121 of 2000 took 0.097s
  training loss:		2.289071
  validation loss:		2.240274
  validation accuracy:		19.67 %
Epoch 122 of 2000 took 0.097s
  training loss:		2.289287
  validation loss:		2.237835
  validation accuracy:		12.83 %
Epoch 123 of 2000 took 0.097s
  training loss:		2.288866
  validation loss:		2.236247
  validation accuracy:		19.46 %
Epoch 124 of 2000 took 0.097s
  training loss:		2.288353
  validation loss:		2.238881
  validation accuracy:		14.46 %
Epoch 125 of 2000 took 0.097s
  training loss:		2.288550
  validation loss:		2.238014
  validation accuracy:		14.46 %
Epoch 126 of 2000 took 0.097s
  training loss:		2.288611
  validation loss:		2.236224
  validation accuracy:		13.80 %
Epoch 127 of 2000 took 0.097s
  training loss:		2.288886
  validation loss:		2.235137
  validation accuracy:		17.83 %
Epoch 128 of 2000 took 0.102s
  training loss:		2.287885
  validation loss:		2.238103
  validation accuracy:		20.76 %
Epoch 129 of 2000 took 0.097s
  training loss:		2.287458
  validation loss:		2.232908
  validation accuracy:		24.57 %
Epoch 130 of 2000 took 0.097s
  training loss:		2.287825
  validation loss:		2.240256
  validation accuracy:		15.00 %
Epoch 131 of 2000 took 0.097s
  training loss:		2.287995
  validation loss:		2.236881
  validation accuracy:		20.43 %
Epoch 132 of 2000 took 0.097s
  training loss:		2.286885
  validation loss:		2.234233
  validation accuracy:		20.98 %
Epoch 133 of 2000 took 0.097s
  training loss:		2.287430
  validation loss:		2.236414
  validation accuracy:		19.35 %
Epoch 134 of 2000 took 0.097s
  training loss:		2.286539
  validation loss:		2.236675
  validation accuracy:		20.00 %
Epoch 135 of 2000 took 0.097s
  training loss:		2.285826
  validation loss:		2.233576
  validation accuracy:		21.74 %
Epoch 136 of 2000 took 0.097s
  training loss:		2.286010
  validation loss:		2.232843
  validation accuracy:		20.98 %
Epoch 137 of 2000 took 0.097s
  training loss:		2.285521
  validation loss:		2.231626
  validation accuracy:		16.41 %
Epoch 138 of 2000 took 0.101s
  training loss:		2.285883
  validation loss:		2.235078
  validation accuracy:		17.07 %
Epoch 139 of 2000 took 0.098s
  training loss:		2.285695
  validation loss:		2.234572
  validation accuracy:		19.24 %
Epoch 140 of 2000 took 0.097s
  training loss:		2.285011
  validation loss:		2.243067
  validation accuracy:		20.00 %
Epoch 141 of 2000 took 0.097s
  training loss:		2.284773
  validation loss:		2.227389
  validation accuracy:		21.52 %
Epoch 142 of 2000 took 0.097s
  training loss:		2.284365
  validation loss:		2.232892
  validation accuracy:		21.30 %
Epoch 143 of 2000 took 0.097s
  training loss:		2.284671
  validation loss:		2.227757
  validation accuracy:		20.22 %
Epoch 144 of 2000 took 0.097s
  training loss:		2.284413
  validation loss:		2.230454
  validation accuracy:		22.61 %
Epoch 145 of 2000 took 0.097s
  training loss:		2.283042
  validation loss:		2.231996
  validation accuracy:		21.30 %
Epoch 146 of 2000 took 0.097s
  training loss:		2.282616
  validation loss:		2.227012
  validation accuracy:		23.15 %
Epoch 147 of 2000 took 0.097s
  training loss:		2.281562
  validation loss:		2.229829
  validation accuracy:		22.17 %
Epoch 148 of 2000 took 0.097s
  training loss:		2.282591
  validation loss:		2.232504
  validation accuracy:		22.93 %
Epoch 149 of 2000 took 0.102s
  training loss:		2.282267
  validation loss:		2.227007
  validation accuracy:		18.48 %
Epoch 150 of 2000 took 0.097s
  training loss:		2.282175
  validation loss:		2.229541
  validation accuracy:		20.87 %
Epoch 151 of 2000 took 0.097s
  training loss:		2.279480
  validation loss:		2.222545
  validation accuracy:		17.07 %
Epoch 152 of 2000 took 0.097s
  training loss:		2.279665
  validation loss:		2.223155
  validation accuracy:		21.96 %
Epoch 153 of 2000 took 0.097s
  training loss:		2.279923
  validation loss:		2.227755
  validation accuracy:		20.98 %
Epoch 154 of 2000 took 0.097s
  training loss:		2.279796
  validation loss:		2.229306
  validation accuracy:		20.76 %
Epoch 155 of 2000 took 0.097s
  training loss:		2.279594
  validation loss:		2.224632
  validation accuracy:		23.48 %
Epoch 156 of 2000 took 0.097s
  training loss:		2.277297
  validation loss:		2.225131
  validation accuracy:		20.22 %
Epoch 157 of 2000 took 0.097s
  training loss:		2.275398
  validation loss:		2.219602
  validation accuracy:		20.00 %
Epoch 158 of 2000 took 0.097s
  training loss:		2.275171
  validation loss:		2.217310
  validation accuracy:		22.72 %
Epoch 159 of 2000 took 0.102s
  training loss:		2.276002
  validation loss:		2.224433
  validation accuracy:		21.41 %
Epoch 160 of 2000 took 0.097s
  training loss:		2.273962
  validation loss:		2.222958
  validation accuracy:		20.43 %
Epoch 161 of 2000 took 0.097s
  training loss:		2.273257
  validation loss:		2.213462
  validation accuracy:		23.91 %
Epoch 162 of 2000 took 0.097s
  training loss:		2.272436
  validation loss:		2.216856
  validation accuracy:		21.41 %
Epoch 163 of 2000 took 0.097s
  training loss:		2.271462
  validation loss:		2.212828
  validation accuracy:		21.63 %
Epoch 164 of 2000 took 0.097s
  training loss:		2.271063
  validation loss:		2.216735
  validation accuracy:		26.63 %
Epoch 165 of 2000 took 0.097s
  training loss:		2.268546
  validation loss:		2.212227
  validation accuracy:		26.20 %
Epoch 166 of 2000 took 0.097s
  training loss:		2.268277
  validation loss:		2.215036
  validation accuracy:		21.63 %
Epoch 167 of 2000 took 0.098s
  training loss:		2.266784
  validation loss:		2.209155
  validation accuracy:		22.50 %
Epoch 168 of 2000 took 0.097s
  training loss:		2.263879
  validation loss:		2.206780
  validation accuracy:		20.22 %
Epoch 169 of 2000 took 0.102s
  training loss:		2.263674
  validation loss:		2.209266
  validation accuracy:		27.28 %
Epoch 170 of 2000 took 0.097s
  training loss:		2.262138
  validation loss:		2.210030
  validation accuracy:		24.78 %
Epoch 171 of 2000 took 0.097s
  training loss:		2.260805
  validation loss:		2.205566
  validation accuracy:		22.50 %
Epoch 172 of 2000 took 0.097s
  training loss:		2.257573
  validation loss:		2.199384
  validation accuracy:		21.63 %
Epoch 173 of 2000 took 0.097s
  training loss:		2.256634
  validation loss:		2.199555
  validation accuracy:		21.74 %
Epoch 174 of 2000 took 0.097s
  training loss:		2.251967
  validation loss:		2.188557
  validation accuracy:		29.02 %
Epoch 175 of 2000 took 0.097s
  training loss:		2.251252
  validation loss:		2.197442
  validation accuracy:		21.74 %
Epoch 176 of 2000 took 0.097s
  training loss:		2.247994
  validation loss:		2.188477
  validation accuracy:		22.72 %
Epoch 177 of 2000 took 0.097s
  training loss:		2.246934
  validation loss:		2.183060
  validation accuracy:		25.43 %
Epoch 178 of 2000 took 0.097s
  training loss:		2.242887
  validation loss:		2.188297
  validation accuracy:		26.74 %
Epoch 179 of 2000 took 0.101s
  training loss:		2.238797
  validation loss:		2.174722
  validation accuracy:		26.52 %
Epoch 180 of 2000 took 0.098s
  training loss:		2.235863
  validation loss:		2.168516
  validation accuracy:		30.98 %
Epoch 181 of 2000 took 0.097s
  training loss:		2.232177
  validation loss:		2.165124
  validation accuracy:		24.67 %
Epoch 182 of 2000 took 0.097s
  training loss:		2.226138
  validation loss:		2.164425
  validation accuracy:		25.98 %
Epoch 183 of 2000 took 0.097s
  training loss:		2.219477
  validation loss:		2.159734
  validation accuracy:		24.57 %
Epoch 184 of 2000 took 0.097s
  training loss:		2.213405
  validation loss:		2.144897
  validation accuracy:		25.00 %
Epoch 185 of 2000 took 0.097s
  training loss:		2.207293
  validation loss:		2.143339
  validation accuracy:		29.57 %
Epoch 186 of 2000 took 0.097s
  training loss:		2.199147
  validation loss:		2.126486
  validation accuracy:		24.89 %
Epoch 187 of 2000 took 0.097s
  training loss:		2.189293
  validation loss:		2.119138
  validation accuracy:		26.85 %
Epoch 188 of 2000 took 0.097s
  training loss:		2.181250
  validation loss:		2.109273
  validation accuracy:		25.98 %
Epoch 189 of 2000 took 0.097s
  training loss:		2.169187
  validation loss:		2.096839
  validation accuracy:		27.28 %
Epoch 190 of 2000 took 0.103s
  training loss:		2.154875
  validation loss:		2.072422
  validation accuracy:		28.80 %
Epoch 191 of 2000 took 0.097s
  training loss:		2.141926
  validation loss:		2.053270
  validation accuracy:		28.48 %
Epoch 192 of 2000 took 0.097s
  training loss:		2.127489
  validation loss:		2.049051
  validation accuracy:		26.85 %
Epoch 193 of 2000 took 0.097s
  training loss:		2.108304
  validation loss:		2.024106
  validation accuracy:		28.48 %
Epoch 194 of 2000 took 0.097s
  training loss:		2.088701
  validation loss:		2.000863
  validation accuracy:		29.13 %
Epoch 195 of 2000 took 0.100s
  training loss:		2.068211
  validation loss:		1.982002
  validation accuracy:		28.80 %
Epoch 196 of 2000 took 0.100s
  training loss:		2.042797
  validation loss:		1.948124
  validation accuracy:		31.30 %
Epoch 197 of 2000 took 0.101s
  training loss:		2.018084
  validation loss:		1.918451
  validation accuracy:		30.54 %
Epoch 198 of 2000 took 0.104s
  training loss:		1.991340
  validation loss:		1.889957
  validation accuracy:		32.17 %
Epoch 199 of 2000 took 0.103s
  training loss:		1.965041
  validation loss:		1.865876
  validation accuracy:		32.61 %
Epoch 200 of 2000 took 0.109s
  training loss:		1.938940
  validation loss:		1.839610
  validation accuracy:		31.20 %
Epoch 201 of 2000 took 0.103s
  training loss:		1.912140
  validation loss:		1.810215
  validation accuracy:		32.72 %
Epoch 202 of 2000 took 0.103s
  training loss:		1.887194
  validation loss:		1.776234
  validation accuracy:		34.13 %
Epoch 203 of 2000 took 0.103s
  training loss:		1.857215
  validation loss:		1.749551
  validation accuracy:		33.80 %
Epoch 204 of 2000 took 0.103s
  training loss:		1.827213
  validation loss:		1.727504
  validation accuracy:		34.89 %
Epoch 205 of 2000 took 0.103s
  training loss:		1.808790
  validation loss:		1.705830
  validation accuracy:		35.98 %
Epoch 206 of 2000 took 0.102s
  training loss:		1.785019
  validation loss:		1.685481
  validation accuracy:		36.09 %
Epoch 207 of 2000 took 0.100s
  training loss:		1.770547
  validation loss:		1.658021
  validation accuracy:		36.96 %
Epoch 208 of 2000 took 0.100s
  training loss:		1.745287
  validation loss:		1.635550
  validation accuracy:		38.48 %
Epoch 209 of 2000 took 0.104s
  training loss:		1.724891
  validation loss:		1.625483
  validation accuracy:		37.93 %
Epoch 210 of 2000 took 0.101s
  training loss:		1.708392
  validation loss:		1.604435
  validation accuracy:		38.37 %
Epoch 211 of 2000 took 0.100s
  training loss:		1.696030
  validation loss:		1.593301
  validation accuracy:		38.59 %
Epoch 212 of 2000 took 0.100s
  training loss:		1.677948
  validation loss:		1.573927
  validation accuracy:		40.43 %
Epoch 213 of 2000 took 0.100s
  training loss:		1.664123
  validation loss:		1.550334
  validation accuracy:		40.00 %
Epoch 214 of 2000 took 0.100s
  training loss:		1.652031
  validation loss:		1.550944
  validation accuracy:		41.20 %
Epoch 215 of 2000 took 0.100s
  training loss:		1.640549
  validation loss:		1.537548
  validation accuracy:		42.39 %
Epoch 216 of 2000 took 0.100s
  training loss:		1.626650
  validation loss:		1.524675
  validation accuracy:		40.65 %
Epoch 217 of 2000 took 0.100s
  training loss:		1.611471
  validation loss:		1.506412
  validation accuracy:		41.52 %
Epoch 218 of 2000 took 0.100s
  training loss:		1.601323
  validation loss:		1.497050
  validation accuracy:		42.17 %
Epoch 219 of 2000 took 0.105s
  training loss:		1.591875
  validation loss:		1.500644
  validation accuracy:		43.15 %
Epoch 220 of 2000 took 0.101s
  training loss:		1.576132
  validation loss:		1.473691
  validation accuracy:		42.61 %
Epoch 221 of 2000 took 0.100s
  training loss:		1.569999
  validation loss:		1.467329
  validation accuracy:		43.80 %
Epoch 222 of 2000 took 0.100s
  training loss:		1.560138
  validation loss:		1.458597
  validation accuracy:		44.02 %
Epoch 223 of 2000 took 0.100s
  training loss:		1.550279
  validation loss:		1.463391
  validation accuracy:		43.91 %
Epoch 224 of 2000 took 0.100s
  training loss:		1.543862
  validation loss:		1.446958
  validation accuracy:		44.57 %
Epoch 225 of 2000 took 0.100s
  training loss:		1.538064
  validation loss:		1.439103
  validation accuracy:		44.46 %
Epoch 226 of 2000 took 0.100s
  training loss:		1.529682
  validation loss:		1.432592
  validation accuracy:		45.11 %
Epoch 227 of 2000 took 0.100s
  training loss:		1.517536
  validation loss:		1.428595
  validation accuracy:		45.33 %
Epoch 228 of 2000 took 0.101s
  training loss:		1.507205
  validation loss:		1.420286
  validation accuracy:		46.20 %
Epoch 229 of 2000 took 0.105s
  training loss:		1.507972
  validation loss:		1.409925
  validation accuracy:		45.87 %
Epoch 230 of 2000 took 0.101s
  training loss:		1.498171
  validation loss:		1.402386
  validation accuracy:		47.17 %
Epoch 231 of 2000 took 0.100s
  training loss:		1.497491
  validation loss:		1.398482
  validation accuracy:		46.74 %
Epoch 232 of 2000 took 0.100s
  training loss:		1.485103
  validation loss:		1.402719
  validation accuracy:		47.83 %
Epoch 233 of 2000 took 0.102s
  training loss:		1.491815
  validation loss:		1.391537
  validation accuracy:		48.48 %
Epoch 234 of 2000 took 0.103s
  training loss:		1.477225
  validation loss:		1.395639
  validation accuracy:		45.22 %
Epoch 235 of 2000 took 0.143s
  training loss:		1.476912
  validation loss:		1.381865
  validation accuracy:		48.26 %
Epoch 236 of 2000 took 0.098s
  training loss:		1.469141
  validation loss:		1.383038
  validation accuracy:		48.80 %
Epoch 237 of 2000 took 0.097s
  training loss:		1.461208
  validation loss:		1.380935
  validation accuracy:		48.37 %
Epoch 238 of 2000 took 0.100s
  training loss:		1.457644
  validation loss:		1.363315
  validation accuracy:		48.26 %
Epoch 239 of 2000 took 0.113s
  training loss:		1.454810
  validation loss:		1.352863
  validation accuracy:		48.48 %
Epoch 240 of 2000 took 0.099s
  training loss:		1.455603
  validation loss:		1.362378
  validation accuracy:		48.91 %
Epoch 241 of 2000 took 0.096s
  training loss:		1.449327
  validation loss:		1.352452
  validation accuracy:		48.37 %
Epoch 242 of 2000 took 0.096s
  training loss:		1.440164
  validation loss:		1.353425
  validation accuracy:		49.57 %
Epoch 243 of 2000 took 0.096s
  training loss:		1.435522
  validation loss:		1.347440
  validation accuracy:		49.02 %
Epoch 244 of 2000 took 0.096s
  training loss:		1.433248
  validation loss:		1.347084
  validation accuracy:		50.00 %
Epoch 245 of 2000 took 0.096s
  training loss:		1.428729
  validation loss:		1.339739
  validation accuracy:		50.33 %
Epoch 246 of 2000 took 0.096s
  training loss:		1.428971
  validation loss:		1.339511
  validation accuracy:		51.41 %
Epoch 247 of 2000 took 0.096s
  training loss:		1.426795
  validation loss:		1.342960
  validation accuracy:		50.43 %
Epoch 248 of 2000 took 0.096s
  training loss:		1.415838
  validation loss:		1.333542
  validation accuracy:		50.98 %
Epoch 249 of 2000 took 0.098s
  training loss:		1.420509
  validation loss:		1.341851
  validation accuracy:		51.85 %
Epoch 250 of 2000 took 0.096s
  training loss:		1.428397
  validation loss:		1.360023
  validation accuracy:		50.87 %
Epoch 251 of 2000 took 0.096s
  training loss:		1.420700
  validation loss:		1.325082
  validation accuracy:		51.09 %
Epoch 252 of 2000 took 0.096s
  training loss:		1.409887
  validation loss:		1.333629
  validation accuracy:		51.20 %
Epoch 253 of 2000 took 0.096s
  training loss:		1.419456
  validation loss:		1.317272
  validation accuracy:		50.43 %
Epoch 254 of 2000 took 0.096s
  training loss:		1.409999
  validation loss:		1.320555
  validation accuracy:		48.80 %
Epoch 255 of 2000 took 0.096s
  training loss:		1.412236
  validation loss:		1.319277
  validation accuracy:		51.20 %
Epoch 256 of 2000 took 0.096s
  training loss:		1.404443
  validation loss:		1.305496
  validation accuracy:		51.30 %
Epoch 257 of 2000 took 0.096s
  training loss:		1.401351
  validation loss:		1.323966
  validation accuracy:		51.74 %
Epoch 258 of 2000 took 0.097s
  training loss:		1.399332
  validation loss:		1.333070
  validation accuracy:		51.85 %
Epoch 259 of 2000 took 0.098s
  training loss:		1.399067
  validation loss:		1.299083
  validation accuracy:		51.20 %
Epoch 260 of 2000 took 0.096s
  training loss:		1.393384
  validation loss:		1.331446
  validation accuracy:		52.17 %
Epoch 261 of 2000 took 0.096s
  training loss:		1.397742
  validation loss:		1.308716
  validation accuracy:		50.33 %
Epoch 262 of 2000 took 0.096s
  training loss:		1.400048
  validation loss:		1.302147
  validation accuracy:		50.11 %
Epoch 263 of 2000 took 0.096s
  training loss:		1.396267
  validation loss:		1.302327
  validation accuracy:		52.07 %
Epoch 264 of 2000 took 0.096s
  training loss:		1.395992
  validation loss:		1.302925
  validation accuracy:		49.24 %
Epoch 265 of 2000 took 0.096s
  training loss:		1.396859
  validation loss:		1.292885
  validation accuracy:		51.52 %
Epoch 266 of 2000 took 0.096s
  training loss:		1.390743
  validation loss:		1.317637
  validation accuracy:		52.93 %
Epoch 267 of 2000 took 0.096s
  training loss:		1.389010
  validation loss:		1.295513
  validation accuracy:		50.00 %
Epoch 268 of 2000 took 0.096s
  training loss:		1.380359
  validation loss:		1.290189
  validation accuracy:		49.78 %
Epoch 269 of 2000 took 0.096s
  training loss:		1.378512
  validation loss:		1.296651
  validation accuracy:		50.65 %
Epoch 270 of 2000 took 0.099s
  training loss:		1.379683
  validation loss:		1.285653
  validation accuracy:		49.67 %
Epoch 271 of 2000 took 0.097s
  training loss:		1.387712
  validation loss:		1.290750
  validation accuracy:		49.24 %
Epoch 272 of 2000 took 0.097s
  training loss:		1.377724
  validation loss:		1.290601
  validation accuracy:		50.87 %
Epoch 273 of 2000 took 0.096s
  training loss:		1.381110
  validation loss:		1.289134
  validation accuracy:		49.46 %
Epoch 274 of 2000 took 0.097s
  training loss:		1.389502
  validation loss:		1.330406
  validation accuracy:		52.61 %
Epoch 275 of 2000 took 0.096s
  training loss:		1.374175
  validation loss:		1.276142
  validation accuracy:		51.41 %
Epoch 276 of 2000 took 0.096s
  training loss:		1.372603
  validation loss:		1.282907
  validation accuracy:		51.20 %
Epoch 277 of 2000 took 0.096s
  training loss:		1.369987
  validation loss:		1.288020
  validation accuracy:		50.11 %
Epoch 278 of 2000 took 0.095s
  training loss:		1.374602
  validation loss:		1.306752
  validation accuracy:		50.11 %
Epoch 279 of 2000 took 0.095s
  training loss:		1.363067
  validation loss:		1.282381
  validation accuracy:		50.54 %
Epoch 280 of 2000 took 0.095s
  training loss:		1.361330
  validation loss:		1.328241
  validation accuracy:		50.65 %
Epoch 281 of 2000 took 0.095s
  training loss:		1.366604
  validation loss:		1.272355
  validation accuracy:		51.85 %
Epoch 282 of 2000 took 0.098s
  training loss:		1.364799
  validation loss:		1.289326
  validation accuracy:		49.02 %
Epoch 283 of 2000 took 0.096s
  training loss:		1.363043
  validation loss:		1.309009
  validation accuracy:		50.54 %
Epoch 284 of 2000 took 0.095s
  training loss:		1.424491
  validation loss:		1.284866
  validation accuracy:		46.85 %
Epoch 285 of 2000 took 0.095s
  training loss:		1.415325
  validation loss:		1.292983
  validation accuracy:		52.61 %
Epoch 286 of 2000 took 0.095s
  training loss:		1.353769
  validation loss:		1.280091
  validation accuracy:		50.87 %
Epoch 287 of 2000 took 0.095s
  training loss:		1.359129
  validation loss:		1.266341
  validation accuracy:		49.13 %
Epoch 288 of 2000 took 0.095s
  training loss:		1.372910
  validation loss:		1.272418
  validation accuracy:		49.35 %
Epoch 289 of 2000 took 0.096s
  training loss:		1.351952
  validation loss:		1.275620
  validation accuracy:		50.00 %
Epoch 290 of 2000 took 0.096s
  training loss:		1.352419
  validation loss:		1.266455
  validation accuracy:		51.09 %
Epoch 291 of 2000 took 0.096s
  training loss:		1.355108
  validation loss:		1.263894
  validation accuracy:		51.20 %
Epoch 292 of 2000 took 0.095s
  training loss:		1.360758
  validation loss:		1.309610
  validation accuracy:		47.61 %
Epoch 293 of 2000 took 0.095s
  training loss:		1.363498
  validation loss:		1.273218
  validation accuracy:		48.15 %
Epoch 294 of 2000 took 0.095s
  training loss:		1.353235
  validation loss:		1.258941
  validation accuracy:		49.02 %
Epoch 295 of 2000 took 0.095s
  training loss:		1.354299
  validation loss:		1.265924
  validation accuracy:		49.24 %
Epoch 296 of 2000 took 0.095s
  training loss:		1.349663
  validation loss:		1.268295
  validation accuracy:		48.59 %
Epoch 297 of 2000 took 0.098s
  training loss:		1.345202
  validation loss:		1.271263
  validation accuracy:		46.63 %
Epoch 298 of 2000 took 0.096s
  training loss:		1.349820
  validation loss:		1.272372
  validation accuracy:		47.39 %
Epoch 299 of 2000 took 0.095s
  training loss:		1.404224
  validation loss:		1.287660
  validation accuracy:		48.91 %
Epoch 300 of 2000 took 0.095s
  training loss:		1.349590
  validation loss:		1.271376
  validation accuracy:		49.46 %
Epoch 301 of 2000 took 0.095s
  training loss:		1.349433
  validation loss:		1.282598
  validation accuracy:		48.91 %
Epoch 302 of 2000 took 0.095s
  training loss:		1.348175
  validation loss:		1.277879
  validation accuracy:		49.02 %
Epoch 303 of 2000 took 0.095s
  training loss:		1.347749
  validation loss:		1.272532
  validation accuracy:		45.98 %
Epoch 304 of 2000 took 0.095s
  training loss:		1.355516
  validation loss:		1.255003
  validation accuracy:		48.91 %
Epoch 305 of 2000 took 0.095s
  training loss:		1.338980
  validation loss:		1.263305
  validation accuracy:		47.17 %
Epoch 306 of 2000 took 0.095s
  training loss:		1.353150
  validation loss:		1.343516
  validation accuracy:		47.50 %
Epoch 307 of 2000 took 0.095s
  training loss:		1.331904
  validation loss:		1.269076
  validation accuracy:		48.48 %
Epoch 308 of 2000 took 0.095s
  training loss:		1.338564
  validation loss:		1.259500
  validation accuracy:		46.96 %
Epoch 309 of 2000 took 0.095s
  training loss:		1.369943
  validation loss:		1.371318
  validation accuracy:		45.33 %
Epoch 310 of 2000 took 0.095s
  training loss:		1.360128
  validation loss:		1.255186
  validation accuracy:		49.35 %
Epoch 311 of 2000 took 0.095s
  training loss:		1.342750
  validation loss:		1.263063
  validation accuracy:		50.87 %
Epoch 312 of 2000 took 0.095s
  training loss:		1.332770
  validation loss:		1.244032
  validation accuracy:		49.89 %
Epoch 313 of 2000 took 0.095s
  training loss:		1.332148
  validation loss:		1.255522
  validation accuracy:		48.59 %
Epoch 314 of 2000 took 0.095s
  training loss:		1.340429
  validation loss:		1.248627
  validation accuracy:		49.78 %
Epoch 315 of 2000 took 0.097s
  training loss:		1.331160
  validation loss:		1.242974
  validation accuracy:		50.43 %
Epoch 316 of 2000 took 0.096s
  training loss:		1.364161
  validation loss:		1.266793
  validation accuracy:		50.76 %
Epoch 317 of 2000 took 0.096s
  training loss:		1.329543
  validation loss:		1.250607
  validation accuracy:		49.35 %
Epoch 318 of 2000 took 0.095s
  training loss:		1.354979
  validation loss:		1.266604
  validation accuracy:		50.11 %
Epoch 319 of 2000 took 0.095s
  training loss:		1.313368
  validation loss:		1.241313
  validation accuracy:		50.33 %
Epoch 320 of 2000 took 0.095s
  training loss:		1.317289
  validation loss:		1.245923
  validation accuracy:		50.87 %
Epoch 321 of 2000 took 0.096s
  training loss:		1.330175
  validation loss:		1.240013
  validation accuracy:		50.54 %
Epoch 322 of 2000 took 0.095s
  training loss:		1.319225
  validation loss:		1.235953
  validation accuracy:		48.91 %
Epoch 323 of 2000 took 0.095s
  training loss:		1.319887
  validation loss:		1.234284
  validation accuracy:		50.76 %
Epoch 324 of 2000 took 0.095s
  training loss:		1.321192
  validation loss:		1.230219
  validation accuracy:		50.11 %
Epoch 325 of 2000 took 0.095s
  training loss:		1.321305
  validation loss:		1.290750
  validation accuracy:		45.87 %
Epoch 326 of 2000 took 0.095s
  training loss:		1.356864
  validation loss:		1.240212
  validation accuracy:		50.54 %
Epoch 327 of 2000 took 0.095s
  training loss:		1.304407
  validation loss:		1.223143
  validation accuracy:		50.98 %
Epoch 328 of 2000 took 0.095s
  training loss:		1.366437
  validation loss:		1.336236
  validation accuracy:		47.07 %
Epoch 329 of 2000 took 0.095s
  training loss:		1.311676
  validation loss:		1.226512
  validation accuracy:		51.85 %
Epoch 330 of 2000 took 0.095s
  training loss:		1.303745
  validation loss:		1.325203
  validation accuracy:		46.41 %
Epoch 331 of 2000 took 0.095s
  training loss:		1.342044
  validation loss:		1.232966
  validation accuracy:		50.98 %
Epoch 332 of 2000 took 0.095s
  training loss:		1.308897
  validation loss:		1.298945
  validation accuracy:		48.91 %
Epoch 333 of 2000 took 0.095s
  training loss:		1.316452
  validation loss:		1.284694
  validation accuracy:		49.67 %
Epoch 334 of 2000 took 0.095s
  training loss:		1.295025
  validation loss:		1.258272
  validation accuracy:		50.65 %
Epoch 335 of 2000 took 0.095s
  training loss:		1.306983
  validation loss:		1.222480
  validation accuracy:		50.76 %
Epoch 336 of 2000 took 0.095s
  training loss:		1.314914
  validation loss:		1.218792
  validation accuracy:		51.85 %
Epoch 337 of 2000 took 0.098s
  training loss:		1.299249
  validation loss:		1.219629
  validation accuracy:		51.30 %
Epoch 338 of 2000 took 0.096s
  training loss:		1.330484
  validation loss:		1.241641
  validation accuracy:		52.07 %
Epoch 339 of 2000 took 0.095s
  training loss:		1.368534
  validation loss:		1.221986
  validation accuracy:		51.30 %
Epoch 340 of 2000 took 0.095s
  training loss:		1.328946
  validation loss:		1.274078
  validation accuracy:		51.41 %
Epoch 341 of 2000 took 0.095s
  training loss:		1.294888
  validation loss:		1.217119
  validation accuracy:		53.15 %
Epoch 342 of 2000 took 0.095s
  training loss:		1.297796
  validation loss:		1.273472
  validation accuracy:		48.26 %
Epoch 343 of 2000 took 0.095s
  training loss:		1.312362
  validation loss:		1.210454
  validation accuracy:		52.72 %
Epoch 344 of 2000 took 0.095s
  training loss:		1.309519
  validation loss:		1.234874
  validation accuracy:		50.98 %
Epoch 345 of 2000 took 0.095s
  training loss:		1.286141
  validation loss:		1.265182
  validation accuracy:		50.43 %
Epoch 346 of 2000 took 0.096s
  training loss:		1.309999
  validation loss:		1.264157
  validation accuracy:		48.80 %
Epoch 347 of 2000 took 0.095s
  training loss:		1.339028
  validation loss:		1.281136
  validation accuracy:		51.41 %
Epoch 348 of 2000 took 0.095s
  training loss:		1.305690
  validation loss:		1.288595
  validation accuracy:		51.20 %
Epoch 349 of 2000 took 0.095s
  training loss:		1.322165
  validation loss:		1.250884
  validation accuracy:		49.78 %
Epoch 350 of 2000 took 0.095s
  training loss:		1.297050
  validation loss:		1.275728
  validation accuracy:		49.78 %
Epoch 351 of 2000 took 0.095s
  training loss:		1.287103
  validation loss:		1.204237
  validation accuracy:		52.17 %
Epoch 352 of 2000 took 0.096s
  training loss:		1.294286
  validation loss:		1.298332
  validation accuracy:		49.78 %
Epoch 353 of 2000 took 0.096s
  training loss:		1.284439
  validation loss:		1.197042
  validation accuracy:		54.57 %
Epoch 354 of 2000 took 0.095s
  training loss:		1.276013
  validation loss:		1.199745
  validation accuracy:		54.46 %
Epoch 355 of 2000 took 0.095s
  training loss:		1.285763
  validation loss:		1.245706
  validation accuracy:		52.39 %
Epoch 356 of 2000 took 0.095s
  training loss:		1.281525
  validation loss:		1.191051
  validation accuracy:		54.46 %
Epoch 357 of 2000 took 0.095s
  training loss:		1.288703
  validation loss:		1.232354
  validation accuracy:		53.15 %
Epoch 358 of 2000 took 0.095s
  training loss:		1.287864
  validation loss:		1.199362
  validation accuracy:		54.35 %
Epoch 359 of 2000 took 0.096s
  training loss:		1.291196
  validation loss:		1.198756
  validation accuracy:		52.93 %
Epoch 360 of 2000 took 0.095s
  training loss:		1.285067
  validation loss:		1.249554
  validation accuracy:		52.72 %
Epoch 361 of 2000 took 0.095s
  training loss:		1.334800
  validation loss:		1.195920
  validation accuracy:		54.02 %
Epoch 362 of 2000 took 0.095s
  training loss:		1.313414
  validation loss:		1.390161
  validation accuracy:		46.20 %
Epoch 363 of 2000 took 0.098s
  training loss:		1.349239
  validation loss:		1.205158
  validation accuracy:		54.13 %
Epoch 364 of 2000 took 0.096s
  training loss:		1.293859
  validation loss:		1.291119
  validation accuracy:		50.43 %
Epoch 365 of 2000 took 0.095s
  training loss:		1.294924
  validation loss:		1.223527
  validation accuracy:		54.13 %
Epoch 366 of 2000 took 0.096s
  training loss:		1.260452
  validation loss:		1.217593
  validation accuracy:		54.35 %
Epoch 367 of 2000 took 0.095s
  training loss:		1.263084
  validation loss:		1.197135
  validation accuracy:		54.35 %
Epoch 368 of 2000 took 0.095s
  training loss:		1.278270
  validation loss:		1.189822
  validation accuracy:		55.22 %
Epoch 369 of 2000 took 0.095s
  training loss:		1.267804
  validation loss:		1.187748
  validation accuracy:		54.46 %
Epoch 370 of 2000 took 0.095s
  training loss:		1.275337
  validation loss:		1.194884
  validation accuracy:		54.46 %
Epoch 371 of 2000 took 0.095s
  training loss:		1.316546
  validation loss:		1.264740
  validation accuracy:		52.72 %
Epoch 372 of 2000 took 0.095s
  training loss:		1.271107
  validation loss:		1.191424
  validation accuracy:		55.11 %
Epoch 373 of 2000 took 0.095s
  training loss:		1.292838
  validation loss:		1.189809
  validation accuracy:		54.57 %
Epoch 374 of 2000 took 0.095s
  training loss:		1.261422
  validation loss:		1.187070
  validation accuracy:		54.46 %
Epoch 375 of 2000 took 0.095s
  training loss:		1.274141
  validation loss:		1.251822
  validation accuracy:		53.37 %
Epoch 376 of 2000 took 0.095s
  training loss:		1.321720
  validation loss:		1.197871
  validation accuracy:		55.54 %
Epoch 377 of 2000 took 0.095s
  training loss:		1.261522
  validation loss:		1.190140
  validation accuracy:		54.78 %
Epoch 378 of 2000 took 0.095s
  training loss:		1.299004
  validation loss:		1.186168
  validation accuracy:		54.67 %
Epoch 379 of 2000 took 0.095s
  training loss:		1.264068
  validation loss:		1.212009
  validation accuracy:		55.43 %
Epoch 380 of 2000 took 0.095s
  training loss:		1.262530
  validation loss:		1.195962
  validation accuracy:		55.54 %
Epoch 381 of 2000 took 0.095s
  training loss:		1.286162
  validation loss:		1.214379
  validation accuracy:		52.07 %
Epoch 382 of 2000 took 0.095s
  training loss:		1.292795
  validation loss:		1.215341
  validation accuracy:		55.87 %
Epoch 383 of 2000 took 0.095s
  training loss:		1.281594
  validation loss:		1.194565
  validation accuracy:		55.33 %
Epoch 384 of 2000 took 0.096s
  training loss:		1.265514
  validation loss:		1.248575
  validation accuracy:		49.67 %
Epoch 385 of 2000 took 0.095s
  training loss:		1.290735
  validation loss:		1.185436
  validation accuracy:		54.78 %
Epoch 386 of 2000 took 0.095s
  training loss:		1.265641
  validation loss:		1.186559
  validation accuracy:		54.89 %
Epoch 387 of 2000 took 0.095s
  training loss:		1.286190
  validation loss:		1.194939
  validation accuracy:		53.80 %
Epoch 388 of 2000 took 0.095s
  training loss:		1.326097
  validation loss:		1.271473
  validation accuracy:		52.50 %
Epoch 389 of 2000 took 0.095s
  training loss:		1.317980
  validation loss:		1.180231
  validation accuracy:		55.22 %
Epoch 390 of 2000 took 0.095s
  training loss:		1.265590
  validation loss:		1.188227
  validation accuracy:		55.43 %
Epoch 391 of 2000 took 0.095s
  training loss:		1.254053
  validation loss:		1.188526
  validation accuracy:		54.24 %
Epoch 392 of 2000 took 0.095s
  training loss:		1.307006
  validation loss:		1.233557
  validation accuracy:		55.00 %
Epoch 393 of 2000 took 0.095s
  training loss:		1.272976
  validation loss:		1.238999
  validation accuracy:		53.70 %
Epoch 394 of 2000 took 0.095s
  training loss:		1.272327
  validation loss:		1.237769
  validation accuracy:		54.13 %
Epoch 395 of 2000 took 0.098s
  training loss:		1.265909
  validation loss:		1.177877
  validation accuracy:		54.57 %
Epoch 396 of 2000 took 0.095s
  training loss:		1.254941
  validation loss:		1.177266
  validation accuracy:		55.65 %
Epoch 397 of 2000 took 0.095s
  training loss:		1.253895
  validation loss:		1.181452
  validation accuracy:		53.48 %
Epoch 398 of 2000 took 0.095s
  training loss:		1.250009
  validation loss:		1.168143
  validation accuracy:		55.98 %
Epoch 399 of 2000 took 0.095s
  training loss:		1.289708
  validation loss:		1.186886
  validation accuracy:		53.26 %
Epoch 400 of 2000 took 0.096s
  training loss:		1.275188
  validation loss:		1.299713
  validation accuracy:		52.28 %
Epoch 401 of 2000 took 0.096s
  training loss:		1.286217
  validation loss:		1.203691
  validation accuracy:		55.11 %
Epoch 402 of 2000 took 0.096s
  training loss:		1.262803
  validation loss:		1.169131
  validation accuracy:		55.43 %
Epoch 403 of 2000 took 0.101s
  training loss:		1.245258
  validation loss:		1.180686
  validation accuracy:		56.30 %
Epoch 404 of 2000 took 0.104s
  training loss:		1.248730
  validation loss:		1.178663
  validation accuracy:		54.67 %
Epoch 405 of 2000 took 0.100s
  training loss:		1.258740
  validation loss:		1.172492
  validation accuracy:		54.57 %
Epoch 406 of 2000 took 0.095s
  training loss:		1.250192
  validation loss:		1.180464
  validation accuracy:		55.98 %
Epoch 407 of 2000 took 0.095s
  training loss:		1.242315
  validation loss:		1.201678
  validation accuracy:		56.30 %
Epoch 408 of 2000 took 0.096s
  training loss:		1.253012
  validation loss:		1.197599
  validation accuracy:		56.74 %
Epoch 409 of 2000 took 0.095s
  training loss:		1.253242
  validation loss:		1.167763
  validation accuracy:		55.98 %
Epoch 410 of 2000 took 0.096s
  training loss:		1.270665
  validation loss:		1.236849
  validation accuracy:		54.78 %
Epoch 411 of 2000 took 0.095s
  training loss:		1.332498
  validation loss:		1.236554
  validation accuracy:		50.33 %
Epoch 412 of 2000 took 0.095s
  training loss:		1.297479
  validation loss:		1.197996
  validation accuracy:		55.76 %
Epoch 413 of 2000 took 0.096s
  training loss:		1.272434
  validation loss:		1.231037
  validation accuracy:		53.26 %
Epoch 414 of 2000 took 0.095s
  training loss:		1.248329
  validation loss:		1.166682
  validation accuracy:		56.30 %
Epoch 415 of 2000 took 0.097s
  training loss:		1.245978
  validation loss:		1.168650
  validation accuracy:		56.30 %
Epoch 416 of 2000 took 0.099s
  training loss:		1.283081
  validation loss:		1.174757
  validation accuracy:		54.57 %
Epoch 417 of 2000 took 0.096s
  training loss:		1.274359
  validation loss:		1.206133
  validation accuracy:		56.74 %
Epoch 418 of 2000 took 0.096s
  training loss:		1.267147
  validation loss:		1.172887
  validation accuracy:		56.20 %
Epoch 419 of 2000 took 0.096s
  training loss:		1.237692
  validation loss:		1.171485
  validation accuracy:		54.35 %
Epoch 420 of 2000 took 0.096s
  training loss:		1.251758
  validation loss:		1.159964
  validation accuracy:		55.33 %
Epoch 421 of 2000 took 0.096s
  training loss:		1.251113
  validation loss:		1.167658
  validation accuracy:		55.76 %
Epoch 422 of 2000 took 0.096s
  training loss:		1.250912
  validation loss:		1.158448
  validation accuracy:		56.30 %
Epoch 423 of 2000 took 0.097s
  training loss:		1.241702
  validation loss:		1.164285
  validation accuracy:		57.07 %
Epoch 424 of 2000 took 0.096s
  training loss:		1.237112
  validation loss:		1.157795
  validation accuracy:		56.41 %
Epoch 425 of 2000 took 0.096s
  training loss:		1.371145
  validation loss:		1.176578
  validation accuracy:		54.02 %
Epoch 426 of 2000 took 0.096s
  training loss:		1.264512
  validation loss:		1.167156
  validation accuracy:		55.00 %
Epoch 427 of 2000 took 0.096s
  training loss:		1.243526
  validation loss:		1.179956
  validation accuracy:		56.63 %
Epoch 428 of 2000 took 0.096s
  training loss:		1.258172
  validation loss:		1.168368
  validation accuracy:		57.50 %
Epoch 429 of 2000 took 0.096s
  training loss:		1.238184
  validation loss:		1.174303
  validation accuracy:		57.61 %
Epoch 430 of 2000 took 0.096s
  training loss:		1.231444
  validation loss:		1.148493
  validation accuracy:		57.17 %
Epoch 431 of 2000 took 0.096s
  training loss:		1.227633
  validation loss:		1.149010
  validation accuracy:		57.61 %
Epoch 432 of 2000 took 0.099s
  training loss:		1.233322
  validation loss:		1.148557
  validation accuracy:		57.17 %
Epoch 433 of 2000 took 0.096s
  training loss:		1.230643
  validation loss:		1.163661
  validation accuracy:		57.39 %
Epoch 434 of 2000 took 0.097s
  training loss:		1.227667
  validation loss:		1.145470
  validation accuracy:		57.17 %
Epoch 435 of 2000 took 0.096s
  training loss:		1.227840
  validation loss:		1.141005
  validation accuracy:		58.15 %
Epoch 436 of 2000 took 0.096s
  training loss:		1.232135
  validation loss:		1.167512
  validation accuracy:		58.59 %
Epoch 437 of 2000 took 0.096s
  training loss:		1.236454
  validation loss:		1.174325
  validation accuracy:		57.72 %
Epoch 438 of 2000 took 0.096s
  training loss:		1.239933
  validation loss:		1.147130
  validation accuracy:		59.67 %
Epoch 439 of 2000 took 0.096s
  training loss:		1.225648
  validation loss:		1.131744
  validation accuracy:		57.93 %
Epoch 440 of 2000 took 0.096s
  training loss:		1.216175
  validation loss:		1.161491
  validation accuracy:		58.59 %
Epoch 441 of 2000 took 0.096s
  training loss:		1.212913
  validation loss:		1.127497
  validation accuracy:		58.48 %
Epoch 442 of 2000 took 0.096s
  training loss:		1.217256
  validation loss:		1.120528
  validation accuracy:		58.70 %
Epoch 443 of 2000 took 0.096s
  training loss:		1.215664
  validation loss:		1.121657
  validation accuracy:		58.80 %
Epoch 444 of 2000 took 0.096s
  training loss:		1.208293
  validation loss:		1.139238
  validation accuracy:		59.57 %
Epoch 445 of 2000 took 0.096s
  training loss:		1.208190
  validation loss:		1.121735
  validation accuracy:		59.46 %
Epoch 446 of 2000 took 0.096s
  training loss:		1.209264
  validation loss:		1.108852
  validation accuracy:		59.78 %
Epoch 447 of 2000 took 0.097s
  training loss:		1.214767
  validation loss:		1.116005
  validation accuracy:		60.00 %
Epoch 448 of 2000 took 0.096s
  training loss:		1.203986
  validation loss:		1.105075
  validation accuracy:		60.11 %
Epoch 449 of 2000 took 0.096s
  training loss:		1.190460
  validation loss:		1.124497
  validation accuracy:		61.41 %
Epoch 450 of 2000 took 0.096s
  training loss:		1.188771
  validation loss:		1.092758
  validation accuracy:		60.98 %
Epoch 451 of 2000 took 0.096s
  training loss:		1.191382
  validation loss:		1.090524
  validation accuracy:		61.52 %
Epoch 452 of 2000 took 0.096s
  training loss:		1.181012
  validation loss:		1.094033
  validation accuracy:		62.17 %
Epoch 453 of 2000 took 0.096s
  training loss:		1.216281
  validation loss:		1.109630
  validation accuracy:		58.04 %
Epoch 454 of 2000 took 0.096s
  training loss:		1.190035
  validation loss:		1.077157
  validation accuracy:		62.93 %
Epoch 455 of 2000 took 0.096s
  training loss:		1.184647
  validation loss:		1.072888
  validation accuracy:		63.04 %
Epoch 456 of 2000 took 0.096s
  training loss:		1.165132
  validation loss:		1.064391
  validation accuracy:		63.59 %
Epoch 457 of 2000 took 0.096s
  training loss:		1.154249
  validation loss:		1.062914
  validation accuracy:		64.46 %
Epoch 458 of 2000 took 0.096s
  training loss:		1.152242
  validation loss:		1.069127
  validation accuracy:		64.67 %
Epoch 459 of 2000 took 0.096s
  training loss:		1.137067
  validation loss:		1.041406
  validation accuracy:		65.43 %
Epoch 460 of 2000 took 0.096s
  training loss:		1.137178
  validation loss:		1.039164
  validation accuracy:		66.09 %
Epoch 461 of 2000 took 0.096s
  training loss:		1.141330
  validation loss:		1.022383
  validation accuracy:		66.85 %
Epoch 462 of 2000 took 0.096s
  training loss:		1.122062
  validation loss:		1.011547
  validation accuracy:		66.20 %
Epoch 463 of 2000 took 0.096s
  training loss:		1.126215
  validation loss:		1.017413
  validation accuracy:		66.85 %
Epoch 464 of 2000 took 0.096s
  training loss:		1.107275
  validation loss:		0.992068
  validation accuracy:		66.41 %
Epoch 465 of 2000 took 0.096s
  training loss:		1.094555
  validation loss:		1.012625
  validation accuracy:		66.41 %
Epoch 466 of 2000 took 0.096s
  training loss:		1.082440
  validation loss:		1.002039
  validation accuracy:		67.72 %
Epoch 467 of 2000 took 0.096s
  training loss:		1.104630
  validation loss:		0.994243
  validation accuracy:		68.04 %
Epoch 468 of 2000 took 0.096s
  training loss:		1.076650
  validation loss:		0.955095
  validation accuracy:		68.91 %
Epoch 469 of 2000 took 0.096s
  training loss:		1.067282
  validation loss:		0.939747
  validation accuracy:		68.80 %
Epoch 470 of 2000 took 0.096s
  training loss:		1.066643
  validation loss:		0.970045
  validation accuracy:		69.13 %
Epoch 471 of 2000 took 0.096s
  training loss:		1.054316
  validation loss:		0.923331
  validation accuracy:		70.00 %
Epoch 472 of 2000 took 0.096s
  training loss:		1.044304
  validation loss:		0.923578
  validation accuracy:		70.54 %
Epoch 473 of 2000 took 0.096s
  training loss:		1.023775
  validation loss:		0.910150
  validation accuracy:		71.20 %
Epoch 474 of 2000 took 0.096s
  training loss:		1.013088
  validation loss:		0.902677
  validation accuracy:		71.30 %
Epoch 475 of 2000 took 0.096s
  training loss:		1.014681
  validation loss:		0.891613
  validation accuracy:		71.63 %
Epoch 476 of 2000 took 0.098s
  training loss:		1.028441
  validation loss:		0.874829
  validation accuracy:		72.61 %
Epoch 477 of 2000 took 0.097s
  training loss:		0.994074
  validation loss:		0.876955
  validation accuracy:		72.39 %
Epoch 478 of 2000 took 0.097s
  training loss:		0.986640
  validation loss:		0.870551
  validation accuracy:		72.61 %
Epoch 479 of 2000 took 0.096s
  training loss:		0.972973
  validation loss:		0.846204
  validation accuracy:		73.37 %
Epoch 480 of 2000 took 0.096s
  training loss:		0.978977
  validation loss:		0.856455
  validation accuracy:		73.48 %
Epoch 481 of 2000 took 0.097s
  training loss:		0.976211
  validation loss:		0.870584
  validation accuracy:		73.04 %
Epoch 482 of 2000 took 0.096s
  training loss:		0.960831
  validation loss:		0.820947
  validation accuracy:		73.59 %
Epoch 483 of 2000 took 0.096s
  training loss:		0.946422
  validation loss:		0.833723
  validation accuracy:		73.91 %
Epoch 484 of 2000 took 0.096s
  training loss:		0.936101
  validation loss:		0.810205
  validation accuracy:		74.57 %
Epoch 485 of 2000 took 0.096s
  training loss:		0.930460
  validation loss:		0.798557
  validation accuracy:		75.54 %
Epoch 486 of 2000 took 0.096s
  training loss:		0.915297
  validation loss:		0.805012
  validation accuracy:		74.89 %
Epoch 487 of 2000 took 0.096s
  training loss:		0.905369
  validation loss:		0.791544
  validation accuracy:		75.11 %
Epoch 488 of 2000 took 0.096s
  training loss:		0.912695
  validation loss:		0.792555
  validation accuracy:		75.54 %
Epoch 489 of 2000 took 0.096s
  training loss:		0.888459
  validation loss:		0.776739
  validation accuracy:		76.41 %
Epoch 490 of 2000 took 0.096s
  training loss:		0.879117
  validation loss:		0.754009
  validation accuracy:		76.20 %
Epoch 491 of 2000 took 0.096s
  training loss:		0.887426
  validation loss:		0.752679
  validation accuracy:		76.09 %
Epoch 492 of 2000 took 0.096s
  training loss:		0.862235
  validation loss:		0.751919
  validation accuracy:		75.98 %
Epoch 493 of 2000 took 0.096s
  training loss:		0.860757
  validation loss:		0.736763
  validation accuracy:		77.07 %
Epoch 494 of 2000 took 0.096s
  training loss:		0.855159
  validation loss:		0.733224
  validation accuracy:		77.83 %
Epoch 495 of 2000 took 0.096s
  training loss:		0.848671
  validation loss:		0.718860
  validation accuracy:		77.07 %
Epoch 496 of 2000 took 0.096s
  training loss:		0.837136
  validation loss:		0.737314
  validation accuracy:		77.28 %
Epoch 497 of 2000 took 0.096s
  training loss:		0.832721
  validation loss:		0.708750
  validation accuracy:		77.17 %
Epoch 498 of 2000 took 0.096s
  training loss:		0.824206
  validation loss:		0.704585
  validation accuracy:		78.04 %
Epoch 499 of 2000 took 0.097s
  training loss:		0.812690
  validation loss:		0.687977
  validation accuracy:		77.93 %
Epoch 500 of 2000 took 0.096s
  training loss:		0.809985
  validation loss:		0.694213
  validation accuracy:		78.70 %
Epoch 501 of 2000 took 0.096s
  training loss:		0.795753
  validation loss:		0.685860
  validation accuracy:		78.48 %
Epoch 502 of 2000 took 0.096s
  training loss:		0.790145
  validation loss:		0.673484
  validation accuracy:		78.91 %
Epoch 503 of 2000 took 0.096s
  training loss:		0.794563
  validation loss:		0.678689
  validation accuracy:		78.26 %
Epoch 504 of 2000 took 0.096s
  training loss:		0.788050
  validation loss:		0.677802
  validation accuracy:		78.37 %
Epoch 505 of 2000 took 0.096s
  training loss:		0.779041
  validation loss:		0.679735
  validation accuracy:		78.80 %
Epoch 506 of 2000 took 0.096s
  training loss:		0.778416
  validation loss:		0.656048
  validation accuracy:		79.46 %
Epoch 507 of 2000 took 0.096s
  training loss:		0.761818
  validation loss:		0.654115
  validation accuracy:		78.91 %
Epoch 508 of 2000 took 0.096s
  training loss:		0.770811
  validation loss:		0.667804
  validation accuracy:		78.59 %
Epoch 509 of 2000 took 0.097s
  training loss:		0.747565
  validation loss:		0.636104
  validation accuracy:		79.46 %
Epoch 510 of 2000 took 0.096s
  training loss:		0.757415
  validation loss:		0.671391
  validation accuracy:		78.91 %
Epoch 511 of 2000 took 0.096s
  training loss:		0.750511
  validation loss:		0.636371
  validation accuracy:		79.57 %
Epoch 512 of 2000 took 0.096s
  training loss:		0.741020
  validation loss:		0.628801
  validation accuracy:		79.67 %
Epoch 513 of 2000 took 0.096s
  training loss:		0.746486
  validation loss:		0.640305
  validation accuracy:		79.57 %
Epoch 514 of 2000 took 0.096s
  training loss:		0.731338
  validation loss:		0.626367
  validation accuracy:		79.57 %
Epoch 515 of 2000 took 0.096s
  training loss:		0.722732
  validation loss:		0.625539
  validation accuracy:		79.57 %
Epoch 516 of 2000 took 0.096s
  training loss:		0.715238
  validation loss:		0.627825
  validation accuracy:		80.11 %
Epoch 517 of 2000 took 0.096s
  training loss:		0.722949
  validation loss:		0.618680
  validation accuracy:		80.11 %
Epoch 518 of 2000 took 0.096s
  training loss:		0.726709
  validation loss:		0.629216
  validation accuracy:		79.57 %
Epoch 519 of 2000 took 0.096s
  training loss:		0.720521
  validation loss:		0.619312
  validation accuracy:		80.22 %
Epoch 520 of 2000 took 0.096s
  training loss:		0.720200
  validation loss:		0.612628
  validation accuracy:		79.78 %
Epoch 521 of 2000 took 0.096s
  training loss:		0.711975
  validation loss:		0.605595
  validation accuracy:		79.89 %
Epoch 522 of 2000 took 0.096s
  training loss:		0.704460
  validation loss:		0.600436
  validation accuracy:		80.11 %
Epoch 523 of 2000 took 0.096s
  training loss:		0.697780
  validation loss:		0.626089
  validation accuracy:		80.65 %
Epoch 524 of 2000 took 0.096s
  training loss:		0.700580
  validation loss:		0.597539
  validation accuracy:		80.00 %
Epoch 525 of 2000 took 0.096s
  training loss:		0.692585
  validation loss:		0.596270
  validation accuracy:		80.54 %
Epoch 526 of 2000 took 0.096s
  training loss:		0.695158
  validation loss:		0.611717
  validation accuracy:		80.54 %
Epoch 527 of 2000 took 0.096s
  training loss:		0.695689
  validation loss:		0.600420
  validation accuracy:		80.65 %
Epoch 528 of 2000 took 0.096s
  training loss:		0.685485
  validation loss:		0.589553
  validation accuracy:		80.54 %
Epoch 529 of 2000 took 0.096s
  training loss:		0.682193
  validation loss:		0.609430
  validation accuracy:		80.98 %
Epoch 530 of 2000 took 0.099s
  training loss:		0.677822
  validation loss:		0.603402
  validation accuracy:		81.09 %
Epoch 531 of 2000 took 0.096s
  training loss:		0.674430
  validation loss:		0.581477
  validation accuracy:		80.87 %
Epoch 532 of 2000 took 0.096s
  training loss:		0.666942
  validation loss:		0.609122
  validation accuracy:		80.65 %
Epoch 533 of 2000 took 0.096s
  training loss:		0.668183
  validation loss:		0.595334
  validation accuracy:		81.41 %
Epoch 534 of 2000 took 0.096s
  training loss:		0.668200
  validation loss:		0.586293
  validation accuracy:		81.63 %
Epoch 535 of 2000 took 0.096s
  training loss:		0.665642
  validation loss:		0.584558
  validation accuracy:		81.52 %
Epoch 536 of 2000 took 0.096s
  training loss:		0.660115
  validation loss:		0.598733
  validation accuracy:		81.41 %
Epoch 537 of 2000 took 0.096s
  training loss:		0.664790
  validation loss:		0.587386
  validation accuracy:		81.52 %
Epoch 538 of 2000 took 0.096s
  training loss:		0.661163
  validation loss:		0.581258
  validation accuracy:		81.41 %
Epoch 539 of 2000 took 0.096s
  training loss:		0.648962
  validation loss:		0.573199
  validation accuracy:		82.17 %
Epoch 540 of 2000 took 0.097s
  training loss:		0.656864
  validation loss:		0.577494
  validation accuracy:		81.41 %
Epoch 541 of 2000 took 0.096s
  training loss:		0.650955
  validation loss:		0.579939
  validation accuracy:		81.41 %
Epoch 542 of 2000 took 0.096s
  training loss:		0.647966
  validation loss:		0.590372
  validation accuracy:		81.41 %
Epoch 543 of 2000 took 0.096s
  training loss:		0.649284
  validation loss:		0.592884
  validation accuracy:		80.87 %
Epoch 544 of 2000 took 0.096s
  training loss:		0.647243
  validation loss:		0.590538
  validation accuracy:		80.54 %
Epoch 545 of 2000 took 0.096s
  training loss:		0.644859
  validation loss:		0.604011
  validation accuracy:		81.09 %
Epoch 546 of 2000 took 0.096s
  training loss:		0.643309
  validation loss:		0.585032
  validation accuracy:		81.52 %
Epoch 547 of 2000 took 0.096s
  training loss:		0.639962
  validation loss:		0.591945
  validation accuracy:		81.52 %
Epoch 548 of 2000 took 0.096s
  training loss:		0.626267
  validation loss:		0.599610
  validation accuracy:		80.98 %
Epoch 549 of 2000 took 0.096s
  training loss:		0.640197
  validation loss:		0.593266
  validation accuracy:		79.78 %
Epoch 550 of 2000 took 0.096s
  training loss:		0.644202
  validation loss:		0.565957
  validation accuracy:		82.07 %
Epoch 551 of 2000 took 0.096s
  training loss:		0.636676
  validation loss:		0.585476
  validation accuracy:		81.30 %
Epoch 552 of 2000 took 0.096s
  training loss:		0.634919
  validation loss:		0.566872
  validation accuracy:		82.50 %
Epoch 553 of 2000 took 0.096s
  training loss:		0.632446
  validation loss:		0.586841
  validation accuracy:		81.85 %
Epoch 554 of 2000 took 0.096s
  training loss:		0.622636
  validation loss:		0.572477
  validation accuracy:		82.17 %
Epoch 555 of 2000 took 0.096s
  training loss:		0.631025
  validation loss:		0.579306
  validation accuracy:		81.96 %
Epoch 556 of 2000 took 0.096s
  training loss:		0.629312
  validation loss:		0.569287
  validation accuracy:		81.85 %
Epoch 557 of 2000 took 0.096s
  training loss:		0.621949
  validation loss:		0.568717
  validation accuracy:		82.39 %
Epoch 558 of 2000 took 0.096s
  training loss:		0.624683
  validation loss:		0.564873
  validation accuracy:		82.39 %
Epoch 559 of 2000 took 0.096s
  training loss:		0.623356
  validation loss:		0.562172
  validation accuracy:		81.52 %
Epoch 560 of 2000 took 0.096s
  training loss:		0.639352
  validation loss:		0.593738
  validation accuracy:		81.09 %
Epoch 561 of 2000 took 0.096s
  training loss:		0.624402
  validation loss:		0.565676
  validation accuracy:		82.28 %
Epoch 562 of 2000 took 0.096s
  training loss:		0.618694
  validation loss:		0.610963
  validation accuracy:		80.43 %
Epoch 563 of 2000 took 0.096s
  training loss:		0.617084
  validation loss:		0.563320
  validation accuracy:		82.17 %
Epoch 564 of 2000 took 0.096s
  training loss:		0.613216
  validation loss:		0.564310
  validation accuracy:		82.39 %
Epoch 565 of 2000 took 0.096s
  training loss:		0.612794
  validation loss:		0.555797
  validation accuracy:		82.28 %
Epoch 566 of 2000 took 0.096s
  training loss:		0.616122
  validation loss:		0.560642
  validation accuracy:		82.28 %
Epoch 567 of 2000 took 0.097s
  training loss:		0.608754
  validation loss:		0.572802
  validation accuracy:		82.28 %
Epoch 568 of 2000 took 0.096s
  training loss:		0.600575
  validation loss:		0.560610
  validation accuracy:		82.72 %
Epoch 569 of 2000 took 0.096s
  training loss:		0.625441
  validation loss:		0.603367
  validation accuracy:		80.22 %
Epoch 570 of 2000 took 0.096s
  training loss:		0.606718
  validation loss:		0.571228
  validation accuracy:		82.07 %
Epoch 571 of 2000 took 0.096s
  training loss:		0.607421
  validation loss:		0.582287
  validation accuracy:		81.20 %
Epoch 572 of 2000 took 0.097s
  training loss:		0.602581
  validation loss:		0.577812
  validation accuracy:		81.30 %
Epoch 573 of 2000 took 0.096s
  training loss:		0.667347
  validation loss:		0.567883
  validation accuracy:		81.74 %
Epoch 574 of 2000 took 0.096s
  training loss:		0.614001
  validation loss:		0.564641
  validation accuracy:		82.17 %
Epoch 575 of 2000 took 0.096s
  training loss:		0.603522
  validation loss:		0.582331
  validation accuracy:		81.52 %
Epoch 576 of 2000 took 0.096s
  training loss:		0.608366
  validation loss:		0.593971
  validation accuracy:		80.43 %
Epoch 577 of 2000 took 0.096s
  training loss:		0.604314
  validation loss:		0.574623
  validation accuracy:		81.74 %
Epoch 578 of 2000 took 0.096s
  training loss:		0.602578
  validation loss:		0.603193
  validation accuracy:		80.11 %
Epoch 579 of 2000 took 0.096s
  training loss:		0.610556
  validation loss:		0.558257
  validation accuracy:		82.17 %
Epoch 580 of 2000 took 0.096s
  training loss:		0.606160
  validation loss:		0.564749
  validation accuracy:		81.52 %
Epoch 581 of 2000 took 0.096s
  training loss:		0.613136
  validation loss:		0.593754
  validation accuracy:		80.54 %
Epoch 582 of 2000 took 0.096s
  training loss:		0.596990
  validation loss:		0.571833
  validation accuracy:		82.28 %
Epoch 583 of 2000 took 0.096s
  training loss:		0.603319
  validation loss:		0.565429
  validation accuracy:		81.96 %
Epoch 584 of 2000 took 0.096s
  training loss:		0.594984
  validation loss:		0.570378
  validation accuracy:		81.41 %
Epoch 585 of 2000 took 0.096s
  training loss:		0.601041
  validation loss:		0.576081
  validation accuracy:		81.41 %
Epoch 586 of 2000 took 0.096s
  training loss:		0.600157
  validation loss:		0.579115
  validation accuracy:		81.20 %
Epoch 587 of 2000 took 0.096s
  training loss:		0.586497
  validation loss:		0.556417
  validation accuracy:		82.50 %
Epoch 588 of 2000 took 0.096s
  training loss:		0.609410
  validation loss:		0.654190
  validation accuracy:		79.13 %
Epoch 589 of 2000 took 0.096s
  training loss:		0.612329
  validation loss:		0.617600
  validation accuracy:		80.00 %
Epoch 590 of 2000 took 0.096s
  training loss:		0.633619
  validation loss:		0.574621
  validation accuracy:		81.20 %
Epoch 591 of 2000 took 0.096s
  training loss:		0.588584
  validation loss:		0.569791
  validation accuracy:		81.85 %
Epoch 592 of 2000 took 0.096s
  training loss:		0.602326
  validation loss:		0.568079
  validation accuracy:		81.41 %
Epoch 593 of 2000 took 0.096s
  training loss:		0.629014
  validation loss:		0.854141
  validation accuracy:		71.52 %
Epoch 594 of 2000 took 0.099s
  training loss:		0.819300
  validation loss:		0.565902
  validation accuracy:		82.93 %
Epoch 595 of 2000 took 0.097s
  training loss:		0.595868
  validation loss:		0.576734
  validation accuracy:		81.41 %
Epoch 596 of 2000 took 0.096s
  training loss:		0.595796
  validation loss:		0.557115
  validation accuracy:		82.07 %
Epoch 597 of 2000 took 0.096s
  training loss:		0.590430
  validation loss:		0.568216
  validation accuracy:		81.74 %
Epoch 598 of 2000 took 0.096s
  training loss:		0.622310
  validation loss:		0.564622
  validation accuracy:		81.63 %
Epoch 599 of 2000 took 0.096s
  training loss:		0.591008
  validation loss:		0.552226
  validation accuracy:		82.39 %
Epoch 600 of 2000 took 0.096s
  training loss:		0.600232
  validation loss:		0.586571
  validation accuracy:		81.30 %
Epoch 601 of 2000 took 0.096s
  training loss:		0.612986
  validation loss:		0.578540
  validation accuracy:		81.52 %
Epoch 602 of 2000 took 0.096s
  training loss:		0.610526
  validation loss:		0.631525
  validation accuracy:		79.13 %
Epoch 603 of 2000 took 0.097s
  training loss:		0.579066
  validation loss:		0.556661
  validation accuracy:		82.28 %
Epoch 604 of 2000 took 0.096s
  training loss:		0.595716
  validation loss:		0.639964
  validation accuracy:		78.91 %
Epoch 605 of 2000 took 0.097s
  training loss:		0.613462
  validation loss:		0.570878
  validation accuracy:		81.85 %
Epoch 606 of 2000 took 0.096s
  training loss:		0.587332
  validation loss:		0.560536
  validation accuracy:		81.96 %
Epoch 607 of 2000 took 0.096s
  training loss:		0.602937
  validation loss:		0.557993
  validation accuracy:		82.50 %
Epoch 608 of 2000 took 0.096s
  training loss:		0.581665
  validation loss:		0.564203
  validation accuracy:		81.41 %
Epoch 609 of 2000 took 0.096s
  training loss:		0.586065
  validation loss:		0.564907
  validation accuracy:		81.74 %
Epoch 610 of 2000 took 0.096s
  training loss:		0.584654
  validation loss:		0.564285
  validation accuracy:		82.17 %
Epoch 611 of 2000 took 0.096s
  training loss:		0.587632
  validation loss:		0.584289
  validation accuracy:		81.74 %
Epoch 612 of 2000 took 0.096s
  training loss:		0.655104
  validation loss:		0.552277
  validation accuracy:		82.39 %
Epoch 613 of 2000 took 0.096s
  training loss:		0.581057
  validation loss:		0.571833
  validation accuracy:		81.74 %
Epoch 614 of 2000 took 0.096s
  training loss:		0.600269
  validation loss:		0.561388
  validation accuracy:		81.96 %
Epoch 615 of 2000 took 0.096s
  training loss:		0.579398
  validation loss:		0.557328
  validation accuracy:		81.74 %
Epoch 616 of 2000 took 0.096s
  training loss:		0.570132
  validation loss:		0.670183
  validation accuracy:		77.93 %
Epoch 617 of 2000 took 0.096s
  training loss:		0.607130
  validation loss:		0.630231
  validation accuracy:		79.57 %
Epoch 618 of 2000 took 0.096s
  training loss:		0.593002
  validation loss:		0.575288
  validation accuracy:		81.30 %
Epoch 619 of 2000 took 0.096s
  training loss:		0.602478
  validation loss:		0.697039
  validation accuracy:		77.07 %
Epoch 620 of 2000 took 0.096s
  training loss:		0.632857
  validation loss:		0.555705
  validation accuracy:		81.96 %
Epoch 621 of 2000 took 0.096s
  training loss:		0.593958
  validation loss:		0.579326
  validation accuracy:		81.74 %
Epoch 622 of 2000 took 0.096s
  training loss:		0.594391
  validation loss:		0.561956
  validation accuracy:		81.85 %
Epoch 623 of 2000 took 0.096s
  training loss:		0.584097
  validation loss:		0.570703
  validation accuracy:		81.96 %
Epoch 624 of 2000 took 0.096s
  training loss:		0.579574
  validation loss:		0.564955
  validation accuracy:		81.85 %
Epoch 625 of 2000 took 0.096s
  training loss:		0.582983
  validation loss:		0.558989
  validation accuracy:		81.96 %
Epoch 626 of 2000 took 0.096s
  training loss:		0.570630
  validation loss:		0.556632
  validation accuracy:		81.96 %
Epoch 627 of 2000 took 0.096s
  training loss:		0.580548
  validation loss:		0.579855
  validation accuracy:		82.07 %
Epoch 628 of 2000 took 0.096s
  training loss:		0.588306
  validation loss:		0.566746
  validation accuracy:		81.52 %
Epoch 629 of 2000 took 0.096s
  training loss:		0.615110
  validation loss:		0.639430
  validation accuracy:		79.13 %
Epoch 630 of 2000 took 0.096s
  training loss:		0.593410
  validation loss:		0.578636
  validation accuracy:		81.74 %
Epoch 631 of 2000 took 0.096s
  training loss:		0.593310
  validation loss:		0.566490
  validation accuracy:		82.07 %
Epoch 632 of 2000 took 0.096s
  training loss:		0.599110
  validation loss:		0.572842
  validation accuracy:		81.85 %
Epoch 633 of 2000 took 0.096s
  training loss:		0.588018
  validation loss:		0.631883
  validation accuracy:		79.57 %
Epoch 634 of 2000 took 0.097s
  training loss:		0.582301
  validation loss:		0.564318
  validation accuracy:		81.96 %
Epoch 635 of 2000 took 0.096s
  training loss:		0.566033
  validation loss:		0.569229
  validation accuracy:		81.74 %
Epoch 636 of 2000 took 0.096s
  training loss:		0.600717
  validation loss:		0.634458
  validation accuracy:		79.24 %
Epoch 637 of 2000 took 0.096s
  training loss:		0.576722
  validation loss:		0.554961
  validation accuracy:		81.41 %
Epoch 638 of 2000 took 0.096s
  training loss:		0.572124
  validation loss:		0.573475
  validation accuracy:		81.96 %
Epoch 639 of 2000 took 0.096s
  training loss:		0.572284
  validation loss:		0.561812
  validation accuracy:		81.30 %
Epoch 640 of 2000 took 0.096s
  training loss:		0.565420
  validation loss:		0.613883
  validation accuracy:		80.22 %
Epoch 641 of 2000 took 0.096s
  training loss:		0.570989
  validation loss:		0.580595
  validation accuracy:		81.52 %
Epoch 642 of 2000 took 0.096s
  training loss:		0.577065
  validation loss:		0.557307
  validation accuracy:		81.85 %
Epoch 643 of 2000 took 0.096s
  training loss:		0.584896
  validation loss:		0.558120
  validation accuracy:		81.20 %
Epoch 644 of 2000 took 0.096s
  training loss:		0.596247
  validation loss:		0.633059
  validation accuracy:		79.46 %
Epoch 645 of 2000 took 0.096s
  training loss:		0.573411
  validation loss:		0.550584
  validation accuracy:		81.74 %
Epoch 646 of 2000 took 0.096s
  training loss:		0.581693
  validation loss:		0.565939
  validation accuracy:		81.30 %
Epoch 647 of 2000 took 0.096s
  training loss:		0.582721
  validation loss:		0.550878
  validation accuracy:		82.17 %
Epoch 648 of 2000 took 0.096s
  training loss:		0.605726
  validation loss:		0.559048
  validation accuracy:		82.17 %
Epoch 649 of 2000 took 0.096s
  training loss:		0.563530
  validation loss:		0.586960
  validation accuracy:		81.52 %
Epoch 650 of 2000 took 0.097s
  training loss:		0.585317
  validation loss:		0.559824
  validation accuracy:		82.07 %
Epoch 651 of 2000 took 0.096s
  training loss:		0.564563
  validation loss:		0.571004
  validation accuracy:		81.52 %
Epoch 652 of 2000 took 0.096s
  training loss:		0.580283
  validation loss:		0.656554
  validation accuracy:		78.59 %
Epoch 653 of 2000 took 0.096s
  training loss:		0.586839
  validation loss:		0.560177
  validation accuracy:		81.30 %
Epoch 654 of 2000 took 0.096s
  training loss:		0.598806
  validation loss:		0.590784
  validation accuracy:		81.20 %
Epoch 655 of 2000 took 0.096s
  training loss:		0.568871
  validation loss:		0.600717
  validation accuracy:		80.87 %
Epoch 656 of 2000 took 0.096s
  training loss:		0.576678
  validation loss:		0.561943
  validation accuracy:		82.17 %
Epoch 657 of 2000 took 0.096s
  training loss:		0.571267
  validation loss:		0.565494
  validation accuracy:		81.63 %
Epoch 658 of 2000 took 0.097s
  training loss:		0.611869
  validation loss:		0.680124
  validation accuracy:		76.96 %
Epoch 659 of 2000 took 0.096s
  training loss:		0.583716
  validation loss:		0.595103
  validation accuracy:		81.09 %
Epoch 660 of 2000 took 0.096s
  training loss:		0.575944
  validation loss:		0.591249
  validation accuracy:		80.87 %
Epoch 661 of 2000 took 0.096s
  training loss:		0.570127
  validation loss:		0.557200
  validation accuracy:		81.41 %
Epoch 662 of 2000 took 0.096s
  training loss:		0.582831
  validation loss:		0.554883
  validation accuracy:		81.41 %
Epoch 663 of 2000 took 0.096s
  training loss:		0.563204
  validation loss:		0.566846
  validation accuracy:		81.63 %
Epoch 664 of 2000 took 0.096s
  training loss:		0.562697
  validation loss:		0.640471
  validation accuracy:		78.80 %
Epoch 665 of 2000 took 0.097s
  training loss:		0.571699
  validation loss:		0.623255
  validation accuracy:		79.89 %
Epoch 666 of 2000 took 0.097s
  training loss:		0.569592
  validation loss:		0.620472
  validation accuracy:		79.78 %
Epoch 667 of 2000 took 0.096s
  training loss:		0.588333
  validation loss:		0.560926
  validation accuracy:		81.63 %
Epoch 668 of 2000 took 0.096s
  training loss:		0.586570
  validation loss:		0.651555
  validation accuracy:		78.48 %
Epoch 669 of 2000 took 0.096s
  training loss:		0.591310
  validation loss:		0.588691
  validation accuracy:		80.54 %
Epoch 670 of 2000 took 0.096s
  training loss:		0.617778
  validation loss:		0.557888
  validation accuracy:		81.96 %
Epoch 671 of 2000 took 0.096s
  training loss:		0.577363
  validation loss:		0.587296
  validation accuracy:		81.30 %
Epoch 672 of 2000 took 0.099s
  training loss:		0.577875
  validation loss:		0.554686
  validation accuracy:		81.63 %
Epoch 673 of 2000 took 0.096s
  training loss:		0.585709
  validation loss:		0.589145
  validation accuracy:		80.65 %
Epoch 674 of 2000 took 0.096s
  training loss:		0.586379
  validation loss:		0.551026
  validation accuracy:		81.52 %
Epoch 675 of 2000 took 0.096s
  training loss:		0.567010
  validation loss:		0.546988
  validation accuracy:		81.85 %
Epoch 676 of 2000 took 0.096s
  training loss:		0.571723
  validation loss:		0.550864
  validation accuracy:		81.20 %
Epoch 677 of 2000 took 0.096s
  training loss:		0.570617
  validation loss:		0.600589
  validation accuracy:		80.22 %
Epoch 678 of 2000 took 0.096s
  training loss:		0.574672
  validation loss:		0.562999
  validation accuracy:		81.52 %
Epoch 679 of 2000 took 0.096s
  training loss:		0.566849
  validation loss:		0.553165
  validation accuracy:		81.52 %
Epoch 680 of 2000 took 0.096s
  training loss:		0.563260
  validation loss:		0.562377
  validation accuracy:		81.52 %
Epoch 681 of 2000 took 0.096s
  training loss:		0.583191
  validation loss:		0.584598
  validation accuracy:		81.30 %
Epoch 682 of 2000 took 0.096s
  training loss:		0.565267
  validation loss:		0.565718
  validation accuracy:		81.63 %
Epoch 683 of 2000 took 0.096s
  training loss:		0.558214
  validation loss:		0.554639
  validation accuracy:		81.63 %
Epoch 684 of 2000 took 0.096s
  training loss:		0.568981
  validation loss:		0.667633
  validation accuracy:		78.04 %
Epoch 685 of 2000 took 0.096s
  training loss:		0.581716
  validation loss:		0.570687
  validation accuracy:		81.85 %
Epoch 686 of 2000 took 0.096s
  training loss:		0.562575
  validation loss:		0.569167
  validation accuracy:		81.20 %
Epoch 687 of 2000 took 0.096s
  training loss:		0.573171
  validation loss:		0.700420
  validation accuracy:		76.85 %
Epoch 688 of 2000 took 0.097s
  training loss:		0.646460
  validation loss:		0.550160
  validation accuracy:		81.09 %
Epoch 689 of 2000 took 0.096s
  training loss:		0.558918
  validation loss:		0.553518
  validation accuracy:		81.09 %
Epoch 690 of 2000 took 0.096s
  training loss:		0.562074
  validation loss:		0.549155
  validation accuracy:		81.09 %
Epoch 691 of 2000 took 0.096s
  training loss:		0.561382
  validation loss:		0.549478
  validation accuracy:		81.52 %
Epoch 692 of 2000 took 0.096s
  training loss:		0.565511
  validation loss:		0.593705
  validation accuracy:		80.65 %
Epoch 693 of 2000 took 0.097s
  training loss:		0.546745
  validation loss:		0.553072
  validation accuracy:		81.52 %
Epoch 694 of 2000 took 0.096s
  training loss:		0.571957
  validation loss:		0.555321
  validation accuracy:		81.63 %
Epoch 695 of 2000 took 0.096s
  training loss:		0.557198
  validation loss:		0.568334
  validation accuracy:		81.41 %
Epoch 696 of 2000 took 0.096s
  training loss:		0.567346
  validation loss:		0.552876
  validation accuracy:		81.52 %
Epoch 697 of 2000 took 0.097s
  training loss:		0.574479
  validation loss:		0.554591
  validation accuracy:		81.41 %
Epoch 698 of 2000 took 0.096s
  training loss:		0.567746
  validation loss:		0.557831
  validation accuracy:		81.30 %
Epoch 699 of 2000 took 0.096s
  training loss:		0.570426
  validation loss:		0.560451
  validation accuracy:		81.52 %
Epoch 700 of 2000 took 0.096s
  training loss:		0.562702
  validation loss:		0.563469
  validation accuracy:		81.30 %
Epoch 701 of 2000 took 0.096s
  training loss:		0.567581
  validation loss:		0.575835
  validation accuracy:		80.98 %
Epoch 702 of 2000 took 0.096s
  training loss:		0.568610
  validation loss:		0.573848
  validation accuracy:		81.30 %
Epoch 703 of 2000 took 0.096s
  training loss:		0.568181
  validation loss:		0.601688
  validation accuracy:		80.43 %
Epoch 704 of 2000 took 0.096s
  training loss:		0.608888
  validation loss:		0.549694
  validation accuracy:		81.52 %
Epoch 705 of 2000 took 0.096s
  training loss:		0.571782
  validation loss:		0.561717
  validation accuracy:		81.63 %
Epoch 706 of 2000 took 0.096s
  training loss:		0.553603
  validation loss:		0.559968
  validation accuracy:		81.09 %
Epoch 707 of 2000 took 0.096s
  training loss:		0.569981
  validation loss:		0.555180
  validation accuracy:		81.41 %
Epoch 708 of 2000 took 0.096s
  training loss:		0.559949
  validation loss:		0.577588
  validation accuracy:		81.96 %
Epoch 709 of 2000 took 0.096s
  training loss:		0.571854
  validation loss:		0.624814
  validation accuracy:		78.80 %
Epoch 710 of 2000 took 0.096s
  training loss:		0.561117
  validation loss:		0.556272
  validation accuracy:		81.41 %
Epoch 711 of 2000 took 0.096s
  training loss:		0.560430
  validation loss:		0.577953
  validation accuracy:		81.85 %
Epoch 712 of 2000 took 0.096s
  training loss:		0.553079
  validation loss:		0.552498
  validation accuracy:		81.20 %
Epoch 713 of 2000 took 0.096s
  training loss:		0.561739
  validation loss:		0.553025
  validation accuracy:		81.52 %
Epoch 714 of 2000 took 0.096s
  training loss:		0.552965
  validation loss:		0.551668
  validation accuracy:		81.09 %
Epoch 715 of 2000 took 0.096s
  training loss:		0.565945
  validation loss:		0.567021
  validation accuracy:		81.20 %
Epoch 716 of 2000 took 0.096s
  training loss:		0.568861
  validation loss:		0.554123
  validation accuracy:		81.74 %
Epoch 717 of 2000 took 0.096s
  training loss:		0.565724
  validation loss:		0.614168
  validation accuracy:		79.46 %
Epoch 718 of 2000 took 0.096s
  training loss:		0.564478
  validation loss:		0.583104
  validation accuracy:		81.85 %
Epoch 719 of 2000 took 0.096s
  training loss:		0.573269
  validation loss:		0.622375
  validation accuracy:		79.02 %
Epoch 720 of 2000 took 0.096s
  training loss:		0.569213
  validation loss:		0.554447
  validation accuracy:		81.52 %
Epoch 721 of 2000 took 0.096s
  training loss:		0.558872
  validation loss:		0.550529
  validation accuracy:		81.41 %
Epoch 722 of 2000 took 0.096s
  training loss:		0.566257
  validation loss:		0.557277
  validation accuracy:		80.98 %
Epoch 723 of 2000 took 0.096s
  training loss:		0.550590
  validation loss:		0.554795
  validation accuracy:		81.20 %
Epoch 724 of 2000 took 0.096s
  training loss:		0.552199
  validation loss:		0.553154
  validation accuracy:		80.87 %
Epoch 725 of 2000 took 0.096s
  training loss:		0.554801
  validation loss:		0.552982
  validation accuracy:		81.41 %
Epoch 726 of 2000 took 0.096s
  training loss:		0.564174
  validation loss:		0.554396
  validation accuracy:		80.87 %
Epoch 727 of 2000 took 0.096s
  training loss:		0.561746
  validation loss:		0.562211
  validation accuracy:		81.09 %
Epoch 728 of 2000 took 0.097s
  training loss:		0.559398
  validation loss:		0.550301
  validation accuracy:		81.74 %
Epoch 729 of 2000 took 0.097s
  training loss:		0.559716
  validation loss:		0.561015
  validation accuracy:		81.30 %
Epoch 730 of 2000 took 0.096s
  training loss:		0.560694
  validation loss:		0.581267
  validation accuracy:		80.98 %
Epoch 731 of 2000 took 0.096s
  training loss:		0.584625
  validation loss:		0.574711
  validation accuracy:		81.09 %
Epoch 732 of 2000 took 0.096s
  training loss:		0.576293
  validation loss:		0.577530
  validation accuracy:		81.20 %
Epoch 733 of 2000 took 0.097s
  training loss:		0.566760
  validation loss:		0.589542
  validation accuracy:		80.87 %
Epoch 734 of 2000 took 0.096s
  training loss:		0.554530
  validation loss:		0.572329
  validation accuracy:		81.20 %
Epoch 735 of 2000 took 0.096s
  training loss:		0.579407
  validation loss:		0.548738
  validation accuracy:		81.52 %
Epoch 736 of 2000 took 0.096s
  training loss:		0.553084
  validation loss:		0.575781
  validation accuracy:		81.41 %
Epoch 737 of 2000 took 0.096s
  training loss:		0.561386
  validation loss:		0.556577
  validation accuracy:		80.87 %
Epoch 738 of 2000 took 0.096s
  training loss:		0.570762
  validation loss:		0.551976
  validation accuracy:		81.20 %
Epoch 739 of 2000 took 0.096s
  training loss:		0.557094
  validation loss:		0.554925
  validation accuracy:		81.20 %
Epoch 740 of 2000 took 0.096s
  training loss:		0.554322
  validation loss:		0.553066
  validation accuracy:		81.20 %
Epoch 741 of 2000 took 0.096s
  training loss:		0.586102
  validation loss:		0.559469
  validation accuracy:		81.52 %
Epoch 742 of 2000 took 0.096s
  training loss:		0.559072
  validation loss:		0.602091
  validation accuracy:		80.11 %
Epoch 743 of 2000 took 0.096s
  training loss:		0.580937
  validation loss:		0.553122
  validation accuracy:		81.30 %
Epoch 744 of 2000 took 0.096s
  training loss:		0.557162
  validation loss:		0.567753
  validation accuracy:		81.09 %
Epoch 745 of 2000 took 0.096s
  training loss:		0.553442
  validation loss:		0.576388
  validation accuracy:		81.30 %
Epoch 746 of 2000 took 0.096s
  training loss:		0.556563
  validation loss:		0.554307
  validation accuracy:		81.52 %
Epoch 747 of 2000 took 0.096s
  training loss:		0.559408
  validation loss:		0.551603
  validation accuracy:		81.63 %
Epoch 748 of 2000 took 0.096s
  training loss:		0.566798
  validation loss:		0.562994
  validation accuracy:		81.41 %
Epoch 749 of 2000 took 0.096s
  training loss:		0.569851
  validation loss:		0.551762
  validation accuracy:		81.41 %
Epoch 750 of 2000 took 0.096s
  training loss:		0.557658
  validation loss:		0.615164
  validation accuracy:		79.89 %
Epoch 751 of 2000 took 0.096s
  training loss:		0.569963
  validation loss:		0.548983
  validation accuracy:		81.09 %
Epoch 752 of 2000 took 0.096s
  training loss:		0.562133
  validation loss:		0.559359
  validation accuracy:		80.98 %
Epoch 753 of 2000 took 0.096s
  training loss:		0.579586
  validation loss:		0.560701
  validation accuracy:		81.41 %
Epoch 754 of 2000 took 0.096s
  training loss:		0.578037
  validation loss:		0.561706
  validation accuracy:		81.20 %
Epoch 755 of 2000 took 0.096s
  training loss:		0.578040
  validation loss:		0.557915
  validation accuracy:		81.74 %
Epoch 756 of 2000 took 0.096s
  training loss:		0.563954
  validation loss:		0.557471
  validation accuracy:		81.30 %
Epoch 757 of 2000 took 0.096s
  training loss:		0.563521
  validation loss:		0.599680
  validation accuracy:		80.11 %
Epoch 758 of 2000 took 0.096s
  training loss:		0.556900
  validation loss:		0.554519
  validation accuracy:		81.63 %
Epoch 759 of 2000 took 0.097s
  training loss:		0.549693
  validation loss:		0.566146
  validation accuracy:		81.41 %
Epoch 760 of 2000 took 0.096s
  training loss:		0.566479
  validation loss:		0.604303
  validation accuracy:		80.33 %
Epoch 761 of 2000 took 0.096s
  training loss:		0.571219
  validation loss:		0.552152
  validation accuracy:		81.20 %
Epoch 762 of 2000 took 0.096s
  training loss:		0.556622
  validation loss:		0.555165
  validation accuracy:		81.74 %
Epoch 763 of 2000 took 0.096s
  training loss:		0.547295
  validation loss:		0.561970
  validation accuracy:		80.87 %
Epoch 764 of 2000 took 0.099s
  training loss:		0.553665
  validation loss:		0.557168
  validation accuracy:		81.74 %
Epoch 765 of 2000 took 0.096s
  training loss:		0.559545
  validation loss:		0.583929
  validation accuracy:		80.98 %
Epoch 766 of 2000 took 0.096s
  training loss:		0.559758
  validation loss:		0.556979
  validation accuracy:		81.30 %
Epoch 767 of 2000 took 0.096s
  training loss:		0.556028
  validation loss:		0.554403
  validation accuracy:		81.63 %
Epoch 768 of 2000 took 0.096s
  training loss:		0.563002
  validation loss:		0.557095
  validation accuracy:		81.63 %
Epoch 769 of 2000 took 0.096s
  training loss:		0.557749
  validation loss:		0.547251
  validation accuracy:		81.52 %
Epoch 770 of 2000 took 0.096s
  training loss:		0.559183
  validation loss:		0.557863
  validation accuracy:		81.09 %
Epoch 771 of 2000 took 0.096s
  training loss:		0.564973
  validation loss:		0.600765
  validation accuracy:		80.22 %
Epoch 772 of 2000 took 0.096s
  training loss:		0.568483
  validation loss:		0.570577
  validation accuracy:		81.74 %
Epoch 773 of 2000 took 0.096s
  training loss:		0.562607
  validation loss:		0.617601
  validation accuracy:		79.46 %
Epoch 774 of 2000 took 0.099s
  training loss:		0.577226
  validation loss:		0.551289
  validation accuracy:		81.30 %
Epoch 775 of 2000 took 0.103s
  training loss:		0.548541
  validation loss:		0.571253
  validation accuracy:		81.30 %
Epoch 776 of 2000 took 0.137s
  training loss:		0.569224
  validation loss:		0.606513
  validation accuracy:		79.46 %
Epoch 777 of 2000 took 0.144s
  training loss:		0.556126
  validation loss:		0.548481
  validation accuracy:		81.30 %
Epoch 778 of 2000 took 0.144s
  training loss:		0.561158
  validation loss:		0.556899
  validation accuracy:		81.74 %
Epoch 779 of 2000 took 0.144s
  training loss:		0.564424
  validation loss:		0.549202
  validation accuracy:		81.41 %
Epoch 780 of 2000 took 0.145s
  training loss:		0.559166
  validation loss:		0.585579
  validation accuracy:		80.76 %
Epoch 781 of 2000 took 0.159s
  training loss:		0.559401
  validation loss:		0.554942
  validation accuracy:		81.20 %
Epoch 782 of 2000 took 0.179s
  training loss:		0.561895
  validation loss:		0.574050
  validation accuracy:		81.09 %
Epoch 783 of 2000 took 0.179s
  training loss:		0.548531
  validation loss:		0.554722
  validation accuracy:		81.74 %
Epoch 784 of 2000 took 0.180s
  training loss:		0.560341
  validation loss:		0.585282
  validation accuracy:		80.98 %
Epoch 785 of 2000 took 0.179s
  training loss:		0.544497
  validation loss:		0.560440
  validation accuracy:		81.41 %
Epoch 786 of 2000 took 0.179s
  training loss:		0.551692
  validation loss:		0.556058
  validation accuracy:		81.41 %
Epoch 787 of 2000 took 0.180s
  training loss:		0.574190
  validation loss:		0.545371
  validation accuracy:		81.85 %
Epoch 788 of 2000 took 0.180s
  training loss:		0.564467
  validation loss:		0.562118
  validation accuracy:		81.30 %
Epoch 789 of 2000 took 0.179s
  training loss:		0.551264
  validation loss:		0.549389
  validation accuracy:		81.41 %
Epoch 790 of 2000 took 0.179s
  training loss:		0.560462
  validation loss:		0.572495
  validation accuracy:		81.30 %
Epoch 791 of 2000 took 0.178s
  training loss:		0.556295
  validation loss:		0.562784
  validation accuracy:		81.30 %
Epoch 792 of 2000 took 0.179s
  training loss:		0.552609
  validation loss:		0.553676
  validation accuracy:		81.63 %
Epoch 793 of 2000 took 0.180s
  training loss:		0.559288
  validation loss:		0.564350
  validation accuracy:		80.98 %
Epoch 794 of 2000 took 0.181s
  training loss:		0.564938
  validation loss:		0.601510
  validation accuracy:		80.33 %
Epoch 795 of 2000 took 0.179s
  training loss:		0.548990
  validation loss:		0.548785
  validation accuracy:		81.85 %
Epoch 796 of 2000 took 0.179s
  training loss:		0.565308
  validation loss:		0.607668
  validation accuracy:		79.67 %
Epoch 797 of 2000 took 0.179s
  training loss:		0.557272
  validation loss:		0.552313
  validation accuracy:		81.20 %
Epoch 798 of 2000 took 0.154s
  training loss:		0.555633
  validation loss:		0.553255
  validation accuracy:		81.63 %
Epoch 799 of 2000 took 0.123s
  training loss:		0.547709
  validation loss:		0.563923
  validation accuracy:		81.30 %
Epoch 800 of 2000 took 0.123s
  training loss:		0.562986
  validation loss:		0.568416
  validation accuracy:		81.30 %
Epoch 801 of 2000 took 0.123s
  training loss:		0.559375
  validation loss:		0.547296
  validation accuracy:		81.74 %
Epoch 802 of 2000 took 0.123s
  training loss:		0.558116
  validation loss:		0.556540
  validation accuracy:		81.09 %
Epoch 803 of 2000 took 0.123s
  training loss:		0.562217
  validation loss:		0.552894
  validation accuracy:		81.85 %
Epoch 804 of 2000 took 0.123s
  training loss:		0.559312
  validation loss:		0.545539
  validation accuracy:		81.20 %
Epoch 805 of 2000 took 0.123s
  training loss:		0.551715
  validation loss:		0.561676
  validation accuracy:		81.09 %
Epoch 806 of 2000 took 0.123s
  training loss:		0.568126
  validation loss:		0.603433
  validation accuracy:		80.43 %
Epoch 807 of 2000 took 0.116s
  training loss:		0.546833
  validation loss:		0.553882
  validation accuracy:		81.63 %
Epoch 808 of 2000 took 0.097s
  training loss:		0.574505
  validation loss:		0.603738
  validation accuracy:		80.54 %
Epoch 809 of 2000 took 0.096s
  training loss:		0.560821
  validation loss:		0.552510
  validation accuracy:		81.20 %
Epoch 810 of 2000 took 0.096s
  training loss:		0.555429
  validation loss:		0.554779
  validation accuracy:		81.41 %
Epoch 811 of 2000 took 0.096s
  training loss:		0.562720
  validation loss:		0.571468
  validation accuracy:		80.98 %
Epoch 812 of 2000 took 0.096s
  training loss:		0.558144
  validation loss:		0.554956
  validation accuracy:		81.41 %
Epoch 813 of 2000 took 0.096s
  training loss:		0.551425
  validation loss:		0.564239
  validation accuracy:		80.98 %
Epoch 814 of 2000 took 0.096s
  training loss:		0.556653
  validation loss:		0.584867
  validation accuracy:		80.98 %
Epoch 815 of 2000 took 0.096s
  training loss:		0.559739
  validation loss:		0.547730
  validation accuracy:		81.96 %
Epoch 816 of 2000 took 0.096s
  training loss:		0.567742
  validation loss:		0.564455
  validation accuracy:		81.30 %
Epoch 817 of 2000 took 0.096s
  training loss:		0.557837
  validation loss:		0.556865
  validation accuracy:		81.52 %
Epoch 818 of 2000 took 0.096s
  training loss:		0.550745
  validation loss:		0.547429
  validation accuracy:		81.41 %
Epoch 819 of 2000 took 0.096s
  training loss:		0.551132
  validation loss:		0.555503
  validation accuracy:		81.41 %
Epoch 820 of 2000 took 0.096s
  training loss:		0.566279
  validation loss:		0.580704
  validation accuracy:		81.09 %
Epoch 821 of 2000 took 0.096s
  training loss:		0.551134
  validation loss:		0.557142
  validation accuracy:		81.41 %
Epoch 822 of 2000 took 0.096s
  training loss:		0.554985
  validation loss:		0.566459
  validation accuracy:		81.30 %
Epoch 823 of 2000 took 0.096s
  training loss:		0.543754
  validation loss:		0.566985
  validation accuracy:		81.52 %
Epoch 824 of 2000 took 0.096s
  training loss:		0.550633
  validation loss:		0.556731
  validation accuracy:		81.41 %
Epoch 825 of 2000 took 0.096s
  training loss:		0.571874
  validation loss:		0.553017
  validation accuracy:		81.63 %
Epoch 826 of 2000 took 0.096s
  training loss:		0.550026
  validation loss:		0.554323
  validation accuracy:		82.07 %
Epoch 827 of 2000 took 0.096s
  training loss:		0.556589
  validation loss:		0.572404
  validation accuracy:		81.20 %
Epoch 828 of 2000 took 0.096s
  training loss:		0.550469
  validation loss:		0.562203
  validation accuracy:		81.30 %
Epoch 829 of 2000 took 0.096s
  training loss:		0.551956
  validation loss:		0.574255
  validation accuracy:		81.20 %
Epoch 830 of 2000 took 0.096s
  training loss:		0.552876
  validation loss:		0.578211
  validation accuracy:		80.87 %
Epoch 831 of 2000 took 0.096s
  training loss:		0.552617
  validation loss:		0.596278
  validation accuracy:		80.76 %
Epoch 832 of 2000 took 0.096s
  training loss:		0.557337
  validation loss:		0.556931
  validation accuracy:		81.30 %
Epoch 833 of 2000 took 0.097s
  training loss:		0.560089
  validation loss:		0.556712
  validation accuracy:		81.63 %
Epoch 834 of 2000 took 0.097s
  training loss:		0.570490
  validation loss:		0.558355
  validation accuracy:		81.41 %
Epoch 835 of 2000 took 0.096s
  training loss:		0.554938
  validation loss:		0.580603
  validation accuracy:		80.54 %
Epoch 836 of 2000 took 0.096s
  training loss:		0.568308
  validation loss:		0.560711
  validation accuracy:		81.52 %
Epoch 837 of 2000 took 0.096s
  training loss:		0.541941
  validation loss:		0.551370
  validation accuracy:		81.63 %
Epoch 838 of 2000 took 0.096s
  training loss:		0.557187
  validation loss:		0.555596
  validation accuracy:		81.20 %
Epoch 839 of 2000 took 0.096s
  training loss:		0.551731
  validation loss:		0.548551
  validation accuracy:		81.85 %
Epoch 840 of 2000 took 0.096s
  training loss:		0.556716
  validation loss:		0.569002
  validation accuracy:		81.52 %
Epoch 841 of 2000 took 0.096s
  training loss:		0.557791
  validation loss:		0.549657
  validation accuracy:		81.41 %
Epoch 842 of 2000 took 0.096s
  training loss:		0.552222
  validation loss:		0.551672
  validation accuracy:		81.85 %
Epoch 843 of 2000 took 0.096s
  training loss:		0.552819
  validation loss:		0.574703
  validation accuracy:		81.52 %
Epoch 844 of 2000 took 0.096s
  training loss:		0.554569
  validation loss:		0.559376
  validation accuracy:		81.20 %
Epoch 845 of 2000 took 0.096s
  training loss:		0.554775
  validation loss:		0.564603
  validation accuracy:		81.09 %
Epoch 846 of 2000 took 0.096s
  training loss:		0.557276
  validation loss:		0.559986
  validation accuracy:		81.41 %
Epoch 847 of 2000 took 0.096s
  training loss:		0.551093
  validation loss:		0.551921
  validation accuracy:		81.41 %
Epoch 848 of 2000 took 0.096s
  training loss:		0.568522
  validation loss:		0.562246
  validation accuracy:		81.52 %
Epoch 849 of 2000 took 0.096s
  training loss:		0.549740
  validation loss:		0.575381
  validation accuracy:		81.63 %
Epoch 850 of 2000 took 0.096s
  training loss:		0.550563
  validation loss:		0.570438
  validation accuracy:		81.41 %
Epoch 851 of 2000 took 0.096s
  training loss:		0.564885
  validation loss:		0.563815
  validation accuracy:		81.52 %
Epoch 852 of 2000 took 0.096s
  training loss:		0.562490
  validation loss:		0.577835
  validation accuracy:		81.96 %
Epoch 853 of 2000 took 0.096s
  training loss:		0.562682
  validation loss:		0.554056
  validation accuracy:		81.41 %
Epoch 854 of 2000 took 0.096s
  training loss:		0.557802
  validation loss:		0.621353
  validation accuracy:		79.35 %
Epoch 855 of 2000 took 0.098s
  training loss:		0.566622
  validation loss:		0.575095
  validation accuracy:		81.30 %
Epoch 856 of 2000 took 0.097s
  training loss:		0.561363
  validation loss:		0.593467
  validation accuracy:		80.43 %
Epoch 857 of 2000 took 0.096s
  training loss:		0.551036
  validation loss:		0.584985
  validation accuracy:		81.20 %
Epoch 858 of 2000 took 0.096s
  training loss:		0.563906
  validation loss:		0.615732
  validation accuracy:		79.89 %
Epoch 859 of 2000 took 0.096s
  training loss:		0.553392
  validation loss:		0.562673
  validation accuracy:		81.20 %
Epoch 860 of 2000 took 0.096s
  training loss:		0.544731
  validation loss:		0.594901
  validation accuracy:		80.76 %
Epoch 861 of 2000 took 0.096s
  training loss:		0.556107
  validation loss:		0.585010
  validation accuracy:		80.98 %
Epoch 862 of 2000 took 0.096s
  training loss:		0.552047
  validation loss:		0.561458
  validation accuracy:		81.41 %
Epoch 863 of 2000 took 0.096s
  training loss:		0.554132
  validation loss:		0.567083
  validation accuracy:		81.20 %
Epoch 864 of 2000 took 0.097s
  training loss:		0.553335
  validation loss:		0.557868
  validation accuracy:		81.41 %
Epoch 865 of 2000 took 0.096s
  training loss:		0.552512
  validation loss:		0.548929
  validation accuracy:		81.85 %
Epoch 866 of 2000 took 0.096s
  training loss:		0.545055
  validation loss:		0.555990
  validation accuracy:		81.63 %
Epoch 867 of 2000 took 0.096s
  training loss:		0.548451
  validation loss:		0.552043
  validation accuracy:		81.63 %
Epoch 868 of 2000 took 0.096s
  training loss:		0.570606
  validation loss:		0.567401
  validation accuracy:		81.30 %
Epoch 869 of 2000 took 0.096s
  training loss:		0.559943
  validation loss:		0.565003
  validation accuracy:		81.52 %
Epoch 870 of 2000 took 0.096s
  training loss:		0.554742
  validation loss:		0.569736
  validation accuracy:		81.41 %
Epoch 871 of 2000 took 0.096s
  training loss:		0.569367
  validation loss:		0.573302
  validation accuracy:		81.63 %
Epoch 872 of 2000 took 0.096s
  training loss:		0.562430
  validation loss:		0.581011
  validation accuracy:		81.41 %
Epoch 873 of 2000 took 0.096s
  training loss:		0.579789
  validation loss:		0.557295
  validation accuracy:		81.52 %
Epoch 874 of 2000 took 0.096s
  training loss:		0.553462
  validation loss:		0.553316
  validation accuracy:		81.96 %
Epoch 875 of 2000 took 0.096s
  training loss:		0.561538
  validation loss:		0.568386
  validation accuracy:		81.20 %
Epoch 876 of 2000 took 0.096s
  training loss:		0.560664
  validation loss:		0.565108
  validation accuracy:		80.98 %
Epoch 877 of 2000 took 0.096s
  training loss:		0.557733
  validation loss:		0.548871
  validation accuracy:		81.85 %
Epoch 878 of 2000 took 0.096s
  training loss:		0.551990
  validation loss:		0.556752
  validation accuracy:		81.63 %
Epoch 879 of 2000 took 0.097s
  training loss:		0.556816
  validation loss:		0.557176
  validation accuracy:		81.09 %
Epoch 880 of 2000 took 0.097s
  training loss:		0.554289
  validation loss:		0.561741
  validation accuracy:		81.63 %
Epoch 881 of 2000 took 0.096s
  training loss:		0.548486
  validation loss:		0.565134
  validation accuracy:		81.20 %
Epoch 882 of 2000 took 0.096s
  training loss:		0.541511
  validation loss:		0.548679
  validation accuracy:		81.20 %
Epoch 883 of 2000 took 0.096s
  training loss:		0.551843
  validation loss:		0.553031
  validation accuracy:		81.74 %
Epoch 884 of 2000 took 0.096s
  training loss:		0.555501
  validation loss:		0.576495
  validation accuracy:		81.09 %
Epoch 885 of 2000 took 0.096s
  training loss:		0.561103
  validation loss:		0.550021
  validation accuracy:		81.41 %
Epoch 886 of 2000 took 0.096s
  training loss:		0.569169
  validation loss:		0.568416
  validation accuracy:		81.30 %
Epoch 887 of 2000 took 0.096s
  training loss:		0.559823
  validation loss:		0.588085
  validation accuracy:		80.87 %
Epoch 888 of 2000 took 0.096s
  training loss:		0.556686
  validation loss:		0.563351
  validation accuracy:		81.52 %
Epoch 889 of 2000 took 0.096s
  training loss:		0.556586
  validation loss:		0.577261
  validation accuracy:		81.74 %
Epoch 890 of 2000 took 0.096s
  training loss:		0.552776
  validation loss:		0.551332
  validation accuracy:		81.63 %
Epoch 891 of 2000 took 0.096s
  training loss:		0.565189
  validation loss:		0.553090
  validation accuracy:		81.52 %
Epoch 892 of 2000 took 0.096s
  training loss:		0.565436
  validation loss:		0.562024
  validation accuracy:		81.41 %
Epoch 893 of 2000 took 0.096s
  training loss:		0.547815
  validation loss:		0.551342
  validation accuracy:		81.63 %
Epoch 894 of 2000 took 0.096s
  training loss:		0.544001
  validation loss:		0.552433
  validation accuracy:		81.09 %
Epoch 895 of 2000 took 0.096s
  training loss:		0.569019
  validation loss:		0.547086
  validation accuracy:		81.74 %
Epoch 896 of 2000 took 0.097s
  training loss:		0.550620
  validation loss:		0.562337
  validation accuracy:		81.52 %
Epoch 897 of 2000 took 0.096s
  training loss:		0.548950
  validation loss:		0.579882
  validation accuracy:		81.09 %
Epoch 898 of 2000 took 0.096s
  training loss:		0.554132
  validation loss:		0.593980
  validation accuracy:		80.43 %
Epoch 899 of 2000 took 0.096s
  training loss:		0.555708
  validation loss:		0.571427
  validation accuracy:		81.41 %
Epoch 900 of 2000 took 0.096s
  training loss:		0.560033
  validation loss:		0.614920
  validation accuracy:		80.11 %
Epoch 901 of 2000 took 0.096s
  training loss:		0.555963
  validation loss:		0.555318
  validation accuracy:		81.85 %
Epoch 902 of 2000 took 0.096s
  training loss:		0.554738
  validation loss:		0.588518
  validation accuracy:		80.87 %
Epoch 903 of 2000 took 0.096s
  training loss:		0.556684
  validation loss:		0.570553
  validation accuracy:		81.41 %
Epoch 904 of 2000 took 0.096s
  training loss:		0.550811
  validation loss:		0.587580
  validation accuracy:		80.87 %
Epoch 905 of 2000 took 0.096s
  training loss:		0.560589
  validation loss:		0.589681
  validation accuracy:		80.98 %
Epoch 906 of 2000 took 0.096s
  training loss:		0.546805
  validation loss:		0.625167
  validation accuracy:		79.46 %
Epoch 907 of 2000 took 0.096s
  training loss:		0.555511
  validation loss:		0.574736
  validation accuracy:		81.09 %
Epoch 908 of 2000 took 0.096s
  training loss:		0.557068
  validation loss:		0.550730
  validation accuracy:		81.63 %
Epoch 909 of 2000 took 0.096s
  training loss:		0.566255
  validation loss:		0.562405
  validation accuracy:		81.52 %
Epoch 910 of 2000 took 0.096s
  training loss:		0.558364
  validation loss:		0.574471
  validation accuracy:		81.74 %
Epoch 911 of 2000 took 0.096s
  training loss:		0.567551
  validation loss:		0.550512
  validation accuracy:		81.85 %
Epoch 912 of 2000 took 0.096s
  training loss:		0.553872
  validation loss:		0.558183
  validation accuracy:		81.85 %
Epoch 913 of 2000 took 0.096s
  training loss:		0.552076
  validation loss:		0.578908
  validation accuracy:		81.09 %
Epoch 914 of 2000 took 0.096s
  training loss:		0.551805
  validation loss:		0.556197
  validation accuracy:		81.63 %
Epoch 915 of 2000 took 0.096s
  training loss:		0.550949
  validation loss:		0.553914
  validation accuracy:		81.52 %
Epoch 916 of 2000 took 0.096s
  training loss:		0.549098
  validation loss:		0.560281
  validation accuracy:		81.74 %
Epoch 917 of 2000 took 0.097s
  training loss:		0.552861
  validation loss:		0.586456
  validation accuracy:		80.76 %
Epoch 918 of 2000 took 0.096s
  training loss:		0.551709
  validation loss:		0.602160
  validation accuracy:		80.33 %
Epoch 919 of 2000 took 0.096s
  training loss:		0.554687
  validation loss:		0.556539
  validation accuracy:		82.07 %
Epoch 920 of 2000 took 0.096s
  training loss:		0.566211
  validation loss:		0.550681
  validation accuracy:		81.63 %
Epoch 921 of 2000 took 0.096s
  training loss:		0.565962
  validation loss:		0.584680
  validation accuracy:		80.87 %
Epoch 922 of 2000 took 0.096s
  training loss:		0.569893
  validation loss:		0.658981
  validation accuracy:		77.93 %
Epoch 923 of 2000 took 0.096s
  training loss:		0.568213
  validation loss:		0.550643
  validation accuracy:		81.52 %
Epoch 924 of 2000 took 0.096s
  training loss:		0.553255
  validation loss:		0.560730
  validation accuracy:		81.52 %
Epoch 925 of 2000 took 0.096s
  training loss:		0.558756
  validation loss:		0.596410
  validation accuracy:		80.43 %
Epoch 926 of 2000 took 0.096s
  training loss:		0.558234
  validation loss:		0.554947
  validation accuracy:		81.30 %
Epoch 927 of 2000 took 0.097s
  training loss:		0.546499
  validation loss:		0.571433
  validation accuracy:		82.17 %
Epoch 928 of 2000 took 0.096s
  training loss:		0.555924
  validation loss:		0.588676
  validation accuracy:		81.20 %
Epoch 929 of 2000 took 0.096s
  training loss:		0.550890
  validation loss:		0.554767
  validation accuracy:		81.30 %
Epoch 930 of 2000 took 0.096s
  training loss:		0.566922
  validation loss:		0.615869
  validation accuracy:		79.67 %
Epoch 931 of 2000 took 0.096s
  training loss:		0.560533
  validation loss:		0.558520
  validation accuracy:		81.85 %
Epoch 932 of 2000 took 0.096s
  training loss:		0.557546
  validation loss:		0.592261
  validation accuracy:		80.76 %
Epoch 933 of 2000 took 0.096s
  training loss:		0.567569
  validation loss:		0.564845
  validation accuracy:		81.85 %
Epoch 934 of 2000 took 0.096s
  training loss:		0.557382
  validation loss:		0.585449
  validation accuracy:		80.76 %
Epoch 935 of 2000 took 0.096s
  training loss:		0.551907
  validation loss:		0.569433
  validation accuracy:		81.52 %
Epoch 936 of 2000 took 0.096s
  training loss:		0.551376
  validation loss:		0.557273
  validation accuracy:		81.09 %
Epoch 937 of 2000 took 0.096s
  training loss:		0.548606
  validation loss:		0.559746
  validation accuracy:		82.17 %
Epoch 938 of 2000 took 0.096s
  training loss:		0.568997
  validation loss:		0.547682
  validation accuracy:		81.52 %
Epoch 939 of 2000 took 0.096s
  training loss:		0.551352
  validation loss:		0.557477
  validation accuracy:		81.63 %
Epoch 940 of 2000 took 0.096s
  training loss:		0.544570
  validation loss:		0.561115
  validation accuracy:		81.09 %
Epoch 941 of 2000 took 0.096s
  training loss:		0.552749
  validation loss:		0.556087
  validation accuracy:		81.52 %
Epoch 942 of 2000 took 0.096s
  training loss:		0.551851
  validation loss:		0.551492
  validation accuracy:		81.96 %
Epoch 943 of 2000 took 0.096s
  training loss:		0.548303
  validation loss:		0.574373
  validation accuracy:		81.74 %
Epoch 944 of 2000 took 0.096s
  training loss:		0.549446
  validation loss:		0.572095
  validation accuracy:		81.20 %
Epoch 945 of 2000 took 0.103s
  training loss:		0.546292
  validation loss:		0.557556
  validation accuracy:		82.07 %
Epoch 946 of 2000 took 0.103s
  training loss:		0.563179
  validation loss:		0.571106
  validation accuracy:		81.30 %
Epoch 947 of 2000 took 0.100s
  training loss:		0.542969
  validation loss:		0.554029
  validation accuracy:		81.41 %
Epoch 948 of 2000 took 0.099s
  training loss:		0.551119
  validation loss:		0.549310
  validation accuracy:		81.52 %
Epoch 949 of 2000 took 0.100s
  training loss:		0.547020
  validation loss:		0.570411
  validation accuracy:		81.52 %
Epoch 950 of 2000 took 0.100s
  training loss:		0.555284
  validation loss:		0.561800
  validation accuracy:		81.63 %
Epoch 951 of 2000 took 0.099s
  training loss:		0.549971
  validation loss:		0.568469
  validation accuracy:		81.20 %
Epoch 952 of 2000 took 0.099s
  training loss:		0.548053
  validation loss:		0.582031
  validation accuracy:		81.30 %
Epoch 953 of 2000 took 0.096s
  training loss:		0.563205
  validation loss:		0.569389
  validation accuracy:		82.07 %
Epoch 954 of 2000 took 0.096s
  training loss:		0.553958
  validation loss:		0.555421
  validation accuracy:		81.52 %
Epoch 955 of 2000 took 0.096s
  training loss:		0.552470
  validation loss:		0.554518
  validation accuracy:		81.63 %
Epoch 956 of 2000 took 0.096s
  training loss:		0.551371
  validation loss:		0.552806
  validation accuracy:		81.41 %
Epoch 957 of 2000 took 0.096s
  training loss:		0.552138
  validation loss:		0.573541
  validation accuracy:		81.20 %
Epoch 958 of 2000 took 0.097s
  training loss:		0.546522
  validation loss:		0.552649
  validation accuracy:		81.63 %
Epoch 959 of 2000 took 0.096s
  training loss:		0.553587
  validation loss:		0.594551
  validation accuracy:		80.33 %
Epoch 960 of 2000 took 0.096s
  training loss:		0.550420
  validation loss:		0.548886
  validation accuracy:		82.17 %
Epoch 961 of 2000 took 0.096s
  training loss:		0.547304
  validation loss:		0.558598
  validation accuracy:		82.07 %
Epoch 962 of 2000 took 0.096s
  training loss:		0.547259
  validation loss:		0.551635
  validation accuracy:		80.87 %
Epoch 963 of 2000 took 0.096s
  training loss:		0.552619
  validation loss:		0.558216
  validation accuracy:		81.85 %
Epoch 964 of 2000 took 0.096s
  training loss:		0.548194
  validation loss:		0.559842
  validation accuracy:		81.63 %
Epoch 965 of 2000 took 0.096s
  training loss:		0.555698
  validation loss:		0.554481
  validation accuracy:		82.17 %
Epoch 966 of 2000 took 0.096s
  training loss:		0.552506
  validation loss:		0.554135
  validation accuracy:		81.85 %
Epoch 967 of 2000 took 0.096s
  training loss:		0.548905
  validation loss:		0.548071
  validation accuracy:		81.41 %
Epoch 968 of 2000 took 0.096s
  training loss:		0.553042
  validation loss:		0.554308
  validation accuracy:		81.63 %
Epoch 969 of 2000 took 0.096s
  training loss:		0.554745
  validation loss:		0.584985
  validation accuracy:		81.30 %
Epoch 970 of 2000 took 0.096s
  training loss:		0.554288
  validation loss:		0.562475
  validation accuracy:		81.96 %
Epoch 971 of 2000 took 0.096s
  training loss:		0.556367
  validation loss:		0.583121
  validation accuracy:		81.52 %
Epoch 972 of 2000 took 0.096s
  training loss:		0.539754
  validation loss:		0.558313
  validation accuracy:		81.74 %
Epoch 973 of 2000 took 0.096s
  training loss:		0.553560
  validation loss:		0.570928
  validation accuracy:		81.41 %
Epoch 974 of 2000 took 0.096s
  training loss:		0.556586
  validation loss:		0.561578
  validation accuracy:		81.41 %
Epoch 975 of 2000 took 0.096s
  training loss:		0.540429
  validation loss:		0.555860
  validation accuracy:		82.07 %
Epoch 976 of 2000 took 0.096s
  training loss:		0.555014
  validation loss:		0.561213
  validation accuracy:		81.74 %
Epoch 977 of 2000 took 0.096s
  training loss:		0.556350
  validation loss:		0.552374
  validation accuracy:		81.52 %
Epoch 978 of 2000 took 0.096s
  training loss:		0.544363
  validation loss:		0.578581
  validation accuracy:		81.30 %
Epoch 979 of 2000 took 0.096s
  training loss:		0.551525
  validation loss:		0.557701
  validation accuracy:		81.74 %
Epoch 980 of 2000 took 0.096s
  training loss:		0.544164
  validation loss:		0.556227
  validation accuracy:		81.41 %
Epoch 981 of 2000 took 0.096s
  training loss:		0.551517
  validation loss:		0.596667
  validation accuracy:		80.98 %
Epoch 982 of 2000 took 0.096s
  training loss:		0.554693
  validation loss:		0.559264
  validation accuracy:		81.52 %
Epoch 983 of 2000 took 0.096s
  training loss:		0.549112
  validation loss:		0.549753
  validation accuracy:		82.17 %
Epoch 984 of 2000 took 0.097s
  training loss:		0.561647
  validation loss:		0.579102
  validation accuracy:		80.98 %
Epoch 985 of 2000 took 0.096s
  training loss:		0.558736
  validation loss:		0.563437
  validation accuracy:		81.85 %
Epoch 986 of 2000 took 0.096s
  training loss:		0.569747
  validation loss:		0.552202
  validation accuracy:		81.85 %
Epoch 987 of 2000 took 0.096s
  training loss:		0.549262
  validation loss:		0.584513
  validation accuracy:		80.87 %
Epoch 988 of 2000 took 0.096s
  training loss:		0.549147
  validation loss:		0.561453
  validation accuracy:		81.63 %
Epoch 989 of 2000 took 0.100s
  training loss:		0.548708
  validation loss:		0.564880
  validation accuracy:		81.63 %
Epoch 990 of 2000 took 0.096s
  training loss:		0.544028
  validation loss:		0.557302
  validation accuracy:		80.98 %
Epoch 991 of 2000 took 0.096s
  training loss:		0.552602
  validation loss:		0.550659
  validation accuracy:		81.52 %
Epoch 992 of 2000 took 0.096s
  training loss:		0.560960
  validation loss:		0.551875
  validation accuracy:		80.98 %
Epoch 993 of 2000 took 0.096s
  training loss:		0.554347
  validation loss:		0.608444
  validation accuracy:		80.11 %
Epoch 994 of 2000 took 0.096s
  training loss:		0.549006
  validation loss:		0.560440
  validation accuracy:		81.30 %
Epoch 995 of 2000 took 0.096s
  training loss:		0.544789
  validation loss:		0.564237
  validation accuracy:		81.41 %
Epoch 996 of 2000 took 0.096s
  training loss:		0.555508
  validation loss:		0.558467
  validation accuracy:		81.20 %
Epoch 997 of 2000 took 0.096s
  training loss:		0.540953
  validation loss:		0.572336
  validation accuracy:		80.98 %
Epoch 998 of 2000 took 0.096s
  training loss:		0.553922
  validation loss:		0.596740
  validation accuracy:		80.54 %
Epoch 999 of 2000 took 0.096s
  training loss:		0.557189
  validation loss:		0.586042
  validation accuracy:		81.41 %
Epoch 1000 of 2000 took 0.096s
  training loss:		0.555048
  validation loss:		0.567312
  validation accuracy:		81.63 %
Epoch 1001 of 2000 took 0.096s
  training loss:		0.547429
  validation loss:		0.623921
  validation accuracy:		79.35 %
Epoch 1002 of 2000 took 0.096s
  training loss:		0.557017
  validation loss:		0.550451
  validation accuracy:		81.30 %
Epoch 1003 of 2000 took 0.096s
  training loss:		0.555269
  validation loss:		0.564863
  validation accuracy:		81.52 %
Epoch 1004 of 2000 took 0.096s
  training loss:		0.549687
  validation loss:		0.556418
  validation accuracy:		81.96 %
Epoch 1005 of 2000 took 0.096s
  training loss:		0.560759
  validation loss:		0.554255
  validation accuracy:		82.07 %
Epoch 1006 of 2000 took 0.096s
  training loss:		0.537787
  validation loss:		0.566288
  validation accuracy:		81.41 %
Epoch 1007 of 2000 took 0.096s
  training loss:		0.556854
  validation loss:		0.554964
  validation accuracy:		82.07 %
Epoch 1008 of 2000 took 0.096s
  training loss:		0.552304
  validation loss:		0.552567
  validation accuracy:		81.52 %
Epoch 1009 of 2000 took 0.096s
  training loss:		0.551664
  validation loss:		0.559703
  validation accuracy:		80.98 %
Epoch 1010 of 2000 took 0.096s
  training loss:		0.566466
  validation loss:		0.554624
  validation accuracy:		81.30 %
Epoch 1011 of 2000 took 0.096s
  training loss:		0.548153
  validation loss:		0.575785
  validation accuracy:		81.20 %
Epoch 1012 of 2000 took 0.096s
  training loss:		0.560926
  validation loss:		0.566455
  validation accuracy:		81.52 %
Epoch 1013 of 2000 took 0.096s
  training loss:		0.547408
  validation loss:		0.563302
  validation accuracy:		81.52 %
Epoch 1014 of 2000 took 0.096s
  training loss:		0.549715
  validation loss:		0.593722
  validation accuracy:		81.09 %
Epoch 1015 of 2000 took 0.096s
  training loss:		0.549816
  validation loss:		0.564724
  validation accuracy:		82.28 %
Epoch 1016 of 2000 took 0.096s
  training loss:		0.552279
  validation loss:		0.559813
  validation accuracy:		81.52 %
Epoch 1017 of 2000 took 0.096s
  training loss:		0.546095
  validation loss:		0.552854
  validation accuracy:		81.30 %
Epoch 1018 of 2000 took 0.096s
  training loss:		0.557651
  validation loss:		0.641942
  validation accuracy:		78.70 %
Epoch 1019 of 2000 took 0.096s
  training loss:		0.554398
  validation loss:		0.556984
  validation accuracy:		81.41 %
Epoch 1020 of 2000 took 0.097s
  training loss:		0.550273
  validation loss:		0.562903
  validation accuracy:		81.63 %
Epoch 1021 of 2000 took 0.096s
  training loss:		0.544302
  validation loss:		0.548737
  validation accuracy:		81.30 %
Epoch 1022 of 2000 took 0.099s
  training loss:		0.556391
  validation loss:		0.560963
  validation accuracy:		81.52 %
Epoch 1023 of 2000 took 0.106s
  training loss:		0.559705
  validation loss:		0.558057
  validation accuracy:		81.20 %
Epoch 1024 of 2000 took 0.108s
  training loss:		0.549656
  validation loss:		0.569725
  validation accuracy:		81.41 %
Epoch 1025 of 2000 took 0.144s
  training loss:		0.551137
  validation loss:		0.567268
  validation accuracy:		81.41 %
Epoch 1026 of 2000 took 0.102s
  training loss:		0.551502
  validation loss:		0.554003
  validation accuracy:		80.98 %
Epoch 1027 of 2000 took 0.102s
  training loss:		0.546821
  validation loss:		0.565206
  validation accuracy:		82.07 %
Epoch 1028 of 2000 took 0.105s
  training loss:		0.546364
  validation loss:		0.553243
  validation accuracy:		80.87 %
Epoch 1029 of 2000 took 0.104s
  training loss:		0.550655
  validation loss:		0.577209
  validation accuracy:		81.63 %
Epoch 1030 of 2000 took 0.103s
  training loss:		0.549759
  validation loss:		0.571644
  validation accuracy:		81.41 %
Epoch 1031 of 2000 took 0.105s
  training loss:		0.552931
  validation loss:		0.587390
  validation accuracy:		80.87 %
Epoch 1032 of 2000 took 0.102s
  training loss:		0.551463
  validation loss:		0.557791
  validation accuracy:		81.20 %
Epoch 1033 of 2000 took 0.100s
  training loss:		0.552107
  validation loss:		0.585930
  validation accuracy:		80.98 %
Epoch 1034 of 2000 took 0.100s
  training loss:		0.551757
  validation loss:		0.593647
  validation accuracy:		80.76 %
Epoch 1035 of 2000 took 0.100s
  training loss:		0.549161
  validation loss:		0.552721
  validation accuracy:		80.98 %
Epoch 1036 of 2000 took 0.100s
  training loss:		0.546575
  validation loss:		0.547720
  validation accuracy:		81.74 %
Epoch 1037 of 2000 took 0.100s
  training loss:		0.542197
  validation loss:		0.574195
  validation accuracy:		81.74 %
Epoch 1038 of 2000 took 0.100s
  training loss:		0.554133
  validation loss:		0.567200
  validation accuracy:		81.30 %
Epoch 1039 of 2000 took 0.101s
  training loss:		0.561498
  validation loss:		0.546992
  validation accuracy:		81.85 %
Epoch 1040 of 2000 took 0.100s
  training loss:		0.554124
  validation loss:		0.593321
  validation accuracy:		81.09 %
Epoch 1041 of 2000 took 0.102s
  training loss:		0.545060
  validation loss:		0.557875
  validation accuracy:		81.52 %
Epoch 1042 of 2000 took 0.100s
  training loss:		0.542944
  validation loss:		0.558372
  validation accuracy:		81.74 %
Epoch 1043 of 2000 took 0.100s
  training loss:		0.547620
  validation loss:		0.572666
  validation accuracy:		81.63 %
Epoch 1044 of 2000 took 0.101s
  training loss:		0.549094
  validation loss:		0.633396
  validation accuracy:		79.02 %
Epoch 1045 of 2000 took 0.100s
  training loss:		0.567098
  validation loss:		0.554592
  validation accuracy:		81.74 %
Epoch 1046 of 2000 took 0.100s
  training loss:		0.558809
  validation loss:		0.553880
  validation accuracy:		81.63 %
Epoch 1047 of 2000 took 0.100s
  training loss:		0.552753
  validation loss:		0.555687
  validation accuracy:		80.76 %
Epoch 1048 of 2000 took 0.101s
  training loss:		0.551903
  validation loss:		0.549202
  validation accuracy:		81.96 %
Epoch 1049 of 2000 took 0.100s
  training loss:		0.552723
  validation loss:		0.550984
  validation accuracy:		82.07 %
Epoch 1050 of 2000 took 0.101s
  training loss:		0.563296
  validation loss:		0.556083
  validation accuracy:		81.30 %
Epoch 1051 of 2000 took 0.100s
  training loss:		0.552065
  validation loss:		0.573532
  validation accuracy:		81.85 %
Epoch 1052 of 2000 took 0.100s
  training loss:		0.552916
  validation loss:		0.569930
  validation accuracy:		82.39 %
Epoch 1053 of 2000 took 0.100s
  training loss:		0.560094
  validation loss:		0.551323
  validation accuracy:		82.07 %
Epoch 1054 of 2000 took 0.100s
  training loss:		0.551509
  validation loss:		0.585086
  validation accuracy:		81.41 %
Epoch 1055 of 2000 took 0.101s
  training loss:		0.558094
  validation loss:		0.565944
  validation accuracy:		81.52 %
Epoch 1056 of 2000 took 0.100s
  training loss:		0.550267
  validation loss:		0.554572
  validation accuracy:		81.74 %
Epoch 1057 of 2000 took 0.100s
  training loss:		0.543580
  validation loss:		0.571479
  validation accuracy:		81.52 %
Epoch 1058 of 2000 took 0.100s
  training loss:		0.546157
  validation loss:		0.602136
  validation accuracy:		80.98 %
Epoch 1059 of 2000 took 0.100s
  training loss:		0.546274
  validation loss:		0.570933
  validation accuracy:		81.30 %
Epoch 1060 of 2000 took 0.100s
  training loss:		0.545400
  validation loss:		0.554200
  validation accuracy:		80.87 %
Epoch 1061 of 2000 took 0.100s
  training loss:		0.556424
  validation loss:		0.566208
  validation accuracy:		81.20 %
Epoch 1062 of 2000 took 0.100s
  training loss:		0.556279
  validation loss:		0.585407
  validation accuracy:		81.41 %
Epoch 1063 of 2000 took 0.100s
  training loss:		0.564714
  validation loss:		0.568933
  validation accuracy:		81.41 %
Epoch 1064 of 2000 took 0.103s
  training loss:		0.546157
  validation loss:		0.548634
  validation accuracy:		82.28 %
Epoch 1065 of 2000 took 0.118s
  training loss:		0.552037
  validation loss:		0.549155
  validation accuracy:		82.07 %
Epoch 1066 of 2000 took 0.165s
  training loss:		0.553752
  validation loss:		0.557391
  validation accuracy:		81.20 %
Epoch 1067 of 2000 took 0.165s
  training loss:		0.547807
  validation loss:		0.565895
  validation accuracy:		81.96 %
Epoch 1068 of 2000 took 0.165s
  training loss:		0.562720
  validation loss:		0.613069
  validation accuracy:		80.22 %
Epoch 1069 of 2000 took 0.165s
  training loss:		0.573080
  validation loss:		0.601116
  validation accuracy:		80.33 %
Epoch 1070 of 2000 took 0.165s
  training loss:		0.557814
  validation loss:		0.544160
  validation accuracy:		82.17 %
Epoch 1071 of 2000 took 0.165s
  training loss:		0.552863
  validation loss:		0.554711
  validation accuracy:		81.41 %
Epoch 1072 of 2000 took 0.165s
  training loss:		0.553661
  validation loss:		0.551345
  validation accuracy:		82.17 %
Epoch 1073 of 2000 took 0.165s
  training loss:		0.542446
  validation loss:		0.586562
  validation accuracy:		80.87 %
Epoch 1074 of 2000 took 0.167s
  training loss:		0.565162
  validation loss:		0.555536
  validation accuracy:		81.41 %
Epoch 1075 of 2000 took 0.168s
  training loss:		0.554423
  validation loss:		0.554325
  validation accuracy:		81.63 %
Epoch 1076 of 2000 took 0.266s
  training loss:		0.554999
  validation loss:		0.591031
  validation accuracy:		80.76 %
Epoch 1077 of 2000 took 0.244s
  training loss:		0.545655
  validation loss:		0.562762
  validation accuracy:		81.74 %
Epoch 1078 of 2000 took 0.277s
  training loss:		0.560164
  validation loss:		0.555543
  validation accuracy:		82.07 %
Epoch 1079 of 2000 took 0.193s
  training loss:		0.554207
  validation loss:		0.574754
  validation accuracy:		81.09 %
Epoch 1080 of 2000 took 0.235s
  training loss:		0.552752
  validation loss:		0.552580
  validation accuracy:		81.41 %
Epoch 1081 of 2000 took 0.310s
  training loss:		0.540442
  validation loss:		0.547661
  validation accuracy:		81.74 %
Epoch 1082 of 2000 took 0.199s
  training loss:		0.556045
  validation loss:		0.546936
  validation accuracy:		81.74 %
Epoch 1083 of 2000 took 0.326s
  training loss:		0.548951
  validation loss:		0.552582
  validation accuracy:		81.85 %
Epoch 1084 of 2000 took 0.336s
  training loss:		0.562663
  validation loss:		0.553318
  validation accuracy:		81.85 %
Epoch 1085 of 2000 took 0.267s
  training loss:		0.553291
  validation loss:		0.558943
  validation accuracy:		81.41 %
Epoch 1086 of 2000 took 0.182s
  training loss:		0.554861
  validation loss:		0.576183
  validation accuracy:		81.63 %
Epoch 1087 of 2000 took 0.192s
  training loss:		0.567009
  validation loss:		0.585181
  validation accuracy:		80.98 %
Epoch 1088 of 2000 took 0.258s
  training loss:		0.561206
  validation loss:		0.576231
  validation accuracy:		81.09 %
Epoch 1089 of 2000 took 0.267s
  training loss:		0.549585
  validation loss:		0.554161
  validation accuracy:		81.09 %
Epoch 1090 of 2000 took 0.214s
  training loss:		0.559755
  validation loss:		0.559667
  validation accuracy:		81.20 %
Epoch 1091 of 2000 took 0.165s
  training loss:		0.544916
  validation loss:		0.549963
  validation accuracy:		81.30 %
Epoch 1092 of 2000 took 0.164s
  training loss:		0.551597
  validation loss:		0.555785
  validation accuracy:		82.17 %
Epoch 1093 of 2000 took 0.165s
  training loss:		0.567800
  validation loss:		0.598548
  validation accuracy:		80.54 %
Epoch 1094 of 2000 took 0.164s
  training loss:		0.556816
  validation loss:		0.552131
  validation accuracy:		81.85 %
Epoch 1095 of 2000 took 0.165s
  training loss:		0.559849
  validation loss:		0.555858
  validation accuracy:		81.74 %
Epoch 1096 of 2000 took 0.165s
  training loss:		0.548360
  validation loss:		0.560857
  validation accuracy:		81.63 %
Epoch 1097 of 2000 took 0.165s
  training loss:		0.547712
  validation loss:		0.569157
  validation accuracy:		81.30 %
Epoch 1098 of 2000 took 0.165s
  training loss:		0.562625
  validation loss:		0.601120
  validation accuracy:		80.33 %
Epoch 1099 of 2000 took 0.165s
  training loss:		0.552921
  validation loss:		0.553683
  validation accuracy:		82.17 %
Epoch 1100 of 2000 took 0.165s
  training loss:		0.567408
  validation loss:		0.554413
  validation accuracy:		81.63 %
Epoch 1101 of 2000 took 0.165s
  training loss:		0.544572
  validation loss:		0.567140
  validation accuracy:		81.96 %
Epoch 1102 of 2000 took 0.165s
  training loss:		0.550941
  validation loss:		0.598131
  validation accuracy:		80.54 %
Epoch 1103 of 2000 took 0.165s
  training loss:		0.548115
  validation loss:		0.592881
  validation accuracy:		80.76 %
Epoch 1104 of 2000 took 0.166s
  training loss:		0.548005
  validation loss:		0.576954
  validation accuracy:		81.09 %
Epoch 1105 of 2000 took 0.294s
  training loss:		0.561563
  validation loss:		0.548361
  validation accuracy:		81.63 %
Epoch 1106 of 2000 took 0.192s
  training loss:		0.550080
  validation loss:		0.574882
  validation accuracy:		81.09 %
Epoch 1107 of 2000 took 0.264s
  training loss:		0.546516
  validation loss:		0.587191
  validation accuracy:		80.65 %
Epoch 1108 of 2000 took 0.226s
  training loss:		0.551784
  validation loss:		0.561145
  validation accuracy:		81.74 %
Epoch 1109 of 2000 took 0.166s
  training loss:		0.548335
  validation loss:		0.560417
  validation accuracy:		81.52 %
Epoch 1110 of 2000 took 0.164s
  training loss:		0.553474
  validation loss:		0.587239
  validation accuracy:		81.41 %
Epoch 1111 of 2000 took 0.164s
  training loss:		0.554473
  validation loss:		0.558819
  validation accuracy:		81.63 %
Epoch 1112 of 2000 took 0.165s
  training loss:		0.546820
  validation loss:		0.568746
  validation accuracy:		81.96 %
Epoch 1113 of 2000 took 0.164s
  training loss:		0.558437
  validation loss:		0.556933
  validation accuracy:		81.63 %
Epoch 1114 of 2000 took 0.165s
  training loss:		0.550451
  validation loss:		0.579924
  validation accuracy:		81.30 %
Epoch 1115 of 2000 took 0.165s
  training loss:		0.554224
  validation loss:		0.560386
  validation accuracy:		81.30 %
Epoch 1116 of 2000 took 0.165s
  training loss:		0.539346
  validation loss:		0.550797
  validation accuracy:		81.52 %
Epoch 1117 of 2000 took 0.165s
  training loss:		0.547037
  validation loss:		0.563477
  validation accuracy:		82.07 %
Epoch 1118 of 2000 took 0.165s
  training loss:		0.555599
  validation loss:		0.583772
  validation accuracy:		81.09 %
Epoch 1119 of 2000 took 0.165s
  training loss:		0.554164
  validation loss:		0.564382
  validation accuracy:		82.17 %
Epoch 1120 of 2000 took 0.165s
  training loss:		0.557480
  validation loss:		0.562163
  validation accuracy:		81.30 %
Epoch 1121 of 2000 took 0.173s
  training loss:		0.549704
  validation loss:		0.553130
  validation accuracy:		81.74 %
Epoch 1122 of 2000 took 0.315s
  training loss:		0.555839
  validation loss:		0.579602
  validation accuracy:		81.30 %
Epoch 1123 of 2000 took 0.167s
  training loss:		0.548657
  validation loss:		0.565094
  validation accuracy:		81.52 %
Epoch 1124 of 2000 took 0.192s
  training loss:		0.554879
  validation loss:		0.555339
  validation accuracy:		82.28 %
Epoch 1125 of 2000 took 0.291s
  training loss:		0.537479
  validation loss:		0.555927
  validation accuracy:		81.85 %
Epoch 1126 of 2000 took 0.167s
  training loss:		0.551874
  validation loss:		0.558437
  validation accuracy:		82.07 %
Epoch 1127 of 2000 took 0.165s
  training loss:		0.544332
  validation loss:		0.563896
  validation accuracy:		81.52 %
Epoch 1128 of 2000 took 0.165s
  training loss:		0.558318
  validation loss:		0.552156
  validation accuracy:		82.39 %
Epoch 1129 of 2000 took 0.165s
  training loss:		0.553639
  validation loss:		0.575659
  validation accuracy:		81.30 %
Epoch 1130 of 2000 took 0.165s
  training loss:		0.546684
  validation loss:		0.553562
  validation accuracy:		82.39 %
Epoch 1131 of 2000 took 0.165s
  training loss:		0.559015
  validation loss:		0.561537
  validation accuracy:		81.63 %
Epoch 1132 of 2000 took 0.165s
  training loss:		0.545275
  validation loss:		0.562447
  validation accuracy:		82.17 %
Epoch 1133 of 2000 took 0.165s
  training loss:		0.548434
  validation loss:		0.566482
  validation accuracy:		81.30 %
Epoch 1134 of 2000 took 0.165s
  training loss:		0.538654
  validation loss:		0.575919
  validation accuracy:		81.63 %
Epoch 1135 of 2000 took 0.165s
  training loss:		0.549961
  validation loss:		0.558797
  validation accuracy:		81.85 %
Epoch 1136 of 2000 took 0.165s
  training loss:		0.554165
  validation loss:		0.564816
  validation accuracy:		81.30 %
Epoch 1137 of 2000 took 0.165s
  training loss:		0.545123
  validation loss:		0.562273
  validation accuracy:		81.74 %
Epoch 1138 of 2000 took 0.165s
  training loss:		0.551532
  validation loss:		0.557277
  validation accuracy:		81.41 %
Epoch 1139 of 2000 took 0.166s
  training loss:		0.547043
  validation loss:		0.562271
  validation accuracy:		81.41 %
Epoch 1140 of 2000 took 0.165s
  training loss:		0.552976
  validation loss:		0.554881
  validation accuracy:		81.30 %
Epoch 1141 of 2000 took 0.165s
  training loss:		0.552990
  validation loss:		0.548683
  validation accuracy:		81.96 %
Epoch 1142 of 2000 took 0.165s
  training loss:		0.550335
  validation loss:		0.590165
  validation accuracy:		81.09 %
Epoch 1143 of 2000 took 0.165s
  training loss:		0.548352
  validation loss:		0.557798
  validation accuracy:		82.28 %
Epoch 1144 of 2000 took 0.165s
  training loss:		0.559160
  validation loss:		0.579853
  validation accuracy:		81.41 %
Epoch 1145 of 2000 took 0.165s
  training loss:		0.549309
  validation loss:		0.560756
  validation accuracy:		81.41 %
Epoch 1146 of 2000 took 0.165s
  training loss:		0.544417
  validation loss:		0.563313
  validation accuracy:		81.96 %
Epoch 1147 of 2000 took 0.165s
  training loss:		0.550238
  validation loss:		0.566708
  validation accuracy:		81.85 %
Epoch 1148 of 2000 took 0.165s
  training loss:		0.552061
  validation loss:		0.587244
  validation accuracy:		80.87 %
Epoch 1149 of 2000 took 0.165s
  training loss:		0.548488
  validation loss:		0.560060
  validation accuracy:		81.63 %
Epoch 1150 of 2000 took 0.165s
  training loss:		0.553632
  validation loss:		0.580499
  validation accuracy:		81.20 %
Epoch 1151 of 2000 took 0.167s
  training loss:		0.556596
  validation loss:		0.565549
  validation accuracy:		81.30 %
Epoch 1152 of 2000 took 0.166s
  training loss:		0.548856
  validation loss:		0.566552
  validation accuracy:		81.96 %
Epoch 1153 of 2000 took 0.165s
  training loss:		0.551989
  validation loss:		0.567427
  validation accuracy:		81.09 %
Epoch 1154 of 2000 took 0.165s
  training loss:		0.550972
  validation loss:		0.556209
  validation accuracy:		82.50 %
Epoch 1155 of 2000 took 0.165s
  training loss:		0.550217
  validation loss:		0.550959
  validation accuracy:		81.52 %
Epoch 1156 of 2000 took 0.165s
  training loss:		0.547990
  validation loss:		0.556572
  validation accuracy:		81.30 %
Epoch 1157 of 2000 took 0.165s
  training loss:		0.543671
  validation loss:		0.559993
  validation accuracy:		81.41 %
Epoch 1158 of 2000 took 0.165s
  training loss:		0.547876
  validation loss:		0.554171
  validation accuracy:		81.85 %
Epoch 1159 of 2000 took 0.165s
  training loss:		0.558899
  validation loss:		0.604861
  validation accuracy:		80.11 %
Epoch 1160 of 2000 took 0.165s
  training loss:		0.549865
  validation loss:		0.584066
  validation accuracy:		81.41 %
Epoch 1161 of 2000 took 0.165s
  training loss:		0.558195
  validation loss:		0.574923
  validation accuracy:		81.41 %
Epoch 1162 of 2000 took 0.166s
  training loss:		0.551939
  validation loss:		0.554520
  validation accuracy:		82.39 %
Epoch 1163 of 2000 took 0.257s
  training loss:		0.560661
  validation loss:		0.556786
  validation accuracy:		81.41 %
Epoch 1164 of 2000 took 0.168s
  training loss:		0.552016
  validation loss:		0.558779
  validation accuracy:		81.63 %
Epoch 1165 of 2000 took 0.165s
  training loss:		0.539766
  validation loss:		0.549868
  validation accuracy:		82.17 %
Epoch 1166 of 2000 took 0.165s
  training loss:		0.536942
  validation loss:		0.597671
  validation accuracy:		80.54 %
Epoch 1167 of 2000 took 0.165s
  training loss:		0.552732
  validation loss:		0.596836
  validation accuracy:		80.43 %
Epoch 1168 of 2000 took 0.165s
  training loss:		0.556790
  validation loss:		0.560802
  validation accuracy:		81.52 %
Epoch 1169 of 2000 took 0.184s
  training loss:		0.544155
  validation loss:		0.555958
  validation accuracy:		81.20 %
Epoch 1170 of 2000 took 0.305s
  training loss:		0.549068
  validation loss:		0.571278
  validation accuracy:		81.41 %
Epoch 1171 of 2000 took 0.205s
  training loss:		0.554647
  validation loss:		0.591998
  validation accuracy:		80.76 %
Epoch 1172 of 2000 took 0.165s
  training loss:		0.537090
  validation loss:		0.552444
  validation accuracy:		81.96 %
Epoch 1173 of 2000 took 0.165s
  training loss:		0.545331
  validation loss:		0.552273
  validation accuracy:		81.85 %
Epoch 1174 of 2000 took 0.165s
  training loss:		0.547091
  validation loss:		0.553403
  validation accuracy:		81.74 %
Epoch 1175 of 2000 took 0.165s
  training loss:		0.547892
  validation loss:		0.552220
  validation accuracy:		82.17 %
Epoch 1176 of 2000 took 0.165s
  training loss:		0.557990
  validation loss:		0.557916
  validation accuracy:		81.52 %
Epoch 1177 of 2000 took 0.165s
  training loss:		0.553146
  validation loss:		0.586330
  validation accuracy:		81.52 %
Epoch 1178 of 2000 took 0.165s
  training loss:		0.556040
  validation loss:		0.555482
  validation accuracy:		81.85 %
Epoch 1179 of 2000 took 0.165s
  training loss:		0.556418
  validation loss:		0.571686
  validation accuracy:		81.41 %
Epoch 1180 of 2000 took 0.181s
  training loss:		0.576017
  validation loss:		0.597244
  validation accuracy:		80.65 %
Epoch 1181 of 2000 took 0.165s
  training loss:		0.556969
  validation loss:		0.557518
  validation accuracy:		82.07 %
Epoch 1182 of 2000 took 0.165s
  training loss:		0.547507
  validation loss:		0.550349
  validation accuracy:		81.74 %
Epoch 1183 of 2000 took 0.165s
  training loss:		0.550887
  validation loss:		0.592161
  validation accuracy:		81.09 %
Epoch 1184 of 2000 took 0.165s
  training loss:		0.554671
  validation loss:		0.564571
  validation accuracy:		81.20 %
Epoch 1185 of 2000 took 0.165s
  training loss:		0.545929
  validation loss:		0.556400
  validation accuracy:		81.63 %
Epoch 1186 of 2000 took 0.165s
  training loss:		0.549092
  validation loss:		0.553640
  validation accuracy:		81.96 %
Epoch 1187 of 2000 took 0.165s
  training loss:		0.558568
  validation loss:		0.569602
  validation accuracy:		81.52 %
Epoch 1188 of 2000 took 0.165s
  training loss:		0.551840
  validation loss:		0.574778
  validation accuracy:		81.74 %
Epoch 1189 of 2000 took 0.165s
  training loss:		0.552705
  validation loss:		0.565417
  validation accuracy:		81.30 %
Epoch 1190 of 2000 took 0.164s
  training loss:		0.560324
  validation loss:		0.591740
  validation accuracy:		80.65 %
Epoch 1191 of 2000 took 0.165s
  training loss:		0.550090
  validation loss:		0.565089
  validation accuracy:		81.85 %
Epoch 1192 of 2000 took 0.165s
  training loss:		0.546066
  validation loss:		0.553475
  validation accuracy:		81.52 %
Epoch 1193 of 2000 took 0.165s
  training loss:		0.547397
  validation loss:		0.558034
  validation accuracy:		81.52 %
Epoch 1194 of 2000 took 0.165s
  training loss:		0.558072
  validation loss:		0.589125
  validation accuracy:		81.30 %
Epoch 1195 of 2000 took 0.213s
  training loss:		0.551469
  validation loss:		0.562492
  validation accuracy:		81.41 %
Epoch 1196 of 2000 took 0.331s
  training loss:		0.553069
  validation loss:		0.556403
  validation accuracy:		81.63 %
Epoch 1197 of 2000 took 0.327s
  training loss:		0.550518
  validation loss:		0.549299
  validation accuracy:		81.63 %
Epoch 1198 of 2000 took 0.302s
  training loss:		0.553671
  validation loss:		0.562889
  validation accuracy:		81.52 %
Epoch 1199 of 2000 took 0.168s
  training loss:		0.550987
  validation loss:		0.577392
  validation accuracy:		81.20 %
Epoch 1200 of 2000 took 0.166s
  training loss:		0.548339
  validation loss:		0.563336
  validation accuracy:		80.87 %
Epoch 1201 of 2000 took 0.165s
  training loss:		0.545460
  validation loss:		0.558638
  validation accuracy:		80.87 %
Epoch 1202 of 2000 took 0.165s
  training loss:		0.552069
  validation loss:		0.585733
  validation accuracy:		80.76 %
Epoch 1203 of 2000 took 0.165s
  training loss:		0.554283
  validation loss:		0.566386
  validation accuracy:		81.52 %
Epoch 1204 of 2000 took 0.165s
  training loss:		0.549844
  validation loss:		0.565748
  validation accuracy:		81.63 %
Epoch 1205 of 2000 took 0.165s
  training loss:		0.553180
  validation loss:		0.562877
  validation accuracy:		81.30 %
Epoch 1206 of 2000 took 0.164s
  training loss:		0.552738
  validation loss:		0.557558
  validation accuracy:		81.41 %
Epoch 1207 of 2000 took 0.164s
  training loss:		0.540095
  validation loss:		0.581446
  validation accuracy:		81.41 %
Epoch 1208 of 2000 took 0.169s
  training loss:		0.549360
  validation loss:		0.561847
  validation accuracy:		81.52 %
Epoch 1209 of 2000 took 0.131s
  training loss:		0.549598
  validation loss:		0.571562
  validation accuracy:		81.96 %
Epoch 1210 of 2000 took 0.107s
  training loss:		0.547202
  validation loss:		0.565643
  validation accuracy:		82.07 %
Epoch 1211 of 2000 took 0.104s
  training loss:		0.544898
  validation loss:		0.560616
  validation accuracy:		80.87 %
Epoch 1212 of 2000 took 0.107s
  training loss:		0.554484
  validation loss:		0.558816
  validation accuracy:		81.74 %
Epoch 1213 of 2000 took 0.108s
  training loss:		0.561881
  validation loss:		0.564219
  validation accuracy:		81.41 %
Epoch 1214 of 2000 took 0.111s
  training loss:		0.555509
  validation loss:		0.552385
  validation accuracy:		81.52 %
Epoch 1215 of 2000 took 0.108s
  training loss:		0.561274
  validation loss:		0.599826
  validation accuracy:		80.43 %
Epoch 1216 of 2000 took 0.111s
  training loss:		0.550242
  validation loss:		0.560996
  validation accuracy:		81.20 %
Epoch 1217 of 2000 took 0.105s
  training loss:		0.547566
  validation loss:		0.553376
  validation accuracy:		81.85 %
Epoch 1218 of 2000 took 0.106s
  training loss:		0.551385
  validation loss:		0.608290
  validation accuracy:		80.33 %
Epoch 1219 of 2000 took 0.105s
  training loss:		0.550752
  validation loss:		0.554779
  validation accuracy:		81.63 %
Epoch 1220 of 2000 took 0.106s
  training loss:		0.546320
  validation loss:		0.562603
  validation accuracy:		81.63 %
Epoch 1221 of 2000 took 0.110s
  training loss:		0.545172
  validation loss:		0.552452
  validation accuracy:		81.74 %
Epoch 1222 of 2000 took 0.107s
  training loss:		0.560929
  validation loss:		0.549550
  validation accuracy:		82.50 %
Epoch 1223 of 2000 took 0.106s
  training loss:		0.566039
  validation loss:		0.569430
  validation accuracy:		81.30 %
Epoch 1224 of 2000 took 0.110s
  training loss:		0.554527
  validation loss:		0.552768
  validation accuracy:		81.41 %
Epoch 1225 of 2000 took 0.108s
  training loss:		0.537503
  validation loss:		0.563538
  validation accuracy:		81.41 %
Epoch 1226 of 2000 took 0.109s
  training loss:		0.549551
  validation loss:		0.549473
  validation accuracy:		82.28 %
Epoch 1227 of 2000 took 0.106s
  training loss:		0.558190
  validation loss:		0.601754
  validation accuracy:		80.43 %
Epoch 1228 of 2000 took 0.105s
  training loss:		0.550458
  validation loss:		0.569661
  validation accuracy:		81.52 %
Epoch 1229 of 2000 took 0.113s
  training loss:		0.548065
  validation loss:		0.573708
  validation accuracy:		81.63 %
Epoch 1230 of 2000 took 0.109s
  training loss:		0.561325
  validation loss:		0.593244
  validation accuracy:		80.87 %
Epoch 1231 of 2000 took 0.107s
  training loss:		0.557824
  validation loss:		0.567021
  validation accuracy:		81.41 %
Epoch 1232 of 2000 took 0.112s
  training loss:		0.549968
  validation loss:		0.571407
  validation accuracy:		81.52 %
Epoch 1233 of 2000 took 0.110s
  training loss:		0.549310
  validation loss:		0.552255
  validation accuracy:		81.41 %
Epoch 1234 of 2000 took 0.108s
  training loss:		0.543873
  validation loss:		0.551177
  validation accuracy:		81.63 %
Epoch 1235 of 2000 took 0.109s
  training loss:		0.548025
  validation loss:		0.553458
  validation accuracy:		81.85 %
Epoch 1236 of 2000 took 0.107s
  training loss:		0.558191
  validation loss:		0.572107
  validation accuracy:		81.74 %
Epoch 1237 of 2000 took 0.117s
  training loss:		0.543575
  validation loss:		0.568704
  validation accuracy:		81.85 %
Epoch 1238 of 2000 took 0.112s
  training loss:		0.548353
  validation loss:		0.565476
  validation accuracy:		81.09 %
Epoch 1239 of 2000 took 0.110s
  training loss:		0.544315
  validation loss:		0.558899
  validation accuracy:		81.30 %
Epoch 1240 of 2000 took 0.117s
  training loss:		0.561962
  validation loss:		0.572516
  validation accuracy:		81.30 %
Epoch 1241 of 2000 took 0.116s
  training loss:		0.549995
  validation loss:		0.554401
  validation accuracy:		81.52 %
Epoch 1242 of 2000 took 0.111s
  training loss:		0.555709
  validation loss:		0.552516
  validation accuracy:		81.96 %
Epoch 1243 of 2000 took 0.112s
  training loss:		0.555052
  validation loss:		0.558335
  validation accuracy:		81.30 %
Epoch 1244 of 2000 took 0.110s
  training loss:		0.548159
  validation loss:		0.554642
  validation accuracy:		81.52 %
Epoch 1245 of 2000 took 0.118s
  training loss:		0.558221
  validation loss:		0.563721
  validation accuracy:		82.17 %
Epoch 1246 of 2000 took 0.113s
  training loss:		0.552394
  validation loss:		0.556080
  validation accuracy:		81.41 %
Epoch 1247 of 2000 took 0.111s
  training loss:		0.548273
  validation loss:		0.567590
  validation accuracy:		81.74 %
Epoch 1248 of 2000 took 0.118s
  training loss:		0.554469
  validation loss:		0.553858
  validation accuracy:		82.28 %
Epoch 1249 of 2000 took 0.114s
  training loss:		0.548638
  validation loss:		0.552269
  validation accuracy:		81.52 %
Epoch 1250 of 2000 took 0.112s
  training loss:		0.543865
  validation loss:		0.550828
  validation accuracy:		82.07 %
Epoch 1251 of 2000 took 0.111s
  training loss:		0.545402
  validation loss:		0.552161
  validation accuracy:		81.85 %
Epoch 1252 of 2000 took 0.111s
  training loss:		0.547303
  validation loss:		0.557646
  validation accuracy:		81.41 %
Epoch 1253 of 2000 took 0.118s
  training loss:		0.549071
  validation loss:		0.574114
  validation accuracy:		81.74 %
Epoch 1254 of 2000 took 0.112s
  training loss:		0.549979
  validation loss:		0.566379
  validation accuracy:		81.41 %
Epoch 1255 of 2000 took 0.112s
  training loss:		0.544422
  validation loss:		0.607942
  validation accuracy:		79.89 %
Epoch 1256 of 2000 took 0.118s
  training loss:		0.551363
  validation loss:		0.554620
  validation accuracy:		81.41 %
Epoch 1257 of 2000 took 0.110s
  training loss:		0.551075
  validation loss:		0.558506
  validation accuracy:		81.20 %
Epoch 1258 of 2000 took 0.115s
  training loss:		0.539926
  validation loss:		0.552266
  validation accuracy:		81.41 %
Epoch 1259 of 2000 took 0.113s
  training loss:		0.556262
  validation loss:		0.561096
  validation accuracy:		81.52 %
Epoch 1260 of 2000 took 0.112s
  training loss:		0.551996
  validation loss:		0.564541
  validation accuracy:		80.98 %
Epoch 1261 of 2000 took 0.116s
  training loss:		0.544663
  validation loss:		0.564885
  validation accuracy:		81.30 %
Epoch 1262 of 2000 took 0.110s
  training loss:		0.554085
  validation loss:		0.556436
  validation accuracy:		81.41 %
Epoch 1263 of 2000 took 0.116s
  training loss:		0.556608
  validation loss:		0.553606
  validation accuracy:		82.07 %
Epoch 1264 of 2000 took 0.112s
  training loss:		0.561790
  validation loss:		0.570487
  validation accuracy:		81.30 %
Epoch 1265 of 2000 took 0.113s
  training loss:		0.543975
  validation loss:		0.577346
  validation accuracy:		81.41 %
Epoch 1266 of 2000 took 0.116s
  training loss:		0.545663
  validation loss:		0.556381
  validation accuracy:		82.17 %
Epoch 1267 of 2000 took 0.110s
  training loss:		0.545345
  validation loss:		0.636927
  validation accuracy:		79.02 %
Epoch 1268 of 2000 took 0.117s
  training loss:		0.544469
  validation loss:		0.557715
  validation accuracy:		81.52 %
Epoch 1269 of 2000 took 0.110s
  training loss:		0.554015
  validation loss:		0.554797
  validation accuracy:		81.85 %
Epoch 1270 of 2000 took 0.113s
  training loss:		0.551838
  validation loss:		0.582942
  validation accuracy:		81.20 %
Epoch 1271 of 2000 took 0.115s
  training loss:		0.563290
  validation loss:		0.560336
  validation accuracy:		81.52 %
Epoch 1272 of 2000 took 0.111s
  training loss:		0.554115
  validation loss:		0.558787
  validation accuracy:		82.28 %
Epoch 1273 of 2000 took 0.117s
  training loss:		0.547958
  validation loss:		0.554164
  validation accuracy:		81.74 %
Epoch 1274 of 2000 took 0.110s
  training loss:		0.556533
  validation loss:		0.556923
  validation accuracy:		82.28 %
Epoch 1275 of 2000 took 0.114s
  training loss:		0.553882
  validation loss:		0.593847
  validation accuracy:		81.09 %
Epoch 1276 of 2000 took 0.114s
  training loss:		0.537505
  validation loss:		0.562526
  validation accuracy:		81.63 %
Epoch 1277 of 2000 took 0.111s
  training loss:		0.554655
  validation loss:		0.588700
  validation accuracy:		80.98 %
Epoch 1278 of 2000 took 0.117s
  training loss:		0.556961
  validation loss:		0.561078
  validation accuracy:		81.20 %
Epoch 1279 of 2000 took 0.110s
  training loss:		0.556913
  validation loss:		0.574380
  validation accuracy:		81.52 %
Epoch 1280 of 2000 took 0.116s
  training loss:		0.546115
  validation loss:		0.554707
  validation accuracy:		81.52 %
Epoch 1281 of 2000 took 0.112s
  training loss:		0.540937
  validation loss:		0.552959
  validation accuracy:		82.28 %
Epoch 1282 of 2000 took 0.113s
  training loss:		0.543616
  validation loss:		0.561243
  validation accuracy:		81.52 %
Epoch 1283 of 2000 took 0.119s
  training loss:		0.556448
  validation loss:		0.590949
  validation accuracy:		80.76 %
Epoch 1284 of 2000 took 0.111s
  training loss:		0.543937
  validation loss:		0.560868
  validation accuracy:		81.85 %
Epoch 1285 of 2000 took 0.113s
  training loss:		0.551333
  validation loss:		0.582807
  validation accuracy:		81.30 %
Epoch 1286 of 2000 took 0.110s
  training loss:		0.547308
  validation loss:		0.554035
  validation accuracy:		81.52 %
Epoch 1287 of 2000 took 0.114s
  training loss:		0.552059
  validation loss:		0.586856
  validation accuracy:		80.98 %
Epoch 1288 of 2000 took 0.117s
  training loss:		0.545015
  validation loss:		0.558708
  validation accuracy:		81.09 %
Epoch 1289 of 2000 took 0.110s
  training loss:		0.547709
  validation loss:		0.568438
  validation accuracy:		81.74 %
Epoch 1290 of 2000 took 0.113s
  training loss:		0.545310
  validation loss:		0.575257
  validation accuracy:		81.63 %
Epoch 1291 of 2000 took 0.120s
  training loss:		0.543349
  validation loss:		0.568210
  validation accuracy:		81.30 %
Epoch 1292 of 2000 took 0.110s
  training loss:		0.555788
  validation loss:		0.564143
  validation accuracy:		81.52 %
Epoch 1293 of 2000 took 0.113s
  training loss:		0.554410
  validation loss:		0.562508
  validation accuracy:		81.41 %
Epoch 1294 of 2000 took 0.110s
  training loss:		0.548780
  validation loss:		0.558067
  validation accuracy:		81.85 %
Epoch 1295 of 2000 took 0.114s
  training loss:		0.553320
  validation loss:		0.571588
  validation accuracy:		81.41 %
Epoch 1296 of 2000 took 0.117s
  training loss:		0.550742
  validation loss:		0.567972
  validation accuracy:		81.09 %
Epoch 1297 of 2000 took 0.104s
  training loss:		0.554747
  validation loss:		0.589991
  validation accuracy:		81.09 %
Epoch 1298 of 2000 took 0.106s
  training loss:		0.549322
  validation loss:		0.559087
  validation accuracy:		81.85 %
Epoch 1299 of 2000 took 0.114s
  training loss:		0.559451
  validation loss:		0.566619
  validation accuracy:		81.96 %
Epoch 1300 of 2000 took 0.104s
  training loss:		0.545538
  validation loss:		0.556699
  validation accuracy:		81.63 %
Epoch 1301 of 2000 took 0.106s
  training loss:		0.542918
  validation loss:		0.554354
  validation accuracy:		81.63 %
Epoch 1302 of 2000 took 0.103s
  training loss:		0.551433
  validation loss:		0.554636
  validation accuracy:		81.41 %
Epoch 1303 of 2000 took 0.105s
  training loss:		0.549313
  validation loss:		0.564050
  validation accuracy:		81.41 %
Epoch 1304 of 2000 took 0.107s
  training loss:		0.557596
  validation loss:		0.557926
  validation accuracy:		81.96 %
Epoch 1305 of 2000 took 0.100s
  training loss:		0.552559
  validation loss:		0.571870
  validation accuracy:		81.30 %
Epoch 1306 of 2000 took 0.102s
  training loss:		0.552551
  validation loss:		0.596230
  validation accuracy:		80.76 %
Epoch 1307 of 2000 took 0.110s
  training loss:		0.558100
  validation loss:		0.561158
  validation accuracy:		81.41 %
Epoch 1308 of 2000 took 0.100s
  training loss:		0.556011
  validation loss:		0.558506
  validation accuracy:		81.52 %
Epoch 1309 of 2000 took 0.103s
  training loss:		0.549537
  validation loss:		0.582785
  validation accuracy:		81.30 %
Epoch 1310 of 2000 took 0.101s
  training loss:		0.556847
  validation loss:		0.566033
  validation accuracy:		81.63 %
Epoch 1311 of 2000 took 0.104s
  training loss:		0.548475
  validation loss:		0.589217
  validation accuracy:		80.98 %
Epoch 1312 of 2000 took 0.107s
  training loss:		0.560303
  validation loss:		0.560715
  validation accuracy:		81.52 %
Epoch 1313 of 2000 took 0.100s
  training loss:		0.544567
  validation loss:		0.560928
  validation accuracy:		82.17 %
Epoch 1314 of 2000 took 0.102s
  training loss:		0.547793
  validation loss:		0.583542
  validation accuracy:		81.20 %
Epoch 1315 of 2000 took 0.110s
  training loss:		0.551284
  validation loss:		0.584949
  validation accuracy:		81.52 %
Epoch 1316 of 2000 took 0.100s
  training loss:		0.551575
  validation loss:		0.567721
  validation accuracy:		81.63 %
Epoch 1317 of 2000 took 0.103s
  training loss:		0.553868
  validation loss:		0.560858
  validation accuracy:		81.52 %
Epoch 1318 of 2000 took 0.100s
  training loss:		0.556271
  validation loss:		0.564180
  validation accuracy:		81.52 %
Epoch 1319 of 2000 took 0.104s
  training loss:		0.552312
  validation loss:		0.593036
  validation accuracy:		80.98 %
Epoch 1320 of 2000 took 0.107s
  training loss:		0.561326
  validation loss:		0.582140
  validation accuracy:		80.98 %
Epoch 1321 of 2000 took 0.100s
  training loss:		0.551575
  validation loss:		0.565972
  validation accuracy:		81.41 %
Epoch 1322 of 2000 took 0.103s
  training loss:		0.550732
  validation loss:		0.554084
  validation accuracy:		81.41 %
Epoch 1323 of 2000 took 0.110s
  training loss:		0.547038
  validation loss:		0.565663
  validation accuracy:		81.41 %
Epoch 1324 of 2000 took 0.100s
  training loss:		0.548308
  validation loss:		0.561686
  validation accuracy:		81.63 %
Epoch 1325 of 2000 took 0.103s
  training loss:		0.540794
  validation loss:		0.568861
  validation accuracy:		81.74 %
Epoch 1326 of 2000 took 0.100s
  training loss:		0.553841
  validation loss:		0.560192
  validation accuracy:		81.74 %
Epoch 1327 of 2000 took 0.104s
  training loss:		0.547004
  validation loss:		0.565274
  validation accuracy:		81.52 %
Epoch 1328 of 2000 took 0.108s
  training loss:		0.547232
  validation loss:		0.567214
  validation accuracy:		81.30 %
Epoch 1329 of 2000 took 0.099s
  training loss:		0.543280
  validation loss:		0.583541
  validation accuracy:		81.09 %
Epoch 1330 of 2000 took 0.102s
  training loss:		0.545070
  validation loss:		0.567447
  validation accuracy:		81.41 %
Epoch 1331 of 2000 took 0.108s
  training loss:		0.544248
  validation loss:		0.570294
  validation accuracy:		81.30 %
Epoch 1332 of 2000 took 0.099s
  training loss:		0.547743
  validation loss:		0.558897
  validation accuracy:		81.74 %
Epoch 1333 of 2000 took 0.102s
  training loss:		0.546844
  validation loss:		0.568514
  validation accuracy:		81.41 %
Epoch 1334 of 2000 took 0.099s
  training loss:		0.544395
  validation loss:		0.613119
  validation accuracy:		80.54 %
Epoch 1335 of 2000 took 0.103s
  training loss:		0.550706
  validation loss:		0.555142
  validation accuracy:		81.52 %
Epoch 1336 of 2000 took 0.106s
  training loss:		0.547167
  validation loss:		0.555758
  validation accuracy:		81.96 %
Epoch 1337 of 2000 took 0.099s
  training loss:		0.552362
  validation loss:		0.572504
  validation accuracy:		81.52 %
Epoch 1338 of 2000 took 0.102s
  training loss:		0.543682
  validation loss:		0.575264
  validation accuracy:		81.74 %
Epoch 1339 of 2000 took 0.110s
  training loss:		0.547006
  validation loss:		0.567523
  validation accuracy:		82.07 %
Epoch 1340 of 2000 took 0.100s
  training loss:		0.552369
  validation loss:		0.558448
  validation accuracy:		80.87 %
Epoch 1341 of 2000 took 0.103s
  training loss:		0.558580
  validation loss:		0.559207
  validation accuracy:		81.09 %
Epoch 1342 of 2000 took 0.100s
  training loss:		0.549342
  validation loss:		0.550726
  validation accuracy:		81.85 %
Epoch 1343 of 2000 took 0.103s
  training loss:		0.544086
  validation loss:		0.559152
  validation accuracy:		81.20 %
Epoch 1344 of 2000 took 0.107s
  training loss:		0.555487
  validation loss:		0.632521
  validation accuracy:		79.46 %
Epoch 1345 of 2000 took 0.100s
  training loss:		0.546612
  validation loss:		0.555919
  validation accuracy:		81.52 %
Epoch 1346 of 2000 took 0.103s
  training loss:		0.549377
  validation loss:		0.556663
  validation accuracy:		80.87 %
Epoch 1347 of 2000 took 0.110s
  training loss:		0.554769
  validation loss:		0.555870
  validation accuracy:		81.63 %
Epoch 1348 of 2000 took 0.100s
  training loss:		0.555760
  validation loss:		0.554649
  validation accuracy:		81.30 %
Epoch 1349 of 2000 took 0.103s
  training loss:		0.542886
  validation loss:		0.560169
  validation accuracy:		81.20 %
Epoch 1350 of 2000 took 0.100s
  training loss:		0.552257
  validation loss:		0.552937
  validation accuracy:		81.52 %
Epoch 1351 of 2000 took 0.104s
  training loss:		0.562708
  validation loss:		0.559595
  validation accuracy:		81.20 %
Epoch 1352 of 2000 took 0.107s
  training loss:		0.545980
  validation loss:		0.594441
  validation accuracy:		80.76 %
Epoch 1353 of 2000 took 0.100s
  training loss:		0.546522
  validation loss:		0.563811
  validation accuracy:		81.63 %
Epoch 1354 of 2000 took 0.103s
  training loss:		0.551607
  validation loss:		0.558826
  validation accuracy:		81.63 %
Epoch 1355 of 2000 took 0.110s
  training loss:		0.548209
  validation loss:		0.636507
  validation accuracy:		79.24 %
Epoch 1356 of 2000 took 0.100s
  training loss:		0.566079
  validation loss:		0.561232
  validation accuracy:		81.52 %
Epoch 1357 of 2000 took 0.103s
  training loss:		0.549069
  validation loss:		0.559307
  validation accuracy:		81.41 %
Epoch 1358 of 2000 took 0.100s
  training loss:		0.533211
  validation loss:		0.555479
  validation accuracy:		82.17 %
Epoch 1359 of 2000 took 0.104s
  training loss:		0.544679
  validation loss:		0.559079
  validation accuracy:		81.52 %
Epoch 1360 of 2000 took 0.107s
  training loss:		0.542345
  validation loss:		0.555015
  validation accuracy:		80.98 %
Epoch 1361 of 2000 took 0.100s
  training loss:		0.550325
  validation loss:		0.571684
  validation accuracy:		81.52 %
Epoch 1362 of 2000 took 0.103s
  training loss:		0.544890
  validation loss:		0.561223
  validation accuracy:		82.17 %
Epoch 1363 of 2000 took 0.109s
  training loss:		0.551994
  validation loss:		0.551780
  validation accuracy:		82.07 %
Epoch 1364 of 2000 took 0.100s
  training loss:		0.548997
  validation loss:		0.556950
  validation accuracy:		81.09 %
Epoch 1365 of 2000 took 0.103s
  training loss:		0.542641
  validation loss:		0.575226
  validation accuracy:		81.30 %
Epoch 1366 of 2000 took 0.100s
  training loss:		0.553343
  validation loss:		0.586523
  validation accuracy:		80.76 %
Epoch 1367 of 2000 took 0.104s
  training loss:		0.546176
  validation loss:		0.558355
  validation accuracy:		81.74 %
Epoch 1368 of 2000 took 0.108s
  training loss:		0.554825
  validation loss:		0.557150
  validation accuracy:		81.30 %
Epoch 1369 of 2000 took 0.100s
  training loss:		0.555133
  validation loss:		0.565575
  validation accuracy:		81.30 %
Epoch 1370 of 2000 took 0.103s
  training loss:		0.541731
  validation loss:		0.588715
  validation accuracy:		81.52 %
Epoch 1371 of 2000 took 0.109s
  training loss:		0.553023
  validation loss:		0.553090
  validation accuracy:		81.41 %
Epoch 1372 of 2000 took 0.100s
  training loss:		0.546216
  validation loss:		0.577659
  validation accuracy:		81.52 %
Epoch 1373 of 2000 took 0.103s
  training loss:		0.549562
  validation loss:		0.562393
  validation accuracy:		81.52 %
Epoch 1374 of 2000 took 0.100s
  training loss:		0.546503
  validation loss:		0.556419
  validation accuracy:		81.85 %
Epoch 1375 of 2000 took 0.104s
  training loss:		0.548341
  validation loss:		0.581887
  validation accuracy:		81.63 %
Epoch 1376 of 2000 took 0.107s
  training loss:		0.536344
  validation loss:		0.566044
  validation accuracy:		82.07 %
Epoch 1377 of 2000 took 0.100s
  training loss:		0.549852
  validation loss:		0.567939
  validation accuracy:		81.63 %
Epoch 1378 of 2000 took 0.103s
  training loss:		0.549297
  validation loss:		0.584476
  validation accuracy:		81.30 %
Epoch 1379 of 2000 took 0.109s
  training loss:		0.555023
  validation loss:		0.558132
  validation accuracy:		81.41 %
Epoch 1380 of 2000 took 0.100s
  training loss:		0.549030
  validation loss:		0.553754
  validation accuracy:		81.85 %
Epoch 1381 of 2000 took 0.103s
  training loss:		0.544863
  validation loss:		0.555437
  validation accuracy:		81.09 %
Epoch 1382 of 2000 took 0.100s
  training loss:		0.548479
  validation loss:		0.566873
  validation accuracy:		81.30 %
Epoch 1383 of 2000 took 0.104s
  training loss:		0.544273
  validation loss:		0.549208
  validation accuracy:		81.85 %
Epoch 1384 of 2000 took 0.107s
  training loss:		0.556689
  validation loss:		0.581166
  validation accuracy:		81.30 %
Epoch 1385 of 2000 took 0.100s
  training loss:		0.556009
  validation loss:		0.551547
  validation accuracy:		82.07 %
Epoch 1386 of 2000 took 0.103s
  training loss:		0.543949
  validation loss:		0.558842
  validation accuracy:		81.63 %
Epoch 1387 of 2000 took 0.109s
  training loss:		0.542076
  validation loss:		0.549264
  validation accuracy:		81.85 %
Epoch 1388 of 2000 took 0.100s
  training loss:		0.546199
  validation loss:		0.555517
  validation accuracy:		81.41 %
Epoch 1389 of 2000 took 0.103s
  training loss:		0.544498
  validation loss:		0.559016
  validation accuracy:		81.41 %
Epoch 1390 of 2000 took 0.100s
  training loss:		0.548089
  validation loss:		0.557589
  validation accuracy:		81.96 %
Epoch 1391 of 2000 took 0.105s
  training loss:		0.555951
  validation loss:		0.558523
  validation accuracy:		81.09 %
Epoch 1392 of 2000 took 0.106s
  training loss:		0.553083
  validation loss:		0.557199
  validation accuracy:		82.17 %
Epoch 1393 of 2000 took 0.100s
  training loss:		0.555782
  validation loss:		0.567957
  validation accuracy:		81.41 %
Epoch 1394 of 2000 took 0.103s
  training loss:		0.546844
  validation loss:		0.566502
  validation accuracy:		81.52 %
Epoch 1395 of 2000 took 0.109s
  training loss:		0.544340
  validation loss:		0.563009
  validation accuracy:		82.39 %
Epoch 1396 of 2000 took 0.103s
  training loss:		0.552842
  validation loss:		0.557482
  validation accuracy:		81.85 %
Epoch 1397 of 2000 took 0.103s
  training loss:		0.550830
  validation loss:		0.576608
  validation accuracy:		81.63 %
Epoch 1398 of 2000 took 0.104s
  training loss:		0.552935
  validation loss:		0.559028
  validation accuracy:		82.28 %
Epoch 1399 of 2000 took 0.103s
  training loss:		0.560124
  validation loss:		0.553440
  validation accuracy:		81.96 %
Epoch 1400 of 2000 took 0.103s
  training loss:		0.554875
  validation loss:		0.576546
  validation accuracy:		81.63 %
Epoch 1401 of 2000 took 0.103s
  training loss:		0.552888
  validation loss:		0.557674
  validation accuracy:		81.09 %
Epoch 1402 of 2000 took 0.103s
  training loss:		0.550377
  validation loss:		0.560519
  validation accuracy:		80.98 %
Epoch 1403 of 2000 took 0.103s
  training loss:		0.555906
  validation loss:		0.577527
  validation accuracy:		81.20 %
Epoch 1404 of 2000 took 0.103s
  training loss:		0.554772
  validation loss:		0.555365
  validation accuracy:		81.63 %
Epoch 1405 of 2000 took 0.103s
  training loss:		0.554440
  validation loss:		0.571552
  validation accuracy:		81.52 %
Epoch 1406 of 2000 took 0.103s
  training loss:		0.544428
  validation loss:		0.579621
  validation accuracy:		81.74 %
Epoch 1407 of 2000 took 0.103s
  training loss:		0.542720
  validation loss:		0.558953
  validation accuracy:		81.20 %
Epoch 1408 of 2000 took 0.103s
  training loss:		0.548418
  validation loss:		0.557360
  validation accuracy:		82.17 %
Epoch 1409 of 2000 took 0.103s
  training loss:		0.547059
  validation loss:		0.566460
  validation accuracy:		81.09 %
Epoch 1410 of 2000 took 0.103s
  training loss:		0.545074
  validation loss:		0.551037
  validation accuracy:		81.41 %
Epoch 1411 of 2000 took 0.103s
  training loss:		0.550003
  validation loss:		0.559978
  validation accuracy:		81.74 %
Epoch 1412 of 2000 took 0.103s
  training loss:		0.545737
  validation loss:		0.553702
  validation accuracy:		81.41 %
Epoch 1413 of 2000 took 0.106s
  training loss:		0.549079
  validation loss:		0.557275
  validation accuracy:		81.52 %
Epoch 1414 of 2000 took 0.106s
  training loss:		0.552825
  validation loss:		0.563797
  validation accuracy:		81.09 %
Epoch 1415 of 2000 took 0.108s
  training loss:		0.545322
  validation loss:		0.593179
  validation accuracy:		80.87 %
Epoch 1416 of 2000 took 0.107s
  training loss:		0.557625
  validation loss:		0.567344
  validation accuracy:		80.87 %
Epoch 1417 of 2000 took 0.106s
  training loss:		0.561599
  validation loss:		0.552342
  validation accuracy:		81.52 %
Epoch 1418 of 2000 took 0.106s
  training loss:		0.547826
  validation loss:		0.555777
  validation accuracy:		80.65 %
Epoch 1419 of 2000 took 0.106s
  training loss:		0.546716
  validation loss:		0.592236
  validation accuracy:		80.87 %
Epoch 1420 of 2000 took 0.106s
  training loss:		0.545091
  validation loss:		0.563073
  validation accuracy:		81.52 %
Epoch 1421 of 2000 took 0.106s
  training loss:		0.547740
  validation loss:		0.552930
  validation accuracy:		81.52 %
Epoch 1422 of 2000 took 0.106s
  training loss:		0.549958
  validation loss:		0.555308
  validation accuracy:		81.30 %
Epoch 1423 of 2000 took 0.107s
  training loss:		0.557917
  validation loss:		0.559748
  validation accuracy:		81.20 %
Epoch 1424 of 2000 took 0.106s
  training loss:		0.554055
  validation loss:		0.578122
  validation accuracy:		81.63 %
Epoch 1425 of 2000 took 0.106s
  training loss:		0.538216
  validation loss:		0.564056
  validation accuracy:		81.30 %
Epoch 1426 of 2000 took 0.106s
  training loss:		0.547734
  validation loss:		0.553961
  validation accuracy:		81.96 %
Epoch 1427 of 2000 took 0.103s
  training loss:		0.547658
  validation loss:		0.612812
  validation accuracy:		79.89 %
Epoch 1428 of 2000 took 0.103s
  training loss:		0.550770
  validation loss:		0.567086
  validation accuracy:		81.52 %
Epoch 1429 of 2000 took 0.103s
  training loss:		0.552378
  validation loss:		0.554275
  validation accuracy:		81.63 %
Epoch 1430 of 2000 took 0.103s
  training loss:		0.550707
  validation loss:		0.559141
  validation accuracy:		81.30 %
Epoch 1431 of 2000 took 0.103s
  training loss:		0.552760
  validation loss:		0.553501
  validation accuracy:		81.74 %
Epoch 1432 of 2000 took 0.103s
  training loss:		0.555629
  validation loss:		0.567961
  validation accuracy:		81.52 %
Epoch 1433 of 2000 took 0.103s
  training loss:		0.553002
  validation loss:		0.556982
  validation accuracy:		81.85 %
Epoch 1434 of 2000 took 0.103s
  training loss:		0.547411
  validation loss:		0.556572
  validation accuracy:		81.30 %
Epoch 1435 of 2000 took 0.103s
  training loss:		0.560182
  validation loss:		0.574963
  validation accuracy:		81.52 %
Epoch 1436 of 2000 took 0.103s
  training loss:		0.548115
  validation loss:		0.559351
  validation accuracy:		81.85 %
Epoch 1437 of 2000 took 0.103s
  training loss:		0.554057
  validation loss:		0.555718
  validation accuracy:		81.20 %
Epoch 1438 of 2000 took 0.103s
  training loss:		0.550620
  validation loss:		0.554685
  validation accuracy:		82.17 %
Epoch 1439 of 2000 took 0.103s
  training loss:		0.548090
  validation loss:		0.551690
  validation accuracy:		82.28 %
Epoch 1440 of 2000 took 0.103s
  training loss:		0.551833
  validation loss:		0.581128
  validation accuracy:		81.74 %
Epoch 1441 of 2000 took 0.103s
  training loss:		0.550054
  validation loss:		0.553851
  validation accuracy:		82.07 %
Epoch 1442 of 2000 took 0.103s
  training loss:		0.553959
  validation loss:		0.555462
  validation accuracy:		81.41 %
Epoch 1443 of 2000 took 0.103s
  training loss:		0.547555
  validation loss:		0.550709
  validation accuracy:		82.07 %
Epoch 1444 of 2000 took 0.103s
  training loss:		0.549797
  validation loss:		0.562399
  validation accuracy:		81.52 %
Epoch 1445 of 2000 took 0.103s
  training loss:		0.547976
  validation loss:		0.573242
  validation accuracy:		81.30 %
Epoch 1446 of 2000 took 0.103s
  training loss:		0.551792
  validation loss:		0.561199
  validation accuracy:		81.52 %
Epoch 1447 of 2000 took 0.103s
  training loss:		0.553980
  validation loss:		0.558228
  validation accuracy:		81.09 %
Epoch 1448 of 2000 took 0.103s
  training loss:		0.548656
  validation loss:		0.556113
  validation accuracy:		81.20 %
Epoch 1449 of 2000 took 0.103s
  training loss:		0.549814
  validation loss:		0.560972
  validation accuracy:		81.52 %
Epoch 1450 of 2000 took 0.103s
  training loss:		0.549027
  validation loss:		0.568430
  validation accuracy:		82.28 %
Epoch 1451 of 2000 took 0.103s
  training loss:		0.528840
  validation loss:		0.554228
  validation accuracy:		81.96 %
Epoch 1452 of 2000 took 0.103s
  training loss:		0.559140
  validation loss:		0.558145
  validation accuracy:		81.41 %
Epoch 1453 of 2000 took 0.103s
  training loss:		0.555737
  validation loss:		0.558861
  validation accuracy:		80.98 %
Epoch 1454 of 2000 took 0.103s
  training loss:		0.550577
  validation loss:		0.575344
  validation accuracy:		81.63 %
Epoch 1455 of 2000 took 0.103s
  training loss:		0.553407
  validation loss:		0.575016
  validation accuracy:		81.09 %
Epoch 1456 of 2000 took 0.104s
  training loss:		0.546019
  validation loss:		0.565405
  validation accuracy:		81.96 %
Epoch 1457 of 2000 took 0.103s
  training loss:		0.549954
  validation loss:		0.553224
  validation accuracy:		82.07 %
Epoch 1458 of 2000 took 0.103s
  training loss:		0.552234
  validation loss:		0.561143
  validation accuracy:		81.30 %
Epoch 1459 of 2000 took 0.103s
  training loss:		0.556250
  validation loss:		0.565167
  validation accuracy:		81.20 %
Epoch 1460 of 2000 took 0.103s
  training loss:		0.555460
  validation loss:		0.558550
  validation accuracy:		82.07 %
Epoch 1461 of 2000 took 0.103s
  training loss:		0.551442
  validation loss:		0.566845
  validation accuracy:		81.41 %
Epoch 1462 of 2000 took 0.103s
  training loss:		0.557173
  validation loss:		0.560566
  validation accuracy:		81.63 %
Epoch 1463 of 2000 took 0.103s
  training loss:		0.545095
  validation loss:		0.569679
  validation accuracy:		81.41 %
Epoch 1464 of 2000 took 0.103s
  training loss:		0.552370
  validation loss:		0.585762
  validation accuracy:		80.65 %
Epoch 1465 of 2000 took 0.103s
  training loss:		0.557040
  validation loss:		0.563228
  validation accuracy:		81.85 %
Epoch 1466 of 2000 took 0.103s
  training loss:		0.552704
  validation loss:		0.558252
  validation accuracy:		81.20 %
Epoch 1467 of 2000 took 0.103s
  training loss:		0.562307
  validation loss:		0.602449
  validation accuracy:		80.33 %
Epoch 1468 of 2000 took 0.103s
  training loss:		0.543611
  validation loss:		0.558995
  validation accuracy:		80.98 %
Epoch 1469 of 2000 took 0.103s
  training loss:		0.549025
  validation loss:		0.564531
  validation accuracy:		81.74 %
Epoch 1470 of 2000 took 0.103s
  training loss:		0.547597
  validation loss:		0.553357
  validation accuracy:		81.30 %
Epoch 1471 of 2000 took 0.103s
  training loss:		0.552834
  validation loss:		0.581020
  validation accuracy:		81.63 %
Epoch 1472 of 2000 took 0.103s
  training loss:		0.553830
  validation loss:		0.589800
  validation accuracy:		80.87 %
Epoch 1473 of 2000 took 0.103s
  training loss:		0.543965
  validation loss:		0.556278
  validation accuracy:		81.20 %
Epoch 1474 of 2000 took 0.103s
  training loss:		0.552154
  validation loss:		0.574762
  validation accuracy:		81.52 %
Epoch 1475 of 2000 took 0.103s
  training loss:		0.549352
  validation loss:		0.563025
  validation accuracy:		81.74 %
Epoch 1476 of 2000 took 0.103s
  training loss:		0.546703
  validation loss:		0.558042
  validation accuracy:		81.41 %
Epoch 1477 of 2000 took 0.103s
  training loss:		0.549599
  validation loss:		0.586041
  validation accuracy:		81.20 %
Epoch 1478 of 2000 took 0.103s
  training loss:		0.558858
  validation loss:		0.557095
  validation accuracy:		81.63 %
Epoch 1479 of 2000 took 0.103s
  training loss:		0.542665
  validation loss:		0.564305
  validation accuracy:		81.20 %
Epoch 1480 of 2000 took 0.103s
  training loss:		0.556789
  validation loss:		0.560983
  validation accuracy:		81.63 %
Epoch 1481 of 2000 took 0.103s
  training loss:		0.551685
  validation loss:		0.576450
  validation accuracy:		81.85 %
Epoch 1482 of 2000 took 0.103s
  training loss:		0.550161
  validation loss:		0.555721
  validation accuracy:		82.07 %
Epoch 1483 of 2000 took 0.103s
  training loss:		0.550314
  validation loss:		0.554152
  validation accuracy:		81.74 %
Epoch 1484 of 2000 took 0.103s
  training loss:		0.556747
  validation loss:		0.602154
  validation accuracy:		80.33 %
Epoch 1485 of 2000 took 0.104s
  training loss:		0.545162
  validation loss:		0.568692
  validation accuracy:		81.41 %
Epoch 1486 of 2000 took 0.103s
  training loss:		0.545768
  validation loss:		0.576647
  validation accuracy:		81.74 %
Epoch 1487 of 2000 took 0.103s
  training loss:		0.547890
  validation loss:		0.573153
  validation accuracy:		82.07 %
Epoch 1488 of 2000 took 0.103s
  training loss:		0.540354
  validation loss:		0.556554
  validation accuracy:		81.63 %
Epoch 1489 of 2000 took 0.103s
  training loss:		0.549267
  validation loss:		0.580568
  validation accuracy:		81.52 %
Epoch 1490 of 2000 took 0.103s
  training loss:		0.543549
  validation loss:		0.595382
  validation accuracy:		80.87 %
Epoch 1491 of 2000 took 0.103s
  training loss:		0.547625
  validation loss:		0.585417
  validation accuracy:		80.87 %
Epoch 1492 of 2000 took 0.103s
  training loss:		0.554840
  validation loss:		0.556943
  validation accuracy:		81.63 %
Epoch 1493 of 2000 took 0.103s
  training loss:		0.552349
  validation loss:		0.572077
  validation accuracy:		82.17 %
Epoch 1494 of 2000 took 0.103s
  training loss:		0.547377
  validation loss:		0.559801
  validation accuracy:		81.20 %
Epoch 1495 of 2000 took 0.103s
  training loss:		0.558720
  validation loss:		0.557705
  validation accuracy:		81.63 %
Epoch 1496 of 2000 took 0.103s
  training loss:		0.541014
  validation loss:		0.553624
  validation accuracy:		82.17 %
Epoch 1497 of 2000 took 0.103s
  training loss:		0.545674
  validation loss:		0.557150
  validation accuracy:		81.30 %
Epoch 1498 of 2000 took 0.103s
  training loss:		0.555777
  validation loss:		0.591130
  validation accuracy:		81.09 %
Epoch 1499 of 2000 took 0.103s
  training loss:		0.547371
  validation loss:		0.568033
  validation accuracy:		81.74 %
Epoch 1500 of 2000 took 0.103s
  training loss:		0.554140
  validation loss:		0.555897
  validation accuracy:		81.52 %
Epoch 1501 of 2000 took 0.103s
  training loss:		0.545010
  validation loss:		0.574084
  validation accuracy:		81.63 %
Epoch 1502 of 2000 took 0.103s
  training loss:		0.552234
  validation loss:		0.552780
  validation accuracy:		81.74 %
Epoch 1503 of 2000 took 0.103s
  training loss:		0.550452
  validation loss:		0.554821
  validation accuracy:		81.74 %
Epoch 1504 of 2000 took 0.103s
  training loss:		0.556694
  validation loss:		0.577846
  validation accuracy:		81.30 %
Epoch 1505 of 2000 took 0.103s
  training loss:		0.553531
  validation loss:		0.584374
  validation accuracy:		80.98 %
Epoch 1506 of 2000 took 0.103s
  training loss:		0.546513
  validation loss:		0.581810
  validation accuracy:		81.30 %
Epoch 1507 of 2000 took 0.103s
  training loss:		0.552585
  validation loss:		0.561242
  validation accuracy:		81.30 %
Epoch 1508 of 2000 took 0.103s
  training loss:		0.545585
  validation loss:		0.558106
  validation accuracy:		81.30 %
Epoch 1509 of 2000 took 0.103s
  training loss:		0.550929
  validation loss:		0.579541
  validation accuracy:		81.09 %
Epoch 1510 of 2000 took 0.103s
  training loss:		0.546760
  validation loss:		0.565297
  validation accuracy:		81.63 %
Epoch 1511 of 2000 took 0.103s
  training loss:		0.556496
  validation loss:		0.561924
  validation accuracy:		81.30 %
Epoch 1512 of 2000 took 0.103s
  training loss:		0.546176
  validation loss:		0.553518
  validation accuracy:		81.74 %
Epoch 1513 of 2000 took 0.103s
  training loss:		0.551672
  validation loss:		0.559142
  validation accuracy:		82.07 %
Epoch 1514 of 2000 took 0.104s
  training loss:		0.543278
  validation loss:		0.552401
  validation accuracy:		81.85 %
Epoch 1515 of 2000 took 0.103s
  training loss:		0.565444
  validation loss:		0.549307
  validation accuracy:		81.85 %
Epoch 1516 of 2000 took 0.103s
  training loss:		0.550232
  validation loss:		0.556315
  validation accuracy:		81.41 %
Epoch 1517 of 2000 took 0.103s
  training loss:		0.553320
  validation loss:		0.563859
  validation accuracy:		81.74 %
Epoch 1518 of 2000 took 0.103s
  training loss:		0.546418
  validation loss:		0.567524
  validation accuracy:		81.09 %
Epoch 1519 of 2000 took 0.103s
  training loss:		0.547764
  validation loss:		0.553749
  validation accuracy:		81.74 %
Epoch 1520 of 2000 took 0.103s
  training loss:		0.554792
  validation loss:		0.603062
  validation accuracy:		80.33 %
Epoch 1521 of 2000 took 0.103s
  training loss:		0.542439
  validation loss:		0.558140
  validation accuracy:		81.09 %
Epoch 1522 of 2000 took 0.103s
  training loss:		0.553358
  validation loss:		0.569295
  validation accuracy:		81.30 %
Epoch 1523 of 2000 took 0.103s
  training loss:		0.546027
  validation loss:		0.567792
  validation accuracy:		81.85 %
Epoch 1524 of 2000 took 0.103s
  training loss:		0.548607
  validation loss:		0.560568
  validation accuracy:		81.85 %
Epoch 1525 of 2000 took 0.103s
  training loss:		0.545332
  validation loss:		0.559792
  validation accuracy:		81.41 %
Epoch 1526 of 2000 took 0.103s
  training loss:		0.530372
  validation loss:		0.583971
  validation accuracy:		81.20 %
Epoch 1527 of 2000 took 0.103s
  training loss:		0.546627
  validation loss:		0.579773
  validation accuracy:		81.30 %
Epoch 1528 of 2000 took 0.103s
  training loss:		0.543590
  validation loss:		0.570831
  validation accuracy:		81.41 %
Epoch 1529 of 2000 took 0.103s
  training loss:		0.549193
  validation loss:		0.562771
  validation accuracy:		81.85 %
Epoch 1530 of 2000 took 0.103s
  training loss:		0.552842
  validation loss:		0.560252
  validation accuracy:		81.20 %
Epoch 1531 of 2000 took 0.103s
  training loss:		0.543546
  validation loss:		0.579469
  validation accuracy:		81.52 %
Epoch 1532 of 2000 took 0.103s
  training loss:		0.555199
  validation loss:		0.594962
  validation accuracy:		80.76 %
Epoch 1533 of 2000 took 0.103s
  training loss:		0.545398
  validation loss:		0.564787
  validation accuracy:		81.74 %
Epoch 1534 of 2000 took 0.103s
  training loss:		0.552682
  validation loss:		0.559982
  validation accuracy:		81.20 %
Epoch 1535 of 2000 took 0.103s
  training loss:		0.547273
  validation loss:		0.572521
  validation accuracy:		81.20 %
Epoch 1536 of 2000 took 0.103s
  training loss:		0.544397
  validation loss:		0.569157
  validation accuracy:		81.30 %
Epoch 1537 of 2000 took 0.103s
  training loss:		0.548361
  validation loss:		0.559554
  validation accuracy:		81.52 %
Epoch 1538 of 2000 took 0.103s
  training loss:		0.548821
  validation loss:		0.621955
  validation accuracy:		79.89 %
Epoch 1539 of 2000 took 0.103s
  training loss:		0.540530
  validation loss:		0.562299
  validation accuracy:		80.87 %
Epoch 1540 of 2000 took 0.103s
  training loss:		0.550886
  validation loss:		0.558787
  validation accuracy:		81.30 %
Epoch 1541 of 2000 took 0.103s
  training loss:		0.560954
  validation loss:		0.564140
  validation accuracy:		81.63 %
Epoch 1542 of 2000 took 0.103s
  training loss:		0.555092
  validation loss:		0.567311
  validation accuracy:		81.63 %
Epoch 1543 of 2000 took 0.104s
  training loss:		0.550985
  validation loss:		0.555261
  validation accuracy:		81.63 %
Epoch 1544 of 2000 took 0.103s
  training loss:		0.556711
  validation loss:		0.561334
  validation accuracy:		81.52 %
Epoch 1545 of 2000 took 0.103s
  training loss:		0.550668
  validation loss:		0.562305
  validation accuracy:		81.74 %
Epoch 1546 of 2000 took 0.103s
  training loss:		0.551182
  validation loss:		0.566718
  validation accuracy:		81.63 %
Epoch 1547 of 2000 took 0.103s
  training loss:		0.550357
  validation loss:		0.562817
  validation accuracy:		82.17 %
Epoch 1548 of 2000 took 0.103s
  training loss:		0.551825
  validation loss:		0.575198
  validation accuracy:		81.30 %
Epoch 1549 of 2000 took 0.103s
  training loss:		0.540889
  validation loss:		0.560191
  validation accuracy:		80.65 %
Epoch 1550 of 2000 took 0.103s
  training loss:		0.542106
  validation loss:		0.585015
  validation accuracy:		80.65 %
Epoch 1551 of 2000 took 0.103s
  training loss:		0.543895
  validation loss:		0.568572
  validation accuracy:		81.30 %
Epoch 1552 of 2000 took 0.103s
  training loss:		0.550238
  validation loss:		0.568004
  validation accuracy:		81.30 %
Epoch 1553 of 2000 took 0.103s
  training loss:		0.547697
  validation loss:		0.566576
  validation accuracy:		81.63 %
Epoch 1554 of 2000 took 0.103s
  training loss:		0.543180
  validation loss:		0.560288
  validation accuracy:		81.63 %
Epoch 1555 of 2000 took 0.103s
  training loss:		0.548868
  validation loss:		0.556723
  validation accuracy:		81.20 %
Epoch 1556 of 2000 took 0.103s
  training loss:		0.546091
  validation loss:		0.566989
  validation accuracy:		81.20 %
Epoch 1557 of 2000 took 0.103s
  training loss:		0.550136
  validation loss:		0.565262
  validation accuracy:		81.20 %
Epoch 1558 of 2000 took 0.103s
  training loss:		0.546010
  validation loss:		0.579730
  validation accuracy:		81.20 %
Epoch 1559 of 2000 took 0.103s
  training loss:		0.544121
  validation loss:		0.566416
  validation accuracy:		80.98 %
Epoch 1560 of 2000 took 0.103s
  training loss:		0.546436
  validation loss:		0.558417
  validation accuracy:		81.52 %
Epoch 1561 of 2000 took 0.103s
  training loss:		0.544500
  validation loss:		0.556029
  validation accuracy:		81.41 %
Epoch 1562 of 2000 took 0.103s
  training loss:		0.543049
  validation loss:		0.559261
  validation accuracy:		80.98 %
Epoch 1563 of 2000 took 0.103s
  training loss:		0.554815
  validation loss:		0.558265
  validation accuracy:		81.30 %
Epoch 1564 of 2000 took 0.103s
  training loss:		0.547646
  validation loss:		0.564723
  validation accuracy:		81.41 %
Epoch 1565 of 2000 took 0.103s
  training loss:		0.550824
  validation loss:		0.566984
  validation accuracy:		81.20 %
Epoch 1566 of 2000 took 0.103s
  training loss:		0.543870
  validation loss:		0.557321
  validation accuracy:		81.30 %
Epoch 1567 of 2000 took 0.103s
  training loss:		0.549019
  validation loss:		0.553931
  validation accuracy:		81.09 %
Epoch 1568 of 2000 took 0.103s
  training loss:		0.554543
  validation loss:		0.574401
  validation accuracy:		81.41 %
Epoch 1569 of 2000 took 0.103s
  training loss:		0.548717
  validation loss:		0.563362
  validation accuracy:		81.20 %
Epoch 1570 of 2000 took 0.103s
  training loss:		0.549814
  validation loss:		0.571231
  validation accuracy:		81.20 %
Epoch 1571 of 2000 took 0.103s
  training loss:		0.550934
  validation loss:		0.552164
  validation accuracy:		81.63 %
Epoch 1572 of 2000 took 0.103s
  training loss:		0.549520
  validation loss:		0.563191
  validation accuracy:		81.52 %
Epoch 1573 of 2000 took 0.104s
  training loss:		0.550181
  validation loss:		0.586997
  validation accuracy:		81.09 %
Epoch 1574 of 2000 took 0.103s
  training loss:		0.551638
  validation loss:		0.555556
  validation accuracy:		81.52 %
Epoch 1575 of 2000 took 0.103s
  training loss:		0.553658
  validation loss:		0.560476
  validation accuracy:		81.63 %
Epoch 1576 of 2000 took 0.103s
  training loss:		0.546818
  validation loss:		0.578757
  validation accuracy:		81.74 %
Epoch 1577 of 2000 took 0.103s
  training loss:		0.552695
  validation loss:		0.563311
  validation accuracy:		81.63 %
Epoch 1578 of 2000 took 0.103s
  training loss:		0.548324
  validation loss:		0.559915
  validation accuracy:		81.52 %
Epoch 1579 of 2000 took 0.103s
  training loss:		0.552242
  validation loss:		0.570967
  validation accuracy:		81.63 %
Epoch 1580 of 2000 took 0.103s
  training loss:		0.548747
  validation loss:		0.585340
  validation accuracy:		81.41 %
Epoch 1581 of 2000 took 0.103s
  training loss:		0.554735
  validation loss:		0.590708
  validation accuracy:		81.41 %
Epoch 1582 of 2000 took 0.103s
  training loss:		0.532811
  validation loss:		0.565046
  validation accuracy:		81.20 %
Epoch 1583 of 2000 took 0.103s
  training loss:		0.552389
  validation loss:		0.564434
  validation accuracy:		81.96 %
Epoch 1584 of 2000 took 0.103s
  training loss:		0.541633
  validation loss:		0.560301
  validation accuracy:		81.41 %
Epoch 1585 of 2000 took 0.103s
  training loss:		0.550751
  validation loss:		0.558116
  validation accuracy:		81.41 %
Epoch 1586 of 2000 took 0.103s
  training loss:		0.546426
  validation loss:		0.551967
  validation accuracy:		81.74 %
Epoch 1587 of 2000 took 0.103s
  training loss:		0.554132
  validation loss:		0.558247
  validation accuracy:		81.30 %
Epoch 1588 of 2000 took 0.103s
  training loss:		0.557073
  validation loss:		0.559610
  validation accuracy:		81.20 %
Epoch 1589 of 2000 took 0.103s
  training loss:		0.552594
  validation loss:		0.572416
  validation accuracy:		81.09 %
Epoch 1590 of 2000 took 0.103s
  training loss:		0.556218
  validation loss:		0.563554
  validation accuracy:		81.30 %
Epoch 1591 of 2000 took 0.103s
  training loss:		0.547512
  validation loss:		0.581374
  validation accuracy:		81.09 %
Epoch 1592 of 2000 took 0.103s
  training loss:		0.550210
  validation loss:		0.572623
  validation accuracy:		80.87 %
Epoch 1593 of 2000 took 0.103s
  training loss:		0.554085
  validation loss:		0.555794
  validation accuracy:		81.30 %
Epoch 1594 of 2000 took 0.103s
  training loss:		0.549451
  validation loss:		0.568088
  validation accuracy:		81.85 %
Epoch 1595 of 2000 took 0.103s
  training loss:		0.552809
  validation loss:		0.561412
  validation accuracy:		81.74 %
Epoch 1596 of 2000 took 0.103s
  training loss:		0.553662
  validation loss:		0.558864
  validation accuracy:		81.74 %
Epoch 1597 of 2000 took 0.103s
  training loss:		0.556447
  validation loss:		0.581892
  validation accuracy:		81.20 %
Epoch 1598 of 2000 took 0.103s
  training loss:		0.554657
  validation loss:		0.555372
  validation accuracy:		81.30 %
Epoch 1599 of 2000 took 0.103s
  training loss:		0.549758
  validation loss:		0.560371
  validation accuracy:		81.74 %
Epoch 1600 of 2000 took 0.103s
  training loss:		0.550152
  validation loss:		0.569800
  validation accuracy:		81.30 %
Epoch 1601 of 2000 took 0.103s
  training loss:		0.544775
  validation loss:		0.552039
  validation accuracy:		81.74 %
Epoch 1602 of 2000 took 0.104s
  training loss:		0.545428
  validation loss:		0.557580
  validation accuracy:		81.85 %
Epoch 1603 of 2000 took 0.103s
  training loss:		0.554332
  validation loss:		0.555169
  validation accuracy:		82.17 %
Epoch 1604 of 2000 took 0.103s
  training loss:		0.551991
  validation loss:		0.569769
  validation accuracy:		81.41 %
Epoch 1605 of 2000 took 0.103s
  training loss:		0.552981
  validation loss:		0.560259
  validation accuracy:		81.30 %
Epoch 1606 of 2000 took 0.103s
  training loss:		0.550062
  validation loss:		0.555898
  validation accuracy:		81.74 %
Epoch 1607 of 2000 took 0.103s
  training loss:		0.542948
  validation loss:		0.575775
  validation accuracy:		81.09 %
Epoch 1608 of 2000 took 0.103s
  training loss:		0.550301
  validation loss:		0.571918
  validation accuracy:		81.85 %
Epoch 1609 of 2000 took 0.103s
  training loss:		0.549671
  validation loss:		0.553076
  validation accuracy:		81.30 %
Epoch 1610 of 2000 took 0.103s
  training loss:		0.543705
  validation loss:		0.558127
  validation accuracy:		81.41 %
Epoch 1611 of 2000 took 0.103s
  training loss:		0.553317
  validation loss:		0.591723
  validation accuracy:		80.76 %
Epoch 1612 of 2000 took 0.103s
  training loss:		0.556012
  validation loss:		0.582141
  validation accuracy:		81.30 %
Epoch 1613 of 2000 took 0.103s
  training loss:		0.533960
  validation loss:		0.563786
  validation accuracy:		81.20 %
Epoch 1614 of 2000 took 0.103s
  training loss:		0.544733
  validation loss:		0.552976
  validation accuracy:		81.63 %
Epoch 1615 of 2000 took 0.103s
  training loss:		0.550270
  validation loss:		0.567538
  validation accuracy:		81.41 %
Epoch 1616 of 2000 took 0.103s
  training loss:		0.552122
  validation loss:		0.577261
  validation accuracy:		81.30 %
Epoch 1617 of 2000 took 0.103s
  training loss:		0.552105
  validation loss:		0.564162
  validation accuracy:		82.07 %
Epoch 1618 of 2000 took 0.103s
  training loss:		0.544826
  validation loss:		0.604994
  validation accuracy:		80.43 %
Epoch 1619 of 2000 took 0.103s
  training loss:		0.555589
  validation loss:		0.564360
  validation accuracy:		81.20 %
Epoch 1620 of 2000 took 0.103s
  training loss:		0.545025
  validation loss:		0.550954
  validation accuracy:		81.85 %
Epoch 1621 of 2000 took 0.103s
  training loss:		0.556831
  validation loss:		0.559128
  validation accuracy:		81.41 %
Epoch 1622 of 2000 took 0.103s
  training loss:		0.543101
  validation loss:		0.574060
  validation accuracy:		81.20 %
Epoch 1623 of 2000 took 0.103s
  training loss:		0.537128
  validation loss:		0.588273
  validation accuracy:		80.87 %
Epoch 1624 of 2000 took 0.103s
  training loss:		0.549490
  validation loss:		0.569028
  validation accuracy:		81.41 %
Epoch 1625 of 2000 took 0.103s
  training loss:		0.557473
  validation loss:		0.597834
  validation accuracy:		80.76 %
Epoch 1626 of 2000 took 0.103s
  training loss:		0.547269
  validation loss:		0.555279
  validation accuracy:		81.96 %
Epoch 1627 of 2000 took 0.103s
  training loss:		0.544324
  validation loss:		0.581705
  validation accuracy:		81.41 %
Epoch 1628 of 2000 took 0.103s
  training loss:		0.545672
  validation loss:		0.563072
  validation accuracy:		81.41 %
Epoch 1629 of 2000 took 0.103s
  training loss:		0.540922
  validation loss:		0.572789
  validation accuracy:		81.52 %
Epoch 1630 of 2000 took 0.103s
  training loss:		0.538922
  validation loss:		0.565908
  validation accuracy:		81.85 %
Epoch 1631 of 2000 took 0.104s
  training loss:		0.539742
  validation loss:		0.560546
  validation accuracy:		81.09 %
Epoch 1632 of 2000 took 0.103s
  training loss:		0.547205
  validation loss:		0.564492
  validation accuracy:		81.41 %
Epoch 1633 of 2000 took 0.103s
  training loss:		0.552353
  validation loss:		0.558369
  validation accuracy:		81.85 %
Epoch 1634 of 2000 took 0.103s
  training loss:		0.547290
  validation loss:		0.566726
  validation accuracy:		81.74 %
Epoch 1635 of 2000 took 0.103s
  training loss:		0.540513
  validation loss:		0.571271
  validation accuracy:		81.20 %
Epoch 1636 of 2000 took 0.103s
  training loss:		0.546774
  validation loss:		0.556900
  validation accuracy:		81.52 %
Epoch 1637 of 2000 took 0.103s
  training loss:		0.548603
  validation loss:		0.558191
  validation accuracy:		81.52 %
Epoch 1638 of 2000 took 0.103s
  training loss:		0.553462
  validation loss:		0.568845
  validation accuracy:		81.20 %
Epoch 1639 of 2000 took 0.103s
  training loss:		0.541028
  validation loss:		0.559369
  validation accuracy:		81.85 %
Epoch 1640 of 2000 took 0.103s
  training loss:		0.551972
  validation loss:		0.561852
  validation accuracy:		81.30 %
Epoch 1641 of 2000 took 0.103s
  training loss:		0.544664
  validation loss:		0.573088
  validation accuracy:		81.41 %
Epoch 1642 of 2000 took 0.103s
  training loss:		0.547259
  validation loss:		0.589224
  validation accuracy:		80.87 %
Epoch 1643 of 2000 took 0.103s
  training loss:		0.553027
  validation loss:		0.558568
  validation accuracy:		81.09 %
Epoch 1644 of 2000 took 0.103s
  training loss:		0.542622
  validation loss:		0.559566
  validation accuracy:		81.85 %
Epoch 1645 of 2000 took 0.103s
  training loss:		0.547093
  validation loss:		0.571679
  validation accuracy:		81.30 %
Epoch 1646 of 2000 took 0.103s
  training loss:		0.540421
  validation loss:		0.557567
  validation accuracy:		81.52 %
Epoch 1647 of 2000 took 0.103s
  training loss:		0.538408
  validation loss:		0.559045
  validation accuracy:		81.52 %
Epoch 1648 of 2000 took 0.103s
  training loss:		0.555929
  validation loss:		0.561123
  validation accuracy:		81.52 %
Epoch 1649 of 2000 took 0.103s
  training loss:		0.552490
  validation loss:		0.553826
  validation accuracy:		82.07 %
Epoch 1650 of 2000 took 0.103s
  training loss:		0.549168
  validation loss:		0.580506
  validation accuracy:		81.63 %
Epoch 1651 of 2000 took 0.103s
  training loss:		0.549131
  validation loss:		0.564186
  validation accuracy:		81.74 %
Epoch 1652 of 2000 took 0.103s
  training loss:		0.542809
  validation loss:		0.569892
  validation accuracy:		81.52 %
Epoch 1653 of 2000 took 0.103s
  training loss:		0.554819
  validation loss:		0.555292
  validation accuracy:		82.17 %
Epoch 1654 of 2000 took 0.103s
  training loss:		0.545719
  validation loss:		0.562810
  validation accuracy:		80.87 %
Epoch 1655 of 2000 took 0.103s
  training loss:		0.550667
  validation loss:		0.562475
  validation accuracy:		81.20 %
Epoch 1656 of 2000 took 0.103s
  training loss:		0.548777
  validation loss:		0.557360
  validation accuracy:		81.85 %
Epoch 1657 of 2000 took 0.103s
  training loss:		0.545062
  validation loss:		0.566255
  validation accuracy:		81.30 %
Epoch 1658 of 2000 took 0.103s
  training loss:		0.547061
  validation loss:		0.572611
  validation accuracy:		81.52 %
Epoch 1659 of 2000 took 0.103s
  training loss:		0.535674
  validation loss:		0.557853
  validation accuracy:		82.17 %
Epoch 1660 of 2000 took 0.104s
  training loss:		0.548715
  validation loss:		0.575389
  validation accuracy:		81.52 %
Epoch 1661 of 2000 took 0.103s
  training loss:		0.551256
  validation loss:		0.560376
  validation accuracy:		81.09 %
Epoch 1662 of 2000 took 0.103s
  training loss:		0.544343
  validation loss:		0.570697
  validation accuracy:		81.41 %
Epoch 1663 of 2000 took 0.103s
  training loss:		0.543657
  validation loss:		0.570189
  validation accuracy:		81.41 %
Epoch 1664 of 2000 took 0.103s
  training loss:		0.539588
  validation loss:		0.565277
  validation accuracy:		81.30 %
Epoch 1665 of 2000 took 0.103s
  training loss:		0.552723
  validation loss:		0.588846
  validation accuracy:		81.30 %
Epoch 1666 of 2000 took 0.103s
  training loss:		0.553584
  validation loss:		0.553723
  validation accuracy:		81.63 %
Epoch 1667 of 2000 took 0.103s
  training loss:		0.545730
  validation loss:		0.552959
  validation accuracy:		82.07 %
Epoch 1668 of 2000 took 0.103s
  training loss:		0.543744
  validation loss:		0.564650
  validation accuracy:		80.87 %
Epoch 1669 of 2000 took 0.103s
  training loss:		0.550125
  validation loss:		0.557980
  validation accuracy:		81.20 %
Epoch 1670 of 2000 took 0.103s
  training loss:		0.549302
  validation loss:		0.566150
  validation accuracy:		81.52 %
Epoch 1671 of 2000 took 0.103s
  training loss:		0.536122
  validation loss:		0.575418
  validation accuracy:		81.96 %
Epoch 1672 of 2000 took 0.103s
  training loss:		0.557691
  validation loss:		0.554580
  validation accuracy:		81.52 %
Epoch 1673 of 2000 took 0.105s
  training loss:		0.555326
  validation loss:		0.563495
  validation accuracy:		81.85 %
Epoch 1674 of 2000 took 0.103s
  training loss:		0.547467
  validation loss:		0.559640
  validation accuracy:		81.30 %
Epoch 1675 of 2000 took 0.103s
  training loss:		0.555079
  validation loss:		0.576992
  validation accuracy:		81.30 %
Epoch 1676 of 2000 took 0.103s
  training loss:		0.546547
  validation loss:		0.566510
  validation accuracy:		82.07 %
Epoch 1677 of 2000 took 0.103s
  training loss:		0.559922
  validation loss:		0.558141
  validation accuracy:		81.30 %
Epoch 1678 of 2000 took 0.103s
  training loss:		0.539826
  validation loss:		0.558252
  validation accuracy:		81.74 %
Epoch 1679 of 2000 took 0.103s
  training loss:		0.554700
  validation loss:		0.564367
  validation accuracy:		81.41 %
Epoch 1680 of 2000 took 0.103s
  training loss:		0.550850
  validation loss:		0.558206
  validation accuracy:		81.20 %
Epoch 1681 of 2000 took 0.103s
  training loss:		0.549448
  validation loss:		0.558649
  validation accuracy:		81.52 %
Epoch 1682 of 2000 took 0.103s
  training loss:		0.547235
  validation loss:		0.582132
  validation accuracy:		81.63 %
Epoch 1683 of 2000 took 0.103s
  training loss:		0.553065
  validation loss:		0.552439
  validation accuracy:		81.96 %
Epoch 1684 of 2000 took 0.103s
  training loss:		0.554683
  validation loss:		0.563863
  validation accuracy:		81.30 %
Epoch 1685 of 2000 took 0.103s
  training loss:		0.540953
  validation loss:		0.579046
  validation accuracy:		81.41 %
Epoch 1686 of 2000 took 0.103s
  training loss:		0.546871
  validation loss:		0.604764
  validation accuracy:		80.43 %
Epoch 1687 of 2000 took 0.103s
  training loss:		0.555716
  validation loss:		0.573401
  validation accuracy:		81.74 %
Epoch 1688 of 2000 took 0.103s
  training loss:		0.542587
  validation loss:		0.556902
  validation accuracy:		80.87 %
Epoch 1689 of 2000 took 0.104s
  training loss:		0.548687
  validation loss:		0.572513
  validation accuracy:		81.52 %
Epoch 1690 of 2000 took 0.103s
  training loss:		0.551607
  validation loss:		0.559488
  validation accuracy:		81.30 %
Epoch 1691 of 2000 took 0.103s
  training loss:		0.549282
  validation loss:		0.557858
  validation accuracy:		81.96 %
Epoch 1692 of 2000 took 0.103s
  training loss:		0.550692
  validation loss:		0.560690
  validation accuracy:		81.74 %
Epoch 1693 of 2000 took 0.103s
  training loss:		0.553853
  validation loss:		0.557321
  validation accuracy:		81.41 %
Epoch 1694 of 2000 took 0.103s
  training loss:		0.553343
  validation loss:		0.551143
  validation accuracy:		82.07 %
Epoch 1695 of 2000 took 0.103s
  training loss:		0.550784
  validation loss:		0.571938
  validation accuracy:		81.96 %
Epoch 1696 of 2000 took 0.103s
  training loss:		0.552105
  validation loss:		0.567415
  validation accuracy:		81.41 %
Epoch 1697 of 2000 took 0.103s
  training loss:		0.555762
  validation loss:		0.560084
  validation accuracy:		81.41 %
Epoch 1698 of 2000 took 0.103s
  training loss:		0.555932
  validation loss:		0.575003
  validation accuracy:		81.09 %
Epoch 1699 of 2000 took 0.103s
  training loss:		0.548878
  validation loss:		0.565797
  validation accuracy:		81.96 %
Epoch 1700 of 2000 took 0.103s
  training loss:		0.542272
  validation loss:		0.583574
  validation accuracy:		81.20 %
Epoch 1701 of 2000 took 0.103s
  training loss:		0.553533
  validation loss:		0.566136
  validation accuracy:		81.20 %
Epoch 1702 of 2000 took 0.103s
  training loss:		0.549318
  validation loss:		0.555719
  validation accuracy:		81.41 %
Epoch 1703 of 2000 took 0.103s
  training loss:		0.553412
  validation loss:		0.553347
  validation accuracy:		81.52 %
Epoch 1704 of 2000 took 0.103s
  training loss:		0.547857
  validation loss:		0.559345
  validation accuracy:		81.63 %
Epoch 1705 of 2000 took 0.103s
  training loss:		0.552977
  validation loss:		0.570367
  validation accuracy:		81.30 %
Epoch 1706 of 2000 took 0.103s
  training loss:		0.548218
  validation loss:		0.576718
  validation accuracy:		81.63 %
Epoch 1707 of 2000 took 0.104s
  training loss:		0.551469
  validation loss:		0.576198
  validation accuracy:		81.63 %
Epoch 1708 of 2000 took 0.103s
  training loss:		0.551280
  validation loss:		0.558451
  validation accuracy:		81.52 %
Epoch 1709 of 2000 took 0.103s
  training loss:		0.542575
  validation loss:		0.558390
  validation accuracy:		81.20 %
Epoch 1710 of 2000 took 0.103s
  training loss:		0.554095
  validation loss:		0.553215
  validation accuracy:		81.20 %
Epoch 1711 of 2000 took 0.103s
  training loss:		0.539531
  validation loss:		0.560677
  validation accuracy:		81.74 %
Epoch 1712 of 2000 took 0.103s
  training loss:		0.545134
  validation loss:		0.566421
  validation accuracy:		81.09 %
Epoch 1713 of 2000 took 0.103s
  training loss:		0.551409
  validation loss:		0.552789
  validation accuracy:		81.41 %
Epoch 1714 of 2000 took 0.103s
  training loss:		0.552213
  validation loss:		0.553604
  validation accuracy:		81.52 %
Epoch 1715 of 2000 took 0.103s
  training loss:		0.543229
  validation loss:		0.573067
  validation accuracy:		81.41 %
Epoch 1716 of 2000 took 0.103s
  training loss:		0.548569
  validation loss:		0.573594
  validation accuracy:		81.20 %
Epoch 1717 of 2000 took 0.103s
  training loss:		0.551570
  validation loss:		0.550930
  validation accuracy:		81.63 %
Epoch 1718 of 2000 took 0.103s
  training loss:		0.558264
  validation loss:		0.571227
  validation accuracy:		80.98 %
Epoch 1719 of 2000 took 0.104s
  training loss:		0.546982
  validation loss:		0.581000
  validation accuracy:		80.76 %
Epoch 1720 of 2000 took 0.103s
  training loss:		0.560155
  validation loss:		0.564365
  validation accuracy:		80.98 %
Epoch 1721 of 2000 took 0.103s
  training loss:		0.555207
  validation loss:		0.552910
  validation accuracy:		81.63 %
Epoch 1722 of 2000 took 0.103s
  training loss:		0.552776
  validation loss:		0.551802
  validation accuracy:		82.07 %
Epoch 1723 of 2000 took 0.103s
  training loss:		0.548059
  validation loss:		0.572117
  validation accuracy:		81.30 %
Epoch 1724 of 2000 took 0.103s
  training loss:		0.554983
  validation loss:		0.583959
  validation accuracy:		81.20 %
Epoch 1725 of 2000 took 0.103s
  training loss:		0.552908
  validation loss:		0.565937
  validation accuracy:		81.52 %
Epoch 1726 of 2000 took 0.103s
  training loss:		0.545362
  validation loss:		0.554594
  validation accuracy:		81.41 %
Epoch 1727 of 2000 took 0.103s
  training loss:		0.548227
  validation loss:		0.612732
  validation accuracy:		79.67 %
Epoch 1728 of 2000 took 0.103s
  training loss:		0.544370
  validation loss:		0.553876
  validation accuracy:		81.41 %
Epoch 1729 of 2000 took 0.103s
  training loss:		0.548782
  validation loss:		0.578963
  validation accuracy:		81.30 %
Epoch 1730 of 2000 took 0.103s
  training loss:		0.550705
  validation loss:		0.555903
  validation accuracy:		81.52 %
Epoch 1731 of 2000 took 0.103s
  training loss:		0.543916
  validation loss:		0.562980
  validation accuracy:		81.41 %
Epoch 1732 of 2000 took 0.103s
  training loss:		0.545374
  validation loss:		0.558913
  validation accuracy:		81.63 %
Epoch 1733 of 2000 took 0.103s
  training loss:		0.556507
  validation loss:		0.560578
  validation accuracy:		81.09 %
Epoch 1734 of 2000 took 0.103s
  training loss:		0.549904
  validation loss:		0.562727
  validation accuracy:		80.98 %
Epoch 1735 of 2000 took 0.103s
  training loss:		0.552729
  validation loss:		0.568555
  validation accuracy:		81.74 %
Epoch 1736 of 2000 took 0.103s
  training loss:		0.551300
  validation loss:		0.575127
  validation accuracy:		81.74 %
Epoch 1737 of 2000 took 0.103s
  training loss:		0.554817
  validation loss:		0.563584
  validation accuracy:		81.09 %
Epoch 1738 of 2000 took 0.104s
  training loss:		0.549820
  validation loss:		0.558048
  validation accuracy:		81.96 %
Epoch 1739 of 2000 took 0.103s
  training loss:		0.545384
  validation loss:		0.551147
  validation accuracy:		81.74 %
Epoch 1740 of 2000 took 0.103s
  training loss:		0.542064
  validation loss:		0.557422
  validation accuracy:		81.30 %
Epoch 1741 of 2000 took 0.103s
  training loss:		0.539918
  validation loss:		0.568739
  validation accuracy:		82.07 %
Epoch 1742 of 2000 took 0.103s
  training loss:		0.552763
  validation loss:		0.555115
  validation accuracy:		81.63 %
Epoch 1743 of 2000 took 0.103s
  training loss:		0.541492
  validation loss:		0.559778
  validation accuracy:		81.85 %
Epoch 1744 of 2000 took 0.103s
  training loss:		0.552468
  validation loss:		0.554658
  validation accuracy:		81.30 %
Epoch 1745 of 2000 took 0.103s
  training loss:		0.544972
  validation loss:		0.561190
  validation accuracy:		81.20 %
Epoch 1746 of 2000 took 0.103s
  training loss:		0.543166
  validation loss:		0.570774
  validation accuracy:		81.41 %
Epoch 1747 of 2000 took 0.103s
  training loss:		0.551153
  validation loss:		0.569120
  validation accuracy:		81.20 %
Epoch 1748 of 2000 took 0.104s
  training loss:		0.547932
  validation loss:		0.559270
  validation accuracy:		81.30 %
Epoch 1749 of 2000 took 0.103s
  training loss:		0.542628
  validation loss:		0.555512
  validation accuracy:		81.30 %
Epoch 1750 of 2000 took 0.103s
  training loss:		0.549347
  validation loss:		0.556352
  validation accuracy:		82.07 %
Epoch 1751 of 2000 took 0.103s
  training loss:		0.542541
  validation loss:		0.560761
  validation accuracy:		81.30 %
Epoch 1752 of 2000 took 0.103s
  training loss:		0.545797
  validation loss:		0.554258
  validation accuracy:		81.20 %
Epoch 1753 of 2000 took 0.103s
  training loss:		0.543863
  validation loss:		0.554710
  validation accuracy:		81.20 %
Epoch 1754 of 2000 took 0.103s
  training loss:		0.549447
  validation loss:		0.565208
  validation accuracy:		81.74 %
Epoch 1755 of 2000 took 0.103s
  training loss:		0.550883
  validation loss:		0.587023
  validation accuracy:		81.41 %
Epoch 1756 of 2000 took 0.103s
  training loss:		0.546712
  validation loss:		0.555656
  validation accuracy:		81.30 %
Epoch 1757 of 2000 took 0.103s
  training loss:		0.545988
  validation loss:		0.570187
  validation accuracy:		81.63 %
Epoch 1758 of 2000 took 0.103s
  training loss:		0.547746
  validation loss:		0.569095
  validation accuracy:		81.63 %
Epoch 1759 of 2000 took 0.103s
  training loss:		0.539340
  validation loss:		0.556170
  validation accuracy:		81.52 %
Epoch 1760 of 2000 took 0.103s
  training loss:		0.548890
  validation loss:		0.562558
  validation accuracy:		81.09 %
Epoch 1761 of 2000 took 0.103s
  training loss:		0.547199
  validation loss:		0.572270
  validation accuracy:		81.52 %
Epoch 1762 of 2000 took 0.103s
  training loss:		0.541254
  validation loss:		0.558397
  validation accuracy:		81.41 %
Epoch 1763 of 2000 took 0.103s
  training loss:		0.544213
  validation loss:		0.562130
  validation accuracy:		81.63 %
Epoch 1764 of 2000 took 0.103s
  training loss:		0.545179
  validation loss:		0.560952
  validation accuracy:		80.98 %
Epoch 1765 of 2000 took 0.103s
  training loss:		0.546103
  validation loss:		0.554679
  validation accuracy:		81.52 %
Epoch 1766 of 2000 took 0.103s
  training loss:		0.548117
  validation loss:		0.573121
  validation accuracy:		81.74 %
Epoch 1767 of 2000 took 0.103s
  training loss:		0.547136
  validation loss:		0.566817
  validation accuracy:		81.63 %
Epoch 1768 of 2000 took 0.103s
  training loss:		0.550258
  validation loss:		0.580851
  validation accuracy:		81.52 %
Epoch 1769 of 2000 took 0.103s
  training loss:		0.553015
  validation loss:		0.553863
  validation accuracy:		81.85 %
Epoch 1770 of 2000 took 0.103s
  training loss:		0.547015
  validation loss:		0.568902
  validation accuracy:		81.30 %
Epoch 1771 of 2000 took 0.103s
  training loss:		0.542350
  validation loss:		0.569058
  validation accuracy:		81.85 %
Epoch 1772 of 2000 took 0.103s
  training loss:		0.544494
  validation loss:		0.562484
  validation accuracy:		81.85 %
Epoch 1773 of 2000 took 0.103s
  training loss:		0.541711
  validation loss:		0.584751
  validation accuracy:		81.09 %
Epoch 1774 of 2000 took 0.103s
  training loss:		0.557850
  validation loss:		0.563327
  validation accuracy:		81.63 %
Epoch 1775 of 2000 took 0.103s
  training loss:		0.551508
  validation loss:		0.576889
  validation accuracy:		80.87 %
Epoch 1776 of 2000 took 0.103s
  training loss:		0.542480
  validation loss:		0.567425
  validation accuracy:		81.20 %
Epoch 1777 of 2000 took 0.104s
  training loss:		0.545521
  validation loss:		0.559948
  validation accuracy:		80.76 %
Epoch 1778 of 2000 took 0.103s
  training loss:		0.540179
  validation loss:		0.585531
  validation accuracy:		80.87 %
Epoch 1779 of 2000 took 0.103s
  training loss:		0.547176
  validation loss:		0.557583
  validation accuracy:		81.52 %
Epoch 1780 of 2000 took 0.103s
  training loss:		0.558775
  validation loss:		0.559588
  validation accuracy:		81.52 %
Epoch 1781 of 2000 took 0.103s
  training loss:		0.549378
  validation loss:		0.553982
  validation accuracy:		81.74 %
Epoch 1782 of 2000 took 0.103s
  training loss:		0.543349
  validation loss:		0.569241
  validation accuracy:		81.09 %
Epoch 1783 of 2000 took 0.103s
  training loss:		0.554849
  validation loss:		0.569161
  validation accuracy:		81.96 %
Epoch 1784 of 2000 took 0.103s
  training loss:		0.556272
  validation loss:		0.558346
  validation accuracy:		81.63 %
Epoch 1785 of 2000 took 0.103s
  training loss:		0.553197
  validation loss:		0.565801
  validation accuracy:		81.20 %
Epoch 1786 of 2000 took 0.103s
  training loss:		0.540639
  validation loss:		0.558094
  validation accuracy:		81.41 %
Epoch 1787 of 2000 took 0.103s
  training loss:		0.543115
  validation loss:		0.564267
  validation accuracy:		81.63 %
Epoch 1788 of 2000 took 0.103s
  training loss:		0.556799
  validation loss:		0.584713
  validation accuracy:		80.87 %
Epoch 1789 of 2000 took 0.103s
  training loss:		0.549994
  validation loss:		0.574440
  validation accuracy:		81.74 %
Epoch 1790 of 2000 took 0.103s
  training loss:		0.540444
  validation loss:		0.562901
  validation accuracy:		81.52 %
Epoch 1791 of 2000 took 0.103s
  training loss:		0.546208
  validation loss:		0.561290
  validation accuracy:		82.07 %
Epoch 1792 of 2000 took 0.103s
  training loss:		0.546963
  validation loss:		0.585170
  validation accuracy:		80.98 %
Epoch 1793 of 2000 took 0.103s
  training loss:		0.554782
  validation loss:		0.569751
  validation accuracy:		81.52 %
Epoch 1794 of 2000 took 0.103s
  training loss:		0.545955
  validation loss:		0.573234
  validation accuracy:		81.30 %
Epoch 1795 of 2000 took 0.103s
  training loss:		0.549746
  validation loss:		0.567417
  validation accuracy:		81.09 %
Epoch 1796 of 2000 took 0.103s
  training loss:		0.544248
  validation loss:		0.569143
  validation accuracy:		81.09 %
Epoch 1797 of 2000 took 0.103s
  training loss:		0.544699
  validation loss:		0.565157
  validation accuracy:		80.98 %
Epoch 1798 of 2000 took 0.103s
  training loss:		0.541072
  validation loss:		0.564039
  validation accuracy:		81.52 %
Epoch 1799 of 2000 took 0.103s
  training loss:		0.546005
  validation loss:		0.562206
  validation accuracy:		81.52 %
Epoch 1800 of 2000 took 0.103s
  training loss:		0.545597
  validation loss:		0.569869
  validation accuracy:		80.98 %
Epoch 1801 of 2000 took 0.103s
  training loss:		0.551915
  validation loss:		0.567238
  validation accuracy:		81.30 %
Epoch 1802 of 2000 took 0.103s
  training loss:		0.550739
  validation loss:		0.560528
  validation accuracy:		81.85 %
Epoch 1803 of 2000 took 0.103s
  training loss:		0.548234
  validation loss:		0.574980
  validation accuracy:		80.87 %
Epoch 1804 of 2000 took 0.103s
  training loss:		0.548089
  validation loss:		0.580654
  validation accuracy:		81.09 %
Epoch 1805 of 2000 took 0.103s
  training loss:		0.539990
  validation loss:		0.593348
  validation accuracy:		80.43 %
Epoch 1806 of 2000 took 0.104s
  training loss:		0.545302
  validation loss:		0.568599
  validation accuracy:		81.30 %
Epoch 1807 of 2000 took 0.103s
  training loss:		0.548612
  validation loss:		0.559531
  validation accuracy:		81.85 %
Epoch 1808 of 2000 took 0.103s
  training loss:		0.550626
  validation loss:		0.563654
  validation accuracy:		80.87 %
Epoch 1809 of 2000 took 0.103s
  training loss:		0.551833
  validation loss:		0.581783
  validation accuracy:		81.20 %
Epoch 1810 of 2000 took 0.103s
  training loss:		0.547423
  validation loss:		0.554575
  validation accuracy:		81.74 %
Epoch 1811 of 2000 took 0.103s
  training loss:		0.552273
  validation loss:		0.602301
  validation accuracy:		80.43 %
Epoch 1812 of 2000 took 0.103s
  training loss:		0.541981
  validation loss:		0.597718
  validation accuracy:		80.76 %
Epoch 1813 of 2000 took 0.103s
  training loss:		0.542645
  validation loss:		0.553605
  validation accuracy:		82.07 %
Epoch 1814 of 2000 took 0.103s
  training loss:		0.552442
  validation loss:		0.564030
  validation accuracy:		81.96 %
Epoch 1815 of 2000 took 0.103s
  training loss:		0.551890
  validation loss:		0.569689
  validation accuracy:		82.07 %
Epoch 1816 of 2000 took 0.103s
  training loss:		0.545590
  validation loss:		0.572730
  validation accuracy:		81.63 %
Epoch 1817 of 2000 took 0.103s
  training loss:		0.545954
  validation loss:		0.558612
  validation accuracy:		81.41 %
Epoch 1818 of 2000 took 0.103s
  training loss:		0.554381
  validation loss:		0.559871
  validation accuracy:		81.09 %
Epoch 1819 of 2000 took 0.103s
  training loss:		0.548092
  validation loss:		0.567346
  validation accuracy:		81.74 %
Epoch 1820 of 2000 took 0.103s
  training loss:		0.549963
  validation loss:		0.557353
  validation accuracy:		82.07 %
Epoch 1821 of 2000 took 0.103s
  training loss:		0.539634
  validation loss:		0.590751
  validation accuracy:		80.87 %
Epoch 1822 of 2000 took 0.103s
  training loss:		0.547560
  validation loss:		0.558292
  validation accuracy:		81.63 %
Epoch 1823 of 2000 took 0.103s
  training loss:		0.538124
  validation loss:		0.559580
  validation accuracy:		81.41 %
Epoch 1824 of 2000 took 0.103s
  training loss:		0.551345
  validation loss:		0.566636
  validation accuracy:		82.07 %
Epoch 1825 of 2000 took 0.103s
  training loss:		0.546118
  validation loss:		0.554825
  validation accuracy:		81.96 %
Epoch 1826 of 2000 took 0.103s
  training loss:		0.550909
  validation loss:		0.559295
  validation accuracy:		81.52 %
Epoch 1827 of 2000 took 0.103s
  training loss:		0.549272
  validation loss:		0.566993
  validation accuracy:		81.74 %
Epoch 1828 of 2000 took 0.103s
  training loss:		0.543501
  validation loss:		0.562348
  validation accuracy:		81.30 %
Epoch 1829 of 2000 took 0.103s
  training loss:		0.550636
  validation loss:		0.565058
  validation accuracy:		80.87 %
Epoch 1830 of 2000 took 0.103s
  training loss:		0.538520
  validation loss:		0.558683
  validation accuracy:		81.85 %
Epoch 1831 of 2000 took 0.103s
  training loss:		0.552406
  validation loss:		0.568961
  validation accuracy:		81.30 %
Epoch 1832 of 2000 took 0.103s
  training loss:		0.542603
  validation loss:		0.567823
  validation accuracy:		81.41 %
Epoch 1833 of 2000 took 0.103s
  training loss:		0.546781
  validation loss:		0.573696
  validation accuracy:		81.30 %
Epoch 1834 of 2000 took 0.103s
  training loss:		0.540409
  validation loss:		0.557997
  validation accuracy:		81.74 %
Epoch 1835 of 2000 took 0.103s
  training loss:		0.548445
  validation loss:		0.579040
  validation accuracy:		81.85 %
Epoch 1836 of 2000 took 0.104s
  training loss:		0.542745
  validation loss:		0.558115
  validation accuracy:		81.41 %
Epoch 1837 of 2000 took 0.103s
  training loss:		0.554207
  validation loss:		0.563610
  validation accuracy:		81.41 %
Epoch 1838 of 2000 took 0.103s
  training loss:		0.547956
  validation loss:		0.552102
  validation accuracy:		82.17 %
Epoch 1839 of 2000 took 0.103s
  training loss:		0.546889
  validation loss:		0.571035
  validation accuracy:		81.09 %
Epoch 1840 of 2000 took 0.103s
  training loss:		0.546447
  validation loss:		0.558576
  validation accuracy:		81.96 %
Epoch 1841 of 2000 took 0.103s
  training loss:		0.541271
  validation loss:		0.566752
  validation accuracy:		81.20 %
Epoch 1842 of 2000 took 0.103s
  training loss:		0.549149
  validation loss:		0.588663
  validation accuracy:		80.65 %
Epoch 1843 of 2000 took 0.103s
  training loss:		0.537065
  validation loss:		0.558929
  validation accuracy:		81.85 %
Epoch 1844 of 2000 took 0.103s
  training loss:		0.550471
  validation loss:		0.557100
  validation accuracy:		81.30 %
Epoch 1845 of 2000 took 0.103s
  training loss:		0.550483
  validation loss:		0.566839
  validation accuracy:		80.98 %
Epoch 1846 of 2000 took 0.103s
  training loss:		0.547690
  validation loss:		0.566796
  validation accuracy:		81.96 %
Epoch 1847 of 2000 took 0.103s
  training loss:		0.551697
  validation loss:		0.562265
  validation accuracy:		81.09 %
Epoch 1848 of 2000 took 0.103s
  training loss:		0.550237
  validation loss:		0.560619
  validation accuracy:		80.87 %
Epoch 1849 of 2000 took 0.103s
  training loss:		0.542643
  validation loss:		0.566034
  validation accuracy:		81.30 %
Epoch 1850 of 2000 took 0.103s
  training loss:		0.542622
  validation loss:		0.558751
  validation accuracy:		81.85 %
Epoch 1851 of 2000 took 0.103s
  training loss:		0.549254
  validation loss:		0.567161
  validation accuracy:		81.09 %
Epoch 1852 of 2000 took 0.103s
  training loss:		0.543325
  validation loss:		0.573021
  validation accuracy:		81.20 %
Epoch 1853 of 2000 took 0.103s
  training loss:		0.558156
  validation loss:		0.598893
  validation accuracy:		80.76 %
Epoch 1854 of 2000 took 0.103s
  training loss:		0.553357
  validation loss:		0.556503
  validation accuracy:		82.17 %
Epoch 1855 of 2000 took 0.103s
  training loss:		0.547186
  validation loss:		0.552818
  validation accuracy:		81.52 %
Epoch 1856 of 2000 took 0.103s
  training loss:		0.551355
  validation loss:		0.578389
  validation accuracy:		80.98 %
Epoch 1857 of 2000 took 0.103s
  training loss:		0.543641
  validation loss:		0.569717
  validation accuracy:		81.96 %
Epoch 1858 of 2000 took 0.103s
  training loss:		0.550507
  validation loss:		0.559718
  validation accuracy:		81.20 %
Epoch 1859 of 2000 took 0.103s
  training loss:		0.547251
  validation loss:		0.573288
  validation accuracy:		81.30 %
Epoch 1860 of 2000 took 0.103s
  training loss:		0.547394
  validation loss:		0.566007
  validation accuracy:		81.74 %
Epoch 1861 of 2000 took 0.103s
  training loss:		0.546309
  validation loss:		0.578896
  validation accuracy:		81.52 %
Epoch 1862 of 2000 took 0.103s
  training loss:		0.536684
  validation loss:		0.559714
  validation accuracy:		81.09 %
Epoch 1863 of 2000 took 0.103s
  training loss:		0.545729
  validation loss:		0.569948
  validation accuracy:		81.52 %
Epoch 1864 of 2000 took 0.103s
  training loss:		0.545132
  validation loss:		0.556847
  validation accuracy:		81.63 %
Epoch 1865 of 2000 took 0.104s
  training loss:		0.542203
  validation loss:		0.574215
  validation accuracy:		81.63 %
Epoch 1866 of 2000 took 0.103s
  training loss:		0.549254
  validation loss:		0.573940
  validation accuracy:		81.52 %
Epoch 1867 of 2000 took 0.103s
  training loss:		0.546238
  validation loss:		0.557206
  validation accuracy:		81.63 %
Epoch 1868 of 2000 took 0.103s
  training loss:		0.552030
  validation loss:		0.566867
  validation accuracy:		80.98 %
Epoch 1869 of 2000 took 0.103s
  training loss:		0.550237
  validation loss:		0.572400
  validation accuracy:		81.09 %
Epoch 1870 of 2000 took 0.103s
  training loss:		0.534258
  validation loss:		0.552524
  validation accuracy:		81.52 %
Epoch 1871 of 2000 took 0.103s
  training loss:		0.550176
  validation loss:		0.574567
  validation accuracy:		81.74 %
Epoch 1872 of 2000 took 0.103s
  training loss:		0.549222
  validation loss:		0.559163
  validation accuracy:		81.41 %
Epoch 1873 of 2000 took 0.103s
  training loss:		0.542725
  validation loss:		0.558245
  validation accuracy:		81.41 %
Epoch 1874 of 2000 took 0.103s
  training loss:		0.552412
  validation loss:		0.564111
  validation accuracy:		80.76 %
Epoch 1875 of 2000 took 0.103s
  training loss:		0.543212
  validation loss:		0.567552
  validation accuracy:		81.74 %
Epoch 1876 of 2000 took 0.103s
  training loss:		0.547127
  validation loss:		0.571477
  validation accuracy:		81.52 %
Epoch 1877 of 2000 took 0.103s
  training loss:		0.544402
  validation loss:		0.568915
  validation accuracy:		81.74 %
Epoch 1878 of 2000 took 0.103s
  training loss:		0.546216
  validation loss:		0.571164
  validation accuracy:		81.09 %
Epoch 1879 of 2000 took 0.103s
  training loss:		0.539643
  validation loss:		0.553466
  validation accuracy:		81.41 %
Epoch 1880 of 2000 took 0.103s
  training loss:		0.550027
  validation loss:		0.570081
  validation accuracy:		81.20 %
Epoch 1881 of 2000 took 0.103s
  training loss:		0.555083
  validation loss:		0.562063
  validation accuracy:		81.09 %
Epoch 1882 of 2000 took 0.103s
  training loss:		0.552462
  validation loss:		0.551412
  validation accuracy:		81.85 %
Epoch 1883 of 2000 took 0.103s
  training loss:		0.551975
  validation loss:		0.559514
  validation accuracy:		81.52 %
Epoch 1884 of 2000 took 0.103s
  training loss:		0.544541
  validation loss:		0.560093
  validation accuracy:		80.76 %
Epoch 1885 of 2000 took 0.103s
  training loss:		0.547250
  validation loss:		0.578250
  validation accuracy:		81.52 %
Epoch 1886 of 2000 took 0.103s
  training loss:		0.552850
  validation loss:		0.563256
  validation accuracy:		81.41 %
Epoch 1887 of 2000 took 0.103s
  training loss:		0.542908
  validation loss:		0.574932
  validation accuracy:		81.09 %
Epoch 1888 of 2000 took 0.103s
  training loss:		0.552530
  validation loss:		0.593716
  validation accuracy:		81.20 %
Epoch 1889 of 2000 took 0.103s
  training loss:		0.545275
  validation loss:		0.557636
  validation accuracy:		81.09 %
Epoch 1890 of 2000 took 0.103s
  training loss:		0.549610
  validation loss:		0.578580
  validation accuracy:		81.41 %
Epoch 1891 of 2000 took 0.103s
  training loss:		0.547730
  validation loss:		0.572869
  validation accuracy:		81.20 %
Epoch 1892 of 2000 took 0.103s
  training loss:		0.551676
  validation loss:		0.554412
  validation accuracy:		81.30 %
Epoch 1893 of 2000 took 0.103s
  training loss:		0.550952
  validation loss:		0.559657
  validation accuracy:		81.20 %
Epoch 1894 of 2000 took 0.104s
  training loss:		0.543368
  validation loss:		0.565839
  validation accuracy:		81.20 %
Epoch 1895 of 2000 took 0.103s
  training loss:		0.547383
  validation loss:		0.564937
  validation accuracy:		81.30 %
Epoch 1896 of 2000 took 0.103s
  training loss:		0.541830
  validation loss:		0.578672
  validation accuracy:		81.09 %
Epoch 1897 of 2000 took 0.103s
  training loss:		0.546466
  validation loss:		0.552158
  validation accuracy:		81.85 %
Epoch 1898 of 2000 took 0.103s
  training loss:		0.556002
  validation loss:		0.556867
  validation accuracy:		81.41 %
Epoch 1899 of 2000 took 0.103s
  training loss:		0.538810
  validation loss:		0.556608
  validation accuracy:		82.07 %
Epoch 1900 of 2000 took 0.103s
  training loss:		0.552324
  validation loss:		0.582077
  validation accuracy:		81.20 %
Epoch 1901 of 2000 took 0.103s
  training loss:		0.552127
  validation loss:		0.563760
  validation accuracy:		81.20 %
Epoch 1902 of 2000 took 0.103s
  training loss:		0.548965
  validation loss:		0.562657
  validation accuracy:		81.85 %
Epoch 1903 of 2000 took 0.103s
  training loss:		0.543391
  validation loss:		0.572904
  validation accuracy:		81.30 %
Epoch 1904 of 2000 took 0.103s
  training loss:		0.544270
  validation loss:		0.569106
  validation accuracy:		81.30 %
Epoch 1905 of 2000 took 0.103s
  training loss:		0.558472
  validation loss:		0.574676
  validation accuracy:		81.30 %
Epoch 1906 of 2000 took 0.103s
  training loss:		0.548819
  validation loss:		0.571254
  validation accuracy:		81.41 %
Epoch 1907 of 2000 took 0.103s
  training loss:		0.546560
  validation loss:		0.555006
  validation accuracy:		81.85 %
Epoch 1908 of 2000 took 0.103s
  training loss:		0.548050
  validation loss:		0.559282
  validation accuracy:		80.98 %
Epoch 1909 of 2000 took 0.103s
  training loss:		0.541293
  validation loss:		0.588509
  validation accuracy:		81.63 %
Epoch 1910 of 2000 took 0.103s
  training loss:		0.546245
  validation loss:		0.559807
  validation accuracy:		81.74 %
Epoch 1911 of 2000 took 0.103s
  training loss:		0.544003
  validation loss:		0.554374
  validation accuracy:		81.85 %
Epoch 1912 of 2000 took 0.103s
  training loss:		0.558476
  validation loss:		0.573841
  validation accuracy:		81.09 %
Epoch 1913 of 2000 took 0.103s
  training loss:		0.549232
  validation loss:		0.587575
  validation accuracy:		81.20 %
Epoch 1914 of 2000 took 0.103s
  training loss:		0.545909
  validation loss:		0.578860
  validation accuracy:		81.30 %
Epoch 1915 of 2000 took 0.103s
  training loss:		0.548743
  validation loss:		0.555963
  validation accuracy:		82.07 %
Epoch 1916 of 2000 took 0.103s
  training loss:		0.554433
  validation loss:		0.558927
  validation accuracy:		80.87 %
Epoch 1917 of 2000 took 0.103s
  training loss:		0.545874
  validation loss:		0.564874
  validation accuracy:		81.74 %
Epoch 1918 of 2000 took 0.103s
  training loss:		0.549630
  validation loss:		0.559502
  validation accuracy:		80.87 %
Epoch 1919 of 2000 took 0.103s
  training loss:		0.555550
  validation loss:		0.558570
  validation accuracy:		81.74 %
Epoch 1920 of 2000 took 0.103s
  training loss:		0.540096
  validation loss:		0.583147
  validation accuracy:		81.09 %
Epoch 1921 of 2000 took 0.103s
  training loss:		0.549010
  validation loss:		0.558557
  validation accuracy:		82.07 %
Epoch 1922 of 2000 took 0.103s
  training loss:		0.550422
  validation loss:		0.555693
  validation accuracy:		81.09 %
Epoch 1923 of 2000 took 0.104s
  training loss:		0.545501
  validation loss:		0.551856
  validation accuracy:		81.96 %
Epoch 1924 of 2000 took 0.103s
  training loss:		0.549996
  validation loss:		0.564032
  validation accuracy:		81.09 %
Epoch 1925 of 2000 took 0.103s
  training loss:		0.549492
  validation loss:		0.566045
  validation accuracy:		82.07 %
Epoch 1926 of 2000 took 0.103s
  training loss:		0.556341
  validation loss:		0.592225
  validation accuracy:		80.87 %
Epoch 1927 of 2000 took 0.103s
  training loss:		0.559557
  validation loss:		0.567973
  validation accuracy:		82.17 %
Epoch 1928 of 2000 took 0.103s
  training loss:		0.537975
  validation loss:		0.556299
  validation accuracy:		81.52 %
Epoch 1929 of 2000 took 0.103s
  training loss:		0.544862
  validation loss:		0.555542
  validation accuracy:		81.41 %
Epoch 1930 of 2000 took 0.103s
  training loss:		0.554851
  validation loss:		0.587469
  validation accuracy:		80.65 %
Epoch 1931 of 2000 took 0.103s
  training loss:		0.553266
  validation loss:		0.577518
  validation accuracy:		80.87 %
Epoch 1932 of 2000 took 0.103s
  training loss:		0.541782
  validation loss:		0.564888
  validation accuracy:		81.30 %
Epoch 1933 of 2000 took 0.103s
  training loss:		0.548828
  validation loss:		0.560660
  validation accuracy:		81.85 %
Epoch 1934 of 2000 took 0.103s
  training loss:		0.543260
  validation loss:		0.555986
  validation accuracy:		81.74 %
Epoch 1935 of 2000 took 0.103s
  training loss:		0.541131
  validation loss:		0.566375
  validation accuracy:		81.74 %
Epoch 1936 of 2000 took 0.103s
  training loss:		0.552959
  validation loss:		0.570117
  validation accuracy:		81.20 %
Epoch 1937 of 2000 took 0.103s
  training loss:		0.545338
  validation loss:		0.561698
  validation accuracy:		81.09 %
Epoch 1938 of 2000 took 0.103s
  training loss:		0.542689
  validation loss:		0.556919
  validation accuracy:		81.20 %
Epoch 1939 of 2000 took 0.103s
  training loss:		0.548650
  validation loss:		0.559759
  validation accuracy:		81.52 %
Epoch 1940 of 2000 took 0.103s
  training loss:		0.546141
  validation loss:		0.554527
  validation accuracy:		81.30 %
Epoch 1941 of 2000 took 0.103s
  training loss:		0.543848
  validation loss:		0.560839
  validation accuracy:		81.20 %
Epoch 1942 of 2000 took 0.103s
  training loss:		0.552732
  validation loss:		0.551878
  validation accuracy:		82.07 %
Epoch 1943 of 2000 took 0.103s
  training loss:		0.549426
  validation loss:		0.563540
  validation accuracy:		81.30 %
Epoch 1944 of 2000 took 0.103s
  training loss:		0.553117
  validation loss:		0.561644
  validation accuracy:		81.09 %
Epoch 1945 of 2000 took 0.103s
  training loss:		0.554694
  validation loss:		0.553432
  validation accuracy:		81.74 %
Epoch 1946 of 2000 took 0.103s
  training loss:		0.540091
  validation loss:		0.566303
  validation accuracy:		81.20 %
Epoch 1947 of 2000 took 0.103s
  training loss:		0.546715
  validation loss:		0.563514
  validation accuracy:		81.09 %
Epoch 1948 of 2000 took 0.103s
  training loss:		0.544615
  validation loss:		0.554425
  validation accuracy:		81.52 %
Epoch 1949 of 2000 took 0.103s
  training loss:		0.548649
  validation loss:		0.569421
  validation accuracy:		81.09 %
Epoch 1950 of 2000 took 0.103s
  training loss:		0.536365
  validation loss:		0.566497
  validation accuracy:		81.09 %
Epoch 1951 of 2000 took 0.103s
  training loss:		0.550944
  validation loss:		0.558678
  validation accuracy:		81.09 %
Epoch 1952 of 2000 took 0.104s
  training loss:		0.544843
  validation loss:		0.582966
  validation accuracy:		81.52 %
Epoch 1953 of 2000 took 0.103s
  training loss:		0.551096
  validation loss:		0.574928
  validation accuracy:		81.09 %
Epoch 1954 of 2000 took 0.103s
  training loss:		0.545202
  validation loss:		0.554722
  validation accuracy:		81.41 %
Epoch 1955 of 2000 took 0.103s
  training loss:		0.533606
  validation loss:		0.560447
  validation accuracy:		81.96 %
Epoch 1956 of 2000 took 0.103s
  training loss:		0.560444
  validation loss:		0.557373
  validation accuracy:		81.09 %
Epoch 1957 of 2000 took 0.103s
  training loss:		0.546482
  validation loss:		0.555522
  validation accuracy:		81.41 %
Epoch 1958 of 2000 took 0.103s
  training loss:		0.538177
  validation loss:		0.597033
  validation accuracy:		80.76 %
Epoch 1959 of 2000 took 0.104s
  training loss:		0.540838
  validation loss:		0.569824
  validation accuracy:		81.20 %
Epoch 1960 of 2000 took 0.103s
  training loss:		0.537293
  validation loss:		0.566451
  validation accuracy:		81.74 %
Epoch 1961 of 2000 took 0.103s
  training loss:		0.545587
  validation loss:		0.555377
  validation accuracy:		81.41 %
Epoch 1962 of 2000 took 0.103s
  training loss:		0.538815
  validation loss:		0.590449
  validation accuracy:		80.54 %
Epoch 1963 of 2000 took 0.103s
  training loss:		0.552012
  validation loss:		0.556450
  validation accuracy:		82.17 %
Epoch 1964 of 2000 took 0.103s
  training loss:		0.543095
  validation loss:		0.563379
  validation accuracy:		80.98 %
Epoch 1965 of 2000 took 0.103s
  training loss:		0.550061
  validation loss:		0.567756
  validation accuracy:		81.41 %
Epoch 1966 of 2000 took 0.103s
  training loss:		0.550708
  validation loss:		0.576107
  validation accuracy:		81.20 %
Epoch 1967 of 2000 took 0.103s
  training loss:		0.539765
  validation loss:		0.556743
  validation accuracy:		81.20 %
Epoch 1968 of 2000 took 0.103s
  training loss:		0.547893
  validation loss:		0.567798
  validation accuracy:		81.20 %
Epoch 1969 of 2000 took 0.103s
  training loss:		0.546091
  validation loss:		0.566011
  validation accuracy:		81.41 %
Epoch 1970 of 2000 took 0.103s
  training loss:		0.546621
  validation loss:		0.567258
  validation accuracy:		81.52 %
Epoch 1971 of 2000 took 0.103s
  training loss:		0.552322
  validation loss:		0.563874
  validation accuracy:		81.20 %
Epoch 1972 of 2000 took 0.103s
  training loss:		0.545512
  validation loss:		0.582905
  validation accuracy:		81.41 %
Epoch 1973 of 2000 took 0.103s
  training loss:		0.546372
  validation loss:		0.563696
  validation accuracy:		81.63 %
Epoch 1974 of 2000 took 0.103s
  training loss:		0.552853
  validation loss:		0.561791
  validation accuracy:		81.41 %
Epoch 1975 of 2000 took 0.103s
  training loss:		0.544392
  validation loss:		0.562772
  validation accuracy:		81.63 %
Epoch 1976 of 2000 took 0.103s
  training loss:		0.544933
  validation loss:		0.577640
  validation accuracy:		80.87 %
Epoch 1977 of 2000 took 0.103s
  training loss:		0.533630
  validation loss:		0.574755
  validation accuracy:		81.63 %
Epoch 1978 of 2000 took 0.103s
  training loss:		0.555240
  validation loss:		0.561198
  validation accuracy:		82.28 %
Epoch 1979 of 2000 took 0.103s
  training loss:		0.548654
  validation loss:		0.574418
  validation accuracy:		81.74 %
Epoch 1980 of 2000 took 0.103s
  training loss:		0.550223
  validation loss:		0.566651
  validation accuracy:		81.74 %
Epoch 1981 of 2000 took 0.103s
  training loss:		0.542563
  validation loss:		0.571235
  validation accuracy:		81.30 %
Epoch 1982 of 2000 took 0.104s
  training loss:		0.555966
  validation loss:		0.560417
  validation accuracy:		81.52 %
Epoch 1983 of 2000 took 0.103s
  training loss:		0.548036
  validation loss:		0.562788
  validation accuracy:		81.41 %
Epoch 1984 of 2000 took 0.105s
  training loss:		0.551050
  validation loss:		0.570651
  validation accuracy:		82.07 %
Epoch 1985 of 2000 took 0.103s
  training loss:		0.544334
  validation loss:		0.554044
  validation accuracy:		82.17 %
Epoch 1986 of 2000 took 0.103s
  training loss:		0.547640
  validation loss:		0.556439
  validation accuracy:		82.17 %
Epoch 1987 of 2000 took 0.103s
  training loss:		0.543515
  validation loss:		0.553164
  validation accuracy:		81.63 %
Epoch 1988 of 2000 took 0.103s
  training loss:		0.548751
  validation loss:		0.560725
  validation accuracy:		81.30 %
Epoch 1989 of 2000 took 0.103s
  training loss:		0.538540
  validation loss:		0.554079
  validation accuracy:		81.09 %
Epoch 1990 of 2000 took 0.103s
  training loss:		0.554633
  validation loss:		0.551827
  validation accuracy:		81.74 %
Epoch 1991 of 2000 took 0.103s
  training loss:		0.538891
  validation loss:		0.567518
  validation accuracy:		81.09 %
Epoch 1992 of 2000 took 0.103s
  training loss:		0.548182
  validation loss:		0.612986
  validation accuracy:		79.89 %
Epoch 1993 of 2000 took 0.103s
  training loss:		0.547133
  validation loss:		0.586659
  validation accuracy:		81.52 %
Epoch 1994 of 2000 took 0.103s
  training loss:		0.554339
  validation loss:		0.583344
  validation accuracy:		81.41 %
Epoch 1995 of 2000 took 0.103s
  training loss:		0.546894
  validation loss:		0.562506
  validation accuracy:		81.96 %
Epoch 1996 of 2000 took 0.109s
  training loss:		0.542022
  validation loss:		0.554850
  validation accuracy:		81.63 %
Epoch 1997 of 2000 took 0.110s
  training loss:		0.536903
  validation loss:		0.561919
  validation accuracy:		81.41 %
Epoch 1998 of 2000 took 0.110s
  training loss:		0.550129
  validation loss:		0.599807
  validation accuracy:		80.33 %
Epoch 1999 of 2000 took 0.110s
  training loss:		0.546565
  validation loss:		0.556573
  validation accuracy:		81.74 %
Epoch 2000 of 2000 took 0.110s
  training loss:		0.553068
  validation loss:		0.560011
  validation accuracy:		82.07 %
Final results:
  test loss:			0.801663
  test accuracy:		74.72 %
