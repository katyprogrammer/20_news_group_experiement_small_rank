Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (4, 50, 50)...
decomposing tensor B of shape (4, 50)...
Starting training...
Epoch 1 of 2000 took 0.167s
  training loss:		3.003430
  validation loss:		3.002595
  validation accuracy:		0.00 %
Epoch 2 of 2000 took 0.168s
  training loss:		2.997158
  validation loss:		2.994377
  validation accuracy:		12.83 %
Epoch 3 of 2000 took 0.163s
  training loss:		2.988848
  validation loss:		2.984877
  validation accuracy:		12.83 %
Epoch 4 of 2000 took 0.168s
  training loss:		2.979983
  validation loss:		2.975081
  validation accuracy:		12.83 %
Epoch 5 of 2000 took 0.163s
  training loss:		2.970848
  validation loss:		2.965245
  validation accuracy:		12.83 %
Epoch 6 of 2000 took 0.168s
  training loss:		2.961631
  validation loss:		2.955486
  validation accuracy:		12.83 %
Epoch 7 of 2000 took 0.163s
  training loss:		2.952670
  validation loss:		2.945865
  validation accuracy:		12.83 %
Epoch 8 of 2000 took 0.167s
  training loss:		2.943652
  validation loss:		2.936428
  validation accuracy:		12.83 %
Epoch 9 of 2000 took 0.164s
  training loss:		2.935095
  validation loss:		2.927219
  validation accuracy:		12.83 %
Epoch 10 of 2000 took 0.165s
  training loss:		2.926709
  validation loss:		2.918089
  validation accuracy:		12.83 %
Epoch 11 of 2000 took 0.167s
  training loss:		2.918268
  validation loss:		2.909081
  validation accuracy:		12.83 %
Epoch 12 of 2000 took 0.163s
  training loss:		2.910069
  validation loss:		2.900194
  validation accuracy:		12.83 %
Epoch 13 of 2000 took 0.168s
  training loss:		2.901909
  validation loss:		2.891397
  validation accuracy:		12.83 %
Epoch 14 of 2000 took 0.163s
  training loss:		2.893902
  validation loss:		2.882722
  validation accuracy:		12.83 %
Epoch 15 of 2000 took 0.168s
  training loss:		2.885707
  validation loss:		2.874240
  validation accuracy:		12.83 %
Epoch 16 of 2000 took 0.163s
  training loss:		2.877963
  validation loss:		2.865882
  validation accuracy:		12.83 %
Epoch 17 of 2000 took 0.170s
  training loss:		2.869779
  validation loss:		2.857486
  validation accuracy:		12.83 %
Epoch 18 of 2000 took 0.166s
  training loss:		2.862956
  validation loss:		2.849256
  validation accuracy:		12.83 %
Epoch 19 of 2000 took 0.166s
  training loss:		2.854961
  validation loss:		2.841158
  validation accuracy:		12.83 %
Epoch 20 of 2000 took 0.164s
  training loss:		2.847253
  validation loss:		2.833088
  validation accuracy:		12.83 %
Epoch 21 of 2000 took 0.165s
  training loss:		2.840356
  validation loss:		2.825142
  validation accuracy:		12.83 %
Epoch 22 of 2000 took 0.168s
  training loss:		2.832814
  validation loss:		2.817320
  validation accuracy:		12.83 %
Epoch 23 of 2000 took 0.163s
  training loss:		2.825805
  validation loss:		2.809502
  validation accuracy:		12.83 %
Epoch 24 of 2000 took 0.168s
  training loss:		2.818903
  validation loss:		2.801758
  validation accuracy:		13.04 %
Epoch 25 of 2000 took 0.163s
  training loss:		2.811738
  validation loss:		2.794131
  validation accuracy:		13.04 %
Epoch 26 of 2000 took 0.168s
  training loss:		2.804909
  validation loss:		2.786540
  validation accuracy:		13.04 %
Epoch 27 of 2000 took 0.162s
  training loss:		2.797384
  validation loss:		2.779013
  validation accuracy:		13.04 %
Epoch 28 of 2000 took 0.167s
  training loss:		2.790995
  validation loss:		2.771533
  validation accuracy:		13.04 %
Epoch 29 of 2000 took 0.164s
  training loss:		2.783874
  validation loss:		2.764156
  validation accuracy:		13.04 %
Epoch 30 of 2000 took 0.165s
  training loss:		2.777622
  validation loss:		2.756788
  validation accuracy:		13.04 %
Epoch 31 of 2000 took 0.167s
  training loss:		2.770452
  validation loss:		2.749507
  validation accuracy:		13.04 %
Epoch 32 of 2000 took 0.163s
  training loss:		2.763909
  validation loss:		2.742275
  validation accuracy:		13.04 %
Epoch 33 of 2000 took 0.168s
  training loss:		2.758233
  validation loss:		2.735131
  validation accuracy:		13.04 %
Epoch 34 of 2000 took 0.161s
  training loss:		2.751031
  validation loss:		2.728067
  validation accuracy:		13.04 %
Epoch 35 of 2000 took 0.168s
  training loss:		2.744772
  validation loss:		2.721065
  validation accuracy:		13.04 %
Epoch 36 of 2000 took 0.163s
  training loss:		2.738494
  validation loss:		2.714018
  validation accuracy:		13.04 %
Epoch 37 of 2000 took 0.167s
  training loss:		2.731118
  validation loss:		2.707066
  validation accuracy:		13.04 %
Epoch 38 of 2000 took 0.163s
  training loss:		2.726366
  validation loss:		2.700236
  validation accuracy:		13.04 %
Epoch 39 of 2000 took 0.166s
  training loss:		2.719432
  validation loss:		2.693486
  validation accuracy:		13.04 %
Epoch 40 of 2000 took 0.165s
  training loss:		2.713366
  validation loss:		2.686697
  validation accuracy:		13.04 %
Epoch 41 of 2000 took 0.164s
  training loss:		2.707512
  validation loss:		2.680103
  validation accuracy:		13.04 %
Epoch 42 of 2000 took 0.168s
  training loss:		2.700701
  validation loss:		2.673483
  validation accuracy:		13.04 %
Epoch 43 of 2000 took 0.163s
  training loss:		2.695579
  validation loss:		2.666873
  validation accuracy:		13.04 %
Epoch 44 of 2000 took 0.168s
  training loss:		2.690055
  validation loss:		2.660351
  validation accuracy:		13.04 %
Epoch 45 of 2000 took 0.163s
  training loss:		2.683755
  validation loss:		2.653952
  validation accuracy:		13.04 %
Epoch 46 of 2000 took 0.167s
  training loss:		2.678266
  validation loss:		2.647586
  validation accuracy:		13.04 %
Epoch 47 of 2000 took 0.167s
  training loss:		2.671455
  validation loss:		2.641287
  validation accuracy:		13.04 %
Epoch 48 of 2000 took 0.167s
  training loss:		2.666108
  validation loss:		2.635054
  validation accuracy:		13.04 %
Epoch 49 of 2000 took 0.164s
  training loss:		2.661280
  validation loss:		2.628933
  validation accuracy:		13.04 %
Epoch 50 of 2000 took 0.165s
  training loss:		2.655251
  validation loss:		2.622857
  validation accuracy:		13.04 %
Epoch 51 of 2000 took 0.167s
  training loss:		2.650553
  validation loss:		2.616845
  validation accuracy:		13.04 %
Epoch 52 of 2000 took 0.163s
  training loss:		2.643960
  validation loss:		2.610835
  validation accuracy:		13.04 %
Epoch 53 of 2000 took 0.168s
  training loss:		2.639223
  validation loss:		2.604858
  validation accuracy:		13.04 %
Epoch 54 of 2000 took 0.163s
  training loss:		2.633763
  validation loss:		2.598938
  validation accuracy:		13.04 %
Epoch 55 of 2000 took 0.168s
  training loss:		2.628233
  validation loss:		2.593145
  validation accuracy:		13.04 %
Epoch 56 of 2000 took 0.163s
  training loss:		2.622987
  validation loss:		2.587297
  validation accuracy:		13.04 %
Epoch 57 of 2000 took 0.167s
  training loss:		2.616857
  validation loss:		2.581621
  validation accuracy:		13.04 %
Epoch 58 of 2000 took 0.164s
  training loss:		2.612398
  validation loss:		2.575895
  validation accuracy:		13.04 %
Epoch 59 of 2000 took 0.166s
  training loss:		2.606905
  validation loss:		2.570292
  validation accuracy:		13.04 %
Epoch 60 of 2000 took 0.167s
  training loss:		2.602473
  validation loss:		2.564713
  validation accuracy:		13.04 %
Epoch 61 of 2000 took 0.163s
  training loss:		2.598760
  validation loss:		2.559226
  validation accuracy:		13.04 %
Epoch 62 of 2000 took 0.168s
  training loss:		2.592849
  validation loss:		2.553840
  validation accuracy:		13.04 %
Epoch 63 of 2000 took 0.163s
  training loss:		2.588193
  validation loss:		2.548519
  validation accuracy:		13.04 %
Epoch 64 of 2000 took 0.168s
  training loss:		2.583419
  validation loss:		2.543330
  validation accuracy:		13.04 %
Epoch 65 of 2000 took 0.163s
  training loss:		2.578919
  validation loss:		2.538177
  validation accuracy:		13.04 %
Epoch 66 of 2000 took 0.168s
  training loss:		2.574798
  validation loss:		2.533077
  validation accuracy:		13.04 %
Epoch 67 of 2000 took 0.163s
  training loss:		2.569855
  validation loss:		2.528032
  validation accuracy:		13.04 %
Epoch 68 of 2000 took 0.166s
  training loss:		2.564863
  validation loss:		2.523059
  validation accuracy:		13.04 %
Epoch 69 of 2000 took 0.164s
  training loss:		2.559798
  validation loss:		2.518112
  validation accuracy:		13.04 %
Epoch 70 of 2000 took 0.165s
  training loss:		2.557257
  validation loss:		2.513314
  validation accuracy:		13.04 %
Epoch 71 of 2000 took 0.167s
  training loss:		2.552802
  validation loss:		2.508648
  validation accuracy:		13.04 %
Epoch 72 of 2000 took 0.163s
  training loss:		2.547921
  validation loss:		2.503971
  validation accuracy:		13.04 %
Epoch 73 of 2000 took 0.168s
  training loss:		2.544111
  validation loss:		2.499455
  validation accuracy:		13.04 %
Epoch 74 of 2000 took 0.163s
  training loss:		2.540121
  validation loss:		2.494938
  validation accuracy:		13.04 %
Epoch 75 of 2000 took 0.168s
  training loss:		2.535827
  validation loss:		2.490496
  validation accuracy:		13.04 %
Epoch 76 of 2000 took 0.162s
  training loss:		2.532395
  validation loss:		2.486064
  validation accuracy:		13.04 %
Epoch 77 of 2000 took 0.167s
  training loss:		2.526965
  validation loss:		2.481762
  validation accuracy:		13.04 %
Epoch 78 of 2000 took 0.164s
  training loss:		2.524402
  validation loss:		2.477513
  validation accuracy:		13.04 %
Epoch 79 of 2000 took 0.165s
  training loss:		2.520796
  validation loss:		2.473377
  validation accuracy:		13.04 %
Epoch 80 of 2000 took 0.166s
  training loss:		2.516080
  validation loss:		2.469185
  validation accuracy:		13.04 %
Epoch 81 of 2000 took 0.163s
  training loss:		2.512694
  validation loss:		2.465170
  validation accuracy:		13.04 %
Epoch 82 of 2000 took 0.168s
  training loss:		2.509710
  validation loss:		2.461195
  validation accuracy:		13.04 %
Epoch 83 of 2000 took 0.163s
  training loss:		2.506091
  validation loss:		2.457336
  validation accuracy:		13.04 %
Epoch 84 of 2000 took 0.168s
  training loss:		2.502842
  validation loss:		2.453492
  validation accuracy:		13.04 %
Epoch 85 of 2000 took 0.163s
  training loss:		2.499199
  validation loss:		2.449729
  validation accuracy:		13.04 %
Epoch 86 of 2000 took 0.171s
  training loss:		2.497100
  validation loss:		2.446051
  validation accuracy:		13.04 %
Epoch 87 of 2000 took 0.164s
  training loss:		2.492732
  validation loss:		2.442537
  validation accuracy:		13.04 %
Epoch 88 of 2000 took 0.166s
  training loss:		2.489782
  validation loss:		2.439050
  validation accuracy:		13.04 %
Epoch 89 of 2000 took 0.166s
  training loss:		2.486923
  validation loss:		2.435544
  validation accuracy:		13.04 %
Epoch 90 of 2000 took 0.163s
  training loss:		2.484083
  validation loss:		2.432132
  validation accuracy:		13.04 %
Epoch 91 of 2000 took 0.168s
  training loss:		2.478834
  validation loss:		2.428790
  validation accuracy:		13.04 %
Epoch 92 of 2000 took 0.163s
  training loss:		2.476764
  validation loss:		2.425450
  validation accuracy:		13.04 %
Epoch 93 of 2000 took 0.168s
  training loss:		2.473434
  validation loss:		2.422160
  validation accuracy:		13.04 %
Epoch 94 of 2000 took 0.163s
  training loss:		2.471063
  validation loss:		2.418982
  validation accuracy:		13.04 %
Epoch 95 of 2000 took 0.167s
  training loss:		2.468393
  validation loss:		2.415833
  validation accuracy:		13.04 %
Epoch 96 of 2000 took 0.163s
  training loss:		2.465111
  validation loss:		2.412842
  validation accuracy:		13.04 %
Epoch 97 of 2000 took 0.166s
  training loss:		2.462198
  validation loss:		2.409788
  validation accuracy:		13.04 %
Epoch 98 of 2000 took 0.163s
  training loss:		2.460595
  validation loss:		2.406875
  validation accuracy:		13.04 %
Epoch 99 of 2000 took 0.164s
  training loss:		2.457180
  validation loss:		2.403960
  validation accuracy:		13.04 %
Epoch 100 of 2000 took 0.168s
  training loss:		2.455502
  validation loss:		2.401279
  validation accuracy:		13.04 %
Epoch 101 of 2000 took 0.163s
  training loss:		2.453051
  validation loss:		2.398506
  validation accuracy:		13.04 %
Epoch 102 of 2000 took 0.168s
  training loss:		2.451037
  validation loss:		2.395835
  validation accuracy:		13.04 %
Epoch 103 of 2000 took 0.163s
  training loss:		2.446994
  validation loss:		2.393221
  validation accuracy:		13.04 %
Epoch 104 of 2000 took 0.168s
  training loss:		2.444601
  validation loss:		2.390550
  validation accuracy:		13.04 %
Epoch 105 of 2000 took 0.163s
  training loss:		2.442361
  validation loss:		2.388003
  validation accuracy:		13.04 %
Epoch 106 of 2000 took 0.166s
  training loss:		2.440025
  validation loss:		2.385441
  validation accuracy:		13.04 %
Epoch 107 of 2000 took 0.165s
  training loss:		2.438354
  validation loss:		2.382988
  validation accuracy:		13.04 %
Epoch 108 of 2000 took 0.164s
  training loss:		2.436858
  validation loss:		2.380574
  validation accuracy:		13.04 %
Epoch 109 of 2000 took 0.168s
  training loss:		2.434422
  validation loss:		2.378283
  validation accuracy:		13.04 %
Epoch 110 of 2000 took 0.163s
  training loss:		2.432058
  validation loss:		2.376110
  validation accuracy:		13.04 %
Epoch 111 of 2000 took 0.168s
  training loss:		2.429544
  validation loss:		2.373846
  validation accuracy:		13.04 %
Epoch 112 of 2000 took 0.163s
  training loss:		2.427482
  validation loss:		2.371610
  validation accuracy:		13.04 %
Epoch 113 of 2000 took 0.168s
  training loss:		2.425356
  validation loss:		2.369456
  validation accuracy:		13.04 %
Epoch 114 of 2000 took 0.163s
  training loss:		2.423209
  validation loss:		2.367280
  validation accuracy:		13.04 %
Epoch 115 of 2000 took 0.167s
  training loss:		2.421056
  validation loss:		2.365128
  validation accuracy:		13.04 %
Epoch 116 of 2000 took 0.164s
  training loss:		2.419833
  validation loss:		2.363102
  validation accuracy:		13.04 %
Epoch 117 of 2000 took 0.165s
  training loss:		2.417943
  validation loss:		2.361109
  validation accuracy:		13.04 %
Epoch 118 of 2000 took 0.167s
  training loss:		2.416986
  validation loss:		2.359203
  validation accuracy:		13.04 %
Epoch 119 of 2000 took 0.163s
  training loss:		2.414183
  validation loss:		2.357429
  validation accuracy:		13.04 %
Epoch 120 of 2000 took 0.168s
  training loss:		2.412314
  validation loss:		2.355633
  validation accuracy:		13.04 %
Epoch 121 of 2000 took 0.163s
  training loss:		2.411455
  validation loss:		2.353861
  validation accuracy:		13.04 %
Epoch 122 of 2000 took 0.168s
  training loss:		2.408191
  validation loss:		2.352023
  validation accuracy:		13.04 %
Epoch 123 of 2000 took 0.163s
  training loss:		2.406732
  validation loss:		2.350227
  validation accuracy:		13.04 %
Epoch 124 of 2000 took 0.167s
  training loss:		2.406145
  validation loss:		2.348501
  validation accuracy:		13.04 %
Epoch 125 of 2000 took 0.162s
  training loss:		2.403966
  validation loss:		2.346849
  validation accuracy:		13.04 %
Epoch 126 of 2000 took 0.166s
  training loss:		2.402939
  validation loss:		2.345161
  validation accuracy:		13.04 %
Epoch 127 of 2000 took 0.165s
  training loss:		2.401250
  validation loss:		2.343661
  validation accuracy:		13.04 %
Epoch 128 of 2000 took 0.165s
  training loss:		2.400114
  validation loss:		2.342111
  validation accuracy:		13.04 %
Epoch 129 of 2000 took 0.167s
  training loss:		2.397811
  validation loss:		2.340584
  validation accuracy:		13.04 %
Epoch 130 of 2000 took 0.163s
  training loss:		2.396489
  validation loss:		2.339132
  validation accuracy:		13.04 %
Epoch 131 of 2000 took 0.168s
  training loss:		2.395811
  validation loss:		2.337717
  validation accuracy:		13.04 %
Epoch 132 of 2000 took 0.163s
  training loss:		2.393001
  validation loss:		2.336282
  validation accuracy:		13.04 %
Epoch 133 of 2000 took 0.172s
  training loss:		2.392955
  validation loss:		2.334873
  validation accuracy:		13.04 %
Epoch 134 of 2000 took 0.163s
  training loss:		2.390855
  validation loss:		2.333456
  validation accuracy:		13.04 %
Epoch 135 of 2000 took 0.167s
  training loss:		2.390363
  validation loss:		2.332219
  validation accuracy:		13.04 %
Epoch 136 of 2000 took 0.163s
  training loss:		2.389444
  validation loss:		2.330971
  validation accuracy:		13.04 %
Epoch 137 of 2000 took 0.166s
  training loss:		2.386824
  validation loss:		2.329663
  validation accuracy:		13.04 %
Epoch 138 of 2000 took 0.166s
  training loss:		2.385713
  validation loss:		2.328286
  validation accuracy:		13.04 %
Epoch 139 of 2000 took 0.163s
  training loss:		2.385438
  validation loss:		2.326994
  validation accuracy:		13.04 %
Epoch 140 of 2000 took 0.168s
  training loss:		2.383720
  validation loss:		2.325761
  validation accuracy:		13.04 %
Epoch 141 of 2000 took 0.163s
  training loss:		2.383782
  validation loss:		2.324631
  validation accuracy:		13.04 %
Epoch 142 of 2000 took 0.168s
  training loss:		2.382319
  validation loss:		2.323646
  validation accuracy:		13.04 %
Epoch 143 of 2000 took 0.163s
  training loss:		2.381121
  validation loss:		2.322514
  validation accuracy:		13.04 %
Epoch 144 of 2000 took 0.167s
  training loss:		2.379361
  validation loss:		2.321481
  validation accuracy:		13.04 %
Epoch 145 of 2000 took 0.163s
  training loss:		2.378323
  validation loss:		2.320419
  validation accuracy:		13.04 %
Epoch 146 of 2000 took 0.166s
  training loss:		2.376243
  validation loss:		2.319323
  validation accuracy:		13.04 %
Epoch 147 of 2000 took 0.165s
  training loss:		2.375574
  validation loss:		2.318184
  validation accuracy:		13.04 %
Epoch 148 of 2000 took 0.165s
  training loss:		2.376823
  validation loss:		2.317331
  validation accuracy:		13.04 %
Epoch 149 of 2000 took 0.167s
  training loss:		2.374875
  validation loss:		2.316546
  validation accuracy:		13.04 %
Epoch 150 of 2000 took 0.163s
  training loss:		2.373072
  validation loss:		2.315567
  validation accuracy:		13.04 %
Epoch 151 of 2000 took 0.168s
  training loss:		2.371685
  validation loss:		2.314565
  validation accuracy:		13.04 %
Epoch 152 of 2000 took 0.163s
  training loss:		2.371808
  validation loss:		2.313620
  validation accuracy:		13.04 %
Epoch 153 of 2000 took 0.168s
  training loss:		2.370118
  validation loss:		2.312828
  validation accuracy:		13.04 %
Epoch 154 of 2000 took 0.163s
  training loss:		2.369734
  validation loss:		2.311940
  validation accuracy:		13.04 %
Epoch 155 of 2000 took 0.167s
  training loss:		2.369049
  validation loss:		2.311180
  validation accuracy:		13.04 %
Epoch 156 of 2000 took 0.164s
  training loss:		2.367952
  validation loss:		2.310339
  validation accuracy:		13.04 %
Epoch 157 of 2000 took 0.165s
  training loss:		2.365769
  validation loss:		2.309464
  validation accuracy:		13.04 %
Epoch 158 of 2000 took 0.167s
  training loss:		2.365468
  validation loss:		2.308528
  validation accuracy:		13.04 %
Epoch 159 of 2000 took 0.163s
  training loss:		2.364546
  validation loss:		2.307752
  validation accuracy:		13.04 %
Epoch 160 of 2000 took 0.168s
  training loss:		2.364973
  validation loss:		2.306958
  validation accuracy:		13.04 %
Epoch 161 of 2000 took 0.163s
  training loss:		2.363403
  validation loss:		2.306101
  validation accuracy:		13.04 %
Epoch 162 of 2000 took 0.168s
  training loss:		2.362287
  validation loss:		2.305376
  validation accuracy:		13.04 %
Epoch 163 of 2000 took 0.163s
  training loss:		2.362432
  validation loss:		2.304674
  validation accuracy:		13.04 %
Epoch 164 of 2000 took 0.167s
  training loss:		2.360752
  validation loss:		2.304023
  validation accuracy:		13.04 %
Epoch 165 of 2000 took 0.163s
  training loss:		2.359482
  validation loss:		2.303275
  validation accuracy:		13.04 %
Epoch 166 of 2000 took 0.166s
  training loss:		2.359780
  validation loss:		2.302627
  validation accuracy:		13.04 %
Epoch 167 of 2000 took 0.165s
  training loss:		2.358767
  validation loss:		2.301984
  validation accuracy:		13.04 %
Epoch 168 of 2000 took 0.164s
  training loss:		2.357627
  validation loss:		2.301391
  validation accuracy:		13.04 %
Epoch 169 of 2000 took 0.167s
  training loss:		2.356910
  validation loss:		2.300635
  validation accuracy:		13.04 %
Epoch 170 of 2000 took 0.163s
  training loss:		2.356313
  validation loss:		2.299959
  validation accuracy:		13.04 %
Epoch 171 of 2000 took 0.168s
  training loss:		2.354857
  validation loss:		2.299267
  validation accuracy:		13.04 %
Epoch 172 of 2000 took 0.163s
  training loss:		2.355685
  validation loss:		2.298668
  validation accuracy:		13.04 %
Epoch 173 of 2000 took 0.168s
  training loss:		2.355208
  validation loss:		2.298073
  validation accuracy:		13.04 %
Epoch 174 of 2000 took 0.163s
  training loss:		2.354006
  validation loss:		2.297509
  validation accuracy:		13.04 %
Epoch 175 of 2000 took 0.166s
  training loss:		2.353327
  validation loss:		2.296954
  validation accuracy:		13.04 %
Epoch 176 of 2000 took 0.164s
  training loss:		2.351658
  validation loss:		2.296191
  validation accuracy:		13.04 %
Epoch 177 of 2000 took 0.165s
  training loss:		2.351262
  validation loss:		2.295579
  validation accuracy:		13.04 %
Epoch 178 of 2000 took 0.167s
  training loss:		2.352226
  validation loss:		2.294945
  validation accuracy:		13.04 %
Epoch 179 of 2000 took 0.163s
  training loss:		2.350131
  validation loss:		2.294376
  validation accuracy:		13.04 %
Epoch 180 of 2000 took 0.168s
  training loss:		2.350446
  validation loss:		2.293802
  validation accuracy:		13.04 %
Epoch 181 of 2000 took 0.162s
  training loss:		2.349766
  validation loss:		2.293250
  validation accuracy:		13.04 %
Epoch 182 of 2000 took 0.164s
  training loss:		2.349852
  validation loss:		2.292847
  validation accuracy:		13.04 %
Epoch 183 of 2000 took 0.167s
  training loss:		2.348597
  validation loss:		2.292505
  validation accuracy:		13.04 %
Epoch 184 of 2000 took 0.147s
  training loss:		2.348006
  validation loss:		2.292139
  validation accuracy:		13.04 %
Epoch 185 of 2000 took 0.164s
  training loss:		2.347129
  validation loss:		2.291525
  validation accuracy:		13.04 %
Epoch 186 of 2000 took 0.168s
  training loss:		2.347289
  validation loss:		2.291028
  validation accuracy:		13.04 %
Epoch 187 of 2000 took 0.163s
  training loss:		2.346103
  validation loss:		2.290441
  validation accuracy:		13.04 %
Epoch 188 of 2000 took 0.168s
  training loss:		2.345901
  validation loss:		2.290056
  validation accuracy:		13.04 %
Epoch 189 of 2000 took 0.167s
  training loss:		2.344972
  validation loss:		2.289523
  validation accuracy:		13.04 %
Epoch 190 of 2000 took 0.168s
  training loss:		2.344450
  validation loss:		2.289078
  validation accuracy:		13.04 %
Epoch 191 of 2000 took 0.163s
  training loss:		2.344005
  validation loss:		2.288610
  validation accuracy:		13.04 %
Epoch 192 of 2000 took 0.166s
  training loss:		2.344080
  validation loss:		2.288115
  validation accuracy:		13.04 %
Epoch 193 of 2000 took 0.164s
  training loss:		2.344177
  validation loss:		2.287817
  validation accuracy:		13.04 %
Epoch 194 of 2000 took 0.164s
  training loss:		2.343039
  validation loss:		2.287424
  validation accuracy:		13.04 %
Epoch 195 of 2000 took 0.168s
  training loss:		2.343496
  validation loss:		2.287131
  validation accuracy:		13.04 %
Epoch 196 of 2000 took 0.163s
  training loss:		2.341920
  validation loss:		2.286764
  validation accuracy:		13.04 %
Epoch 197 of 2000 took 0.167s
  training loss:		2.340431
  validation loss:		2.286371
  validation accuracy:		13.04 %
Epoch 198 of 2000 took 0.163s
  training loss:		2.341562
  validation loss:		2.286010
  validation accuracy:		13.04 %
Epoch 199 of 2000 took 0.167s
  training loss:		2.340636
  validation loss:		2.285688
  validation accuracy:		13.04 %
Epoch 200 of 2000 took 0.163s
  training loss:		2.340466
  validation loss:		2.285363
  validation accuracy:		13.04 %
Epoch 201 of 2000 took 0.166s
  training loss:		2.339593
  validation loss:		2.284971
  validation accuracy:		13.04 %
Epoch 202 of 2000 took 0.164s
  training loss:		2.338917
  validation loss:		2.284551
  validation accuracy:		13.04 %
Epoch 203 of 2000 took 0.165s
  training loss:		2.339023
  validation loss:		2.284188
  validation accuracy:		13.04 %
Epoch 204 of 2000 took 0.168s
  training loss:		2.338504
  validation loss:		2.283805
  validation accuracy:		13.04 %
Epoch 205 of 2000 took 0.163s
  training loss:		2.337933
  validation loss:		2.283408
  validation accuracy:		13.04 %
Epoch 206 of 2000 took 0.167s
  training loss:		2.338212
  validation loss:		2.283050
  validation accuracy:		13.04 %
Epoch 207 of 2000 took 0.163s
  training loss:		2.337869
  validation loss:		2.282843
  validation accuracy:		13.04 %
Epoch 208 of 2000 took 0.164s
  training loss:		2.337834
  validation loss:		2.282573
  validation accuracy:		13.04 %
Epoch 209 of 2000 took 0.167s
  training loss:		2.337696
  validation loss:		2.282467
  validation accuracy:		13.04 %
Epoch 210 of 2000 took 0.164s
  training loss:		2.336051
  validation loss:		2.282127
  validation accuracy:		13.04 %
Epoch 211 of 2000 took 0.165s
  training loss:		2.336426
  validation loss:		2.281797
  validation accuracy:		13.04 %
Epoch 212 of 2000 took 0.167s
  training loss:		2.334312
  validation loss:		2.281459
  validation accuracy:		13.04 %
Epoch 213 of 2000 took 0.155s
  training loss:		2.334687
  validation loss:		2.281022
  validation accuracy:		13.04 %
Epoch 214 of 2000 took 0.167s
  training loss:		2.334229
  validation loss:		2.280579
  validation accuracy:		13.04 %
Epoch 215 of 2000 took 0.151s
  training loss:		2.334740
  validation loss:		2.280313
  validation accuracy:		13.04 %
Epoch 216 of 2000 took 0.166s
  training loss:		2.334030
  validation loss:		2.279994
  validation accuracy:		13.04 %
Epoch 217 of 2000 took 0.160s
  training loss:		2.333402
  validation loss:		2.279533
  validation accuracy:		13.04 %
Epoch 218 of 2000 took 0.150s
  training loss:		2.332586
  validation loss:		2.279137
  validation accuracy:		13.04 %
Epoch 219 of 2000 took 0.164s
  training loss:		2.333357
  validation loss:		2.278847
  validation accuracy:		13.04 %
Epoch 220 of 2000 took 0.163s
  training loss:		2.333546
  validation loss:		2.278677
  validation accuracy:		13.04 %
Epoch 221 of 2000 took 0.162s
  training loss:		2.331974
  validation loss:		2.278341
  validation accuracy:		13.04 %
Epoch 222 of 2000 took 0.166s
  training loss:		2.331649
  validation loss:		2.278169
  validation accuracy:		13.04 %
Epoch 223 of 2000 took 0.160s
  training loss:		2.331060
  validation loss:		2.277801
  validation accuracy:		13.04 %
Epoch 224 of 2000 took 0.165s
  training loss:		2.331894
  validation loss:		2.277588
  validation accuracy:		13.04 %
Epoch 225 of 2000 took 0.161s
  training loss:		2.331565
  validation loss:		2.277320
  validation accuracy:		13.04 %
Epoch 226 of 2000 took 0.165s
  training loss:		2.331330
  validation loss:		2.277055
  validation accuracy:		13.04 %
Epoch 227 of 2000 took 0.166s
  training loss:		2.330142
  validation loss:		2.276639
  validation accuracy:		13.04 %
Epoch 228 of 2000 took 0.160s
  training loss:		2.330297
  validation loss:		2.276315
  validation accuracy:		13.04 %
Epoch 229 of 2000 took 0.172s
  training loss:		2.329497
  validation loss:		2.275987
  validation accuracy:		13.04 %
Epoch 230 of 2000 took 0.165s
  training loss:		2.328638
  validation loss:		2.275743
  validation accuracy:		13.04 %
Epoch 231 of 2000 took 0.161s
  training loss:		2.329810
  validation loss:		2.275479
  validation accuracy:		13.04 %
Epoch 232 of 2000 took 0.153s
  training loss:		2.329123
  validation loss:		2.275139
  validation accuracy:		13.04 %
Epoch 233 of 2000 took 0.166s
  training loss:		2.328233
  validation loss:		2.274808
  validation accuracy:		13.04 %
Epoch 234 of 2000 took 0.160s
  training loss:		2.327048
  validation loss:		2.274514
  validation accuracy:		13.04 %
Epoch 235 of 2000 took 0.165s
  training loss:		2.328104
  validation loss:		2.274130
  validation accuracy:		13.04 %
Epoch 236 of 2000 took 0.161s
  training loss:		2.327612
  validation loss:		2.273996
  validation accuracy:		13.04 %
Epoch 237 of 2000 took 0.165s
  training loss:		2.327734
  validation loss:		2.273736
  validation accuracy:		13.04 %
Epoch 238 of 2000 took 0.167s
  training loss:		2.328101
  validation loss:		2.273450
  validation accuracy:		13.04 %
Epoch 239 of 2000 took 0.159s
  training loss:		2.327309
  validation loss:		2.273226
  validation accuracy:		13.04 %
Epoch 240 of 2000 took 0.166s
  training loss:		2.327898
  validation loss:		2.273156
  validation accuracy:		13.04 %
Epoch 241 of 2000 took 0.160s
  training loss:		2.326119
  validation loss:		2.272945
  validation accuracy:		12.93 %
Epoch 242 of 2000 took 0.165s
  training loss:		2.326915
  validation loss:		2.272773
  validation accuracy:		13.04 %
Epoch 243 of 2000 took 0.166s
  training loss:		2.326598
  validation loss:		2.272704
  validation accuracy:		13.04 %
Epoch 244 of 2000 took 0.159s
  training loss:		2.325018
  validation loss:		2.272419
  validation accuracy:		13.04 %
Epoch 245 of 2000 took 0.165s
  training loss:		2.324976
  validation loss:		2.272101
  validation accuracy:		13.04 %
Epoch 246 of 2000 took 0.161s
  training loss:		2.324503
  validation loss:		2.271836
  validation accuracy:		13.15 %
Epoch 247 of 2000 took 0.165s
  training loss:		2.325713
  validation loss:		2.271594
  validation accuracy:		13.04 %
Epoch 248 of 2000 took 0.162s
  training loss:		2.325163
  validation loss:		2.271274
  validation accuracy:		13.04 %
Epoch 249 of 2000 took 0.164s
  training loss:		2.324965
  validation loss:		2.271060
  validation accuracy:		13.04 %
Epoch 250 of 2000 took 0.167s
  training loss:		2.324101
  validation loss:		2.270959
  validation accuracy:		13.04 %
Epoch 251 of 2000 took 0.160s
  training loss:		2.324200
  validation loss:		2.270822
  validation accuracy:		13.04 %
Epoch 252 of 2000 took 0.165s
  training loss:		2.323997
  validation loss:		2.270652
  validation accuracy:		13.04 %
Epoch 253 of 2000 took 0.161s
  training loss:		2.323927
  validation loss:		2.270514
  validation accuracy:		13.04 %
Epoch 254 of 2000 took 0.165s
  training loss:		2.323521
  validation loss:		2.270314
  validation accuracy:		13.04 %
Epoch 255 of 2000 took 0.163s
  training loss:		2.324015
  validation loss:		2.270272
  validation accuracy:		13.04 %
Epoch 256 of 2000 took 0.163s
  training loss:		2.323530
  validation loss:		2.270118
  validation accuracy:		12.93 %
Epoch 257 of 2000 took 0.171s
  training loss:		2.323304
  validation loss:		2.269948
  validation accuracy:		13.04 %
Epoch 258 of 2000 took 0.160s
  training loss:		2.322217
  validation loss:		2.269796
  validation accuracy:		13.04 %
Epoch 259 of 2000 took 0.165s
  training loss:		2.323426
  validation loss:		2.269625
  validation accuracy:		13.04 %
Epoch 260 of 2000 took 0.161s
  training loss:		2.323021
  validation loss:		2.269561
  validation accuracy:		13.04 %
Epoch 261 of 2000 took 0.183s
  training loss:		2.322614
  validation loss:		2.269441
  validation accuracy:		13.04 %
Epoch 262 of 2000 took 0.179s
  training loss:		2.322000
  validation loss:		2.269387
  validation accuracy:		13.04 %
Epoch 263 of 2000 took 0.168s
  training loss:		2.321470
  validation loss:		2.269348
  validation accuracy:		12.93 %
Epoch 264 of 2000 took 0.180s
  training loss:		2.322330
  validation loss:		2.269103
  validation accuracy:		12.93 %
Epoch 265 of 2000 took 0.163s
  training loss:		2.321278
  validation loss:		2.268875
  validation accuracy:		13.04 %
Epoch 266 of 2000 took 0.166s
  training loss:		2.321335
  validation loss:		2.268532
  validation accuracy:		12.93 %
Epoch 267 of 2000 took 0.160s
  training loss:		2.320709
  validation loss:		2.268403
  validation accuracy:		12.93 %
Epoch 268 of 2000 took 0.180s
  training loss:		2.322039
  validation loss:		2.268375
  validation accuracy:		13.04 %
Epoch 269 of 2000 took 0.162s
  training loss:		2.320409
  validation loss:		2.268271
  validation accuracy:		12.93 %
Epoch 270 of 2000 took 0.165s
  training loss:		2.320331
  validation loss:		2.268073
  validation accuracy:		13.04 %
Epoch 271 of 2000 took 0.166s
  training loss:		2.319177
  validation loss:		2.267855
  validation accuracy:		13.04 %
Epoch 272 of 2000 took 0.160s
  training loss:		2.319038
  validation loss:		2.267400
  validation accuracy:		13.15 %
Epoch 273 of 2000 took 0.166s
  training loss:		2.319747
  validation loss:		2.266981
  validation accuracy:		12.93 %
Epoch 274 of 2000 took 0.160s
  training loss:		2.319826
  validation loss:		2.266873
  validation accuracy:		12.93 %
Epoch 275 of 2000 took 0.165s
  training loss:		2.319783
  validation loss:		2.266671
  validation accuracy:		12.93 %
Epoch 276 of 2000 took 0.161s
  training loss:		2.320144
  validation loss:		2.266509
  validation accuracy:		12.93 %
Epoch 277 of 2000 took 0.165s
  training loss:		2.319324
  validation loss:		2.266399
  validation accuracy:		12.93 %
Epoch 278 of 2000 took 0.167s
  training loss:		2.319540
  validation loss:		2.266366
  validation accuracy:		12.93 %
Epoch 279 of 2000 took 0.159s
  training loss:		2.319138
  validation loss:		2.266201
  validation accuracy:		12.50 %
Epoch 280 of 2000 took 0.184s
  training loss:		2.319300
  validation loss:		2.266166
  validation accuracy:		13.04 %
Epoch 281 of 2000 took 0.162s
  training loss:		2.318711
  validation loss:		2.266122
  validation accuracy:		13.04 %
Epoch 282 of 2000 took 0.168s
  training loss:		2.318502
  validation loss:		2.266026
  validation accuracy:		13.15 %
Epoch 283 of 2000 took 0.179s
  training loss:		2.317642
  validation loss:		2.265682
  validation accuracy:		13.04 %
Epoch 284 of 2000 took 0.217s
  training loss:		2.318528
  validation loss:		2.265648
  validation accuracy:		13.04 %
Epoch 285 of 2000 took 0.161s
  training loss:		2.318263
  validation loss:		2.265461
  validation accuracy:		13.04 %
Epoch 286 of 2000 took 0.165s
  training loss:		2.318592
  validation loss:		2.265352
  validation accuracy:		13.04 %
Epoch 287 of 2000 took 0.167s
  training loss:		2.317340
  validation loss:		2.265373
  validation accuracy:		13.04 %
Epoch 288 of 2000 took 0.159s
  training loss:		2.318028
  validation loss:		2.265403
  validation accuracy:		13.04 %
Epoch 289 of 2000 took 0.164s
  training loss:		2.317291
  validation loss:		2.265317
  validation accuracy:		13.04 %
Epoch 290 of 2000 took 0.162s
  training loss:		2.318054
  validation loss:		2.265178
  validation accuracy:		13.04 %
Epoch 291 of 2000 took 0.164s
  training loss:		2.317365
  validation loss:		2.265121
  validation accuracy:		13.04 %
Epoch 292 of 2000 took 0.167s
  training loss:		2.316678
  validation loss:		2.264988
  validation accuracy:		13.04 %
Epoch 293 of 2000 took 0.159s
  training loss:		2.317260
  validation loss:		2.264741
  validation accuracy:		13.04 %
Epoch 294 of 2000 took 0.165s
  training loss:		2.316748
  validation loss:		2.264547
  validation accuracy:		13.04 %
Epoch 295 of 2000 took 0.160s
  training loss:		2.316681
  validation loss:		2.264475
  validation accuracy:		13.04 %
Epoch 296 of 2000 took 0.165s
  training loss:		2.316242
  validation loss:		2.264397
  validation accuracy:		13.04 %
Epoch 297 of 2000 took 0.163s
  training loss:		2.316167
  validation loss:		2.264258
  validation accuracy:		13.04 %
Epoch 298 of 2000 took 0.162s
  training loss:		2.315759
  validation loss:		2.263950
  validation accuracy:		13.04 %
Epoch 299 of 2000 took 0.167s
  training loss:		2.316282
  validation loss:		2.263892
  validation accuracy:		13.04 %
Epoch 300 of 2000 took 0.159s
  training loss:		2.316404
  validation loss:		2.263822
  validation accuracy:		13.04 %
Epoch 301 of 2000 took 0.165s
  training loss:		2.315762
  validation loss:		2.263629
  validation accuracy:		13.04 %
Epoch 302 of 2000 took 0.161s
  training loss:		2.316464
  validation loss:		2.263603
  validation accuracy:		13.04 %
Epoch 303 of 2000 took 0.165s
  training loss:		2.316171
  validation loss:		2.263618
  validation accuracy:		13.04 %
Epoch 304 of 2000 took 0.164s
  training loss:		2.315229
  validation loss:		2.263463
  validation accuracy:		13.04 %
Epoch 305 of 2000 took 0.161s
  training loss:		2.315490
  validation loss:		2.263344
  validation accuracy:		13.04 %
Epoch 306 of 2000 took 0.167s
  training loss:		2.315376
  validation loss:		2.263395
  validation accuracy:		13.04 %
Epoch 307 of 2000 took 0.160s
  training loss:		2.315666
  validation loss:		2.263427
  validation accuracy:		13.04 %
Epoch 308 of 2000 took 0.165s
  training loss:		2.315450
  validation loss:		2.263175
  validation accuracy:		13.04 %
Epoch 309 of 2000 took 0.161s
  training loss:		2.315135
  validation loss:		2.263253
  validation accuracy:		13.04 %
Epoch 310 of 2000 took 0.165s
  training loss:		2.314382
  validation loss:		2.263101
  validation accuracy:		13.04 %
Epoch 311 of 2000 took 0.165s
  training loss:		2.315119
  validation loss:		2.262950
  validation accuracy:		13.04 %
Epoch 312 of 2000 took 0.161s
  training loss:		2.315186
  validation loss:		2.262960
  validation accuracy:		13.04 %
Epoch 313 of 2000 took 0.166s
  training loss:		2.315707
  validation loss:		2.263016
  validation accuracy:		13.04 %
Epoch 314 of 2000 took 0.160s
  training loss:		2.314991
  validation loss:		2.263136
  validation accuracy:		13.04 %
Epoch 315 of 2000 took 0.165s
  training loss:		2.313011
  validation loss:		2.262975
  validation accuracy:		13.04 %
Epoch 316 of 2000 took 0.166s
  training loss:		2.314132
  validation loss:		2.262809
  validation accuracy:		13.04 %
Epoch 317 of 2000 took 0.151s
  training loss:		2.313900
  validation loss:		2.262826
  validation accuracy:		13.04 %
Epoch 318 of 2000 took 0.166s
  training loss:		2.314003
  validation loss:		2.262566
  validation accuracy:		13.04 %
Epoch 319 of 2000 took 0.160s
  training loss:		2.314499
  validation loss:		2.262520
  validation accuracy:		13.04 %
Epoch 320 of 2000 took 0.165s
  training loss:		2.313485
  validation loss:		2.262379
  validation accuracy:		13.04 %
Epoch 321 of 2000 took 0.161s
  training loss:		2.312791
  validation loss:		2.262275
  validation accuracy:		13.04 %
Epoch 322 of 2000 took 0.165s
  training loss:		2.313053
  validation loss:		2.261803
  validation accuracy:		13.04 %
Epoch 323 of 2000 took 0.167s
  training loss:		2.314403
  validation loss:		2.261747
  validation accuracy:		13.04 %
Epoch 324 of 2000 took 0.158s
  training loss:		2.313415
  validation loss:		2.261709
  validation accuracy:		13.04 %
Epoch 325 of 2000 took 0.166s
  training loss:		2.313943
  validation loss:		2.261826
  validation accuracy:		13.04 %
Epoch 326 of 2000 took 0.160s
  training loss:		2.312606
  validation loss:		2.261679
  validation accuracy:		13.04 %
Epoch 327 of 2000 took 0.165s
  training loss:		2.312397
  validation loss:		2.261506
  validation accuracy:		13.04 %
Epoch 328 of 2000 took 0.161s
  training loss:		2.312464
  validation loss:		2.261212
  validation accuracy:		13.04 %
Epoch 329 of 2000 took 0.165s
  training loss:		2.312542
  validation loss:		2.261150
  validation accuracy:		13.04 %
Epoch 330 of 2000 took 0.167s
  training loss:		2.311957
  validation loss:		2.260914
  validation accuracy:		13.04 %
Epoch 331 of 2000 took 0.159s
  training loss:		2.312708
  validation loss:		2.260769
  validation accuracy:		13.04 %
Epoch 332 of 2000 took 0.166s
  training loss:		2.312506
  validation loss:		2.260561
  validation accuracy:		13.04 %
Epoch 333 of 2000 took 0.160s
  training loss:		2.312243
  validation loss:		2.260531
  validation accuracy:		13.04 %
Epoch 334 of 2000 took 0.165s
  training loss:		2.312374
  validation loss:		2.260432
  validation accuracy:		12.93 %
Epoch 335 of 2000 took 0.161s
  training loss:		2.313009
  validation loss:		2.260467
  validation accuracy:		12.93 %
Epoch 336 of 2000 took 0.165s
  training loss:		2.311870
  validation loss:		2.260524
  validation accuracy:		12.93 %
Epoch 337 of 2000 took 0.167s
  training loss:		2.311897
  validation loss:		2.260407
  validation accuracy:		12.93 %
Epoch 338 of 2000 took 0.163s
  training loss:		2.312779
  validation loss:		2.260263
  validation accuracy:		13.04 %
Epoch 339 of 2000 took 0.166s
  training loss:		2.311694
  validation loss:		2.260236
  validation accuracy:		12.93 %
Epoch 340 of 2000 took 0.160s
  training loss:		2.311287
  validation loss:		2.260198
  validation accuracy:		13.04 %
Epoch 341 of 2000 took 0.165s
  training loss:		2.311486
  validation loss:		2.259992
  validation accuracy:		13.04 %
Epoch 342 of 2000 took 0.160s
  training loss:		2.311354
  validation loss:		2.260014
  validation accuracy:		13.04 %
Epoch 343 of 2000 took 0.165s
  training loss:		2.311343
  validation loss:		2.259954
  validation accuracy:		13.04 %
Epoch 344 of 2000 took 0.167s
  training loss:		2.311816
  validation loss:		2.259870
  validation accuracy:		13.04 %
Epoch 345 of 2000 took 0.159s
  training loss:		2.310730
  validation loss:		2.259855
  validation accuracy:		13.04 %
Epoch 346 of 2000 took 0.166s
  training loss:		2.311079
  validation loss:		2.259779
  validation accuracy:		12.93 %
Epoch 347 of 2000 took 0.160s
  training loss:		2.311797
  validation loss:		2.259660
  validation accuracy:		12.93 %
Epoch 348 of 2000 took 0.165s
  training loss:		2.310996
  validation loss:		2.259605
  validation accuracy:		12.93 %
Epoch 349 of 2000 took 0.161s
  training loss:		2.310679
  validation loss:		2.259507
  validation accuracy:		13.04 %
Epoch 350 of 2000 took 0.165s
  training loss:		2.310141
  validation loss:		2.259382
  validation accuracy:		13.04 %
Epoch 351 of 2000 took 0.167s
  training loss:		2.310818
  validation loss:		2.259430
  validation accuracy:		13.04 %
Epoch 352 of 2000 took 0.158s
  training loss:		2.310201
  validation loss:		2.259195
  validation accuracy:		13.04 %
Epoch 353 of 2000 took 0.166s
  training loss:		2.310931
  validation loss:		2.259104
  validation accuracy:		13.04 %
Epoch 354 of 2000 took 0.140s
  training loss:		2.309962
  validation loss:		2.258939
  validation accuracy:		13.04 %
Epoch 355 of 2000 took 0.161s
  training loss:		2.310740
  validation loss:		2.258883
  validation accuracy:		13.04 %
Epoch 356 of 2000 took 0.167s
  training loss:		2.309866
  validation loss:		2.258796
  validation accuracy:		13.04 %
Epoch 357 of 2000 took 0.134s
  training loss:		2.310600
  validation loss:		2.258764
  validation accuracy:		13.04 %
Epoch 358 of 2000 took 0.166s
  training loss:		2.309924
  validation loss:		2.258745
  validation accuracy:		13.04 %
Epoch 359 of 2000 took 0.138s
  training loss:		2.309241
  validation loss:		2.258641
  validation accuracy:		13.04 %
Epoch 360 of 2000 took 0.162s
  training loss:		2.308854
  validation loss:		2.258280
  validation accuracy:		13.04 %
Epoch 361 of 2000 took 0.167s
  training loss:		2.309391
  validation loss:		2.258072
  validation accuracy:		13.04 %
Epoch 362 of 2000 took 0.135s
  training loss:		2.309753
  validation loss:		2.257745
  validation accuracy:		13.04 %
Epoch 363 of 2000 took 0.166s
  training loss:		2.308699
  validation loss:		2.257580
  validation accuracy:		13.04 %
Epoch 364 of 2000 took 0.136s
  training loss:		2.310216
  validation loss:		2.257574
  validation accuracy:		13.04 %
Epoch 365 of 2000 took 0.164s
  training loss:		2.309932
  validation loss:		2.257658
  validation accuracy:		13.04 %
Epoch 366 of 2000 took 0.165s
  training loss:		2.308903
  validation loss:		2.257450
  validation accuracy:		13.04 %
Epoch 367 of 2000 took 0.136s
  training loss:		2.309788
  validation loss:		2.257547
  validation accuracy:		13.04 %
Epoch 368 of 2000 took 0.166s
  training loss:		2.309825
  validation loss:		2.257494
  validation accuracy:		13.04 %
Epoch 369 of 2000 took 0.136s
  training loss:		2.307772
  validation loss:		2.257356
  validation accuracy:		13.04 %
Epoch 370 of 2000 took 0.165s
  training loss:		2.309531
  validation loss:		2.257198
  validation accuracy:		13.04 %
Epoch 371 of 2000 took 0.163s
  training loss:		2.308356
  validation loss:		2.257125
  validation accuracy:		13.04 %
Epoch 372 of 2000 took 0.138s
  training loss:		2.308895
  validation loss:		2.256942
  validation accuracy:		13.04 %
Epoch 373 of 2000 took 0.166s
  training loss:		2.309530
  validation loss:		2.256900
  validation accuracy:		13.04 %
Epoch 374 of 2000 took 0.136s
  training loss:		2.309690
  validation loss:		2.257007
  validation accuracy:		13.04 %
Epoch 375 of 2000 took 0.165s
  training loss:		2.308228
  validation loss:		2.257046
  validation accuracy:		13.04 %
Epoch 376 of 2000 took 0.162s
  training loss:		2.309855
  validation loss:		2.257119
  validation accuracy:		13.04 %
Epoch 377 of 2000 took 0.140s
  training loss:		2.309794
  validation loss:		2.257248
  validation accuracy:		13.04 %
Epoch 378 of 2000 took 0.166s
  training loss:		2.308734
  validation loss:		2.257309
  validation accuracy:		13.04 %
Epoch 379 of 2000 took 0.136s
  training loss:		2.308116
  validation loss:		2.257273
  validation accuracy:		13.04 %
Epoch 380 of 2000 took 0.165s
  training loss:		2.308377
  validation loss:		2.257290
  validation accuracy:		13.04 %
Epoch 381 of 2000 took 0.160s
  training loss:		2.309111
  validation loss:		2.257351
  validation accuracy:		13.04 %
Epoch 382 of 2000 took 0.142s
  training loss:		2.309017
  validation loss:		2.257348
  validation accuracy:		13.04 %
Epoch 383 of 2000 took 0.166s
  training loss:		2.308216
  validation loss:		2.257286
  validation accuracy:		13.04 %
Epoch 384 of 2000 took 0.136s
  training loss:		2.309233
  validation loss:		2.257362
  validation accuracy:		13.04 %
Epoch 385 of 2000 took 0.165s
  training loss:		2.307394
  validation loss:		2.257154
  validation accuracy:		13.04 %
Epoch 386 of 2000 took 0.158s
  training loss:		2.307686
  validation loss:		2.257156
  validation accuracy:		13.04 %
Epoch 387 of 2000 took 0.143s
  training loss:		2.309025
  validation loss:		2.257197
  validation accuracy:		13.04 %
Epoch 388 of 2000 took 0.167s
  training loss:		2.309166
  validation loss:		2.257245
  validation accuracy:		13.04 %
Epoch 389 of 2000 took 0.135s
  training loss:		2.307830
  validation loss:		2.257260
  validation accuracy:		12.93 %
Epoch 390 of 2000 took 0.165s
  training loss:		2.307591
  validation loss:		2.257103
  validation accuracy:		12.93 %
Epoch 391 of 2000 took 0.157s
  training loss:		2.307307
  validation loss:		2.256821
  validation accuracy:		12.93 %
Epoch 392 of 2000 took 0.145s
  training loss:		2.307356
  validation loss:		2.256591
  validation accuracy:		12.93 %
Epoch 393 of 2000 took 0.167s
  training loss:		2.306983
  validation loss:		2.256345
  validation accuracy:		12.93 %
Epoch 394 of 2000 took 0.129s
  training loss:		2.308602
  validation loss:		2.256281
  validation accuracy:		12.93 %
Epoch 395 of 2000 took 0.165s
  training loss:		2.308586
  validation loss:		2.256321
  validation accuracy:		12.93 %
Epoch 396 of 2000 took 0.163s
  training loss:		2.307922
  validation loss:		2.256441
  validation accuracy:		13.04 %
Epoch 397 of 2000 took 0.137s
  training loss:		2.307781
  validation loss:		2.256478
  validation accuracy:		13.04 %
Epoch 398 of 2000 took 0.166s
  training loss:		2.307571
  validation loss:		2.256445
  validation accuracy:		13.04 %
Epoch 399 of 2000 took 0.136s
  training loss:		2.307742
  validation loss:		2.256379
  validation accuracy:		13.04 %
Epoch 400 of 2000 took 0.165s
  training loss:		2.306160
  validation loss:		2.256261
  validation accuracy:		13.04 %
Epoch 401 of 2000 took 0.162s
  training loss:		2.307848
  validation loss:		2.256178
  validation accuracy:		13.04 %
Epoch 402 of 2000 took 0.140s
  training loss:		2.308386
  validation loss:		2.256320
  validation accuracy:		13.04 %
Epoch 403 of 2000 took 0.166s
  training loss:		2.306722
  validation loss:		2.256167
  validation accuracy:		12.93 %
Epoch 404 of 2000 took 0.136s
  training loss:		2.307733
  validation loss:		2.256088
  validation accuracy:		12.93 %
Epoch 405 of 2000 took 0.165s
  training loss:		2.306777
  validation loss:		2.256087
  validation accuracy:		12.93 %
Epoch 406 of 2000 took 0.158s
  training loss:		2.306856
  validation loss:		2.256028
  validation accuracy:		13.04 %
Epoch 407 of 2000 took 0.143s
  training loss:		2.307232
  validation loss:		2.255963
  validation accuracy:		13.04 %
Epoch 408 of 2000 took 0.167s
  training loss:		2.307130
  validation loss:		2.255847
  validation accuracy:		13.04 %
Epoch 409 of 2000 took 0.135s
  training loss:		2.306991
  validation loss:		2.255988
  validation accuracy:		13.04 %
Epoch 410 of 2000 took 0.165s
  training loss:		2.307174
  validation loss:		2.255971
  validation accuracy:		13.04 %
Epoch 411 of 2000 took 0.156s
  training loss:		2.306869
  validation loss:		2.255981
  validation accuracy:		13.04 %
Epoch 412 of 2000 took 0.146s
  training loss:		2.306895
  validation loss:		2.256011
  validation accuracy:		13.04 %
Epoch 413 of 2000 took 0.167s
  training loss:		2.306531
  validation loss:		2.255912
  validation accuracy:		13.04 %
Epoch 414 of 2000 took 0.136s
  training loss:		2.306955
  validation loss:		2.255692
  validation accuracy:		13.04 %
Epoch 415 of 2000 took 0.165s
  training loss:		2.306654
  validation loss:		2.255602
  validation accuracy:		13.04 %
Epoch 416 of 2000 took 0.152s
  training loss:		2.306518
  validation loss:		2.255632
  validation accuracy:		13.04 %
Epoch 417 of 2000 took 0.149s
  training loss:		2.306116
  validation loss:		2.255506
  validation accuracy:		13.04 %
Epoch 418 of 2000 took 0.167s
  training loss:		2.306002
  validation loss:		2.255484
  validation accuracy:		13.04 %
Epoch 419 of 2000 took 0.135s
  training loss:		2.306057
  validation loss:		2.255351
  validation accuracy:		13.04 %
Epoch 420 of 2000 took 0.165s
  training loss:		2.305533
  validation loss:		2.255106
  validation accuracy:		13.04 %
Epoch 421 of 2000 took 0.149s
  training loss:		2.306656
  validation loss:		2.255178
  validation accuracy:		13.04 %
Epoch 422 of 2000 took 0.153s
  training loss:		2.307005
  validation loss:		2.255233
  validation accuracy:		13.04 %
Epoch 423 of 2000 took 0.167s
  training loss:		2.305794
  validation loss:		2.255234
  validation accuracy:		13.04 %
Epoch 424 of 2000 took 0.135s
  training loss:		2.306756
  validation loss:		2.255203
  validation accuracy:		13.04 %
Epoch 425 of 2000 took 0.165s
  training loss:		2.305892
  validation loss:		2.255156
  validation accuracy:		13.04 %
Epoch 426 of 2000 took 0.145s
  training loss:		2.306043
  validation loss:		2.255064
  validation accuracy:		13.04 %
Epoch 427 of 2000 took 0.156s
  training loss:		2.305080
  validation loss:		2.254860
  validation accuracy:		13.04 %
Epoch 428 of 2000 took 0.167s
  training loss:		2.305366
  validation loss:		2.254811
  validation accuracy:		12.93 %
Epoch 429 of 2000 took 0.136s
  training loss:		2.305723
  validation loss:		2.254868
  validation accuracy:		13.04 %
Epoch 430 of 2000 took 0.165s
  training loss:		2.305736
  validation loss:		2.254812
  validation accuracy:		13.04 %
Epoch 431 of 2000 took 0.141s
  training loss:		2.305833
  validation loss:		2.254839
  validation accuracy:		13.04 %
Epoch 432 of 2000 took 0.160s
  training loss:		2.305640
  validation loss:		2.254631
  validation accuracy:		13.04 %
Epoch 433 of 2000 took 0.167s
  training loss:		2.305368
  validation loss:		2.254531
  validation accuracy:		13.04 %
Epoch 434 of 2000 took 0.135s
  training loss:		2.305998
  validation loss:		2.254437
  validation accuracy:		13.04 %
Epoch 435 of 2000 took 0.166s
  training loss:		2.306521
  validation loss:		2.254715
  validation accuracy:		13.04 %
Epoch 436 of 2000 took 0.138s
  training loss:		2.306239
  validation loss:		2.254799
  validation accuracy:		13.04 %
Epoch 437 of 2000 took 0.163s
  training loss:		2.306173
  validation loss:		2.254793
  validation accuracy:		13.04 %
Epoch 438 of 2000 took 0.166s
  training loss:		2.304970
  validation loss:		2.254872
  validation accuracy:		13.04 %
Epoch 439 of 2000 took 0.136s
  training loss:		2.304428
  validation loss:		2.254675
  validation accuracy:		13.04 %
Epoch 440 of 2000 took 0.166s
  training loss:		2.305877
  validation loss:		2.254344
  validation accuracy:		13.04 %
Epoch 441 of 2000 took 0.139s
  training loss:		2.305221
  validation loss:		2.254482
  validation accuracy:		13.04 %
Epoch 442 of 2000 took 0.165s
  training loss:		2.305705
  validation loss:		2.254511
  validation accuracy:		13.04 %
Epoch 443 of 2000 took 0.161s
  training loss:		2.305316
  validation loss:		2.254347
  validation accuracy:		13.04 %
Epoch 444 of 2000 took 0.141s
  training loss:		2.305896
  validation loss:		2.254284
  validation accuracy:		13.04 %
Epoch 445 of 2000 took 0.166s
  training loss:		2.304525
  validation loss:		2.254227
  validation accuracy:		13.04 %
Epoch 446 of 2000 took 0.135s
  training loss:		2.305327
  validation loss:		2.254216
  validation accuracy:		13.04 %
Epoch 447 of 2000 took 0.165s
  training loss:		2.304992
  validation loss:		2.254173
  validation accuracy:		13.04 %
Epoch 448 of 2000 took 0.158s
  training loss:		2.305338
  validation loss:		2.254153
  validation accuracy:		13.04 %
Epoch 449 of 2000 took 0.144s
  training loss:		2.304756
  validation loss:		2.253976
  validation accuracy:		13.04 %
Epoch 450 of 2000 took 0.167s
  training loss:		2.304571
  validation loss:		2.253817
  validation accuracy:		13.04 %
Epoch 451 of 2000 took 0.135s
  training loss:		2.304659
  validation loss:		2.253806
  validation accuracy:		13.04 %
Epoch 452 of 2000 took 0.165s
  training loss:		2.305360
  validation loss:		2.253896
  validation accuracy:		13.04 %
Epoch 453 of 2000 took 0.155s
  training loss:		2.305428
  validation loss:		2.253865
  validation accuracy:		13.04 %
Epoch 454 of 2000 took 0.148s
  training loss:		2.305425
  validation loss:		2.253882
  validation accuracy:		13.04 %
Epoch 455 of 2000 took 0.167s
  training loss:		2.304048
  validation loss:		2.253757
  validation accuracy:		13.04 %
Epoch 456 of 2000 took 0.135s
  training loss:		2.305111
  validation loss:		2.253724
  validation accuracy:		13.04 %
Epoch 457 of 2000 took 0.165s
  training loss:		2.304333
  validation loss:		2.253627
  validation accuracy:		13.04 %
Epoch 458 of 2000 took 0.155s
  training loss:		2.305385
  validation loss:		2.253687
  validation accuracy:		13.04 %
Epoch 459 of 2000 took 0.147s
  training loss:		2.304979
  validation loss:		2.253709
  validation accuracy:		13.04 %
Epoch 460 of 2000 took 0.167s
  training loss:		2.304943
  validation loss:		2.253844
  validation accuracy:		13.04 %
Epoch 461 of 2000 took 0.135s
  training loss:		2.304869
  validation loss:		2.253848
  validation accuracy:		13.04 %
Epoch 462 of 2000 took 0.165s
  training loss:		2.304937
  validation loss:		2.253788
  validation accuracy:		12.93 %
Epoch 463 of 2000 took 0.152s
  training loss:		2.304332
  validation loss:		2.253720
  validation accuracy:		12.93 %
Epoch 464 of 2000 took 0.151s
  training loss:		2.304578
  validation loss:		2.253836
  validation accuracy:		12.93 %
Epoch 465 of 2000 took 0.167s
  training loss:		2.303894
  validation loss:		2.253750
  validation accuracy:		12.93 %
Epoch 466 of 2000 took 0.135s
  training loss:		2.304494
  validation loss:		2.253732
  validation accuracy:		12.93 %
Epoch 467 of 2000 took 0.165s
  training loss:		2.303565
  validation loss:		2.253819
  validation accuracy:		12.93 %
Epoch 468 of 2000 took 0.148s
  training loss:		2.304413
  validation loss:		2.253682
  validation accuracy:		12.93 %
Epoch 469 of 2000 took 0.154s
  training loss:		2.304413
  validation loss:		2.253632
  validation accuracy:		12.93 %
Epoch 470 of 2000 took 0.167s
  training loss:		2.303788
  validation loss:		2.253378
  validation accuracy:		12.93 %
Epoch 471 of 2000 took 0.134s
  training loss:		2.303989
  validation loss:		2.253173
  validation accuracy:		12.93 %
Epoch 472 of 2000 took 0.165s
  training loss:		2.303874
  validation loss:		2.253220
  validation accuracy:		12.93 %
Epoch 473 of 2000 took 0.144s
  training loss:		2.304805
  validation loss:		2.253246
  validation accuracy:		12.93 %
Epoch 474 of 2000 took 0.158s
  training loss:		2.303783
  validation loss:		2.253121
  validation accuracy:		12.93 %
Epoch 475 of 2000 took 0.167s
  training loss:		2.303599
  validation loss:		2.253043
  validation accuracy:		12.93 %
Epoch 476 of 2000 took 0.135s
  training loss:		2.303276
  validation loss:		2.252921
  validation accuracy:		12.93 %
Epoch 477 of 2000 took 0.165s
  training loss:		2.303651
  validation loss:		2.252807
  validation accuracy:		12.93 %
Epoch 478 of 2000 took 0.146s
  training loss:		2.304595
  validation loss:		2.252791
  validation accuracy:		13.04 %
Epoch 479 of 2000 took 0.156s
  training loss:		2.303294
  validation loss:		2.252902
  validation accuracy:		13.04 %
Epoch 480 of 2000 took 0.167s
  training loss:		2.304206
  validation loss:		2.252885
  validation accuracy:		12.93 %
Epoch 481 of 2000 took 0.134s
  training loss:		2.303817
  validation loss:		2.252801
  validation accuracy:		12.93 %
Epoch 482 of 2000 took 0.165s
  training loss:		2.303822
  validation loss:		2.252775
  validation accuracy:		12.93 %
Epoch 483 of 2000 took 0.142s
  training loss:		2.303666
  validation loss:		2.252757
  validation accuracy:		12.93 %
Epoch 484 of 2000 took 0.160s
  training loss:		2.304759
  validation loss:		2.252702
  validation accuracy:		12.93 %
Epoch 485 of 2000 took 0.167s
  training loss:		2.303189
  validation loss:		2.252784
  validation accuracy:		12.93 %
Epoch 486 of 2000 took 0.134s
  training loss:		2.303606
  validation loss:		2.252650
  validation accuracy:		12.93 %
Epoch 487 of 2000 took 0.166s
  training loss:		2.303395
  validation loss:		2.252559
  validation accuracy:		13.04 %
Epoch 488 of 2000 took 0.138s
  training loss:		2.303665
  validation loss:		2.252436
  validation accuracy:		13.04 %
Epoch 489 of 2000 took 0.163s
  training loss:		2.303063
  validation loss:		2.252307
  validation accuracy:		13.04 %
Epoch 490 of 2000 took 0.165s
  training loss:		2.303164
  validation loss:		2.252320
  validation accuracy:		13.04 %
Epoch 491 of 2000 took 0.136s
  training loss:		2.303665
  validation loss:		2.252280
  validation accuracy:		13.04 %
Epoch 492 of 2000 took 0.166s
  training loss:		2.303165
  validation loss:		2.252260
  validation accuracy:		13.04 %
Epoch 493 of 2000 took 0.136s
  training loss:		2.303309
  validation loss:		2.252187
  validation accuracy:		13.04 %
Epoch 494 of 2000 took 0.165s
  training loss:		2.304284
  validation loss:		2.252122
  validation accuracy:		13.04 %
Epoch 495 of 2000 took 0.167s
  training loss:		2.303908
  validation loss:		2.252364
  validation accuracy:		13.04 %
Epoch 496 of 2000 took 0.134s
  training loss:		2.303177
  validation loss:		2.252458
  validation accuracy:		13.04 %
Epoch 497 of 2000 took 0.166s
  training loss:		2.303092
  validation loss:		2.252421
  validation accuracy:		13.04 %
Epoch 498 of 2000 took 0.136s
  training loss:		2.304401
  validation loss:		2.252510
  validation accuracy:		13.04 %
Epoch 499 of 2000 took 0.165s
  training loss:		2.303529
  validation loss:		2.252774
  validation accuracy:		13.04 %
Epoch 500 of 2000 took 0.164s
  training loss:		2.302915
  validation loss:		2.252710
  validation accuracy:		12.93 %
Epoch 501 of 2000 took 0.137s
  training loss:		2.302953
  validation loss:		2.252542
  validation accuracy:		13.04 %
Epoch 502 of 2000 took 0.166s
  training loss:		2.303973
  validation loss:		2.252664
  validation accuracy:		12.93 %
Epoch 503 of 2000 took 0.136s
  training loss:		2.303488
  validation loss:		2.252703
  validation accuracy:		12.93 %
Epoch 504 of 2000 took 0.165s
  training loss:		2.302584
  validation loss:		2.252672
  validation accuracy:		12.93 %
Epoch 505 of 2000 took 0.161s
  training loss:		2.302406
  validation loss:		2.252402
  validation accuracy:		12.93 %
Epoch 506 of 2000 took 0.140s
  training loss:		2.302619
  validation loss:		2.252215
  validation accuracy:		12.93 %
Epoch 507 of 2000 took 0.166s
  training loss:		2.303329
  validation loss:		2.252241
  validation accuracy:		12.93 %
Epoch 508 of 2000 took 0.136s
  training loss:		2.303609
  validation loss:		2.252280
  validation accuracy:		12.93 %
Epoch 509 of 2000 took 0.165s
  training loss:		2.302069
  validation loss:		2.252143
  validation accuracy:		12.93 %
Epoch 510 of 2000 took 0.158s
  training loss:		2.303668
  validation loss:		2.252187
  validation accuracy:		13.04 %
Epoch 511 of 2000 took 0.143s
  training loss:		2.302312
  validation loss:		2.252121
  validation accuracy:		13.04 %
Epoch 512 of 2000 took 0.167s
  training loss:		2.303305
  validation loss:		2.252016
  validation accuracy:		13.04 %
Epoch 513 of 2000 took 0.136s
  training loss:		2.303578
  validation loss:		2.252291
  validation accuracy:		13.04 %
Epoch 514 of 2000 took 0.165s
  training loss:		2.302739
  validation loss:		2.252290
  validation accuracy:		13.04 %
Epoch 515 of 2000 took 0.155s
  training loss:		2.303002
  validation loss:		2.252144
  validation accuracy:		13.04 %
Epoch 516 of 2000 took 0.147s
  training loss:		2.302684
  validation loss:		2.252095
  validation accuracy:		13.04 %
Epoch 517 of 2000 took 0.167s
  training loss:		2.302434
  validation loss:		2.252105
  validation accuracy:		12.93 %
Epoch 518 of 2000 took 0.136s
  training loss:		2.302938
  validation loss:		2.252069
  validation accuracy:		13.04 %
Epoch 519 of 2000 took 0.165s
  training loss:		2.302705
  validation loss:		2.252111
  validation accuracy:		12.93 %
Epoch 520 of 2000 took 0.151s
  training loss:		2.302554
  validation loss:		2.252001
  validation accuracy:		12.93 %
Epoch 521 of 2000 took 0.151s
  training loss:		2.302775
  validation loss:		2.251901
  validation accuracy:		12.93 %
Epoch 522 of 2000 took 0.167s
  training loss:		2.302558
  validation loss:		2.251953
  validation accuracy:		13.04 %
Epoch 523 of 2000 took 0.135s
  training loss:		2.303204
  validation loss:		2.251737
  validation accuracy:		12.93 %
Epoch 524 of 2000 took 0.165s
  training loss:		2.303026
  validation loss:		2.251776
  validation accuracy:		12.93 %
Epoch 525 of 2000 took 0.148s
  training loss:		2.302525
  validation loss:		2.251891
  validation accuracy:		12.93 %
Epoch 526 of 2000 took 0.154s
  training loss:		2.302875
  validation loss:		2.252052
  validation accuracy:		17.07 %
Epoch 527 of 2000 took 0.167s
  training loss:		2.302184
  validation loss:		2.251984
  validation accuracy:		12.93 %
Epoch 528 of 2000 took 0.135s
  training loss:		2.301666
  validation loss:		2.251818
  validation accuracy:		12.93 %
Epoch 529 of 2000 took 0.165s
  training loss:		2.302686
  validation loss:		2.251699
  validation accuracy:		12.93 %
Epoch 530 of 2000 took 0.142s
  training loss:		2.302747
  validation loss:		2.251705
  validation accuracy:		12.93 %
Epoch 531 of 2000 took 0.159s
  training loss:		2.302191
  validation loss:		2.251530
  validation accuracy:		12.93 %
Epoch 532 of 2000 took 0.167s
  training loss:		2.301994
  validation loss:		2.251522
  validation accuracy:		12.93 %
Epoch 533 of 2000 took 0.135s
  training loss:		2.302550
  validation loss:		2.251406
  validation accuracy:		13.04 %
Epoch 534 of 2000 took 0.166s
  training loss:		2.301863
  validation loss:		2.251358
  validation accuracy:		13.04 %
Epoch 535 of 2000 took 0.138s
  training loss:		2.303054
  validation loss:		2.251158
  validation accuracy:		13.04 %
Epoch 536 of 2000 took 0.163s
  training loss:		2.302245
  validation loss:		2.251159
  validation accuracy:		13.04 %
Epoch 537 of 2000 took 0.164s
  training loss:		2.302454
  validation loss:		2.251374
  validation accuracy:		13.04 %
Epoch 538 of 2000 took 0.138s
  training loss:		2.302629
  validation loss:		2.251617
  validation accuracy:		13.04 %
Epoch 539 of 2000 took 0.166s
  training loss:		2.301596
  validation loss:		2.251537
  validation accuracy:		13.04 %
Epoch 540 of 2000 took 0.135s
  training loss:		2.301930
  validation loss:		2.251422
  validation accuracy:		13.04 %
Epoch 541 of 2000 took 0.165s
  training loss:		2.301682
  validation loss:		2.251306
  validation accuracy:		13.04 %
Epoch 542 of 2000 took 0.162s
  training loss:		2.301600
  validation loss:		2.251261
  validation accuracy:		13.04 %
Epoch 543 of 2000 took 0.141s
  training loss:		2.301840
  validation loss:		2.251141
  validation accuracy:		13.04 %
Epoch 544 of 2000 took 0.166s
  training loss:		2.302278
  validation loss:		2.251188
  validation accuracy:		13.04 %
Epoch 545 of 2000 took 0.135s
  training loss:		2.301560
  validation loss:		2.251054
  validation accuracy:		13.04 %
Epoch 546 of 2000 took 0.165s
  training loss:		2.302181
  validation loss:		2.251032
  validation accuracy:		13.04 %
Epoch 547 of 2000 took 0.158s
  training loss:		2.302067
  validation loss:		2.250987
  validation accuracy:		12.93 %
Epoch 548 of 2000 took 0.144s
  training loss:		2.301898
  validation loss:		2.250945
  validation accuracy:		13.04 %
Epoch 549 of 2000 took 0.167s
  training loss:		2.300633
  validation loss:		2.250880
  validation accuracy:		13.04 %
Epoch 550 of 2000 took 0.135s
  training loss:		2.301525
  validation loss:		2.250711
  validation accuracy:		13.04 %
Epoch 551 of 2000 took 0.165s
  training loss:		2.302107
  validation loss:		2.250633
  validation accuracy:		12.93 %
Epoch 552 of 2000 took 0.156s
  training loss:		2.302594
  validation loss:		2.250775
  validation accuracy:		12.93 %
Epoch 553 of 2000 took 0.146s
  training loss:		2.301566
  validation loss:		2.250706
  validation accuracy:		13.04 %
Epoch 554 of 2000 took 0.167s
  training loss:		2.302490
  validation loss:		2.250696
  validation accuracy:		13.04 %
Epoch 555 of 2000 took 0.134s
  training loss:		2.301514
  validation loss:		2.250876
  validation accuracy:		13.04 %
Epoch 556 of 2000 took 0.165s
  training loss:		2.302361
  validation loss:		2.250855
  validation accuracy:		13.04 %
Epoch 557 of 2000 took 0.154s
  training loss:		2.302612
  validation loss:		2.251106
  validation accuracy:		13.04 %
Epoch 558 of 2000 took 0.149s
  training loss:		2.301626
  validation loss:		2.251068
  validation accuracy:		13.04 %
Epoch 559 of 2000 took 0.167s
  training loss:		2.302279
  validation loss:		2.251271
  validation accuracy:		13.04 %
Epoch 560 of 2000 took 0.135s
  training loss:		2.301601
  validation loss:		2.251177
  validation accuracy:		13.04 %
Epoch 561 of 2000 took 0.165s
  training loss:		2.301517
  validation loss:		2.251155
  validation accuracy:		13.04 %
Epoch 562 of 2000 took 0.150s
  training loss:		2.301121
  validation loss:		2.251092
  validation accuracy:		12.93 %
Epoch 563 of 2000 took 0.152s
  training loss:		2.301606
  validation loss:		2.251004
  validation accuracy:		13.04 %
Epoch 564 of 2000 took 0.167s
  training loss:		2.302290
  validation loss:		2.250910
  validation accuracy:		12.93 %
Epoch 565 of 2000 took 0.134s
  training loss:		2.301039
  validation loss:		2.250946
  validation accuracy:		13.04 %
Epoch 566 of 2000 took 0.169s
  training loss:		2.300892
  validation loss:		2.250715
  validation accuracy:		13.04 %
Epoch 567 of 2000 took 0.146s
  training loss:		2.302130
  validation loss:		2.250996
  validation accuracy:		13.04 %
Epoch 568 of 2000 took 0.155s
  training loss:		2.301873
  validation loss:		2.250980
  validation accuracy:		13.04 %
Epoch 569 of 2000 took 0.167s
  training loss:		2.301014
  validation loss:		2.250792
  validation accuracy:		13.04 %
Epoch 570 of 2000 took 0.134s
  training loss:		2.301046
  validation loss:		2.250814
  validation accuracy:		13.04 %
Epoch 571 of 2000 took 0.165s
  training loss:		2.301081
  validation loss:		2.250698
  validation accuracy:		13.04 %
Epoch 572 of 2000 took 0.145s
  training loss:		2.301464
  validation loss:		2.250661
  validation accuracy:		12.93 %
Epoch 573 of 2000 took 0.157s
  training loss:		2.301270
  validation loss:		2.250695
  validation accuracy:		12.93 %
Epoch 574 of 2000 took 0.167s
  training loss:		2.302430
  validation loss:		2.250843
  validation accuracy:		12.93 %
Epoch 575 of 2000 took 0.134s
  training loss:		2.302002
  validation loss:		2.250867
  validation accuracy:		12.93 %
Epoch 576 of 2000 took 0.165s
  training loss:		2.301022
  validation loss:		2.250858
  validation accuracy:		12.93 %
Epoch 577 of 2000 took 0.145s
  training loss:		2.301499
  validation loss:		2.250809
  validation accuracy:		12.93 %
Epoch 578 of 2000 took 0.157s
  training loss:		2.302172
  validation loss:		2.250781
  validation accuracy:		12.93 %
Epoch 579 of 2000 took 0.167s
  training loss:		2.302238
  validation loss:		2.250942
  validation accuracy:		13.04 %
Epoch 580 of 2000 took 0.134s
  training loss:		2.301638
  validation loss:		2.251030
  validation accuracy:		13.04 %
Epoch 581 of 2000 took 0.165s
  training loss:		2.301373
  validation loss:		2.251098
  validation accuracy:		13.04 %
Epoch 582 of 2000 took 0.141s
  training loss:		2.301644
  validation loss:		2.251181
  validation accuracy:		13.04 %
Epoch 583 of 2000 took 0.160s
  training loss:		2.300773
  validation loss:		2.251124
  validation accuracy:		13.04 %
Epoch 584 of 2000 took 0.167s
  training loss:		2.301777
  validation loss:		2.251120
  validation accuracy:		13.04 %
Epoch 585 of 2000 took 0.134s
  training loss:		2.300447
  validation loss:		2.251069
  validation accuracy:		13.04 %
Epoch 586 of 2000 took 0.166s
  training loss:		2.301791
  validation loss:		2.250832
  validation accuracy:		13.04 %
Epoch 587 of 2000 took 0.138s
  training loss:		2.300281
  validation loss:		2.250962
  validation accuracy:		13.04 %
Epoch 588 of 2000 took 0.163s
  training loss:		2.301127
  validation loss:		2.250885
  validation accuracy:		13.04 %
Epoch 589 of 2000 took 0.163s
  training loss:		2.301056
  validation loss:		2.250859
  validation accuracy:		13.04 %
Epoch 590 of 2000 took 0.138s
  training loss:		2.301197
  validation loss:		2.250812
  validation accuracy:		13.04 %
Epoch 591 of 2000 took 0.166s
  training loss:		2.301432
  validation loss:		2.250774
  validation accuracy:		13.04 %
Epoch 592 of 2000 took 0.135s
  training loss:		2.300873
  validation loss:		2.250608
  validation accuracy:		13.04 %
Epoch 593 of 2000 took 0.165s
  training loss:		2.301746
  validation loss:		2.250684
  validation accuracy:		13.04 %
Epoch 594 of 2000 took 0.165s
  training loss:		2.300841
  validation loss:		2.250392
  validation accuracy:		13.04 %
Epoch 595 of 2000 took 0.136s
  training loss:		2.300561
  validation loss:		2.250527
  validation accuracy:		13.04 %
Epoch 596 of 2000 took 0.166s
  training loss:		2.301297
  validation loss:		2.250478
  validation accuracy:		13.04 %
Epoch 597 of 2000 took 0.135s
  training loss:		2.301222
  validation loss:		2.250550
  validation accuracy:		13.04 %
Epoch 598 of 2000 took 0.165s
  training loss:		2.301087
  validation loss:		2.250411
  validation accuracy:		13.04 %
Epoch 599 of 2000 took 0.161s
  training loss:		2.300619
  validation loss:		2.250407
  validation accuracy:		13.04 %
Epoch 600 of 2000 took 0.141s
  training loss:		2.300060
  validation loss:		2.250416
  validation accuracy:		13.04 %
Epoch 601 of 2000 took 0.166s
  training loss:		2.301558
  validation loss:		2.250387
  validation accuracy:		13.04 %
Epoch 602 of 2000 took 0.136s
  training loss:		2.300622
  validation loss:		2.250348
  validation accuracy:		13.04 %
Epoch 603 of 2000 took 0.165s
  training loss:		2.300576
  validation loss:		2.250381
  validation accuracy:		13.04 %
Epoch 604 of 2000 took 0.158s
  training loss:		2.301651
  validation loss:		2.250269
  validation accuracy:		13.04 %
Epoch 605 of 2000 took 0.143s
  training loss:		2.302086
  validation loss:		2.250384
  validation accuracy:		13.04 %
Epoch 606 of 2000 took 0.166s
  training loss:		2.301021
  validation loss:		2.250493
  validation accuracy:		13.04 %
Epoch 607 of 2000 took 0.135s
  training loss:		2.301155
  validation loss:		2.250592
  validation accuracy:		12.93 %
Epoch 608 of 2000 took 0.165s
  training loss:		2.299839
  validation loss:		2.250500
  validation accuracy:		12.93 %
Epoch 609 of 2000 took 0.156s
  training loss:		2.300581
  validation loss:		2.250432
  validation accuracy:		12.93 %
Epoch 610 of 2000 took 0.146s
  training loss:		2.300773
  validation loss:		2.250331
  validation accuracy:		12.93 %
Epoch 611 of 2000 took 0.167s
  training loss:		2.300669
  validation loss:		2.250267
  validation accuracy:		12.93 %
Epoch 612 of 2000 took 0.136s
  training loss:		2.299956
  validation loss:		2.250264
  validation accuracy:		12.93 %
Epoch 613 of 2000 took 0.165s
  training loss:		2.300407
  validation loss:		2.250177
  validation accuracy:		12.93 %
Epoch 614 of 2000 took 0.153s
  training loss:		2.300943
  validation loss:		2.250156
  validation accuracy:		13.04 %
Epoch 615 of 2000 took 0.148s
  training loss:		2.299955
  validation loss:		2.249864
  validation accuracy:		13.04 %
Epoch 616 of 2000 took 0.167s
  training loss:		2.301345
  validation loss:		2.249992
  validation accuracy:		13.04 %
Epoch 617 of 2000 took 0.135s
  training loss:		2.301320
  validation loss:		2.250201
  validation accuracy:		12.93 %
Epoch 618 of 2000 took 0.165s
  training loss:		2.300539
  validation loss:		2.250091
  validation accuracy:		12.93 %
Epoch 619 of 2000 took 0.150s
  training loss:		2.300470
  validation loss:		2.249999
  validation accuracy:		12.93 %
Epoch 620 of 2000 took 0.152s
  training loss:		2.300528
  validation loss:		2.249965
  validation accuracy:		12.93 %
Epoch 621 of 2000 took 0.167s
  training loss:		2.300517
  validation loss:		2.249859
  validation accuracy:		12.93 %
Epoch 622 of 2000 took 0.135s
  training loss:		2.300698
  validation loss:		2.249958
  validation accuracy:		12.93 %
Epoch 623 of 2000 took 0.165s
  training loss:		2.300611
  validation loss:		2.250071
  validation accuracy:		12.93 %
Epoch 624 of 2000 took 0.147s
  training loss:		2.301292
  validation loss:		2.250185
  validation accuracy:		13.04 %
Epoch 625 of 2000 took 0.154s
  training loss:		2.301480
  validation loss:		2.250392
  validation accuracy:		13.04 %
Epoch 626 of 2000 took 0.167s
  training loss:		2.301073
  validation loss:		2.250305
  validation accuracy:		13.04 %
Epoch 627 of 2000 took 0.135s
  training loss:		2.301244
  validation loss:		2.250310
  validation accuracy:		12.93 %
Epoch 628 of 2000 took 0.165s
  training loss:		2.300399
  validation loss:		2.250607
  validation accuracy:		13.04 %
Epoch 629 of 2000 took 0.143s
  training loss:		2.300842
  validation loss:		2.250628
  validation accuracy:		13.04 %
Epoch 630 of 2000 took 0.158s
  training loss:		2.299981
  validation loss:		2.250573
  validation accuracy:		13.04 %
Epoch 631 of 2000 took 0.167s
  training loss:		2.300756
  validation loss:		2.250383
  validation accuracy:		13.04 %
Epoch 632 of 2000 took 0.135s
  training loss:		2.299700
  validation loss:		2.250365
  validation accuracy:		13.04 %
Epoch 633 of 2000 took 0.166s
  training loss:		2.300060
  validation loss:		2.249933
  validation accuracy:		13.04 %
Epoch 634 of 2000 took 0.140s
  training loss:		2.300206
  validation loss:		2.249878
  validation accuracy:		13.04 %
Epoch 635 of 2000 took 0.161s
  training loss:		2.300275
  validation loss:		2.249765
  validation accuracy:		13.04 %
Epoch 636 of 2000 took 0.167s
  training loss:		2.300086
  validation loss:		2.249824
  validation accuracy:		13.04 %
Epoch 637 of 2000 took 0.134s
  training loss:		2.299943
  validation loss:		2.249747
  validation accuracy:		13.04 %
Epoch 638 of 2000 took 0.166s
  training loss:		2.299692
  validation loss:		2.249789
  validation accuracy:		13.04 %
Epoch 639 of 2000 took 0.137s
  training loss:		2.300114
  validation loss:		2.249761
  validation accuracy:		13.04 %
Epoch 640 of 2000 took 0.164s
  training loss:		2.300271
  validation loss:		2.249804
  validation accuracy:		13.04 %
Epoch 641 of 2000 took 0.165s
  training loss:		2.300273
  validation loss:		2.249705
  validation accuracy:		13.04 %
Epoch 642 of 2000 took 0.136s
  training loss:		2.300280
  validation loss:		2.249718
  validation accuracy:		13.04 %
Epoch 643 of 2000 took 0.166s
  training loss:		2.300511
  validation loss:		2.249616
  validation accuracy:		13.04 %
Epoch 644 of 2000 took 0.135s
  training loss:		2.299954
  validation loss:		2.249500
  validation accuracy:		13.04 %
Epoch 645 of 2000 took 0.165s
  training loss:		2.299585
  validation loss:		2.249496
  validation accuracy:		13.04 %
Epoch 646 of 2000 took 0.163s
  training loss:		2.300508
  validation loss:		2.249602
  validation accuracy:		13.04 %
Epoch 647 of 2000 took 0.139s
  training loss:		2.300730
  validation loss:		2.249661
  validation accuracy:		13.04 %
Epoch 648 of 2000 took 0.166s
  training loss:		2.300117
  validation loss:		2.249675
  validation accuracy:		13.04 %
Epoch 649 of 2000 took 0.135s
  training loss:		2.300055
  validation loss:		2.249692
  validation accuracy:		13.04 %
Epoch 650 of 2000 took 0.165s
  training loss:		2.299591
  validation loss:		2.249583
  validation accuracy:		13.04 %
Epoch 651 of 2000 took 0.160s
  training loss:		2.300542
  validation loss:		2.249687
  validation accuracy:		13.04 %
Epoch 652 of 2000 took 0.142s
  training loss:		2.300721
  validation loss:		2.249771
  validation accuracy:		12.93 %
Epoch 653 of 2000 took 0.166s
  training loss:		2.299766
  validation loss:		2.249807
  validation accuracy:		13.04 %
Epoch 654 of 2000 took 0.135s
  training loss:		2.299939
  validation loss:		2.249811
  validation accuracy:		13.04 %
Epoch 655 of 2000 took 0.165s
  training loss:		2.300272
  validation loss:		2.249606
  validation accuracy:		13.04 %
Epoch 656 of 2000 took 0.157s
  training loss:		2.300432
  validation loss:		2.249615
  validation accuracy:		13.04 %
Epoch 657 of 2000 took 0.145s
  training loss:		2.300879
  validation loss:		2.249667
  validation accuracy:		12.93 %
Epoch 658 of 2000 took 0.166s
  training loss:		2.299402
  validation loss:		2.249632
  validation accuracy:		13.04 %
Epoch 659 of 2000 took 0.135s
  training loss:		2.299317
  validation loss:		2.249531
  validation accuracy:		12.93 %
Epoch 660 of 2000 took 0.165s
  training loss:		2.299184
  validation loss:		2.249362
  validation accuracy:		12.93 %
Epoch 661 of 2000 took 0.155s
  training loss:		2.299905
  validation loss:		2.249182
  validation accuracy:		12.93 %
Epoch 662 of 2000 took 0.148s
  training loss:		2.300102
  validation loss:		2.248999
  validation accuracy:		12.93 %
Epoch 663 of 2000 took 0.167s
  training loss:		2.299812
  validation loss:		2.248944
  validation accuracy:		12.93 %
Epoch 664 of 2000 took 0.135s
  training loss:		2.299249
  validation loss:		2.248908
  validation accuracy:		12.93 %
Epoch 665 of 2000 took 0.165s
  training loss:		2.299156
  validation loss:		2.248701
  validation accuracy:		12.93 %
Epoch 666 of 2000 took 0.151s
  training loss:		2.299998
  validation loss:		2.248798
  validation accuracy:		13.04 %
Epoch 667 of 2000 took 0.151s
  training loss:		2.299637
  validation loss:		2.248880
  validation accuracy:		13.04 %
Epoch 668 of 2000 took 0.162s
  training loss:		2.299722
  validation loss:		2.248883
  validation accuracy:		12.93 %
Epoch 669 of 2000 took 0.146s
  training loss:		2.300038
  validation loss:		2.248691
  validation accuracy:		12.93 %
Epoch 670 of 2000 took 0.107s
  training loss:		2.298574
  validation loss:		2.248583
  validation accuracy:		12.93 %
Epoch 671 of 2000 took 0.106s
  training loss:		2.300941
  validation loss:		2.248673
  validation accuracy:		12.93 %
Epoch 672 of 2000 took 0.106s
  training loss:		2.299731
  validation loss:		2.248856
  validation accuracy:		12.93 %
Epoch 673 of 2000 took 0.106s
  training loss:		2.299902
  validation loss:		2.248676
  validation accuracy:		12.93 %
Epoch 674 of 2000 took 0.106s
  training loss:		2.299950
  validation loss:		2.248657
  validation accuracy:		12.93 %
Epoch 675 of 2000 took 0.106s
  training loss:		2.300329
  validation loss:		2.248848
  validation accuracy:		12.93 %
Epoch 676 of 2000 took 0.106s
  training loss:		2.300084
  validation loss:		2.249048
  validation accuracy:		12.93 %
Epoch 677 of 2000 took 0.107s
  training loss:		2.299575
  validation loss:		2.249246
  validation accuracy:		12.93 %
Epoch 678 of 2000 took 0.107s
  training loss:		2.299716
  validation loss:		2.249273
  validation accuracy:		12.93 %
Epoch 679 of 2000 took 0.106s
  training loss:		2.299757
  validation loss:		2.249263
  validation accuracy:		12.93 %
Epoch 680 of 2000 took 0.106s
  training loss:		2.300015
  validation loss:		2.249217
  validation accuracy:		12.93 %
Epoch 681 of 2000 took 0.106s
  training loss:		2.299458
  validation loss:		2.249166
  validation accuracy:		13.04 %
Epoch 682 of 2000 took 0.106s
  training loss:		2.300168
  validation loss:		2.249282
  validation accuracy:		13.04 %
Epoch 683 of 2000 took 0.106s
  training loss:		2.299418
  validation loss:		2.249146
  validation accuracy:		13.04 %
Epoch 684 of 2000 took 0.106s
  training loss:		2.299061
  validation loss:		2.249009
  validation accuracy:		13.04 %
Epoch 685 of 2000 took 0.106s
  training loss:		2.299034
  validation loss:		2.248903
  validation accuracy:		13.04 %
Epoch 686 of 2000 took 0.107s
  training loss:		2.299392
  validation loss:		2.248989
  validation accuracy:		13.04 %
Epoch 687 of 2000 took 0.106s
  training loss:		2.299556
  validation loss:		2.249031
  validation accuracy:		13.04 %
Epoch 688 of 2000 took 0.106s
  training loss:		2.299924
  validation loss:		2.248952
  validation accuracy:		13.04 %
Epoch 689 of 2000 took 0.106s
  training loss:		2.299735
  validation loss:		2.248997
  validation accuracy:		13.04 %
Epoch 690 of 2000 took 0.106s
  training loss:		2.300190
  validation loss:		2.249032
  validation accuracy:		13.04 %
Epoch 691 of 2000 took 0.106s
  training loss:		2.299339
  validation loss:		2.248940
  validation accuracy:		12.93 %
Epoch 692 of 2000 took 0.106s
  training loss:		2.299675
  validation loss:		2.249157
  validation accuracy:		12.93 %
Epoch 693 of 2000 took 0.106s
  training loss:		2.298638
  validation loss:		2.249165
  validation accuracy:		13.04 %
Epoch 694 of 2000 took 0.106s
  training loss:		2.299489
  validation loss:		2.248977
  validation accuracy:		13.04 %
Epoch 695 of 2000 took 0.106s
  training loss:		2.298756
  validation loss:		2.248845
  validation accuracy:		13.04 %
Epoch 696 of 2000 took 0.106s
  training loss:		2.300018
  validation loss:		2.248923
  validation accuracy:		13.04 %
Epoch 697 of 2000 took 0.106s
  training loss:		2.299747
  validation loss:		2.248973
  validation accuracy:		13.04 %
Epoch 698 of 2000 took 0.106s
  training loss:		2.299235
  validation loss:		2.248992
  validation accuracy:		13.04 %
Epoch 699 of 2000 took 0.106s
  training loss:		2.300253
  validation loss:		2.249083
  validation accuracy:		12.93 %
Epoch 700 of 2000 took 0.107s
  training loss:		2.299534
  validation loss:		2.248983
  validation accuracy:		12.93 %
Epoch 701 of 2000 took 0.106s
  training loss:		2.300218
  validation loss:		2.249180
  validation accuracy:		12.93 %
Epoch 702 of 2000 took 0.106s
  training loss:		2.299331
  validation loss:		2.249149
  validation accuracy:		12.93 %
Epoch 703 of 2000 took 0.106s
  training loss:		2.299270
  validation loss:		2.249094
  validation accuracy:		12.93 %
Epoch 704 of 2000 took 0.106s
  training loss:		2.299526
  validation loss:		2.249178
  validation accuracy:		12.93 %
Epoch 705 of 2000 took 0.107s
  training loss:		2.299011
  validation loss:		2.249103
  validation accuracy:		12.93 %
Epoch 706 of 2000 took 0.106s
  training loss:		2.299239
  validation loss:		2.249048
  validation accuracy:		12.93 %
Epoch 707 of 2000 took 0.106s
  training loss:		2.299449
  validation loss:		2.248883
  validation accuracy:		12.93 %
Epoch 708 of 2000 took 0.107s
  training loss:		2.298633
  validation loss:		2.248710
  validation accuracy:		12.93 %
Epoch 709 of 2000 took 0.106s
  training loss:		2.299094
  validation loss:		2.248815
  validation accuracy:		12.93 %
Epoch 710 of 2000 took 0.106s
  training loss:		2.299415
  validation loss:		2.248665
  validation accuracy:		12.93 %
Epoch 711 of 2000 took 0.106s
  training loss:		2.299752
  validation loss:		2.248671
  validation accuracy:		12.93 %
Epoch 712 of 2000 took 0.106s
  training loss:		2.299604
  validation loss:		2.248579
  validation accuracy:		13.04 %
Epoch 713 of 2000 took 0.106s
  training loss:		2.298902
  validation loss:		2.248645
  validation accuracy:		13.04 %
Epoch 714 of 2000 took 0.107s
  training loss:		2.298455
  validation loss:		2.248522
  validation accuracy:		13.04 %
Epoch 715 of 2000 took 0.106s
  training loss:		2.299126
  validation loss:		2.248404
  validation accuracy:		13.04 %
Epoch 716 of 2000 took 0.106s
  training loss:		2.298675
  validation loss:		2.248232
  validation accuracy:		13.04 %
Epoch 717 of 2000 took 0.106s
  training loss:		2.298934
  validation loss:		2.248171
  validation accuracy:		13.04 %
Epoch 718 of 2000 took 0.106s
  training loss:		2.299834
  validation loss:		2.248228
  validation accuracy:		13.04 %
Epoch 719 of 2000 took 0.106s
  training loss:		2.299857
  validation loss:		2.248441
  validation accuracy:		13.04 %
Epoch 720 of 2000 took 0.106s
  training loss:		2.298324
  validation loss:		2.248379
  validation accuracy:		13.04 %
Epoch 721 of 2000 took 0.106s
  training loss:		2.298442
  validation loss:		2.248431
  validation accuracy:		13.04 %
Epoch 722 of 2000 took 0.106s
  training loss:		2.299668
  validation loss:		2.248365
  validation accuracy:		12.93 %
Epoch 723 of 2000 took 0.106s
  training loss:		2.299191
  validation loss:		2.248498
  validation accuracy:		12.93 %
Epoch 724 of 2000 took 0.107s
  training loss:		2.298585
  validation loss:		2.248275
  validation accuracy:		13.04 %
Epoch 725 of 2000 took 0.106s
  training loss:		2.298479
  validation loss:		2.248063
  validation accuracy:		13.04 %
Epoch 726 of 2000 took 0.106s
  training loss:		2.299201
  validation loss:		2.247942
  validation accuracy:		13.04 %
Epoch 727 of 2000 took 0.106s
  training loss:		2.298581
  validation loss:		2.247970
  validation accuracy:		13.04 %
Epoch 728 of 2000 took 0.106s
  training loss:		2.298163
  validation loss:		2.248165
  validation accuracy:		13.04 %
Epoch 729 of 2000 took 0.107s
  training loss:		2.298857
  validation loss:		2.248219
  validation accuracy:		13.04 %
Epoch 730 of 2000 took 0.106s
  training loss:		2.298405
  validation loss:		2.248014
  validation accuracy:		13.04 %
Epoch 731 of 2000 took 0.106s
  training loss:		2.299100
  validation loss:		2.247994
  validation accuracy:		13.04 %
Epoch 732 of 2000 took 0.106s
  training loss:		2.299226
  validation loss:		2.247877
  validation accuracy:		13.04 %
Epoch 733 of 2000 took 0.107s
  training loss:		2.299195
  validation loss:		2.248024
  validation accuracy:		13.04 %
Epoch 734 of 2000 took 0.106s
  training loss:		2.298899
  validation loss:		2.248177
  validation accuracy:		13.04 %
Epoch 735 of 2000 took 0.106s
  training loss:		2.298264
  validation loss:		2.247933
  validation accuracy:		13.04 %
Epoch 736 of 2000 took 0.106s
  training loss:		2.299003
  validation loss:		2.248033
  validation accuracy:		13.04 %
Epoch 737 of 2000 took 0.109s
  training loss:		2.299044
  validation loss:		2.248008
  validation accuracy:		13.04 %
Epoch 738 of 2000 took 0.106s
  training loss:		2.299052
  validation loss:		2.247937
  validation accuracy:		13.04 %
Epoch 739 of 2000 took 0.106s
  training loss:		2.299309
  validation loss:		2.248143
  validation accuracy:		13.04 %
Epoch 740 of 2000 took 0.106s
  training loss:		2.298733
  validation loss:		2.248092
  validation accuracy:		13.04 %
Epoch 741 of 2000 took 0.106s
  training loss:		2.298272
  validation loss:		2.247906
  validation accuracy:		13.04 %
Epoch 742 of 2000 took 0.107s
  training loss:		2.299158
  validation loss:		2.248103
  validation accuracy:		13.04 %
Epoch 743 of 2000 took 0.106s
  training loss:		2.297092
  validation loss:		2.247678
  validation accuracy:		13.04 %
Epoch 744 of 2000 took 0.106s
  training loss:		2.299741
  validation loss:		2.247513
  validation accuracy:		13.04 %
Epoch 745 of 2000 took 0.106s
  training loss:		2.299509
  validation loss:		2.247785
  validation accuracy:		13.04 %
Epoch 746 of 2000 took 0.106s
  training loss:		2.299607
  validation loss:		2.247965
  validation accuracy:		13.04 %
Epoch 747 of 2000 took 0.106s
  training loss:		2.299554
  validation loss:		2.248090
  validation accuracy:		13.04 %
Epoch 748 of 2000 took 0.106s
  training loss:		2.297995
  validation loss:		2.248160
  validation accuracy:		13.04 %
Epoch 749 of 2000 took 0.106s
  training loss:		2.298579
  validation loss:		2.248300
  validation accuracy:		12.83 %
Epoch 750 of 2000 took 0.106s
  training loss:		2.299245
  validation loss:		2.248135
  validation accuracy:		13.04 %
Epoch 751 of 2000 took 0.106s
  training loss:		2.299371
  validation loss:		2.248088
  validation accuracy:		13.04 %
Epoch 752 of 2000 took 0.107s
  training loss:		2.298315
  validation loss:		2.248114
  validation accuracy:		13.04 %
Epoch 753 of 2000 took 0.106s
  training loss:		2.299596
  validation loss:		2.248091
  validation accuracy:		13.04 %
Epoch 754 of 2000 took 0.106s
  training loss:		2.298336
  validation loss:		2.248274
  validation accuracy:		13.04 %
Epoch 755 of 2000 took 0.106s
  training loss:		2.297983
  validation loss:		2.248107
  validation accuracy:		12.93 %
Epoch 756 of 2000 took 0.106s
  training loss:		2.298897
  validation loss:		2.248232
  validation accuracy:		12.93 %
Epoch 757 of 2000 took 0.107s
  training loss:		2.298893
  validation loss:		2.248152
  validation accuracy:		12.93 %
Epoch 758 of 2000 took 0.106s
  training loss:		2.299233
  validation loss:		2.248273
  validation accuracy:		12.93 %
Epoch 759 of 2000 took 0.106s
  training loss:		2.299118
  validation loss:		2.248516
  validation accuracy:		12.93 %
Epoch 760 of 2000 took 0.106s
  training loss:		2.298416
  validation loss:		2.248243
  validation accuracy:		13.04 %
Epoch 761 of 2000 took 0.107s
  training loss:		2.298779
  validation loss:		2.248141
  validation accuracy:		12.93 %
Epoch 762 of 2000 took 0.106s
  training loss:		2.299247
  validation loss:		2.248420
  validation accuracy:		12.93 %
Epoch 763 of 2000 took 0.107s
  training loss:		2.299331
  validation loss:		2.248667
  validation accuracy:		12.93 %
Epoch 764 of 2000 took 0.106s
  training loss:		2.298735
  validation loss:		2.248492
  validation accuracy:		13.04 %
Epoch 765 of 2000 took 0.106s
  training loss:		2.299559
  validation loss:		2.248840
  validation accuracy:		13.04 %
Epoch 766 of 2000 took 0.106s
  training loss:		2.298740
  validation loss:		2.248855
  validation accuracy:		13.04 %
Epoch 767 of 2000 took 0.106s
  training loss:		2.298932
  validation loss:		2.248721
  validation accuracy:		13.04 %
Epoch 768 of 2000 took 0.106s
  training loss:		2.298198
  validation loss:		2.248731
  validation accuracy:		13.04 %
Epoch 769 of 2000 took 0.106s
  training loss:		2.298480
  validation loss:		2.248706
  validation accuracy:		13.04 %
Epoch 770 of 2000 took 0.106s
  training loss:		2.298013
  validation loss:		2.248426
  validation accuracy:		13.04 %
Epoch 771 of 2000 took 0.107s
  training loss:		2.297851
  validation loss:		2.248272
  validation accuracy:		13.04 %
Epoch 772 of 2000 took 0.106s
  training loss:		2.297635
  validation loss:		2.247948
  validation accuracy:		13.04 %
Epoch 773 of 2000 took 0.106s
  training loss:		2.298475
  validation loss:		2.248125
  validation accuracy:		13.04 %
Epoch 774 of 2000 took 0.106s
  training loss:		2.298606
  validation loss:		2.247999
  validation accuracy:		13.04 %
Epoch 775 of 2000 took 0.106s
  training loss:		2.297991
  validation loss:		2.247947
  validation accuracy:		13.04 %
Epoch 776 of 2000 took 0.106s
  training loss:		2.299136
  validation loss:		2.247907
  validation accuracy:		13.04 %
Epoch 777 of 2000 took 0.106s
  training loss:		2.298180
  validation loss:		2.247846
  validation accuracy:		13.04 %
Epoch 778 of 2000 took 0.106s
  training loss:		2.298229
  validation loss:		2.247782
  validation accuracy:		13.04 %
Epoch 779 of 2000 took 0.106s
  training loss:		2.299066
  validation loss:		2.247852
  validation accuracy:		13.04 %
Epoch 780 of 2000 took 0.106s
  training loss:		2.298936
  validation loss:		2.247767
  validation accuracy:		13.04 %
Epoch 781 of 2000 took 0.106s
  training loss:		2.299708
  validation loss:		2.247869
  validation accuracy:		13.04 %
Epoch 782 of 2000 took 0.106s
  training loss:		2.298175
  validation loss:		2.247976
  validation accuracy:		12.93 %
Epoch 783 of 2000 took 0.106s
  training loss:		2.299263
  validation loss:		2.248140
  validation accuracy:		12.93 %
Epoch 784 of 2000 took 0.106s
  training loss:		2.298249
  validation loss:		2.248142
  validation accuracy:		12.93 %
Epoch 785 of 2000 took 0.107s
  training loss:		2.298718
  validation loss:		2.248176
  validation accuracy:		12.93 %
Epoch 786 of 2000 took 0.107s
  training loss:		2.297928
  validation loss:		2.248277
  validation accuracy:		12.93 %
Epoch 787 of 2000 took 0.106s
  training loss:		2.298170
  validation loss:		2.248125
  validation accuracy:		12.93 %
Epoch 788 of 2000 took 0.106s
  training loss:		2.298093
  validation loss:		2.248207
  validation accuracy:		12.93 %
Epoch 789 of 2000 took 0.107s
  training loss:		2.298798
  validation loss:		2.248198
  validation accuracy:		12.93 %
Epoch 790 of 2000 took 0.106s
  training loss:		2.299193
  validation loss:		2.248311
  validation accuracy:		12.93 %
Epoch 791 of 2000 took 0.106s
  training loss:		2.297768
  validation loss:		2.248182
  validation accuracy:		12.93 %
Epoch 792 of 2000 took 0.106s
  training loss:		2.298817
  validation loss:		2.248084
  validation accuracy:		12.93 %
Epoch 793 of 2000 took 0.106s
  training loss:		2.297941
  validation loss:		2.248056
  validation accuracy:		12.93 %
Epoch 794 of 2000 took 0.106s
  training loss:		2.298136
  validation loss:		2.247777
  validation accuracy:		12.93 %
Epoch 795 of 2000 took 0.106s
  training loss:		2.297672
  validation loss:		2.247699
  validation accuracy:		12.93 %
Epoch 796 of 2000 took 0.106s
  training loss:		2.298811
  validation loss:		2.247798
  validation accuracy:		12.93 %
Epoch 797 of 2000 took 0.106s
  training loss:		2.298571
  validation loss:		2.247816
  validation accuracy:		12.93 %
Epoch 798 of 2000 took 0.106s
  training loss:		2.298285
  validation loss:		2.247751
  validation accuracy:		12.93 %
Epoch 799 of 2000 took 0.107s
  training loss:		2.298401
  validation loss:		2.247656
  validation accuracy:		12.93 %
Epoch 800 of 2000 took 0.106s
  training loss:		2.298256
  validation loss:		2.247924
  validation accuracy:		12.93 %
Epoch 801 of 2000 took 0.106s
  training loss:		2.298846
  validation loss:		2.247970
  validation accuracy:		12.93 %
Epoch 802 of 2000 took 0.106s
  training loss:		2.298830
  validation loss:		2.248116
  validation accuracy:		12.93 %
Epoch 803 of 2000 took 0.106s
  training loss:		2.297731
  validation loss:		2.248200
  validation accuracy:		12.93 %
Epoch 804 of 2000 took 0.106s
  training loss:		2.297889
  validation loss:		2.248071
  validation accuracy:		12.93 %
Epoch 805 of 2000 took 0.106s
  training loss:		2.299021
  validation loss:		2.247858
  validation accuracy:		12.93 %
Epoch 806 of 2000 took 0.106s
  training loss:		2.298401
  validation loss:		2.247987
  validation accuracy:		12.93 %
Epoch 807 of 2000 took 0.106s
  training loss:		2.298360
  validation loss:		2.248052
  validation accuracy:		12.93 %
Epoch 808 of 2000 took 0.106s
  training loss:		2.296974
  validation loss:		2.247822
  validation accuracy:		12.93 %
Epoch 809 of 2000 took 0.106s
  training loss:		2.298153
  validation loss:		2.247833
  validation accuracy:		12.93 %
Epoch 810 of 2000 took 0.106s
  training loss:		2.298894
  validation loss:		2.247867
  validation accuracy:		12.93 %
Epoch 811 of 2000 took 0.106s
  training loss:		2.298512
  validation loss:		2.247821
  validation accuracy:		12.93 %
Epoch 812 of 2000 took 0.106s
  training loss:		2.298407
  validation loss:		2.247855
  validation accuracy:		12.93 %
Epoch 813 of 2000 took 0.106s
  training loss:		2.298366
  validation loss:		2.247722
  validation accuracy:		12.93 %
Epoch 814 of 2000 took 0.107s
  training loss:		2.297703
  validation loss:		2.247742
  validation accuracy:		12.93 %
Epoch 815 of 2000 took 0.106s
  training loss:		2.299013
  validation loss:		2.247885
  validation accuracy:		12.93 %
Epoch 816 of 2000 took 0.106s
  training loss:		2.298343
  validation loss:		2.247980
  validation accuracy:		12.93 %
Epoch 817 of 2000 took 0.106s
  training loss:		2.297933
  validation loss:		2.247993
  validation accuracy:		12.93 %
Epoch 818 of 2000 took 0.106s
  training loss:		2.298036
  validation loss:		2.248072
  validation accuracy:		12.93 %
Epoch 819 of 2000 took 0.106s
  training loss:		2.298040
  validation loss:		2.247935
  validation accuracy:		12.93 %
Epoch 820 of 2000 took 0.106s
  training loss:		2.298003
  validation loss:		2.247798
  validation accuracy:		12.93 %
Epoch 821 of 2000 took 0.106s
  training loss:		2.298449
  validation loss:		2.247676
  validation accuracy:		12.93 %
Epoch 822 of 2000 took 0.106s
  training loss:		2.298149
  validation loss:		2.247725
  validation accuracy:		12.83 %
Epoch 823 of 2000 took 0.106s
  training loss:		2.297608
  validation loss:		2.247673
  validation accuracy:		12.93 %
Epoch 824 of 2000 took 0.106s
  training loss:		2.298890
  validation loss:		2.247587
  validation accuracy:		12.93 %
Epoch 825 of 2000 took 0.106s
  training loss:		2.297428
  validation loss:		2.247615
  validation accuracy:		12.93 %
Epoch 826 of 2000 took 0.106s
  training loss:		2.298098
  validation loss:		2.247474
  validation accuracy:		12.83 %
Epoch 827 of 2000 took 0.106s
  training loss:		2.298401
  validation loss:		2.247554
  validation accuracy:		12.83 %
Epoch 828 of 2000 took 0.106s
  training loss:		2.298669
  validation loss:		2.247934
  validation accuracy:		12.83 %
Epoch 829 of 2000 took 0.106s
  training loss:		2.298343
  validation loss:		2.248018
  validation accuracy:		12.93 %
Epoch 830 of 2000 took 0.106s
  training loss:		2.297489
  validation loss:		2.247952
  validation accuracy:		12.93 %
Epoch 831 of 2000 took 0.106s
  training loss:		2.298758
  validation loss:		2.247991
  validation accuracy:		13.15 %
Epoch 832 of 2000 took 0.106s
  training loss:		2.297802
  validation loss:		2.248038
  validation accuracy:		12.93 %
Epoch 833 of 2000 took 0.106s
  training loss:		2.297779
  validation loss:		2.247698
  validation accuracy:		12.93 %
Epoch 834 of 2000 took 0.106s
  training loss:		2.297875
  validation loss:		2.247815
  validation accuracy:		12.93 %
Epoch 835 of 2000 took 0.106s
  training loss:		2.297834
  validation loss:		2.247841
  validation accuracy:		12.93 %
Epoch 836 of 2000 took 0.106s
  training loss:		2.298225
  validation loss:		2.247946
  validation accuracy:		12.93 %
Epoch 837 of 2000 took 0.106s
  training loss:		2.298046
  validation loss:		2.247915
  validation accuracy:		12.93 %
Epoch 838 of 2000 took 0.106s
  training loss:		2.297491
  validation loss:		2.247665
  validation accuracy:		12.93 %
Epoch 839 of 2000 took 0.106s
  training loss:		2.297709
  validation loss:		2.247468
  validation accuracy:		12.93 %
Epoch 840 of 2000 took 0.106s
  training loss:		2.298048
  validation loss:		2.247380
  validation accuracy:		12.93 %
Epoch 841 of 2000 took 0.106s
  training loss:		2.298007
  validation loss:		2.247262
  validation accuracy:		12.93 %
Epoch 842 of 2000 took 0.107s
  training loss:		2.298428
  validation loss:		2.247439
  validation accuracy:		12.93 %
Epoch 843 of 2000 took 0.106s
  training loss:		2.299210
  validation loss:		2.247685
  validation accuracy:		12.93 %
Epoch 844 of 2000 took 0.106s
  training loss:		2.298538
  validation loss:		2.247847
  validation accuracy:		12.93 %
Epoch 845 of 2000 took 0.106s
  training loss:		2.298052
  validation loss:		2.248008
  validation accuracy:		13.04 %
Epoch 846 of 2000 took 0.106s
  training loss:		2.298563
  validation loss:		2.248058
  validation accuracy:		13.04 %
Epoch 847 of 2000 took 0.106s
  training loss:		2.298618
  validation loss:		2.248125
  validation accuracy:		13.04 %
Epoch 848 of 2000 took 0.106s
  training loss:		2.297353
  validation loss:		2.248044
  validation accuracy:		12.93 %
Epoch 849 of 2000 took 0.106s
  training loss:		2.297876
  validation loss:		2.248006
  validation accuracy:		13.04 %
Epoch 850 of 2000 took 0.106s
  training loss:		2.297674
  validation loss:		2.247885
  validation accuracy:		12.93 %
Epoch 851 of 2000 took 0.106s
  training loss:		2.298477
  validation loss:		2.247858
  validation accuracy:		13.04 %
Epoch 852 of 2000 took 0.106s
  training loss:		2.298146
  validation loss:		2.247872
  validation accuracy:		13.04 %
Epoch 853 of 2000 took 0.106s
  training loss:		2.298383
  validation loss:		2.247963
  validation accuracy:		13.04 %
Epoch 854 of 2000 took 0.106s
  training loss:		2.298151
  validation loss:		2.248006
  validation accuracy:		13.04 %
Epoch 855 of 2000 took 0.106s
  training loss:		2.297928
  validation loss:		2.247886
  validation accuracy:		13.04 %
Epoch 856 of 2000 took 0.106s
  training loss:		2.297505
  validation loss:		2.247989
  validation accuracy:		13.04 %
Epoch 857 of 2000 took 0.106s
  training loss:		2.298200
  validation loss:		2.247959
  validation accuracy:		13.04 %
Epoch 858 of 2000 took 0.106s
  training loss:		2.298301
  validation loss:		2.248058
  validation accuracy:		13.04 %
Epoch 859 of 2000 took 0.106s
  training loss:		2.297788
  validation loss:		2.248026
  validation accuracy:		13.04 %
Epoch 860 of 2000 took 0.106s
  training loss:		2.297511
  validation loss:		2.248065
  validation accuracy:		13.04 %
Epoch 861 of 2000 took 0.106s
  training loss:		2.298421
  validation loss:		2.248051
  validation accuracy:		13.04 %
Epoch 862 of 2000 took 0.106s
  training loss:		2.297783
  validation loss:		2.247986
  validation accuracy:		12.93 %
Epoch 863 of 2000 took 0.106s
  training loss:		2.298245
  validation loss:		2.248090
  validation accuracy:		13.04 %
Epoch 864 of 2000 took 0.106s
  training loss:		2.297608
  validation loss:		2.248039
  validation accuracy:		13.04 %
Epoch 865 of 2000 took 0.106s
  training loss:		2.297593
  validation loss:		2.247942
  validation accuracy:		13.04 %
Epoch 866 of 2000 took 0.106s
  training loss:		2.297892
  validation loss:		2.247907
  validation accuracy:		12.83 %
Epoch 867 of 2000 took 0.106s
  training loss:		2.297820
  validation loss:		2.247795
  validation accuracy:		12.93 %
Epoch 868 of 2000 took 0.106s
  training loss:		2.298239
  validation loss:		2.247667
  validation accuracy:		13.04 %
Epoch 869 of 2000 took 0.106s
  training loss:		2.297343
  validation loss:		2.247698
  validation accuracy:		13.04 %
Epoch 870 of 2000 took 0.107s
  training loss:		2.296836
  validation loss:		2.247490
  validation accuracy:		12.83 %
Epoch 871 of 2000 took 0.106s
  training loss:		2.297358
  validation loss:		2.247402
  validation accuracy:		12.93 %
Epoch 872 of 2000 took 0.106s
  training loss:		2.297697
  validation loss:		2.247343
  validation accuracy:		12.93 %
Epoch 873 of 2000 took 0.106s
  training loss:		2.298192
  validation loss:		2.247410
  validation accuracy:		12.93 %
Epoch 874 of 2000 took 0.106s
  training loss:		2.298559
  validation loss:		2.247501
  validation accuracy:		12.93 %
Epoch 875 of 2000 took 0.106s
  training loss:		2.298059
  validation loss:		2.247776
  validation accuracy:		12.93 %
Epoch 876 of 2000 took 0.106s
  training loss:		2.297794
  validation loss:		2.247609
  validation accuracy:		12.93 %
Epoch 877 of 2000 took 0.106s
  training loss:		2.298119
  validation loss:		2.247601
  validation accuracy:		12.93 %
Epoch 878 of 2000 took 0.106s
  training loss:		2.298133
  validation loss:		2.247586
  validation accuracy:		12.93 %
Epoch 879 of 2000 took 0.106s
  training loss:		2.297393
  validation loss:		2.247568
  validation accuracy:		12.93 %
Epoch 880 of 2000 took 0.106s
  training loss:		2.297729
  validation loss:		2.247336
  validation accuracy:		12.93 %
Epoch 881 of 2000 took 0.106s
  training loss:		2.297226
  validation loss:		2.247263
  validation accuracy:		12.93 %
Epoch 882 of 2000 took 0.106s
  training loss:		2.297989
  validation loss:		2.247214
  validation accuracy:		12.93 %
Epoch 883 of 2000 took 0.106s
  training loss:		2.298580
  validation loss:		2.247415
  validation accuracy:		12.93 %
Epoch 884 of 2000 took 0.106s
  training loss:		2.297397
  validation loss:		2.247548
  validation accuracy:		12.93 %
Epoch 885 of 2000 took 0.106s
  training loss:		2.297821
  validation loss:		2.247570
  validation accuracy:		12.93 %
Epoch 886 of 2000 took 0.106s
  training loss:		2.296773
  validation loss:		2.247515
  validation accuracy:		12.93 %
Epoch 887 of 2000 took 0.106s
  training loss:		2.297821
  validation loss:		2.247299
  validation accuracy:		13.04 %
Epoch 888 of 2000 took 0.106s
  training loss:		2.297849
  validation loss:		2.247188
  validation accuracy:		13.04 %
Epoch 889 of 2000 took 0.106s
  training loss:		2.297506
  validation loss:		2.247253
  validation accuracy:		13.04 %
Epoch 890 of 2000 took 0.106s
  training loss:		2.297523
  validation loss:		2.247188
  validation accuracy:		12.83 %
Epoch 891 of 2000 took 0.106s
  training loss:		2.296996
  validation loss:		2.247035
  validation accuracy:		13.04 %
Epoch 892 of 2000 took 0.106s
  training loss:		2.298016
  validation loss:		2.247041
  validation accuracy:		13.04 %
Epoch 893 of 2000 took 0.106s
  training loss:		2.298057
  validation loss:		2.247110
  validation accuracy:		13.04 %
Epoch 894 of 2000 took 0.106s
  training loss:		2.297661
  validation loss:		2.247010
  validation accuracy:		13.04 %
Epoch 895 of 2000 took 0.106s
  training loss:		2.297650
  validation loss:		2.247040
  validation accuracy:		13.04 %
Epoch 896 of 2000 took 0.106s
  training loss:		2.297273
  validation loss:		2.247005
  validation accuracy:		13.04 %
Epoch 897 of 2000 took 0.107s
  training loss:		2.297984
  validation loss:		2.247012
  validation accuracy:		13.04 %
Epoch 898 of 2000 took 0.107s
  training loss:		2.297536
  validation loss:		2.246992
  validation accuracy:		13.04 %
Epoch 899 of 2000 took 0.107s
  training loss:		2.297915
  validation loss:		2.247026
  validation accuracy:		12.83 %
Epoch 900 of 2000 took 0.107s
  training loss:		2.297846
  validation loss:		2.247360
  validation accuracy:		13.04 %
Epoch 901 of 2000 took 0.107s
  training loss:		2.297803
  validation loss:		2.247483
  validation accuracy:		12.93 %
Epoch 902 of 2000 took 0.107s
  training loss:		2.297312
  validation loss:		2.247532
  validation accuracy:		12.93 %
Epoch 903 of 2000 took 0.107s
  training loss:		2.297959
  validation loss:		2.247486
  validation accuracy:		12.93 %
Epoch 904 of 2000 took 0.107s
  training loss:		2.297945
  validation loss:		2.247334
  validation accuracy:		12.93 %
Epoch 905 of 2000 took 0.107s
  training loss:		2.297515
  validation loss:		2.247315
  validation accuracy:		12.93 %
Epoch 906 of 2000 took 0.107s
  training loss:		2.297269
  validation loss:		2.247296
  validation accuracy:		12.93 %
Epoch 907 of 2000 took 0.107s
  training loss:		2.298146
  validation loss:		2.247454
  validation accuracy:		12.93 %
Epoch 908 of 2000 took 0.107s
  training loss:		2.296196
  validation loss:		2.247231
  validation accuracy:		12.93 %
Epoch 909 of 2000 took 0.107s
  training loss:		2.297624
  validation loss:		2.246986
  validation accuracy:		12.93 %
Epoch 910 of 2000 took 0.107s
  training loss:		2.297558
  validation loss:		2.246923
  validation accuracy:		12.93 %
Epoch 911 of 2000 took 0.107s
  training loss:		2.297946
  validation loss:		2.246968
  validation accuracy:		12.93 %
Epoch 912 of 2000 took 0.107s
  training loss:		2.298655
  validation loss:		2.247268
  validation accuracy:		12.93 %
Epoch 913 of 2000 took 0.107s
  training loss:		2.297087
  validation loss:		2.247284
  validation accuracy:		15.76 %
Epoch 914 of 2000 took 0.107s
  training loss:		2.297396
  validation loss:		2.247282
  validation accuracy:		12.93 %
Epoch 915 of 2000 took 0.107s
  training loss:		2.297151
  validation loss:		2.247176
  validation accuracy:		12.93 %
Epoch 916 of 2000 took 0.107s
  training loss:		2.297090
  validation loss:		2.247079
  validation accuracy:		12.93 %
Epoch 917 of 2000 took 0.107s
  training loss:		2.298487
  validation loss:		2.247043
  validation accuracy:		12.93 %
Epoch 918 of 2000 took 0.107s
  training loss:		2.297739
  validation loss:		2.247422
  validation accuracy:		12.93 %
Epoch 919 of 2000 took 0.107s
  training loss:		2.297559
  validation loss:		2.247473
  validation accuracy:		12.93 %
Epoch 920 of 2000 took 0.107s
  training loss:		2.297619
  validation loss:		2.247430
  validation accuracy:		12.93 %
Epoch 921 of 2000 took 0.107s
  training loss:		2.297652
  validation loss:		2.247404
  validation accuracy:		12.93 %
Epoch 922 of 2000 took 0.107s
  training loss:		2.297565
  validation loss:		2.247462
  validation accuracy:		12.93 %
Epoch 923 of 2000 took 0.107s
  training loss:		2.297754
  validation loss:		2.247346
  validation accuracy:		12.93 %
Epoch 924 of 2000 took 0.107s
  training loss:		2.297558
  validation loss:		2.247233
  validation accuracy:		12.93 %
Epoch 925 of 2000 took 0.107s
  training loss:		2.297625
  validation loss:		2.247263
  validation accuracy:		12.93 %
Epoch 926 of 2000 took 0.107s
  training loss:		2.297550
  validation loss:		2.247281
  validation accuracy:		12.93 %
Epoch 927 of 2000 took 0.107s
  training loss:		2.298034
  validation loss:		2.247293
  validation accuracy:		12.93 %
Epoch 928 of 2000 took 0.107s
  training loss:		2.296922
  validation loss:		2.247442
  validation accuracy:		12.93 %
Epoch 929 of 2000 took 0.107s
  training loss:		2.297184
  validation loss:		2.247394
  validation accuracy:		12.93 %
Epoch 930 of 2000 took 0.107s
  training loss:		2.297600
  validation loss:		2.247271
  validation accuracy:		12.93 %
Epoch 931 of 2000 took 0.107s
  training loss:		2.296607
  validation loss:		2.247179
  validation accuracy:		12.93 %
Epoch 932 of 2000 took 0.107s
  training loss:		2.296798
  validation loss:		2.246844
  validation accuracy:		12.93 %
Epoch 933 of 2000 took 0.107s
  training loss:		2.297732
  validation loss:		2.246769
  validation accuracy:		12.93 %
Epoch 934 of 2000 took 0.107s
  training loss:		2.297239
  validation loss:		2.246751
  validation accuracy:		12.93 %
Epoch 935 of 2000 took 0.107s
  training loss:		2.297843
  validation loss:		2.247110
  validation accuracy:		12.93 %
Epoch 936 of 2000 took 0.107s
  training loss:		2.297665
  validation loss:		2.247007
  validation accuracy:		12.93 %
Epoch 937 of 2000 took 0.107s
  training loss:		2.297046
  validation loss:		2.247053
  validation accuracy:		12.93 %
Epoch 938 of 2000 took 0.107s
  training loss:		2.297759
  validation loss:		2.247143
  validation accuracy:		12.93 %
Epoch 939 of 2000 took 0.107s
  training loss:		2.297703
  validation loss:		2.247061
  validation accuracy:		12.93 %
Epoch 940 of 2000 took 0.107s
  training loss:		2.296562
  validation loss:		2.246735
  validation accuracy:		12.83 %
Epoch 941 of 2000 took 0.107s
  training loss:		2.297257
  validation loss:		2.246601
  validation accuracy:		13.04 %
Epoch 942 of 2000 took 0.107s
  training loss:		2.296243
  validation loss:		2.246341
  validation accuracy:		13.04 %
Epoch 943 of 2000 took 0.107s
  training loss:		2.296783
  validation loss:		2.246216
  validation accuracy:		13.04 %
Epoch 944 of 2000 took 0.107s
  training loss:		2.297362
  validation loss:		2.246347
  validation accuracy:		13.04 %
Epoch 945 of 2000 took 0.107s
  training loss:		2.297845
  validation loss:		2.246317
  validation accuracy:		13.04 %
Epoch 946 of 2000 took 0.107s
  training loss:		2.297289
  validation loss:		2.246567
  validation accuracy:		13.04 %
Epoch 947 of 2000 took 0.107s
  training loss:		2.298032
  validation loss:		2.246641
  validation accuracy:		13.04 %
Epoch 948 of 2000 took 0.107s
  training loss:		2.296849
  validation loss:		2.246697
  validation accuracy:		13.04 %
Epoch 949 of 2000 took 0.107s
  training loss:		2.297227
  validation loss:		2.246703
  validation accuracy:		13.04 %
Epoch 950 of 2000 took 0.107s
  training loss:		2.297064
  validation loss:		2.246728
  validation accuracy:		13.04 %
Epoch 951 of 2000 took 0.107s
  training loss:		2.296760
  validation loss:		2.246449
  validation accuracy:		13.04 %
Epoch 952 of 2000 took 0.107s
  training loss:		2.297472
  validation loss:		2.246528
  validation accuracy:		13.04 %
Epoch 953 of 2000 took 0.107s
  training loss:		2.297492
  validation loss:		2.246603
  validation accuracy:		13.04 %
Epoch 954 of 2000 took 0.107s
  training loss:		2.297706
  validation loss:		2.246774
  validation accuracy:		12.83 %
Epoch 955 of 2000 took 0.107s
  training loss:		2.297358
  validation loss:		2.247217
  validation accuracy:		12.93 %
Epoch 956 of 2000 took 0.107s
  training loss:		2.297804
  validation loss:		2.247127
  validation accuracy:		12.93 %
Epoch 957 of 2000 took 0.107s
  training loss:		2.296636
  validation loss:		2.246977
  validation accuracy:		12.93 %
Epoch 958 of 2000 took 0.107s
  training loss:		2.297214
  validation loss:		2.247133
  validation accuracy:		13.04 %
Epoch 959 of 2000 took 0.103s
  training loss:		2.297354
  validation loss:		2.247165
  validation accuracy:		13.04 %
Epoch 960 of 2000 took 0.100s
  training loss:		2.297392
  validation loss:		2.247091
  validation accuracy:		12.93 %
Epoch 961 of 2000 took 0.102s
  training loss:		2.297736
  validation loss:		2.247042
  validation accuracy:		12.93 %
Epoch 962 of 2000 took 0.102s
  training loss:		2.297266
  validation loss:		2.247030
  validation accuracy:		13.04 %
Epoch 963 of 2000 took 0.109s
  training loss:		2.296483
  validation loss:		2.246920
  validation accuracy:		13.04 %
Epoch 964 of 2000 took 0.108s
  training loss:		2.297258
  validation loss:		2.246823
  validation accuracy:		13.04 %
Epoch 965 of 2000 took 0.107s
  training loss:		2.297279
  validation loss:		2.246856
  validation accuracy:		13.04 %
Epoch 966 of 2000 took 0.101s
  training loss:		2.297894
  validation loss:		2.246706
  validation accuracy:		13.04 %
Epoch 967 of 2000 took 0.099s
  training loss:		2.296777
  validation loss:		2.246707
  validation accuracy:		13.04 %
Epoch 968 of 2000 took 0.100s
  training loss:		2.296429
  validation loss:		2.246550
  validation accuracy:		13.04 %
Epoch 969 of 2000 took 0.099s
  training loss:		2.296881
  validation loss:		2.246511
  validation accuracy:		13.04 %
Epoch 970 of 2000 took 0.102s
  training loss:		2.296770
  validation loss:		2.246499
  validation accuracy:		13.04 %
Epoch 971 of 2000 took 0.101s
  training loss:		2.297818
  validation loss:		2.246626
  validation accuracy:		13.04 %
Epoch 972 of 2000 took 0.100s
  training loss:		2.297831
  validation loss:		2.246690
  validation accuracy:		13.04 %
Epoch 973 of 2000 took 0.100s
  training loss:		2.297787
  validation loss:		2.246799
  validation accuracy:		17.50 %
Epoch 974 of 2000 took 0.100s
  training loss:		2.297812
  validation loss:		2.247058
  validation accuracy:		13.04 %
Epoch 975 of 2000 took 0.100s
  training loss:		2.297043
  validation loss:		2.247018
  validation accuracy:		13.04 %
Epoch 976 of 2000 took 0.104s
  training loss:		2.297099
  validation loss:		2.247049
  validation accuracy:		13.04 %
Epoch 977 of 2000 took 0.104s
  training loss:		2.297082
  validation loss:		2.246923
  validation accuracy:		13.04 %
Epoch 978 of 2000 took 0.100s
  training loss:		2.296511
  validation loss:		2.246918
  validation accuracy:		13.04 %
Epoch 979 of 2000 took 0.102s
  training loss:		2.297747
  validation loss:		2.246830
  validation accuracy:		13.04 %
Epoch 980 of 2000 took 0.103s
  training loss:		2.297960
  validation loss:		2.247018
  validation accuracy:		13.04 %
Epoch 981 of 2000 took 0.103s
  training loss:		2.297023
  validation loss:		2.247138
  validation accuracy:		12.93 %
Epoch 982 of 2000 took 0.103s
  training loss:		2.296462
  validation loss:		2.247023
  validation accuracy:		12.93 %
Epoch 983 of 2000 took 0.102s
  training loss:		2.297892
  validation loss:		2.246977
  validation accuracy:		12.93 %
Epoch 984 of 2000 took 0.103s
  training loss:		2.296620
  validation loss:		2.247252
  validation accuracy:		12.93 %
Epoch 985 of 2000 took 0.103s
  training loss:		2.296981
  validation loss:		2.247138
  validation accuracy:		12.93 %
Epoch 986 of 2000 took 0.103s
  training loss:		2.295598
  validation loss:		2.247128
  validation accuracy:		12.93 %
Epoch 987 of 2000 took 0.103s
  training loss:		2.296084
  validation loss:		2.246826
  validation accuracy:		12.93 %
Epoch 988 of 2000 took 0.103s
  training loss:		2.296313
  validation loss:		2.246596
  validation accuracy:		12.93 %
Epoch 989 of 2000 took 0.103s
  training loss:		2.297179
  validation loss:		2.246473
  validation accuracy:		12.93 %
Epoch 990 of 2000 took 0.103s
  training loss:		2.297306
  validation loss:		2.246489
  validation accuracy:		12.93 %
Epoch 991 of 2000 took 0.103s
  training loss:		2.296712
  validation loss:		2.246417
  validation accuracy:		12.93 %
Epoch 992 of 2000 took 0.103s
  training loss:		2.296777
  validation loss:		2.246295
  validation accuracy:		12.93 %
Epoch 993 of 2000 took 0.102s
  training loss:		2.297645
  validation loss:		2.246377
  validation accuracy:		12.93 %
Epoch 994 of 2000 took 0.103s
  training loss:		2.297348
  validation loss:		2.246439
  validation accuracy:		12.93 %
Epoch 995 of 2000 took 0.102s
  training loss:		2.297384
  validation loss:		2.246452
  validation accuracy:		12.93 %
Epoch 996 of 2000 took 0.102s
  training loss:		2.296473
  validation loss:		2.246603
  validation accuracy:		12.93 %
Epoch 997 of 2000 took 0.103s
  training loss:		2.297587
  validation loss:		2.246595
  validation accuracy:		12.93 %
Epoch 998 of 2000 took 0.102s
  training loss:		2.297679
  validation loss:		2.246617
  validation accuracy:		12.93 %
Epoch 999 of 2000 took 0.105s
  training loss:		2.297365
  validation loss:		2.246893
  validation accuracy:		12.93 %
Epoch 1000 of 2000 took 0.103s
  training loss:		2.297877
  validation loss:		2.246868
  validation accuracy:		12.93 %
Epoch 1001 of 2000 took 0.103s
  training loss:		2.297090
  validation loss:		2.247124
  validation accuracy:		12.93 %
Epoch 1002 of 2000 took 0.103s
  training loss:		2.297414
  validation loss:		2.247200
  validation accuracy:		12.93 %
Epoch 1003 of 2000 took 0.102s
  training loss:		2.296666
  validation loss:		2.246942
  validation accuracy:		12.93 %
Epoch 1004 of 2000 took 0.102s
  training loss:		2.297031
  validation loss:		2.246861
  validation accuracy:		12.93 %
Epoch 1005 of 2000 took 0.103s
  training loss:		2.297783
  validation loss:		2.246827
  validation accuracy:		12.93 %
Epoch 1006 of 2000 took 0.102s
  training loss:		2.295998
  validation loss:		2.246633
  validation accuracy:		12.93 %
Epoch 1007 of 2000 took 0.103s
  training loss:		2.297651
  validation loss:		2.246666
  validation accuracy:		12.93 %
Epoch 1008 of 2000 took 0.102s
  training loss:		2.296629
  validation loss:		2.246664
  validation accuracy:		12.93 %
Epoch 1009 of 2000 took 0.103s
  training loss:		2.297049
  validation loss:		2.246685
  validation accuracy:		12.93 %
Epoch 1010 of 2000 took 0.103s
  training loss:		2.296770
  validation loss:		2.246496
  validation accuracy:		12.93 %
Epoch 1011 of 2000 took 0.103s
  training loss:		2.297469
  validation loss:		2.246624
  validation accuracy:		12.93 %
Epoch 1012 of 2000 took 0.102s
  training loss:		2.295539
  validation loss:		2.246341
  validation accuracy:		13.04 %
Epoch 1013 of 2000 took 0.102s
  training loss:		2.296472
  validation loss:		2.246495
  validation accuracy:		13.04 %
Epoch 1014 of 2000 took 0.104s
  training loss:		2.297648
  validation loss:		2.246208
  validation accuracy:		13.04 %
Epoch 1015 of 2000 took 0.102s
  training loss:		2.297732
  validation loss:		2.246343
  validation accuracy:		13.04 %
Epoch 1016 of 2000 took 0.102s
  training loss:		2.296951
  validation loss:		2.246559
  validation accuracy:		13.04 %
Epoch 1017 of 2000 took 0.103s
  training loss:		2.296404
  validation loss:		2.246504
  validation accuracy:		13.04 %
Epoch 1018 of 2000 took 0.103s
  training loss:		2.296778
  validation loss:		2.246465
  validation accuracy:		13.04 %
Epoch 1019 of 2000 took 0.103s
  training loss:		2.296367
  validation loss:		2.246427
  validation accuracy:		13.04 %
Epoch 1020 of 2000 took 0.103s
  training loss:		2.296791
  validation loss:		2.246414
  validation accuracy:		13.04 %
Epoch 1021 of 2000 took 0.103s
  training loss:		2.297294
  validation loss:		2.246463
  validation accuracy:		13.04 %
Epoch 1022 of 2000 took 0.102s
  training loss:		2.296902
  validation loss:		2.246458
  validation accuracy:		14.02 %
Epoch 1023 of 2000 took 0.103s
  training loss:		2.297052
  validation loss:		2.246370
  validation accuracy:		12.83 %
Epoch 1024 of 2000 took 0.102s
  training loss:		2.297211
  validation loss:		2.246371
  validation accuracy:		13.04 %
Epoch 1025 of 2000 took 0.103s
  training loss:		2.297038
  validation loss:		2.246566
  validation accuracy:		13.04 %
Epoch 1026 of 2000 took 0.102s
  training loss:		2.296891
  validation loss:		2.246407
  validation accuracy:		13.04 %
Epoch 1027 of 2000 took 0.102s
  training loss:		2.297064
  validation loss:		2.246404
  validation accuracy:		13.04 %
Epoch 1028 of 2000 took 0.103s
  training loss:		2.297670
  validation loss:		2.246524
  validation accuracy:		13.04 %
Epoch 1029 of 2000 took 0.103s
  training loss:		2.296852
  validation loss:		2.246590
  validation accuracy:		13.04 %
Epoch 1030 of 2000 took 0.103s
  training loss:		2.297231
  validation loss:		2.246663
  validation accuracy:		13.04 %
Epoch 1031 of 2000 took 0.103s
  training loss:		2.297223
  validation loss:		2.246584
  validation accuracy:		13.04 %
Epoch 1032 of 2000 took 0.103s
  training loss:		2.296583
  validation loss:		2.246775
  validation accuracy:		12.93 %
Epoch 1033 of 2000 took 0.103s
  training loss:		2.297031
  validation loss:		2.246700
  validation accuracy:		12.93 %
Epoch 1034 of 2000 took 0.103s
  training loss:		2.297019
  validation loss:		2.246739
  validation accuracy:		12.93 %
Epoch 1035 of 2000 took 0.102s
  training loss:		2.296342
  validation loss:		2.246716
  validation accuracy:		12.93 %
Epoch 1036 of 2000 took 0.103s
  training loss:		2.297360
  validation loss:		2.246716
  validation accuracy:		12.93 %
Epoch 1037 of 2000 took 0.102s
  training loss:		2.295268
  validation loss:		2.246464
  validation accuracy:		12.93 %
Epoch 1038 of 2000 took 0.103s
  training loss:		2.297374
  validation loss:		2.246473
  validation accuracy:		12.93 %
Epoch 1039 of 2000 took 0.102s
  training loss:		2.297176
  validation loss:		2.246363
  validation accuracy:		12.93 %
Epoch 1040 of 2000 took 0.103s
  training loss:		2.297592
  validation loss:		2.246639
  validation accuracy:		12.93 %
Epoch 1041 of 2000 took 0.102s
  training loss:		2.297851
  validation loss:		2.246739
  validation accuracy:		12.93 %
Epoch 1042 of 2000 took 0.102s
  training loss:		2.296683
  validation loss:		2.246834
  validation accuracy:		12.93 %
Epoch 1043 of 2000 took 0.103s
  training loss:		2.296364
  validation loss:		2.246734
  validation accuracy:		12.93 %
Epoch 1044 of 2000 took 0.103s
  training loss:		2.297086
  validation loss:		2.246681
  validation accuracy:		12.93 %
Epoch 1045 of 2000 took 0.103s
  training loss:		2.297143
  validation loss:		2.246680
  validation accuracy:		12.93 %
Epoch 1046 of 2000 took 0.102s
  training loss:		2.297332
  validation loss:		2.246892
  validation accuracy:		12.93 %
Epoch 1047 of 2000 took 0.102s
  training loss:		2.296482
  validation loss:		2.246852
  validation accuracy:		13.04 %
Epoch 1048 of 2000 took 0.102s
  training loss:		2.296406
  validation loss:		2.246802
  validation accuracy:		13.04 %
Epoch 1049 of 2000 took 0.103s
  training loss:		2.296554
  validation loss:		2.246709
  validation accuracy:		13.04 %
Epoch 1050 of 2000 took 0.102s
  training loss:		2.297810
  validation loss:		2.247039
  validation accuracy:		13.04 %
Epoch 1051 of 2000 took 0.102s
  training loss:		2.296834
  validation loss:		2.247089
  validation accuracy:		13.04 %
Epoch 1052 of 2000 took 0.102s
  training loss:		2.296772
  validation loss:		2.246963
  validation accuracy:		13.04 %
Epoch 1053 of 2000 took 0.102s
  training loss:		2.297248
  validation loss:		2.247009
  validation accuracy:		13.04 %
Epoch 1054 of 2000 took 0.102s
  training loss:		2.296972
  validation loss:		2.247021
  validation accuracy:		13.04 %
Epoch 1055 of 2000 took 0.102s
  training loss:		2.297055
  validation loss:		2.247108
  validation accuracy:		13.04 %
Epoch 1056 of 2000 took 0.103s
  training loss:		2.296507
  validation loss:		2.246994
  validation accuracy:		13.04 %
Epoch 1057 of 2000 took 0.102s
  training loss:		2.296823
  validation loss:		2.246981
  validation accuracy:		13.04 %
Epoch 1058 of 2000 took 0.102s
  training loss:		2.296942
  validation loss:		2.246955
  validation accuracy:		13.04 %
Epoch 1059 of 2000 took 0.103s
  training loss:		2.297144
  validation loss:		2.246917
  validation accuracy:		13.04 %
Epoch 1060 of 2000 took 0.103s
  training loss:		2.297088
  validation loss:		2.246850
  validation accuracy:		13.04 %
Epoch 1061 of 2000 took 0.102s
  training loss:		2.297099
  validation loss:		2.247112
  validation accuracy:		13.04 %
Epoch 1062 of 2000 took 0.103s
  training loss:		2.295994
  validation loss:		2.247023
  validation accuracy:		13.04 %
Epoch 1063 of 2000 took 0.102s
  training loss:		2.297502
  validation loss:		2.247160
  validation accuracy:		13.04 %
Epoch 1064 of 2000 took 0.103s
  training loss:		2.296876
  validation loss:		2.246899
  validation accuracy:		14.13 %
Epoch 1065 of 2000 took 0.102s
  training loss:		2.297424
  validation loss:		2.246847
  validation accuracy:		12.93 %
Epoch 1066 of 2000 took 0.103s
  training loss:		2.296814
  validation loss:		2.246901
  validation accuracy:		12.93 %
Epoch 1067 of 2000 took 0.103s
  training loss:		2.297627
  validation loss:		2.247046
  validation accuracy:		12.93 %
Epoch 1068 of 2000 took 0.103s
  training loss:		2.296880
  validation loss:		2.247229
  validation accuracy:		12.83 %
Epoch 1069 of 2000 took 0.103s
  training loss:		2.296972
  validation loss:		2.247281
  validation accuracy:		12.93 %
Epoch 1070 of 2000 took 0.103s
  training loss:		2.296539
  validation loss:		2.247095
  validation accuracy:		12.83 %
Epoch 1071 of 2000 took 0.103s
  training loss:		2.296282
  validation loss:		2.247269
  validation accuracy:		13.04 %
Epoch 1072 of 2000 took 0.103s
  training loss:		2.295688
  validation loss:		2.247005
  validation accuracy:		12.93 %
Epoch 1073 of 2000 took 0.104s
  training loss:		2.296463
  validation loss:		2.246714
  validation accuracy:		12.93 %
Epoch 1074 of 2000 took 0.103s
  training loss:		2.296668
  validation loss:		2.246649
  validation accuracy:		12.93 %
Epoch 1075 of 2000 took 0.102s
  training loss:		2.296699
  validation loss:		2.246594
  validation accuracy:		12.93 %
Epoch 1076 of 2000 took 0.102s
  training loss:		2.297059
  validation loss:		2.246637
  validation accuracy:		12.93 %
Epoch 1077 of 2000 took 0.102s
  training loss:		2.296577
  validation loss:		2.246336
  validation accuracy:		13.04 %
Epoch 1078 of 2000 took 0.102s
  training loss:		2.297001
  validation loss:		2.246260
  validation accuracy:		13.04 %
Epoch 1079 of 2000 took 0.103s
  training loss:		2.297184
  validation loss:		2.246275
  validation accuracy:		13.04 %
Epoch 1080 of 2000 took 0.103s
  training loss:		2.297628
  validation loss:		2.246532
  validation accuracy:		13.04 %
Epoch 1081 of 2000 took 0.103s
  training loss:		2.296935
  validation loss:		2.246810
  validation accuracy:		13.04 %
Epoch 1082 of 2000 took 0.103s
  training loss:		2.297616
  validation loss:		2.246992
  validation accuracy:		13.04 %
Epoch 1083 of 2000 took 0.102s
  training loss:		2.296399
  validation loss:		2.247046
  validation accuracy:		13.04 %
Epoch 1084 of 2000 took 0.102s
  training loss:		2.296449
  validation loss:		2.246989
  validation accuracy:		13.04 %
Epoch 1085 of 2000 took 0.103s
  training loss:		2.296683
  validation loss:		2.246726
  validation accuracy:		12.83 %
Epoch 1086 of 2000 took 0.102s
  training loss:		2.297116
  validation loss:		2.246610
  validation accuracy:		12.93 %
Epoch 1087 of 2000 took 0.102s
  training loss:		2.297468
  validation loss:		2.246830
  validation accuracy:		12.93 %
Epoch 1088 of 2000 took 0.103s
  training loss:		2.295971
  validation loss:		2.246715
  validation accuracy:		12.93 %
Epoch 1089 of 2000 took 0.102s
  training loss:		2.296361
  validation loss:		2.246519
  validation accuracy:		12.93 %
Epoch 1090 of 2000 took 0.102s
  training loss:		2.296296
  validation loss:		2.246471
  validation accuracy:		12.93 %
Epoch 1091 of 2000 took 0.102s
  training loss:		2.295979
  validation loss:		2.246512
  validation accuracy:		12.93 %
Epoch 1092 of 2000 took 0.103s
  training loss:		2.297427
  validation loss:		2.246520
  validation accuracy:		13.26 %
Epoch 1093 of 2000 took 0.102s
  training loss:		2.296488
  validation loss:		2.246440
  validation accuracy:		12.93 %
Epoch 1094 of 2000 took 0.102s
  training loss:		2.296480
  validation loss:		2.246313
  validation accuracy:		12.93 %
Epoch 1095 of 2000 took 0.103s
  training loss:		2.295912
  validation loss:		2.246127
  validation accuracy:		12.93 %
Epoch 1096 of 2000 took 0.102s
  training loss:		2.295832
  validation loss:		2.245879
  validation accuracy:		13.04 %
Epoch 1097 of 2000 took 0.102s
  training loss:		2.296366
  validation loss:		2.245652
  validation accuracy:		13.04 %
Epoch 1098 of 2000 took 0.103s
  training loss:		2.297101
  validation loss:		2.245788
  validation accuracy:		12.93 %
Epoch 1099 of 2000 took 0.103s
  training loss:		2.296882
  validation loss:		2.245746
  validation accuracy:		12.93 %
Epoch 1100 of 2000 took 0.102s
  training loss:		2.296846
  validation loss:		2.245843
  validation accuracy:		12.93 %
Epoch 1101 of 2000 took 0.103s
  training loss:		2.296087
  validation loss:		2.245825
  validation accuracy:		12.93 %
Epoch 1102 of 2000 took 0.103s
  training loss:		2.297553
  validation loss:		2.246113
  validation accuracy:		12.93 %
Epoch 1103 of 2000 took 0.103s
  training loss:		2.296624
  validation loss:		2.246254
  validation accuracy:		12.93 %
Epoch 1104 of 2000 took 0.102s
  training loss:		2.296850
  validation loss:		2.246309
  validation accuracy:		13.04 %
Epoch 1105 of 2000 took 0.103s
  training loss:		2.296161
  validation loss:		2.246381
  validation accuracy:		13.04 %
Epoch 1106 of 2000 took 0.103s
  training loss:		2.297053
  validation loss:		2.246357
  validation accuracy:		13.04 %
Epoch 1107 of 2000 took 0.103s
  training loss:		2.297491
  validation loss:		2.246482
  validation accuracy:		12.93 %
Epoch 1108 of 2000 took 0.103s
  training loss:		2.296950
  validation loss:		2.246591
  validation accuracy:		12.93 %
Epoch 1109 of 2000 took 0.103s
  training loss:		2.296428
  validation loss:		2.246802
  validation accuracy:		12.93 %
Epoch 1110 of 2000 took 0.103s
  training loss:		2.296506
  validation loss:		2.246533
  validation accuracy:		12.93 %
Epoch 1111 of 2000 took 0.103s
  training loss:		2.296501
  validation loss:		2.246497
  validation accuracy:		12.93 %
Epoch 1112 of 2000 took 0.102s
  training loss:		2.295647
  validation loss:		2.246348
  validation accuracy:		12.93 %
Epoch 1113 of 2000 took 0.102s
  training loss:		2.296619
  validation loss:		2.246208
  validation accuracy:		12.93 %
Epoch 1114 of 2000 took 0.103s
  training loss:		2.296710
  validation loss:		2.246201
  validation accuracy:		12.93 %
Epoch 1115 of 2000 took 0.102s
  training loss:		2.297278
  validation loss:		2.246330
  validation accuracy:		12.93 %
Epoch 1116 of 2000 took 0.103s
  training loss:		2.296697
  validation loss:		2.246362
  validation accuracy:		16.20 %
Epoch 1117 of 2000 took 0.102s
  training loss:		2.297096
  validation loss:		2.246694
  validation accuracy:		12.93 %
Epoch 1118 of 2000 took 0.103s
  training loss:		2.296987
  validation loss:		2.246779
  validation accuracy:		12.93 %
Epoch 1119 of 2000 took 0.102s
  training loss:		2.297393
  validation loss:		2.246900
  validation accuracy:		12.93 %
Epoch 1120 of 2000 took 0.103s
  training loss:		2.296584
  validation loss:		2.247161
  validation accuracy:		12.93 %
Epoch 1121 of 2000 took 0.103s
  training loss:		2.297036
  validation loss:		2.247040
  validation accuracy:		14.57 %
Epoch 1122 of 2000 took 0.103s
  training loss:		2.297266
  validation loss:		2.247100
  validation accuracy:		12.93 %
Epoch 1123 of 2000 took 0.103s
  training loss:		2.296488
  validation loss:		2.247017
  validation accuracy:		12.93 %
Epoch 1124 of 2000 took 0.103s
  training loss:		2.297307
  validation loss:		2.247242
  validation accuracy:		12.83 %
Epoch 1125 of 2000 took 0.102s
  training loss:		2.296911
  validation loss:		2.247137
  validation accuracy:		13.04 %
Epoch 1126 of 2000 took 0.102s
  training loss:		2.296094
  validation loss:		2.246973
  validation accuracy:		13.04 %
Epoch 1127 of 2000 took 0.103s
  training loss:		2.295629
  validation loss:		2.246747
  validation accuracy:		13.04 %
Epoch 1128 of 2000 took 0.103s
  training loss:		2.296671
  validation loss:		2.246637
  validation accuracy:		13.04 %
Epoch 1129 of 2000 took 0.102s
  training loss:		2.296496
  validation loss:		2.246482
  validation accuracy:		13.04 %
Epoch 1130 of 2000 took 0.103s
  training loss:		2.296061
  validation loss:		2.246287
  validation accuracy:		13.04 %
Epoch 1131 of 2000 took 0.104s
  training loss:		2.296536
  validation loss:		2.246144
  validation accuracy:		13.04 %
Epoch 1132 of 2000 took 0.102s
  training loss:		2.296668
  validation loss:		2.246011
  validation accuracy:		13.04 %
Epoch 1133 of 2000 took 0.102s
  training loss:		2.296230
  validation loss:		2.245968
  validation accuracy:		13.04 %
Epoch 1134 of 2000 took 0.103s
  training loss:		2.296931
  validation loss:		2.246102
  validation accuracy:		13.04 %
Epoch 1135 of 2000 took 0.102s
  training loss:		2.296487
  validation loss:		2.246180
  validation accuracy:		13.04 %
Epoch 1136 of 2000 took 0.102s
  training loss:		2.296595
  validation loss:		2.246161
  validation accuracy:		13.04 %
Epoch 1137 of 2000 took 0.103s
  training loss:		2.296598
  validation loss:		2.246154
  validation accuracy:		13.04 %
Epoch 1138 of 2000 took 0.102s
  training loss:		2.296500
  validation loss:		2.246281
  validation accuracy:		13.04 %
Epoch 1139 of 2000 took 0.102s
  training loss:		2.295972
  validation loss:		2.245996
  validation accuracy:		13.04 %
Epoch 1140 of 2000 took 0.102s
  training loss:		2.296268
  validation loss:		2.246106
  validation accuracy:		13.04 %
Epoch 1141 of 2000 took 0.103s
  training loss:		2.296583
  validation loss:		2.246131
  validation accuracy:		13.04 %
Epoch 1142 of 2000 took 0.103s
  training loss:		2.295716
  validation loss:		2.246001
  validation accuracy:		13.04 %
Epoch 1143 of 2000 took 0.102s
  training loss:		2.296789
  validation loss:		2.245849
  validation accuracy:		13.04 %
Epoch 1144 of 2000 took 0.103s
  training loss:		2.296013
  validation loss:		2.245726
  validation accuracy:		13.04 %
Epoch 1145 of 2000 took 0.101s
  training loss:		2.297327
  validation loss:		2.245740
  validation accuracy:		13.04 %
Epoch 1146 of 2000 took 0.099s
  training loss:		2.296571
  validation loss:		2.245895
  validation accuracy:		13.04 %
Epoch 1147 of 2000 took 0.099s
  training loss:		2.296360
  validation loss:		2.245694
  validation accuracy:		12.93 %
Epoch 1148 of 2000 took 0.099s
  training loss:		2.296866
  validation loss:		2.246014
  validation accuracy:		12.93 %
Epoch 1149 of 2000 took 0.099s
  training loss:		2.296561
  validation loss:		2.245892
  validation accuracy:		12.93 %
Epoch 1150 of 2000 took 0.099s
  training loss:		2.296580
  validation loss:		2.245996
  validation accuracy:		12.93 %
Epoch 1151 of 2000 took 0.099s
  training loss:		2.296793
  validation loss:		2.245984
  validation accuracy:		12.93 %
Epoch 1152 of 2000 took 0.099s
  training loss:		2.296416
  validation loss:		2.246121
  validation accuracy:		12.83 %
Epoch 1153 of 2000 took 0.099s
  training loss:		2.296838
  validation loss:		2.246229
  validation accuracy:		12.93 %
Epoch 1154 of 2000 took 0.099s
  training loss:		2.296862
  validation loss:		2.246240
  validation accuracy:		12.93 %
Epoch 1155 of 2000 took 0.099s
  training loss:		2.297123
  validation loss:		2.246475
  validation accuracy:		12.83 %
Epoch 1156 of 2000 took 0.099s
  training loss:		2.296185
  validation loss:		2.246415
  validation accuracy:		12.93 %
Epoch 1157 of 2000 took 0.099s
  training loss:		2.296801
  validation loss:		2.246590
  validation accuracy:		12.93 %
Epoch 1158 of 2000 took 0.099s
  training loss:		2.296151
  validation loss:		2.246635
  validation accuracy:		12.93 %
Epoch 1159 of 2000 took 0.099s
  training loss:		2.297582
  validation loss:		2.246672
  validation accuracy:		12.93 %
Epoch 1160 of 2000 took 0.099s
  training loss:		2.296871
  validation loss:		2.246928
  validation accuracy:		12.93 %
Epoch 1161 of 2000 took 0.100s
  training loss:		2.297076
  validation loss:		2.247062
  validation accuracy:		12.93 %
Epoch 1162 of 2000 took 0.099s
  training loss:		2.296554
  validation loss:		2.247037
  validation accuracy:		12.93 %
Epoch 1163 of 2000 took 0.099s
  training loss:		2.295831
  validation loss:		2.246660
  validation accuracy:		12.93 %
Epoch 1164 of 2000 took 0.099s
  training loss:		2.296678
  validation loss:		2.246796
  validation accuracy:		12.93 %
Epoch 1165 of 2000 took 0.099s
  training loss:		2.296607
  validation loss:		2.246756
  validation accuracy:		12.93 %
Epoch 1166 of 2000 took 0.099s
  training loss:		2.296122
  validation loss:		2.246613
  validation accuracy:		12.93 %
Epoch 1167 of 2000 took 0.099s
  training loss:		2.296376
  validation loss:		2.246659
  validation accuracy:		12.93 %
Epoch 1168 of 2000 took 0.099s
  training loss:		2.295836
  validation loss:		2.246498
  validation accuracy:		12.93 %
Epoch 1169 of 2000 took 0.099s
  training loss:		2.295932
  validation loss:		2.246377
  validation accuracy:		12.93 %
Epoch 1170 of 2000 took 0.099s
  training loss:		2.296613
  validation loss:		2.246327
  validation accuracy:		12.93 %
Epoch 1171 of 2000 took 0.099s
  training loss:		2.297062
  validation loss:		2.246500
  validation accuracy:		12.93 %
Epoch 1172 of 2000 took 0.099s
  training loss:		2.296518
  validation loss:		2.246569
  validation accuracy:		12.93 %
Epoch 1173 of 2000 took 0.099s
  training loss:		2.296445
  validation loss:		2.246549
  validation accuracy:		15.87 %
Epoch 1174 of 2000 took 0.099s
  training loss:		2.296098
  validation loss:		2.246465
  validation accuracy:		13.04 %
Epoch 1175 of 2000 took 0.099s
  training loss:		2.295363
  validation loss:		2.246411
  validation accuracy:		13.04 %
Epoch 1176 of 2000 took 0.099s
  training loss:		2.296409
  validation loss:		2.246145
  validation accuracy:		12.93 %
Epoch 1177 of 2000 took 0.099s
  training loss:		2.297725
  validation loss:		2.246344
  validation accuracy:		13.04 %
Epoch 1178 of 2000 took 0.099s
  training loss:		2.296609
  validation loss:		2.246531
  validation accuracy:		12.93 %
Epoch 1179 of 2000 took 0.099s
  training loss:		2.296804
  validation loss:		2.246692
  validation accuracy:		12.93 %
Epoch 1180 of 2000 took 0.099s
  training loss:		2.296341
  validation loss:		2.246924
  validation accuracy:		12.93 %
Epoch 1181 of 2000 took 0.099s
  training loss:		2.295582
  validation loss:		2.246750
  validation accuracy:		12.93 %
Epoch 1182 of 2000 took 0.099s
  training loss:		2.296161
  validation loss:		2.246403
  validation accuracy:		12.83 %
Epoch 1183 of 2000 took 0.099s
  training loss:		2.296057
  validation loss:		2.246579
  validation accuracy:		15.00 %
Epoch 1184 of 2000 took 0.099s
  training loss:		2.296881
  validation loss:		2.246552
  validation accuracy:		13.04 %
Epoch 1185 of 2000 took 0.099s
  training loss:		2.297118
  validation loss:		2.246717
  validation accuracy:		12.93 %
Epoch 1186 of 2000 took 0.099s
  training loss:		2.296544
  validation loss:		2.246648
  validation accuracy:		13.59 %
Epoch 1187 of 2000 took 0.099s
  training loss:		2.295990
  validation loss:		2.246770
  validation accuracy:		12.93 %
Epoch 1188 of 2000 took 0.099s
  training loss:		2.296350
  validation loss:		2.246881
  validation accuracy:		12.93 %
Epoch 1189 of 2000 took 0.099s
  training loss:		2.295903
  validation loss:		2.246610
  validation accuracy:		12.93 %
Epoch 1190 of 2000 took 0.099s
  training loss:		2.295584
  validation loss:		2.246436
  validation accuracy:		12.93 %
Epoch 1191 of 2000 took 0.099s
  training loss:		2.296576
  validation loss:		2.246442
  validation accuracy:		12.93 %
Epoch 1192 of 2000 took 0.100s
  training loss:		2.296592
  validation loss:		2.246361
  validation accuracy:		12.93 %
Epoch 1193 of 2000 took 0.099s
  training loss:		2.295727
  validation loss:		2.246217
  validation accuracy:		12.93 %
Epoch 1194 of 2000 took 0.099s
  training loss:		2.296677
  validation loss:		2.246197
  validation accuracy:		12.93 %
Epoch 1195 of 2000 took 0.099s
  training loss:		2.295996
  validation loss:		2.246169
  validation accuracy:		12.93 %
Epoch 1196 of 2000 took 0.099s
  training loss:		2.296199
  validation loss:		2.246143
  validation accuracy:		12.93 %
Epoch 1197 of 2000 took 0.099s
  training loss:		2.295629
  validation loss:		2.246119
  validation accuracy:		12.93 %
Epoch 1198 of 2000 took 0.099s
  training loss:		2.296715
  validation loss:		2.245986
  validation accuracy:		12.93 %
Epoch 1199 of 2000 took 0.099s
  training loss:		2.296422
  validation loss:		2.245850
  validation accuracy:		12.93 %
Epoch 1200 of 2000 took 0.099s
  training loss:		2.295693
  validation loss:		2.245809
  validation accuracy:		12.93 %
Epoch 1201 of 2000 took 0.099s
  training loss:		2.296109
  validation loss:		2.245739
  validation accuracy:		16.74 %
Epoch 1202 of 2000 took 0.099s
  training loss:		2.296783
  validation loss:		2.245998
  validation accuracy:		12.93 %
Epoch 1203 of 2000 took 0.099s
  training loss:		2.296385
  validation loss:		2.246081
  validation accuracy:		13.04 %
Epoch 1204 of 2000 took 0.099s
  training loss:		2.295420
  validation loss:		2.245824
  validation accuracy:		12.93 %
Epoch 1205 of 2000 took 0.099s
  training loss:		2.295808
  validation loss:		2.245638
  validation accuracy:		12.93 %
Epoch 1206 of 2000 took 0.099s
  training loss:		2.296307
  validation loss:		2.245614
  validation accuracy:		14.78 %
Epoch 1207 of 2000 took 0.099s
  training loss:		2.295395
  validation loss:		2.245430
  validation accuracy:		13.04 %
Epoch 1208 of 2000 took 0.099s
  training loss:		2.296561
  validation loss:		2.245471
  validation accuracy:		13.04 %
Epoch 1209 of 2000 took 0.099s
  training loss:		2.296025
  validation loss:		2.245574
  validation accuracy:		13.04 %
Epoch 1210 of 2000 took 0.099s
  training loss:		2.295198
  validation loss:		2.245369
  validation accuracy:		13.04 %
Epoch 1211 of 2000 took 0.099s
  training loss:		2.295559
  validation loss:		2.245169
  validation accuracy:		13.04 %
Epoch 1212 of 2000 took 0.099s
  training loss:		2.296695
  validation loss:		2.245217
  validation accuracy:		13.04 %
Epoch 1213 of 2000 took 0.099s
  training loss:		2.296227
  validation loss:		2.245332
  validation accuracy:		13.04 %
Epoch 1214 of 2000 took 0.099s
  training loss:		2.296442
  validation loss:		2.245219
  validation accuracy:		13.04 %
Epoch 1215 of 2000 took 0.099s
  training loss:		2.296081
  validation loss:		2.244985
  validation accuracy:		13.04 %
Epoch 1216 of 2000 took 0.102s
  training loss:		2.295932
  validation loss:		2.245125
  validation accuracy:		13.04 %
Epoch 1217 of 2000 took 0.105s
  training loss:		2.296120
  validation loss:		2.245223
  validation accuracy:		13.04 %
Epoch 1218 of 2000 took 0.142s
  training loss:		2.295875
  validation loss:		2.245313
  validation accuracy:		13.04 %
Epoch 1219 of 2000 took 0.097s
  training loss:		2.296410
  validation loss:		2.245397
  validation accuracy:		13.04 %
Epoch 1220 of 2000 took 0.098s
  training loss:		2.296906
  validation loss:		2.245474
  validation accuracy:		13.04 %
Epoch 1221 of 2000 took 0.102s
  training loss:		2.296435
  validation loss:		2.245810
  validation accuracy:		12.93 %
Epoch 1222 of 2000 took 0.102s
  training loss:		2.296623
  validation loss:		2.245694
  validation accuracy:		12.93 %
Epoch 1223 of 2000 took 0.100s
  training loss:		2.295467
  validation loss:		2.245807
  validation accuracy:		12.93 %
Epoch 1224 of 2000 took 0.099s
  training loss:		2.295827
  validation loss:		2.245763
  validation accuracy:		12.93 %
Epoch 1225 of 2000 took 0.098s
  training loss:		2.296437
  validation loss:		2.245658
  validation accuracy:		12.93 %
Epoch 1226 of 2000 took 0.096s
  training loss:		2.295372
  validation loss:		2.245478
  validation accuracy:		12.93 %
Epoch 1227 of 2000 took 0.096s
  training loss:		2.296514
  validation loss:		2.245536
  validation accuracy:		13.04 %
Epoch 1228 of 2000 took 0.095s
  training loss:		2.295253
  validation loss:		2.245383
  validation accuracy:		13.04 %
Epoch 1229 of 2000 took 0.096s
  training loss:		2.295619
  validation loss:		2.245306
  validation accuracy:		13.04 %
Epoch 1230 of 2000 took 0.096s
  training loss:		2.294736
  validation loss:		2.245087
  validation accuracy:		13.04 %
Epoch 1231 of 2000 took 0.096s
  training loss:		2.296568
  validation loss:		2.245137
  validation accuracy:		13.04 %
Epoch 1232 of 2000 took 0.096s
  training loss:		2.296624
  validation loss:		2.245291
  validation accuracy:		13.04 %
Epoch 1233 of 2000 took 0.095s
  training loss:		2.296601
  validation loss:		2.245380
  validation accuracy:		13.04 %
Epoch 1234 of 2000 took 0.096s
  training loss:		2.295871
  validation loss:		2.245343
  validation accuracy:		13.04 %
Epoch 1235 of 2000 took 0.095s
  training loss:		2.296464
  validation loss:		2.245316
  validation accuracy:		13.04 %
Epoch 1236 of 2000 took 0.096s
  training loss:		2.295936
  validation loss:		2.245187
  validation accuracy:		12.93 %
Epoch 1237 of 2000 took 0.096s
  training loss:		2.295621
  validation loss:		2.245081
  validation accuracy:		12.93 %
Epoch 1238 of 2000 took 0.097s
  training loss:		2.295098
  validation loss:		2.245048
  validation accuracy:		12.93 %
Epoch 1239 of 2000 took 0.096s
  training loss:		2.296033
  validation loss:		2.244893
  validation accuracy:		12.93 %
Epoch 1240 of 2000 took 0.096s
  training loss:		2.296565
  validation loss:		2.244938
  validation accuracy:		12.93 %
Epoch 1241 of 2000 took 0.096s
  training loss:		2.295350
  validation loss:		2.244841
  validation accuracy:		12.93 %
Epoch 1242 of 2000 took 0.096s
  training loss:		2.296983
  validation loss:		2.244935
  validation accuracy:		12.93 %
Epoch 1243 of 2000 took 0.096s
  training loss:		2.296011
  validation loss:		2.244990
  validation accuracy:		12.93 %
Epoch 1244 of 2000 took 0.096s
  training loss:		2.296206
  validation loss:		2.245173
  validation accuracy:		12.93 %
Epoch 1245 of 2000 took 0.096s
  training loss:		2.296654
  validation loss:		2.245375
  validation accuracy:		12.93 %
Epoch 1246 of 2000 took 0.096s
  training loss:		2.297132
  validation loss:		2.245396
  validation accuracy:		12.93 %
Epoch 1247 of 2000 took 0.096s
  training loss:		2.296735
  validation loss:		2.245688
  validation accuracy:		12.93 %
Epoch 1248 of 2000 took 0.096s
  training loss:		2.296360
  validation loss:		2.246024
  validation accuracy:		12.93 %
Epoch 1249 of 2000 took 0.096s
  training loss:		2.296449
  validation loss:		2.246328
  validation accuracy:		12.93 %
Epoch 1250 of 2000 took 0.096s
  training loss:		2.296397
  validation loss:		2.246148
  validation accuracy:		12.93 %
Epoch 1251 of 2000 took 0.096s
  training loss:		2.296554
  validation loss:		2.246192
  validation accuracy:		12.93 %
Epoch 1252 of 2000 took 0.096s
  training loss:		2.296672
  validation loss:		2.246046
  validation accuracy:		16.09 %
Epoch 1253 of 2000 took 0.097s
  training loss:		2.296142
  validation loss:		2.246231
  validation accuracy:		12.93 %
Epoch 1254 of 2000 took 0.096s
  training loss:		2.297481
  validation loss:		2.246371
  validation accuracy:		13.04 %
Epoch 1255 of 2000 took 0.096s
  training loss:		2.296109
  validation loss:		2.246435
  validation accuracy:		13.04 %
Epoch 1256 of 2000 took 0.096s
  training loss:		2.295892
  validation loss:		2.246447
  validation accuracy:		13.04 %
Epoch 1257 of 2000 took 0.098s
  training loss:		2.296305
  validation loss:		2.246457
  validation accuracy:		13.48 %
Epoch 1258 of 2000 took 0.096s
  training loss:		2.295767
  validation loss:		2.246624
  validation accuracy:		12.93 %
Epoch 1259 of 2000 took 0.096s
  training loss:		2.296798
  validation loss:		2.246551
  validation accuracy:		12.93 %
Epoch 1260 of 2000 took 0.096s
  training loss:		2.297250
  validation loss:		2.246644
  validation accuracy:		12.93 %
Epoch 1261 of 2000 took 0.096s
  training loss:		2.296550
  validation loss:		2.246691
  validation accuracy:		13.59 %
Epoch 1262 of 2000 took 0.099s
  training loss:		2.297409
  validation loss:		2.246908
  validation accuracy:		12.93 %
Epoch 1263 of 2000 took 0.103s
  training loss:		2.295262
  validation loss:		2.246815
  validation accuracy:		12.93 %
Epoch 1264 of 2000 took 0.105s
  training loss:		2.295147
  validation loss:		2.246479
  validation accuracy:		12.93 %
Epoch 1265 of 2000 took 0.099s
  training loss:		2.296595
  validation loss:		2.246237
  validation accuracy:		13.04 %
Epoch 1266 of 2000 took 0.095s
  training loss:		2.296333
  validation loss:		2.246226
  validation accuracy:		13.59 %
Epoch 1267 of 2000 took 0.096s
  training loss:		2.296108
  validation loss:		2.246493
  validation accuracy:		12.93 %
Epoch 1268 of 2000 took 0.095s
  training loss:		2.296948
  validation loss:		2.246324
  validation accuracy:		12.93 %
Epoch 1269 of 2000 took 0.095s
  training loss:		2.295409
  validation loss:		2.246417
  validation accuracy:		12.93 %
Epoch 1270 of 2000 took 0.095s
  training loss:		2.295694
  validation loss:		2.246275
  validation accuracy:		16.52 %
Epoch 1271 of 2000 took 0.095s
  training loss:		2.295711
  validation loss:		2.246053
  validation accuracy:		13.04 %
Epoch 1272 of 2000 took 0.095s
  training loss:		2.296329
  validation loss:		2.245888
  validation accuracy:		14.46 %
Epoch 1273 of 2000 took 0.095s
  training loss:		2.296875
  validation loss:		2.246243
  validation accuracy:		12.83 %
Epoch 1274 of 2000 took 0.095s
  training loss:		2.296200
  validation loss:		2.246180
  validation accuracy:		13.04 %
Epoch 1275 of 2000 took 0.098s
  training loss:		2.296100
  validation loss:		2.246278
  validation accuracy:		12.83 %
Epoch 1276 of 2000 took 0.098s
  training loss:		2.296617
  validation loss:		2.246397
  validation accuracy:		13.04 %
Epoch 1277 of 2000 took 0.095s
  training loss:		2.295373
  validation loss:		2.246111
  validation accuracy:		13.04 %
Epoch 1278 of 2000 took 0.097s
  training loss:		2.296144
  validation loss:		2.245941
  validation accuracy:		13.04 %
Epoch 1279 of 2000 took 0.097s
  training loss:		2.295827
  validation loss:		2.246011
  validation accuracy:		13.04 %
Epoch 1280 of 2000 took 0.096s
  training loss:		2.295874
  validation loss:		2.246032
  validation accuracy:		13.04 %
Epoch 1281 of 2000 took 0.096s
  training loss:		2.295645
  validation loss:		2.245827
  validation accuracy:		13.04 %
Epoch 1282 of 2000 took 0.096s
  training loss:		2.295951
  validation loss:		2.245791
  validation accuracy:		13.04 %
Epoch 1283 of 2000 took 0.096s
  training loss:		2.296374
  validation loss:		2.245738
  validation accuracy:		13.04 %
Epoch 1284 of 2000 took 0.097s
  training loss:		2.295772
  validation loss:		2.245714
  validation accuracy:		13.04 %
Epoch 1285 of 2000 took 0.096s
  training loss:		2.296158
  validation loss:		2.245558
  validation accuracy:		13.26 %
Epoch 1286 of 2000 took 0.096s
  training loss:		2.296278
  validation loss:		2.245777
  validation accuracy:		13.04 %
Epoch 1287 of 2000 took 0.096s
  training loss:		2.296117
  validation loss:		2.245752
  validation accuracy:		13.04 %
Epoch 1288 of 2000 took 0.096s
  training loss:		2.295869
  validation loss:		2.245754
  validation accuracy:		13.04 %
Epoch 1289 of 2000 took 0.096s
  training loss:		2.295599
  validation loss:		2.245735
  validation accuracy:		13.70 %
Epoch 1290 of 2000 took 0.096s
  training loss:		2.296842
  validation loss:		2.246054
  validation accuracy:		12.93 %
Epoch 1291 of 2000 took 0.096s
  training loss:		2.296203
  validation loss:		2.246104
  validation accuracy:		12.93 %
Epoch 1292 of 2000 took 0.096s
  training loss:		2.295142
  validation loss:		2.245843
  validation accuracy:		12.93 %
Epoch 1293 of 2000 took 0.096s
  training loss:		2.295732
  validation loss:		2.245822
  validation accuracy:		20.22 %
Epoch 1294 of 2000 took 0.096s
  training loss:		2.295988
  validation loss:		2.245736
  validation accuracy:		13.04 %
Epoch 1295 of 2000 took 0.096s
  training loss:		2.296263
  validation loss:		2.245762
  validation accuracy:		13.04 %
Epoch 1296 of 2000 took 0.096s
  training loss:		2.296050
  validation loss:		2.245634
  validation accuracy:		13.48 %
Epoch 1297 of 2000 took 0.096s
  training loss:		2.295720
  validation loss:		2.245488
  validation accuracy:		13.04 %
Epoch 1298 of 2000 took 0.096s
  training loss:		2.296752
  validation loss:		2.245498
  validation accuracy:		12.93 %
Epoch 1299 of 2000 took 0.096s
  training loss:		2.295079
  validation loss:		2.245588
  validation accuracy:		12.93 %
Epoch 1300 of 2000 took 0.096s
  training loss:		2.295966
  validation loss:		2.245380
  validation accuracy:		12.93 %
Epoch 1301 of 2000 took 0.096s
  training loss:		2.295820
  validation loss:		2.245637
  validation accuracy:		12.93 %
Epoch 1302 of 2000 took 0.096s
  training loss:		2.296120
  validation loss:		2.245669
  validation accuracy:		12.93 %
Epoch 1303 of 2000 took 0.096s
  training loss:		2.295257
  validation loss:		2.245501
  validation accuracy:		12.93 %
Epoch 1304 of 2000 took 0.096s
  training loss:		2.295267
  validation loss:		2.245285
  validation accuracy:		12.93 %
Epoch 1305 of 2000 took 0.097s
  training loss:		2.295693
  validation loss:		2.245119
  validation accuracy:		13.04 %
Epoch 1306 of 2000 took 0.096s
  training loss:		2.296274
  validation loss:		2.245138
  validation accuracy:		13.04 %
Epoch 1307 of 2000 took 0.096s
  training loss:		2.295883
  validation loss:		2.245328
  validation accuracy:		12.83 %
Epoch 1308 of 2000 took 0.096s
  training loss:		2.296290
  validation loss:		2.245299
  validation accuracy:		12.93 %
Epoch 1309 of 2000 took 0.096s
  training loss:		2.296232
  validation loss:		2.245591
  validation accuracy:		12.93 %
Epoch 1310 of 2000 took 0.096s
  training loss:		2.296554
  validation loss:		2.245761
  validation accuracy:		12.93 %
Epoch 1311 of 2000 took 0.096s
  training loss:		2.296293
  validation loss:		2.245878
  validation accuracy:		12.93 %
Epoch 1312 of 2000 took 0.096s
  training loss:		2.295738
  validation loss:		2.245967
  validation accuracy:		12.93 %
Epoch 1313 of 2000 took 0.096s
  training loss:		2.296032
  validation loss:		2.245882
  validation accuracy:		12.93 %
Epoch 1314 of 2000 took 0.096s
  training loss:		2.296345
  validation loss:		2.245946
  validation accuracy:		12.93 %
Epoch 1315 of 2000 took 0.097s
  training loss:		2.296110
  validation loss:		2.246110
  validation accuracy:		17.50 %
Epoch 1316 of 2000 took 0.096s
  training loss:		2.295799
  validation loss:		2.246220
  validation accuracy:		13.04 %
Epoch 1317 of 2000 took 0.096s
  training loss:		2.296444
  validation loss:		2.246136
  validation accuracy:		13.04 %
Epoch 1318 of 2000 took 0.096s
  training loss:		2.295202
  validation loss:		2.246120
  validation accuracy:		13.04 %
Epoch 1319 of 2000 took 0.096s
  training loss:		2.295286
  validation loss:		2.245876
  validation accuracy:		13.04 %
Epoch 1320 of 2000 took 0.096s
  training loss:		2.295721
  validation loss:		2.245795
  validation accuracy:		13.04 %
Epoch 1321 of 2000 took 0.096s
  training loss:		2.295981
  validation loss:		2.245738
  validation accuracy:		13.04 %
Epoch 1322 of 2000 took 0.096s
  training loss:		2.296314
  validation loss:		2.245730
  validation accuracy:		13.04 %
Epoch 1323 of 2000 took 0.096s
  training loss:		2.296282
  validation loss:		2.245931
  validation accuracy:		13.04 %
Epoch 1324 of 2000 took 0.096s
  training loss:		2.295664
  validation loss:		2.245770
  validation accuracy:		16.09 %
Epoch 1325 of 2000 took 0.096s
  training loss:		2.295622
  validation loss:		2.246017
  validation accuracy:		13.48 %
Epoch 1326 of 2000 took 0.096s
  training loss:		2.296202
  validation loss:		2.245720
  validation accuracy:		12.93 %
Epoch 1327 of 2000 took 0.096s
  training loss:		2.295970
  validation loss:		2.245833
  validation accuracy:		12.93 %
Epoch 1328 of 2000 took 0.096s
  training loss:		2.295149
  validation loss:		2.245529
  validation accuracy:		12.93 %
Epoch 1329 of 2000 took 0.096s
  training loss:		2.296232
  validation loss:		2.245487
  validation accuracy:		12.93 %
Epoch 1330 of 2000 took 0.096s
  training loss:		2.295141
  validation loss:		2.245490
  validation accuracy:		12.93 %
Epoch 1331 of 2000 took 0.099s
  training loss:		2.295675
  validation loss:		2.245198
  validation accuracy:		13.04 %
Epoch 1332 of 2000 took 0.096s
  training loss:		2.295839
  validation loss:		2.245462
  validation accuracy:		13.04 %
Epoch 1333 of 2000 took 0.096s
  training loss:		2.296069
  validation loss:		2.245314
  validation accuracy:		13.04 %
Epoch 1334 of 2000 took 0.096s
  training loss:		2.296012
  validation loss:		2.245493
  validation accuracy:		13.04 %
Epoch 1335 of 2000 took 0.096s
  training loss:		2.295876
  validation loss:		2.245469
  validation accuracy:		13.04 %
Epoch 1336 of 2000 took 0.096s
  training loss:		2.296174
  validation loss:		2.245482
  validation accuracy:		13.04 %
Epoch 1337 of 2000 took 0.096s
  training loss:		2.296552
  validation loss:		2.245727
  validation accuracy:		13.04 %
Epoch 1338 of 2000 took 0.096s
  training loss:		2.295586
  validation loss:		2.245856
  validation accuracy:		13.04 %
Epoch 1339 of 2000 took 0.096s
  training loss:		2.295355
  validation loss:		2.245707
  validation accuracy:		13.04 %
Epoch 1340 of 2000 took 0.096s
  training loss:		2.295861
  validation loss:		2.245799
  validation accuracy:		13.04 %
Epoch 1341 of 2000 took 0.096s
  training loss:		2.296937
  validation loss:		2.245981
  validation accuracy:		17.61 %
Epoch 1342 of 2000 took 0.096s
  training loss:		2.294979
  validation loss:		2.245734
  validation accuracy:		12.93 %
Epoch 1343 of 2000 took 0.096s
  training loss:		2.296226
  validation loss:		2.245894
  validation accuracy:		12.93 %
Epoch 1344 of 2000 took 0.096s
  training loss:		2.296632
  validation loss:		2.245970
  validation accuracy:		12.93 %
Epoch 1345 of 2000 took 0.096s
  training loss:		2.295897
  validation loss:		2.245952
  validation accuracy:		12.93 %
Epoch 1346 of 2000 took 0.097s
  training loss:		2.295939
  validation loss:		2.245539
  validation accuracy:		12.93 %
Epoch 1347 of 2000 took 0.097s
  training loss:		2.295999
  validation loss:		2.245684
  validation accuracy:		12.93 %
Epoch 1348 of 2000 took 0.096s
  training loss:		2.295448
  validation loss:		2.245872
  validation accuracy:		12.93 %
Epoch 1349 of 2000 took 0.096s
  training loss:		2.295396
  validation loss:		2.245728
  validation accuracy:		12.93 %
Epoch 1350 of 2000 took 0.097s
  training loss:		2.295689
  validation loss:		2.245633
  validation accuracy:		12.93 %
Epoch 1351 of 2000 took 0.096s
  training loss:		2.295993
  validation loss:		2.245560
  validation accuracy:		12.93 %
Epoch 1352 of 2000 took 0.096s
  training loss:		2.295176
  validation loss:		2.245562
  validation accuracy:		12.93 %
Epoch 1353 of 2000 took 0.096s
  training loss:		2.294361
  validation loss:		2.245462
  validation accuracy:		12.93 %
Epoch 1354 of 2000 took 0.096s
  training loss:		2.296233
  validation loss:		2.245285
  validation accuracy:		12.93 %
Epoch 1355 of 2000 took 0.096s
  training loss:		2.295651
  validation loss:		2.245178
  validation accuracy:		12.93 %
Epoch 1356 of 2000 took 0.096s
  training loss:		2.295773
  validation loss:		2.245136
  validation accuracy:		13.04 %
Epoch 1357 of 2000 took 0.096s
  training loss:		2.296260
  validation loss:		2.245146
  validation accuracy:		13.04 %
Epoch 1358 of 2000 took 0.096s
  training loss:		2.295845
  validation loss:		2.245193
  validation accuracy:		12.93 %
Epoch 1359 of 2000 took 0.096s
  training loss:		2.296222
  validation loss:		2.245212
  validation accuracy:		12.93 %
Epoch 1360 of 2000 took 0.096s
  training loss:		2.295194
  validation loss:		2.245252
  validation accuracy:		12.83 %
Epoch 1361 of 2000 took 0.096s
  training loss:		2.296640
  validation loss:		2.245145
  validation accuracy:		12.83 %
Epoch 1362 of 2000 took 0.096s
  training loss:		2.295774
  validation loss:		2.245055
  validation accuracy:		12.93 %
Epoch 1363 of 2000 took 0.096s
  training loss:		2.296291
  validation loss:		2.245034
  validation accuracy:		12.93 %
Epoch 1364 of 2000 took 0.096s
  training loss:		2.295887
  validation loss:		2.245180
  validation accuracy:		12.93 %
Epoch 1365 of 2000 took 0.096s
  training loss:		2.295922
  validation loss:		2.245387
  validation accuracy:		12.93 %
Epoch 1366 of 2000 took 0.096s
  training loss:		2.296276
  validation loss:		2.245641
  validation accuracy:		12.93 %
Epoch 1367 of 2000 took 0.096s
  training loss:		2.296834
  validation loss:		2.245959
  validation accuracy:		12.93 %
Epoch 1368 of 2000 took 0.096s
  training loss:		2.296130
  validation loss:		2.246051
  validation accuracy:		12.93 %
Epoch 1369 of 2000 took 0.096s
  training loss:		2.296512
  validation loss:		2.246275
  validation accuracy:		15.11 %
Epoch 1370 of 2000 took 0.096s
  training loss:		2.296148
  validation loss:		2.245972
  validation accuracy:		13.04 %
Epoch 1371 of 2000 took 0.096s
  training loss:		2.295671
  validation loss:		2.246262
  validation accuracy:		12.93 %
Epoch 1372 of 2000 took 0.096s
  training loss:		2.295670
  validation loss:		2.246076
  validation accuracy:		12.93 %
Epoch 1373 of 2000 took 0.096s
  training loss:		2.296953
  validation loss:		2.246393
  validation accuracy:		12.93 %
Epoch 1374 of 2000 took 0.096s
  training loss:		2.295415
  validation loss:		2.246467
  validation accuracy:		12.83 %
Epoch 1375 of 2000 took 0.096s
  training loss:		2.295281
  validation loss:		2.245993
  validation accuracy:		13.04 %
Epoch 1376 of 2000 took 0.096s
  training loss:		2.296079
  validation loss:		2.245754
  validation accuracy:		13.04 %
Epoch 1377 of 2000 took 0.096s
  training loss:		2.295904
  validation loss:		2.245857
  validation accuracy:		13.04 %
Epoch 1378 of 2000 took 0.097s
  training loss:		2.296117
  validation loss:		2.245941
  validation accuracy:		13.04 %
Epoch 1379 of 2000 took 0.096s
  training loss:		2.296452
  validation loss:		2.245940
  validation accuracy:		13.04 %
Epoch 1380 of 2000 took 0.096s
  training loss:		2.294883
  validation loss:		2.245743
  validation accuracy:		13.04 %
Epoch 1381 of 2000 took 0.096s
  training loss:		2.295571
  validation loss:		2.245751
  validation accuracy:		13.04 %
Epoch 1382 of 2000 took 0.096s
  training loss:		2.295796
  validation loss:		2.245870
  validation accuracy:		13.04 %
Epoch 1383 of 2000 took 0.096s
  training loss:		2.295808
  validation loss:		2.245685
  validation accuracy:		13.04 %
Epoch 1384 of 2000 took 0.096s
  training loss:		2.296848
  validation loss:		2.245648
  validation accuracy:		13.04 %
Epoch 1385 of 2000 took 0.096s
  training loss:		2.296000
  validation loss:		2.245788
  validation accuracy:		13.04 %
Epoch 1386 of 2000 took 0.096s
  training loss:		2.295525
  validation loss:		2.245723
  validation accuracy:		12.93 %
Epoch 1387 of 2000 took 0.096s
  training loss:		2.295295
  validation loss:		2.245632
  validation accuracy:		12.83 %
Epoch 1388 of 2000 took 0.096s
  training loss:		2.296438
  validation loss:		2.245580
  validation accuracy:		13.04 %
Epoch 1389 of 2000 took 0.096s
  training loss:		2.295958
  validation loss:		2.245668
  validation accuracy:		12.83 %
Epoch 1390 of 2000 took 0.096s
  training loss:		2.295989
  validation loss:		2.245690
  validation accuracy:		13.04 %
Epoch 1391 of 2000 took 0.096s
  training loss:		2.295177
  validation loss:		2.245818
  validation accuracy:		13.04 %
Epoch 1392 of 2000 took 0.096s
  training loss:		2.296210
  validation loss:		2.245845
  validation accuracy:		13.04 %
Epoch 1393 of 2000 took 0.096s
  training loss:		2.296728
  validation loss:		2.245895
  validation accuracy:		13.04 %
Epoch 1394 of 2000 took 0.096s
  training loss:		2.294940
  validation loss:		2.245733
  validation accuracy:		13.04 %
Epoch 1395 of 2000 took 0.096s
  training loss:		2.296512
  validation loss:		2.245850
  validation accuracy:		13.04 %
Epoch 1396 of 2000 took 0.096s
  training loss:		2.295812
  validation loss:		2.245939
  validation accuracy:		13.04 %
Epoch 1397 of 2000 took 0.096s
  training loss:		2.296854
  validation loss:		2.245839
  validation accuracy:		13.04 %
Epoch 1398 of 2000 took 0.096s
  training loss:		2.296483
  validation loss:		2.245769
  validation accuracy:		13.04 %
Epoch 1399 of 2000 took 0.096s
  training loss:		2.295322
  validation loss:		2.246090
  validation accuracy:		13.04 %
Epoch 1400 of 2000 took 0.096s
  training loss:		2.296049
  validation loss:		2.246072
  validation accuracy:		13.04 %
Epoch 1401 of 2000 took 0.096s
  training loss:		2.296420
  validation loss:		2.245979
  validation accuracy:		12.93 %
Epoch 1402 of 2000 took 0.096s
  training loss:		2.295902
  validation loss:		2.246059
  validation accuracy:		13.04 %
Epoch 1403 of 2000 took 0.096s
  training loss:		2.295684
  validation loss:		2.246076
  validation accuracy:		13.04 %
Epoch 1404 of 2000 took 0.096s
  training loss:		2.294941
  validation loss:		2.245805
  validation accuracy:		13.04 %
Epoch 1405 of 2000 took 0.096s
  training loss:		2.295944
  validation loss:		2.245536
  validation accuracy:		13.04 %
Epoch 1406 of 2000 took 0.096s
  training loss:		2.295480
  validation loss:		2.245572
  validation accuracy:		14.02 %
Epoch 1407 of 2000 took 0.096s
  training loss:		2.295932
  validation loss:		2.245487
  validation accuracy:		13.04 %
Epoch 1408 of 2000 took 0.096s
  training loss:		2.295176
  validation loss:		2.245476
  validation accuracy:		13.04 %
Epoch 1409 of 2000 took 0.097s
  training loss:		2.295810
  validation loss:		2.245433
  validation accuracy:		12.93 %
Epoch 1410 of 2000 took 0.096s
  training loss:		2.295939
  validation loss:		2.245323
  validation accuracy:		12.93 %
Epoch 1411 of 2000 took 0.096s
  training loss:		2.294795
  validation loss:		2.245272
  validation accuracy:		15.22 %
Epoch 1412 of 2000 took 0.096s
  training loss:		2.295741
  validation loss:		2.245126
  validation accuracy:		12.93 %
Epoch 1413 of 2000 took 0.096s
  training loss:		2.295787
  validation loss:		2.245097
  validation accuracy:		12.93 %
Epoch 1414 of 2000 took 0.096s
  training loss:		2.295201
  validation loss:		2.245083
  validation accuracy:		12.93 %
Epoch 1415 of 2000 took 0.096s
  training loss:		2.295352
  validation loss:		2.245025
  validation accuracy:		12.93 %
Epoch 1416 of 2000 took 0.096s
  training loss:		2.296111
  validation loss:		2.245123
  validation accuracy:		12.93 %
Epoch 1417 of 2000 took 0.096s
  training loss:		2.295577
  validation loss:		2.245055
  validation accuracy:		12.93 %
Epoch 1418 of 2000 took 0.096s
  training loss:		2.296132
  validation loss:		2.245134
  validation accuracy:		13.04 %
Epoch 1419 of 2000 took 0.097s
  training loss:		2.296083
  validation loss:		2.245246
  validation accuracy:		12.61 %
Epoch 1420 of 2000 took 0.096s
  training loss:		2.295649
  validation loss:		2.245291
  validation accuracy:		12.83 %
Epoch 1421 of 2000 took 0.096s
  training loss:		2.295802
  validation loss:		2.245282
  validation accuracy:		12.83 %
Epoch 1422 of 2000 took 0.096s
  training loss:		2.296262
  validation loss:		2.245365
  validation accuracy:		12.83 %
Epoch 1423 of 2000 took 0.096s
  training loss:		2.295886
  validation loss:		2.245395
  validation accuracy:		13.91 %
Epoch 1424 of 2000 took 0.096s
  training loss:		2.295708
  validation loss:		2.245593
  validation accuracy:		12.93 %
Epoch 1425 of 2000 took 0.096s
  training loss:		2.296098
  validation loss:		2.245591
  validation accuracy:		12.93 %
Epoch 1426 of 2000 took 0.096s
  training loss:		2.295639
  validation loss:		2.245728
  validation accuracy:		12.93 %
Epoch 1427 of 2000 took 0.096s
  training loss:		2.295328
  validation loss:		2.245556
  validation accuracy:		12.93 %
Epoch 1428 of 2000 took 0.097s
  training loss:		2.296184
  validation loss:		2.245562
  validation accuracy:		13.59 %
Epoch 1429 of 2000 took 0.096s
  training loss:		2.296263
  validation loss:		2.245836
  validation accuracy:		12.93 %
Epoch 1430 of 2000 took 0.096s
  training loss:		2.295042
  validation loss:		2.245613
  validation accuracy:		12.93 %
Epoch 1431 of 2000 took 0.096s
  training loss:		2.295570
  validation loss:		2.245465
  validation accuracy:		12.93 %
Epoch 1432 of 2000 took 0.096s
  training loss:		2.296018
  validation loss:		2.245425
  validation accuracy:		12.93 %
Epoch 1433 of 2000 took 0.096s
  training loss:		2.295228
  validation loss:		2.245374
  validation accuracy:		12.93 %
Epoch 1434 of 2000 took 0.096s
  training loss:		2.296497
  validation loss:		2.245361
  validation accuracy:		12.93 %
Epoch 1435 of 2000 took 0.096s
  training loss:		2.296275
  validation loss:		2.245639
  validation accuracy:		12.93 %
Epoch 1436 of 2000 took 0.096s
  training loss:		2.295513
  validation loss:		2.245636
  validation accuracy:		12.93 %
Epoch 1437 of 2000 took 0.096s
  training loss:		2.295658
  validation loss:		2.245686
  validation accuracy:		12.93 %
Epoch 1438 of 2000 took 0.096s
  training loss:		2.295804
  validation loss:		2.245550
  validation accuracy:		12.93 %
Epoch 1439 of 2000 took 0.096s
  training loss:		2.295782
  validation loss:		2.245684
  validation accuracy:		12.93 %
Epoch 1440 of 2000 took 0.098s
  training loss:		2.295749
  validation loss:		2.245575
  validation accuracy:		12.93 %
Epoch 1441 of 2000 took 0.096s
  training loss:		2.295964
  validation loss:		2.245505
  validation accuracy:		12.93 %
Epoch 1442 of 2000 took 0.096s
  training loss:		2.295991
  validation loss:		2.245594
  validation accuracy:		12.93 %
Epoch 1443 of 2000 took 0.096s
  training loss:		2.296335
  validation loss:		2.245829
  validation accuracy:		13.04 %
Epoch 1444 of 2000 took 0.096s
  training loss:		2.295680
  validation loss:		2.245749
  validation accuracy:		12.93 %
Epoch 1445 of 2000 took 0.096s
  training loss:		2.295593
  validation loss:		2.245607
  validation accuracy:		12.93 %
Epoch 1446 of 2000 took 0.096s
  training loss:		2.295516
  validation loss:		2.245664
  validation accuracy:		12.93 %
Epoch 1447 of 2000 took 0.096s
  training loss:		2.295520
  validation loss:		2.245558
  validation accuracy:		12.93 %
Epoch 1448 of 2000 took 0.096s
  training loss:		2.296250
  validation loss:		2.245647
  validation accuracy:		12.93 %
Epoch 1449 of 2000 took 0.096s
  training loss:		2.296769
  validation loss:		2.245756
  validation accuracy:		12.93 %
Epoch 1450 of 2000 took 0.096s
  training loss:		2.296304
  validation loss:		2.246089
  validation accuracy:		12.93 %
Epoch 1451 of 2000 took 0.096s
  training loss:		2.295129
  validation loss:		2.246104
  validation accuracy:		12.93 %
Epoch 1452 of 2000 took 0.096s
  training loss:		2.295874
  validation loss:		2.246109
  validation accuracy:		12.93 %
Epoch 1453 of 2000 took 0.096s
  training loss:		2.294749
  validation loss:		2.245919
  validation accuracy:		12.93 %
Epoch 1454 of 2000 took 0.096s
  training loss:		2.295808
  validation loss:		2.245814
  validation accuracy:		12.93 %
Epoch 1455 of 2000 took 0.096s
  training loss:		2.295043
  validation loss:		2.245635
  validation accuracy:		12.93 %
Epoch 1456 of 2000 took 0.096s
  training loss:		2.295696
  validation loss:		2.245458
  validation accuracy:		12.93 %
Epoch 1457 of 2000 took 0.096s
  training loss:		2.295132
  validation loss:		2.245064
  validation accuracy:		12.93 %
Epoch 1458 of 2000 took 0.096s
  training loss:		2.295682
  validation loss:		2.244959
  validation accuracy:		17.93 %
Epoch 1459 of 2000 took 0.096s
  training loss:		2.295380
  validation loss:		2.244978
  validation accuracy:		13.04 %
Epoch 1460 of 2000 took 0.096s
  training loss:		2.295180
  validation loss:		2.244912
  validation accuracy:		13.04 %
Epoch 1461 of 2000 took 0.096s
  training loss:		2.295715
  validation loss:		2.244739
  validation accuracy:		13.04 %
Epoch 1462 of 2000 took 0.096s
  training loss:		2.295762
  validation loss:		2.245023
  validation accuracy:		13.04 %
Epoch 1463 of 2000 took 0.096s
  training loss:		2.295694
  validation loss:		2.244872
  validation accuracy:		13.04 %
Epoch 1464 of 2000 took 0.096s
  training loss:		2.295659
  validation loss:		2.245028
  validation accuracy:		13.04 %
Epoch 1465 of 2000 took 0.096s
  training loss:		2.295774
  validation loss:		2.245206
  validation accuracy:		13.04 %
Epoch 1466 of 2000 took 0.096s
  training loss:		2.296114
  validation loss:		2.245316
  validation accuracy:		13.04 %
Epoch 1467 of 2000 took 0.096s
  training loss:		2.295756
  validation loss:		2.245362
  validation accuracy:		13.04 %
Epoch 1468 of 2000 took 0.096s
  training loss:		2.295528
  validation loss:		2.245377
  validation accuracy:		13.04 %
Epoch 1469 of 2000 took 0.096s
  training loss:		2.295823
  validation loss:		2.245272
  validation accuracy:		12.93 %
Epoch 1470 of 2000 took 0.096s
  training loss:		2.296260
  validation loss:		2.245491
  validation accuracy:		13.04 %
Epoch 1471 of 2000 took 0.097s
  training loss:		2.295438
  validation loss:		2.245489
  validation accuracy:		12.83 %
Epoch 1472 of 2000 took 0.097s
  training loss:		2.295608
  validation loss:		2.245501
  validation accuracy:		12.83 %
Epoch 1473 of 2000 took 0.096s
  training loss:		2.295985
  validation loss:		2.245471
  validation accuracy:		12.93 %
Epoch 1474 of 2000 took 0.096s
  training loss:		2.295255
  validation loss:		2.245507
  validation accuracy:		17.72 %
Epoch 1475 of 2000 took 0.096s
  training loss:		2.295575
  validation loss:		2.245427
  validation accuracy:		12.83 %
Epoch 1476 of 2000 took 0.096s
  training loss:		2.295924
  validation loss:		2.245507
  validation accuracy:		12.83 %
Epoch 1477 of 2000 took 0.096s
  training loss:		2.294987
  validation loss:		2.245391
  validation accuracy:		12.83 %
Epoch 1478 of 2000 took 0.096s
  training loss:		2.294903
  validation loss:		2.245487
  validation accuracy:		13.04 %
Epoch 1479 of 2000 took 0.096s
  training loss:		2.296394
  validation loss:		2.245162
  validation accuracy:		13.04 %
Epoch 1480 of 2000 took 0.097s
  training loss:		2.295732
  validation loss:		2.245082
  validation accuracy:		13.04 %
Epoch 1481 of 2000 took 0.096s
  training loss:		2.295739
  validation loss:		2.245343
  validation accuracy:		13.04 %
Epoch 1482 of 2000 took 0.096s
  training loss:		2.295728
  validation loss:		2.245220
  validation accuracy:		13.04 %
Epoch 1483 of 2000 took 0.096s
  training loss:		2.295308
  validation loss:		2.245199
  validation accuracy:		13.04 %
Epoch 1484 of 2000 took 0.096s
  training loss:		2.295611
  validation loss:		2.245288
  validation accuracy:		13.04 %
Epoch 1485 of 2000 took 0.096s
  training loss:		2.294856
  validation loss:		2.245067
  validation accuracy:		13.04 %
Epoch 1486 of 2000 took 0.096s
  training loss:		2.294900
  validation loss:		2.244985
  validation accuracy:		12.93 %
Epoch 1487 of 2000 took 0.096s
  training loss:		2.295780
  validation loss:		2.244948
  validation accuracy:		12.93 %
Epoch 1488 of 2000 took 0.096s
  training loss:		2.295601
  validation loss:		2.245015
  validation accuracy:		12.93 %
Epoch 1489 of 2000 took 0.096s
  training loss:		2.295580
  validation loss:		2.244884
  validation accuracy:		12.93 %
Epoch 1490 of 2000 took 0.096s
  training loss:		2.295502
  validation loss:		2.244624
  validation accuracy:		12.93 %
Epoch 1491 of 2000 took 0.096s
  training loss:		2.295650
  validation loss:		2.244767
  validation accuracy:		12.93 %
Epoch 1492 of 2000 took 0.097s
  training loss:		2.295082
  validation loss:		2.244805
  validation accuracy:		19.89 %
Epoch 1493 of 2000 took 0.096s
  training loss:		2.295892
  validation loss:		2.244814
  validation accuracy:		13.04 %
Epoch 1494 of 2000 took 0.096s
  training loss:		2.294422
  validation loss:		2.244658
  validation accuracy:		20.76 %
Epoch 1495 of 2000 took 0.096s
  training loss:		2.296033
  validation loss:		2.244596
  validation accuracy:		12.93 %
Epoch 1496 of 2000 took 0.096s
  training loss:		2.295262
  validation loss:		2.244639
  validation accuracy:		12.93 %
Epoch 1497 of 2000 took 0.096s
  training loss:		2.296003
  validation loss:		2.244860
  validation accuracy:		12.93 %
Epoch 1498 of 2000 took 0.096s
  training loss:		2.295726
  validation loss:		2.244993
  validation accuracy:		12.93 %
Epoch 1499 of 2000 took 0.096s
  training loss:		2.295464
  validation loss:		2.244941
  validation accuracy:		12.83 %
Epoch 1500 of 2000 took 0.096s
  training loss:		2.295450
  validation loss:		2.244993
  validation accuracy:		12.93 %
Epoch 1501 of 2000 took 0.096s
  training loss:		2.295203
  validation loss:		2.245127
  validation accuracy:		17.93 %
Epoch 1502 of 2000 took 0.096s
  training loss:		2.295552
  validation loss:		2.245167
  validation accuracy:		16.85 %
Epoch 1503 of 2000 took 0.097s
  training loss:		2.295989
  validation loss:		2.245422
  validation accuracy:		12.93 %
Epoch 1504 of 2000 took 0.096s
  training loss:		2.296510
  validation loss:		2.245634
  validation accuracy:		14.24 %
Epoch 1505 of 2000 took 0.096s
  training loss:		2.295568
  validation loss:		2.245708
  validation accuracy:		13.04 %
Epoch 1506 of 2000 took 0.096s
  training loss:		2.295408
  validation loss:		2.245829
  validation accuracy:		13.04 %
Epoch 1507 of 2000 took 0.096s
  training loss:		2.295779
  validation loss:		2.245882
  validation accuracy:		12.83 %
Epoch 1508 of 2000 took 0.096s
  training loss:		2.294704
  validation loss:		2.245415
  validation accuracy:		12.93 %
Epoch 1509 of 2000 took 0.096s
  training loss:		2.295204
  validation loss:		2.245339
  validation accuracy:		12.93 %
Epoch 1510 of 2000 took 0.096s
  training loss:		2.295101
  validation loss:		2.245248
  validation accuracy:		15.33 %
Epoch 1511 of 2000 took 0.096s
  training loss:		2.295479
  validation loss:		2.245294
  validation accuracy:		12.93 %
Epoch 1512 of 2000 took 0.096s
  training loss:		2.296072
  validation loss:		2.245375
  validation accuracy:		13.59 %
Epoch 1513 of 2000 took 0.096s
  training loss:		2.295097
  validation loss:		2.245209
  validation accuracy:		13.04 %
Epoch 1514 of 2000 took 0.096s
  training loss:		2.295121
  validation loss:		2.245023
  validation accuracy:		13.04 %
Epoch 1515 of 2000 took 0.096s
  training loss:		2.295983
  validation loss:		2.245109
  validation accuracy:		13.04 %
Epoch 1516 of 2000 took 0.097s
  training loss:		2.296586
  validation loss:		2.245377
  validation accuracy:		13.04 %
Epoch 1517 of 2000 took 0.099s
  training loss:		2.295580
  validation loss:		2.245438
  validation accuracy:		13.04 %
Epoch 1518 of 2000 took 0.099s
  training loss:		2.294930
  validation loss:		2.245540
  validation accuracy:		12.93 %
Epoch 1519 of 2000 took 0.099s
  training loss:		2.295591
  validation loss:		2.245580
  validation accuracy:		12.93 %
Epoch 1520 of 2000 took 0.099s
  training loss:		2.294997
  validation loss:		2.245253
  validation accuracy:		12.93 %
Epoch 1521 of 2000 took 0.099s
  training loss:		2.295350
  validation loss:		2.245140
  validation accuracy:		12.93 %
Epoch 1522 of 2000 took 0.099s
  training loss:		2.295845
  validation loss:		2.245111
  validation accuracy:		12.93 %
Epoch 1523 of 2000 took 0.099s
  training loss:		2.296789
  validation loss:		2.245347
  validation accuracy:		12.93 %
Epoch 1524 of 2000 took 0.099s
  training loss:		2.296732
  validation loss:		2.245434
  validation accuracy:		12.93 %
Epoch 1525 of 2000 took 0.099s
  training loss:		2.295484
  validation loss:		2.245636
  validation accuracy:		12.93 %
Epoch 1526 of 2000 took 0.099s
  training loss:		2.295037
  validation loss:		2.245719
  validation accuracy:		12.93 %
Epoch 1527 of 2000 took 0.099s
  training loss:		2.294879
  validation loss:		2.245489
  validation accuracy:		12.93 %
Epoch 1528 of 2000 took 0.099s
  training loss:		2.295694
  validation loss:		2.245438
  validation accuracy:		12.93 %
Epoch 1529 of 2000 took 0.099s
  training loss:		2.295428
  validation loss:		2.245269
  validation accuracy:		13.15 %
Epoch 1530 of 2000 took 0.099s
  training loss:		2.295207
  validation loss:		2.245373
  validation accuracy:		13.04 %
Epoch 1531 of 2000 took 0.100s
  training loss:		2.295435
  validation loss:		2.245279
  validation accuracy:		13.04 %
Epoch 1532 of 2000 took 0.099s
  training loss:		2.295670
  validation loss:		2.245166
  validation accuracy:		13.04 %
Epoch 1533 of 2000 took 0.099s
  training loss:		2.296156
  validation loss:		2.245281
  validation accuracy:		13.04 %
Epoch 1534 of 2000 took 0.100s
  training loss:		2.295666
  validation loss:		2.245540
  validation accuracy:		13.91 %
Epoch 1535 of 2000 took 0.099s
  training loss:		2.294787
  validation loss:		2.245594
  validation accuracy:		17.83 %
Epoch 1536 of 2000 took 0.099s
  training loss:		2.295536
  validation loss:		2.245552
  validation accuracy:		13.04 %
Epoch 1537 of 2000 took 0.099s
  training loss:		2.295078
  validation loss:		2.245379
  validation accuracy:		13.04 %
Epoch 1538 of 2000 took 0.099s
  training loss:		2.295009
  validation loss:		2.245138
  validation accuracy:		13.04 %
Epoch 1539 of 2000 took 0.099s
  training loss:		2.295605
  validation loss:		2.244921
  validation accuracy:		12.83 %
Epoch 1540 of 2000 took 0.099s
  training loss:		2.295501
  validation loss:		2.244791
  validation accuracy:		15.43 %
Epoch 1541 of 2000 took 0.099s
  training loss:		2.295105
  validation loss:		2.244693
  validation accuracy:		12.93 %
Epoch 1542 of 2000 took 0.099s
  training loss:		2.295483
  validation loss:		2.244843
  validation accuracy:		12.93 %
Epoch 1543 of 2000 took 0.099s
  training loss:		2.294535
  validation loss:		2.244745
  validation accuracy:		12.93 %
Epoch 1544 of 2000 took 0.099s
  training loss:		2.296412
  validation loss:		2.244729
  validation accuracy:		12.93 %
Epoch 1545 of 2000 took 0.099s
  training loss:		2.294546
  validation loss:		2.244883
  validation accuracy:		12.93 %
Epoch 1546 of 2000 took 0.099s
  training loss:		2.295453
  validation loss:		2.244819
  validation accuracy:		12.93 %
Epoch 1547 of 2000 took 0.099s
  training loss:		2.294456
  validation loss:		2.244505
  validation accuracy:		12.93 %
Epoch 1548 of 2000 took 0.099s
  training loss:		2.295847
  validation loss:		2.244592
  validation accuracy:		12.93 %
Epoch 1549 of 2000 took 0.099s
  training loss:		2.295227
  validation loss:		2.244675
  validation accuracy:		12.93 %
Epoch 1550 of 2000 took 0.099s
  training loss:		2.296043
  validation loss:		2.244704
  validation accuracy:		12.93 %
Epoch 1551 of 2000 took 0.099s
  training loss:		2.294865
  validation loss:		2.245008
  validation accuracy:		12.93 %
Epoch 1552 of 2000 took 0.099s
  training loss:		2.295718
  validation loss:		2.244721
  validation accuracy:		12.93 %
Epoch 1553 of 2000 took 0.100s
  training loss:		2.295074
  validation loss:		2.244633
  validation accuracy:		12.93 %
Epoch 1554 of 2000 took 0.099s
  training loss:		2.296199
  validation loss:		2.244928
  validation accuracy:		12.93 %
Epoch 1555 of 2000 took 0.099s
  training loss:		2.295566
  validation loss:		2.244898
  validation accuracy:		12.93 %
Epoch 1556 of 2000 took 0.099s
  training loss:		2.294423
  validation loss:		2.244788
  validation accuracy:		12.93 %
Epoch 1557 of 2000 took 0.099s
  training loss:		2.295482
  validation loss:		2.244783
  validation accuracy:		12.93 %
Epoch 1558 of 2000 took 0.099s
  training loss:		2.296386
  validation loss:		2.244785
  validation accuracy:		12.93 %
Epoch 1559 of 2000 took 0.099s
  training loss:		2.295587
  validation loss:		2.244924
  validation accuracy:		12.93 %
Epoch 1560 of 2000 took 0.099s
  training loss:		2.294772
  validation loss:		2.244920
  validation accuracy:		12.93 %
Epoch 1561 of 2000 took 0.099s
  training loss:		2.296646
  validation loss:		2.245038
  validation accuracy:		12.93 %
Epoch 1562 of 2000 took 0.099s
  training loss:		2.295584
  validation loss:		2.245218
  validation accuracy:		12.93 %
Epoch 1563 of 2000 took 0.100s
  training loss:		2.295074
  validation loss:		2.245081
  validation accuracy:		12.93 %
Epoch 1564 of 2000 took 0.100s
  training loss:		2.295541
  validation loss:		2.245116
  validation accuracy:		12.93 %
Epoch 1565 of 2000 took 0.099s
  training loss:		2.295669
  validation loss:		2.245085
  validation accuracy:		12.93 %
Epoch 1566 of 2000 took 0.099s
  training loss:		2.295844
  validation loss:		2.245361
  validation accuracy:		12.93 %
Epoch 1567 of 2000 took 0.099s
  training loss:		2.295475
  validation loss:		2.245439
  validation accuracy:		14.89 %
Epoch 1568 of 2000 took 0.099s
  training loss:		2.295502
  validation loss:		2.245680
  validation accuracy:		13.48 %
Epoch 1569 of 2000 took 0.099s
  training loss:		2.295215
  validation loss:		2.245588
  validation accuracy:		18.15 %
Epoch 1570 of 2000 took 0.099s
  training loss:		2.295035
  validation loss:		2.245549
  validation accuracy:		13.04 %
Epoch 1571 of 2000 took 0.099s
  training loss:		2.295579
  validation loss:		2.245473
  validation accuracy:		12.93 %
Epoch 1572 of 2000 took 0.099s
  training loss:		2.296324
  validation loss:		2.245508
  validation accuracy:		13.04 %
Epoch 1573 of 2000 took 0.099s
  training loss:		2.295743
  validation loss:		2.245630
  validation accuracy:		13.04 %
Epoch 1574 of 2000 took 0.099s
  training loss:		2.294998
  validation loss:		2.245131
  validation accuracy:		13.04 %
Epoch 1575 of 2000 took 0.099s
  training loss:		2.294461
  validation loss:		2.245017
  validation accuracy:		13.04 %
Epoch 1576 of 2000 took 0.099s
  training loss:		2.295004
  validation loss:		2.244712
  validation accuracy:		13.04 %
Epoch 1577 of 2000 took 0.099s
  training loss:		2.295370
  validation loss:		2.244692
  validation accuracy:		13.04 %
Epoch 1578 of 2000 took 0.099s
  training loss:		2.293859
  validation loss:		2.244507
  validation accuracy:		13.04 %
Epoch 1579 of 2000 took 0.099s
  training loss:		2.295012
  validation loss:		2.244384
  validation accuracy:		12.83 %
Epoch 1580 of 2000 took 0.099s
  training loss:		2.294044
  validation loss:		2.244191
  validation accuracy:		12.93 %
Epoch 1581 of 2000 took 0.099s
  training loss:		2.295907
  validation loss:		2.244217
  validation accuracy:		12.93 %
Epoch 1582 of 2000 took 0.099s
  training loss:		2.294994
  validation loss:		2.244208
  validation accuracy:		12.93 %
Epoch 1583 of 2000 took 0.099s
  training loss:		2.295049
  validation loss:		2.244063
  validation accuracy:		12.93 %
Epoch 1584 of 2000 took 0.099s
  training loss:		2.295589
  validation loss:		2.244111
  validation accuracy:		12.93 %
Epoch 1585 of 2000 took 0.099s
  training loss:		2.295522
  validation loss:		2.244285
  validation accuracy:		12.93 %
Epoch 1586 of 2000 took 0.099s
  training loss:		2.295702
  validation loss:		2.244301
  validation accuracy:		13.04 %
Epoch 1587 of 2000 took 0.099s
  training loss:		2.295900
  validation loss:		2.244566
  validation accuracy:		13.04 %
Epoch 1588 of 2000 took 0.099s
  training loss:		2.295315
  validation loss:		2.244722
  validation accuracy:		13.04 %
Epoch 1589 of 2000 took 0.099s
  training loss:		2.295802
  validation loss:		2.245019
  validation accuracy:		13.04 %
Epoch 1590 of 2000 took 0.099s
  training loss:		2.295690
  validation loss:		2.245169
  validation accuracy:		13.04 %
Epoch 1591 of 2000 took 0.099s
  training loss:		2.296012
  validation loss:		2.245392
  validation accuracy:		12.93 %
Epoch 1592 of 2000 took 0.099s
  training loss:		2.296197
  validation loss:		2.245646
  validation accuracy:		12.93 %
Epoch 1593 of 2000 took 0.100s
  training loss:		2.295220
  validation loss:		2.245567
  validation accuracy:		12.93 %
Epoch 1594 of 2000 took 0.100s
  training loss:		2.295544
  validation loss:		2.245473
  validation accuracy:		12.93 %
Epoch 1595 of 2000 took 0.100s
  training loss:		2.295231
  validation loss:		2.245406
  validation accuracy:		12.93 %
Epoch 1596 of 2000 took 0.099s
  training loss:		2.295426
  validation loss:		2.245276
  validation accuracy:		12.93 %
Epoch 1597 of 2000 took 0.099s
  training loss:		2.295085
  validation loss:		2.245318
  validation accuracy:		12.93 %
Epoch 1598 of 2000 took 0.099s
  training loss:		2.295951
  validation loss:		2.245409
  validation accuracy:		17.28 %
Epoch 1599 of 2000 took 0.099s
  training loss:		2.295364
  validation loss:		2.245493
  validation accuracy:		12.83 %
Epoch 1600 of 2000 took 0.099s
  training loss:		2.295263
  validation loss:		2.245430
  validation accuracy:		13.04 %
Epoch 1601 of 2000 took 0.099s
  training loss:		2.295778
  validation loss:		2.245468
  validation accuracy:		13.04 %
Epoch 1602 of 2000 took 0.099s
  training loss:		2.296190
  validation loss:		2.245444
  validation accuracy:		13.04 %
Epoch 1603 of 2000 took 0.099s
  training loss:		2.296696
  validation loss:		2.245545
  validation accuracy:		13.04 %
Epoch 1604 of 2000 took 0.099s
  training loss:		2.296222
  validation loss:		2.245756
  validation accuracy:		13.59 %
Epoch 1605 of 2000 took 0.100s
  training loss:		2.295854
  validation loss:		2.245958
  validation accuracy:		12.93 %
Epoch 1606 of 2000 took 0.099s
  training loss:		2.295928
  validation loss:		2.245974
  validation accuracy:		12.93 %
Epoch 1607 of 2000 took 0.099s
  training loss:		2.295001
  validation loss:		2.245986
  validation accuracy:		12.93 %
Epoch 1608 of 2000 took 0.099s
  training loss:		2.294929
  validation loss:		2.246040
  validation accuracy:		12.93 %
Epoch 1609 of 2000 took 0.099s
  training loss:		2.295440
  validation loss:		2.246162
  validation accuracy:		12.93 %
Epoch 1610 of 2000 took 0.099s
  training loss:		2.295764
  validation loss:		2.246120
  validation accuracy:		12.93 %
Epoch 1611 of 2000 took 0.099s
  training loss:		2.294606
  validation loss:		2.246034
  validation accuracy:		12.93 %
Epoch 1612 of 2000 took 0.099s
  training loss:		2.294655
  validation loss:		2.245776
  validation accuracy:		12.93 %
Epoch 1613 of 2000 took 0.099s
  training loss:		2.295085
  validation loss:		2.245615
  validation accuracy:		12.93 %
Epoch 1614 of 2000 took 0.099s
  training loss:		2.295910
  validation loss:		2.245669
  validation accuracy:		12.93 %
Epoch 1615 of 2000 took 0.099s
  training loss:		2.294957
  validation loss:		2.245561
  validation accuracy:		12.93 %
Epoch 1616 of 2000 took 0.099s
  training loss:		2.295406
  validation loss:		2.245390
  validation accuracy:		12.93 %
Epoch 1617 of 2000 took 0.099s
  training loss:		2.295281
  validation loss:		2.245349
  validation accuracy:		12.93 %
Epoch 1618 of 2000 took 0.100s
  training loss:		2.294905
  validation loss:		2.245317
  validation accuracy:		12.93 %
Epoch 1619 of 2000 took 0.099s
  training loss:		2.296277
  validation loss:		2.245418
  validation accuracy:		12.93 %
Epoch 1620 of 2000 took 0.099s
  training loss:		2.296012
  validation loss:		2.245648
  validation accuracy:		12.93 %
Epoch 1621 of 2000 took 0.099s
  training loss:		2.294844
  validation loss:		2.245553
  validation accuracy:		12.93 %
Epoch 1622 of 2000 took 0.099s
  training loss:		2.295583
  validation loss:		2.245732
  validation accuracy:		13.80 %
Epoch 1623 of 2000 took 0.099s
  training loss:		2.294977
  validation loss:		2.245407
  validation accuracy:		12.93 %
Epoch 1624 of 2000 took 0.099s
  training loss:		2.294810
  validation loss:		2.244912
  validation accuracy:		12.93 %
Epoch 1625 of 2000 took 0.100s
  training loss:		2.295223
  validation loss:		2.244641
  validation accuracy:		12.83 %
Epoch 1626 of 2000 took 0.099s
  training loss:		2.295714
  validation loss:		2.244943
  validation accuracy:		13.04 %
Epoch 1627 of 2000 took 0.099s
  training loss:		2.295509
  validation loss:		2.244927
  validation accuracy:		12.93 %
Epoch 1628 of 2000 took 0.099s
  training loss:		2.295304
  validation loss:		2.244899
  validation accuracy:		13.04 %
Epoch 1629 of 2000 took 0.100s
  training loss:		2.294728
  validation loss:		2.244691
  validation accuracy:		13.04 %
Epoch 1630 of 2000 took 0.099s
  training loss:		2.295564
  validation loss:		2.244644
  validation accuracy:		13.04 %
Epoch 1631 of 2000 took 0.099s
  training loss:		2.295748
  validation loss:		2.244662
  validation accuracy:		13.04 %
Epoch 1632 of 2000 took 0.099s
  training loss:		2.295707
  validation loss:		2.244868
  validation accuracy:		13.04 %
Epoch 1633 of 2000 took 0.099s
  training loss:		2.295043
  validation loss:		2.244873
  validation accuracy:		13.04 %
Epoch 1634 of 2000 took 0.099s
  training loss:		2.295391
  validation loss:		2.244751
  validation accuracy:		13.04 %
Epoch 1635 of 2000 took 0.099s
  training loss:		2.294265
  validation loss:		2.244928
  validation accuracy:		13.04 %
Epoch 1636 of 2000 took 0.099s
  training loss:		2.294627
  validation loss:		2.244832
  validation accuracy:		13.04 %
Epoch 1637 of 2000 took 0.098s
  training loss:		2.295305
  validation loss:		2.244805
  validation accuracy:		13.37 %
Epoch 1638 of 2000 took 0.096s
  training loss:		2.294837
  validation loss:		2.244355
  validation accuracy:		12.93 %
Epoch 1639 of 2000 took 0.096s
  training loss:		2.295680
  validation loss:		2.244440
  validation accuracy:		13.70 %
Epoch 1640 of 2000 took 0.096s
  training loss:		2.295269
  validation loss:		2.244547
  validation accuracy:		12.93 %
Epoch 1641 of 2000 took 0.096s
  training loss:		2.294158
  validation loss:		2.244598
  validation accuracy:		13.37 %
Epoch 1642 of 2000 took 0.096s
  training loss:		2.294775
  validation loss:		2.244408
  validation accuracy:		12.93 %
Epoch 1643 of 2000 took 0.096s
  training loss:		2.295052
  validation loss:		2.244679
  validation accuracy:		13.15 %
Epoch 1644 of 2000 took 0.096s
  training loss:		2.295610
  validation loss:		2.244687
  validation accuracy:		13.04 %
Epoch 1645 of 2000 took 0.096s
  training loss:		2.295592
  validation loss:		2.244654
  validation accuracy:		13.48 %
Epoch 1646 of 2000 took 0.096s
  training loss:		2.294799
  validation loss:		2.244686
  validation accuracy:		13.04 %
Epoch 1647 of 2000 took 0.096s
  training loss:		2.295161
  validation loss:		2.244885
  validation accuracy:		13.37 %
Epoch 1648 of 2000 took 0.096s
  training loss:		2.296363
  validation loss:		2.244969
  validation accuracy:		12.93 %
Epoch 1649 of 2000 took 0.096s
  training loss:		2.295474
  validation loss:		2.245051
  validation accuracy:		12.93 %
Epoch 1650 of 2000 took 0.096s
  training loss:		2.295584
  validation loss:		2.245227
  validation accuracy:		12.93 %
Epoch 1651 of 2000 took 0.096s
  training loss:		2.296099
  validation loss:		2.245104
  validation accuracy:		13.04 %
Epoch 1652 of 2000 took 0.096s
  training loss:		2.295502
  validation loss:		2.245219
  validation accuracy:		13.04 %
Epoch 1653 of 2000 took 0.096s
  training loss:		2.295602
  validation loss:		2.245325
  validation accuracy:		13.04 %
Epoch 1654 of 2000 took 0.096s
  training loss:		2.295261
  validation loss:		2.245506
  validation accuracy:		13.04 %
Epoch 1655 of 2000 took 0.096s
  training loss:		2.295533
  validation loss:		2.245523
  validation accuracy:		13.04 %
Epoch 1656 of 2000 took 0.097s
  training loss:		2.295073
  validation loss:		2.245476
  validation accuracy:		13.04 %
Epoch 1657 of 2000 took 0.096s
  training loss:		2.295075
  validation loss:		2.245389
  validation accuracy:		13.04 %
Epoch 1658 of 2000 took 0.096s
  training loss:		2.295583
  validation loss:		2.245270
  validation accuracy:		13.04 %
Epoch 1659 of 2000 took 0.096s
  training loss:		2.294862
  validation loss:		2.245253
  validation accuracy:		13.04 %
Epoch 1660 of 2000 took 0.096s
  training loss:		2.295304
  validation loss:		2.245112
  validation accuracy:		13.04 %
Epoch 1661 of 2000 took 0.096s
  training loss:		2.295165
  validation loss:		2.245224
  validation accuracy:		13.04 %
Epoch 1662 of 2000 took 0.096s
  training loss:		2.295132
  validation loss:		2.245234
  validation accuracy:		13.04 %
Epoch 1663 of 2000 took 0.096s
  training loss:		2.295254
  validation loss:		2.245240
  validation accuracy:		13.04 %
Epoch 1664 of 2000 took 0.096s
  training loss:		2.295550
  validation loss:		2.245216
  validation accuracy:		13.04 %
Epoch 1665 of 2000 took 0.096s
  training loss:		2.295066
  validation loss:		2.244884
  validation accuracy:		13.04 %
Epoch 1666 of 2000 took 0.096s
  training loss:		2.295847
  validation loss:		2.244896
  validation accuracy:		13.04 %
Epoch 1667 of 2000 took 0.096s
  training loss:		2.295124
  validation loss:		2.245069
  validation accuracy:		13.04 %
Epoch 1668 of 2000 took 0.096s
  training loss:		2.294776
  validation loss:		2.245151
  validation accuracy:		13.04 %
Epoch 1669 of 2000 took 0.096s
  training loss:		2.295801
  validation loss:		2.245127
  validation accuracy:		13.04 %
Epoch 1670 of 2000 took 0.096s
  training loss:		2.296187
  validation loss:		2.245233
  validation accuracy:		13.04 %
Epoch 1671 of 2000 took 0.096s
  training loss:		2.295687
  validation loss:		2.245263
  validation accuracy:		13.04 %
Epoch 1672 of 2000 took 0.096s
  training loss:		2.295087
  validation loss:		2.245447
  validation accuracy:		13.04 %
Epoch 1673 of 2000 took 0.096s
  training loss:		2.295395
  validation loss:		2.245450
  validation accuracy:		13.04 %
Epoch 1674 of 2000 took 0.096s
  training loss:		2.295819
  validation loss:		2.245707
  validation accuracy:		13.04 %
Epoch 1675 of 2000 took 0.096s
  training loss:		2.294778
  validation loss:		2.245694
  validation accuracy:		13.04 %
Epoch 1676 of 2000 took 0.096s
  training loss:		2.294515
  validation loss:		2.245359
  validation accuracy:		13.04 %
Epoch 1677 of 2000 took 0.096s
  training loss:		2.295015
  validation loss:		2.245188
  validation accuracy:		13.04 %
Epoch 1678 of 2000 took 0.096s
  training loss:		2.295720
  validation loss:		2.245123
  validation accuracy:		16.09 %
Epoch 1679 of 2000 took 0.096s
  training loss:		2.294999
  validation loss:		2.245060
  validation accuracy:		16.85 %
Epoch 1680 of 2000 took 0.096s
  training loss:		2.294175
  validation loss:		2.244877
  validation accuracy:		12.93 %
Epoch 1681 of 2000 took 0.096s
  training loss:		2.294652
  validation loss:		2.244779
  validation accuracy:		12.93 %
Epoch 1682 of 2000 took 0.096s
  training loss:		2.294673
  validation loss:		2.244533
  validation accuracy:		15.33 %
Epoch 1683 of 2000 took 0.096s
  training loss:		2.296359
  validation loss:		2.244658
  validation accuracy:		13.04 %
Epoch 1684 of 2000 took 0.096s
  training loss:		2.295728
  validation loss:		2.244823
  validation accuracy:		13.04 %
Epoch 1685 of 2000 took 0.096s
  training loss:		2.294248
  validation loss:		2.244734
  validation accuracy:		12.83 %
Epoch 1686 of 2000 took 0.096s
  training loss:		2.295951
  validation loss:		2.245111
  validation accuracy:		12.93 %
Epoch 1687 of 2000 took 0.097s
  training loss:		2.294721
  validation loss:		2.245194
  validation accuracy:		12.93 %
Epoch 1688 of 2000 took 0.096s
  training loss:		2.295761
  validation loss:		2.244960
  validation accuracy:		17.83 %
Epoch 1689 of 2000 took 0.096s
  training loss:		2.296185
  validation loss:		2.245135
  validation accuracy:		13.04 %
Epoch 1690 of 2000 took 0.096s
  training loss:		2.295730
  validation loss:		2.245240
  validation accuracy:		13.04 %
Epoch 1691 of 2000 took 0.096s
  training loss:		2.294568
  validation loss:		2.245363
  validation accuracy:		13.04 %
Epoch 1692 of 2000 took 0.096s
  training loss:		2.295194
  validation loss:		2.245160
  validation accuracy:		13.04 %
Epoch 1693 of 2000 took 0.096s
  training loss:		2.295655
  validation loss:		2.245208
  validation accuracy:		13.04 %
Epoch 1694 of 2000 took 0.096s
  training loss:		2.295149
  validation loss:		2.245050
  validation accuracy:		13.04 %
Epoch 1695 of 2000 took 0.096s
  training loss:		2.294553
  validation loss:		2.244950
  validation accuracy:		13.04 %
Epoch 1696 of 2000 took 0.096s
  training loss:		2.294778
  validation loss:		2.244855
  validation accuracy:		13.04 %
Epoch 1697 of 2000 took 0.096s
  training loss:		2.294907
  validation loss:		2.244590
  validation accuracy:		13.04 %
Epoch 1698 of 2000 took 0.096s
  training loss:		2.296376
  validation loss:		2.244957
  validation accuracy:		13.04 %
Epoch 1699 of 2000 took 0.096s
  training loss:		2.295448
  validation loss:		2.244918
  validation accuracy:		13.04 %
Epoch 1700 of 2000 took 0.096s
  training loss:		2.294993
  validation loss:		2.244923
  validation accuracy:		13.04 %
Epoch 1701 of 2000 took 0.096s
  training loss:		2.295365
  validation loss:		2.245076
  validation accuracy:		13.04 %
Epoch 1702 of 2000 took 0.096s
  training loss:		2.294098
  validation loss:		2.244895
  validation accuracy:		13.91 %
Epoch 1703 of 2000 took 0.096s
  training loss:		2.295802
  validation loss:		2.244910
  validation accuracy:		16.96 %
Epoch 1704 of 2000 took 0.096s
  training loss:		2.295299
  validation loss:		2.244909
  validation accuracy:		12.93 %
Epoch 1705 of 2000 took 0.096s
  training loss:		2.295105
  validation loss:		2.244810
  validation accuracy:		12.93 %
Epoch 1706 of 2000 took 0.096s
  training loss:		2.295677
  validation loss:		2.245005
  validation accuracy:		12.93 %
Epoch 1707 of 2000 took 0.096s
  training loss:		2.295550
  validation loss:		2.245102
  validation accuracy:		12.93 %
Epoch 1708 of 2000 took 0.096s
  training loss:		2.294719
  validation loss:		2.245294
  validation accuracy:		12.93 %
Epoch 1709 of 2000 took 0.096s
  training loss:		2.296049
  validation loss:		2.245246
  validation accuracy:		12.93 %
Epoch 1710 of 2000 took 0.096s
  training loss:		2.295657
  validation loss:		2.245329
  validation accuracy:		12.93 %
Epoch 1711 of 2000 took 0.096s
  training loss:		2.295949
  validation loss:		2.245378
  validation accuracy:		12.93 %
Epoch 1712 of 2000 took 0.096s
  training loss:		2.295021
  validation loss:		2.245313
  validation accuracy:		12.93 %
Epoch 1713 of 2000 took 0.096s
  training loss:		2.294654
  validation loss:		2.245271
  validation accuracy:		12.93 %
Epoch 1714 of 2000 took 0.096s
  training loss:		2.295004
  validation loss:		2.245026
  validation accuracy:		12.93 %
Epoch 1715 of 2000 took 0.096s
  training loss:		2.294779
  validation loss:		2.244784
  validation accuracy:		14.46 %
Epoch 1716 of 2000 took 0.096s
  training loss:		2.295116
  validation loss:		2.244623
  validation accuracy:		12.93 %
Epoch 1717 of 2000 took 0.096s
  training loss:		2.294928
  validation loss:		2.244650
  validation accuracy:		13.04 %
Epoch 1718 of 2000 took 0.097s
  training loss:		2.295905
  validation loss:		2.244817
  validation accuracy:		13.04 %
Epoch 1719 of 2000 took 0.096s
  training loss:		2.295135
  validation loss:		2.244683
  validation accuracy:		13.70 %
Epoch 1720 of 2000 took 0.096s
  training loss:		2.296027
  validation loss:		2.244948
  validation accuracy:		12.93 %
Epoch 1721 of 2000 took 0.096s
  training loss:		2.294514
  validation loss:		2.244908
  validation accuracy:		12.93 %
Epoch 1722 of 2000 took 0.096s
  training loss:		2.295177
  validation loss:		2.244771
  validation accuracy:		12.93 %
Epoch 1723 of 2000 took 0.096s
  training loss:		2.294740
  validation loss:		2.244894
  validation accuracy:		12.93 %
Epoch 1724 of 2000 took 0.096s
  training loss:		2.295358
  validation loss:		2.244933
  validation accuracy:		12.93 %
Epoch 1725 of 2000 took 0.096s
  training loss:		2.295216
  validation loss:		2.244936
  validation accuracy:		12.93 %
Epoch 1726 of 2000 took 0.096s
  training loss:		2.295057
  validation loss:		2.244939
  validation accuracy:		12.93 %
Epoch 1727 of 2000 took 0.096s
  training loss:		2.296120
  validation loss:		2.244888
  validation accuracy:		12.93 %
Epoch 1728 of 2000 took 0.096s
  training loss:		2.294402
  validation loss:		2.244890
  validation accuracy:		12.83 %
Epoch 1729 of 2000 took 0.096s
  training loss:		2.295948
  validation loss:		2.244980
  validation accuracy:		12.93 %
Epoch 1730 of 2000 took 0.096s
  training loss:		2.294946
  validation loss:		2.244884
  validation accuracy:		16.63 %
Epoch 1731 of 2000 took 0.096s
  training loss:		2.295543
  validation loss:		2.245241
  validation accuracy:		12.83 %
Epoch 1732 of 2000 took 0.096s
  training loss:		2.295098
  validation loss:		2.245133
  validation accuracy:		13.04 %
Epoch 1733 of 2000 took 0.096s
  training loss:		2.295059
  validation loss:		2.245058
  validation accuracy:		12.93 %
Epoch 1734 of 2000 took 0.096s
  training loss:		2.295096
  validation loss:		2.245255
  validation accuracy:		12.83 %
Epoch 1735 of 2000 took 0.096s
  training loss:		2.294918
  validation loss:		2.245152
  validation accuracy:		13.04 %
Epoch 1736 of 2000 took 0.096s
  training loss:		2.294968
  validation loss:		2.244947
  validation accuracy:		13.04 %
Epoch 1737 of 2000 took 0.096s
  training loss:		2.294707
  validation loss:		2.244754
  validation accuracy:		13.04 %
Epoch 1738 of 2000 took 0.096s
  training loss:		2.295477
  validation loss:		2.244799
  validation accuracy:		13.04 %
Epoch 1739 of 2000 took 0.096s
  training loss:		2.294909
  validation loss:		2.244733
  validation accuracy:		12.93 %
Epoch 1740 of 2000 took 0.096s
  training loss:		2.295551
  validation loss:		2.244826
  validation accuracy:		13.04 %
Epoch 1741 of 2000 took 0.096s
  training loss:		2.295403
  validation loss:		2.244925
  validation accuracy:		13.04 %
Epoch 1742 of 2000 took 0.099s
  training loss:		2.294570
  validation loss:		2.245104
  validation accuracy:		14.13 %
Epoch 1743 of 2000 took 0.096s
  training loss:		2.295165
  validation loss:		2.244890
  validation accuracy:		13.15 %
Epoch 1744 of 2000 took 0.096s
  training loss:		2.295055
  validation loss:		2.244783
  validation accuracy:		12.83 %
Epoch 1745 of 2000 took 0.096s
  training loss:		2.294718
  validation loss:		2.244580
  validation accuracy:		13.70 %
Epoch 1746 of 2000 took 0.096s
  training loss:		2.294727
  validation loss:		2.244436
  validation accuracy:		18.26 %
Epoch 1747 of 2000 took 0.096s
  training loss:		2.295214
  validation loss:		2.244479
  validation accuracy:		13.59 %
Epoch 1748 of 2000 took 0.096s
  training loss:		2.295502
  validation loss:		2.244592
  validation accuracy:		15.98 %
Epoch 1749 of 2000 took 0.097s
  training loss:		2.295273
  validation loss:		2.244720
  validation accuracy:		12.93 %
Epoch 1750 of 2000 took 0.096s
  training loss:		2.295410
  validation loss:		2.244850
  validation accuracy:		16.85 %
Epoch 1751 of 2000 took 0.096s
  training loss:		2.295678
  validation loss:		2.244935
  validation accuracy:		12.83 %
Epoch 1752 of 2000 took 0.096s
  training loss:		2.295055
  validation loss:		2.244947
  validation accuracy:		13.04 %
Epoch 1753 of 2000 took 0.096s
  training loss:		2.295495
  validation loss:		2.245010
  validation accuracy:		13.04 %
Epoch 1754 of 2000 took 0.096s
  training loss:		2.295402
  validation loss:		2.245106
  validation accuracy:		12.93 %
Epoch 1755 of 2000 took 0.096s
  training loss:		2.295160
  validation loss:		2.245066
  validation accuracy:		16.85 %
Epoch 1756 of 2000 took 0.096s
  training loss:		2.294664
  validation loss:		2.245085
  validation accuracy:		13.26 %
Epoch 1757 of 2000 took 0.096s
  training loss:		2.296037
  validation loss:		2.244875
  validation accuracy:		12.93 %
Epoch 1758 of 2000 took 0.097s
  training loss:		2.295041
  validation loss:		2.244972
  validation accuracy:		12.93 %
Epoch 1759 of 2000 took 0.096s
  training loss:		2.293924
  validation loss:		2.244772
  validation accuracy:		15.11 %
Epoch 1760 of 2000 took 0.096s
  training loss:		2.295236
  validation loss:		2.244747
  validation accuracy:		16.63 %
Epoch 1761 of 2000 took 0.096s
  training loss:		2.294777
  validation loss:		2.244661
  validation accuracy:		14.78 %
Epoch 1762 of 2000 took 0.096s
  training loss:		2.295280
  validation loss:		2.244632
  validation accuracy:		13.59 %
Epoch 1763 of 2000 took 0.096s
  training loss:		2.295143
  validation loss:		2.244652
  validation accuracy:		12.93 %
Epoch 1764 of 2000 took 0.096s
  training loss:		2.295689
  validation loss:		2.244768
  validation accuracy:		18.70 %
Epoch 1765 of 2000 took 0.096s
  training loss:		2.295929
  validation loss:		2.245001
  validation accuracy:		12.93 %
Epoch 1766 of 2000 took 0.096s
  training loss:		2.295297
  validation loss:		2.245193
  validation accuracy:		12.93 %
Epoch 1767 of 2000 took 0.096s
  training loss:		2.294270
  validation loss:		2.245200
  validation accuracy:		12.93 %
Epoch 1768 of 2000 took 0.096s
  training loss:		2.295129
  validation loss:		2.244937
  validation accuracy:		12.93 %
Epoch 1769 of 2000 took 0.096s
  training loss:		2.295657
  validation loss:		2.244951
  validation accuracy:		12.93 %
Epoch 1770 of 2000 took 0.096s
  training loss:		2.295170
  validation loss:		2.245014
  validation accuracy:		12.93 %
Epoch 1771 of 2000 took 0.096s
  training loss:		2.294713
  validation loss:		2.244896
  validation accuracy:		12.93 %
Epoch 1772 of 2000 took 0.096s
  training loss:		2.294550
  validation loss:		2.245006
  validation accuracy:		12.93 %
Epoch 1773 of 2000 took 0.096s
  training loss:		2.295859
  validation loss:		2.245038
  validation accuracy:		12.93 %
Epoch 1774 of 2000 took 0.096s
  training loss:		2.294699
  validation loss:		2.245048
  validation accuracy:		12.93 %
Epoch 1775 of 2000 took 0.096s
  training loss:		2.294307
  validation loss:		2.244903
  validation accuracy:		12.93 %
Epoch 1776 of 2000 took 0.096s
  training loss:		2.294986
  validation loss:		2.244762
  validation accuracy:		12.93 %
Epoch 1777 of 2000 took 0.096s
  training loss:		2.294835
  validation loss:		2.244614
  validation accuracy:		12.93 %
Epoch 1778 of 2000 took 0.096s
  training loss:		2.295401
  validation loss:		2.244855
  validation accuracy:		13.04 %
Epoch 1779 of 2000 took 0.096s
  training loss:		2.294011
  validation loss:		2.244585
  validation accuracy:		12.83 %
Epoch 1780 of 2000 took 0.096s
  training loss:		2.296241
  validation loss:		2.244492
  validation accuracy:		15.11 %
Epoch 1781 of 2000 took 0.097s
  training loss:		2.295545
  validation loss:		2.244591
  validation accuracy:		12.93 %
Epoch 1782 of 2000 took 0.096s
  training loss:		2.294737
  validation loss:		2.244636
  validation accuracy:		12.93 %
Epoch 1783 of 2000 took 0.096s
  training loss:		2.294359
  validation loss:		2.244517
  validation accuracy:		12.93 %
Epoch 1784 of 2000 took 0.096s
  training loss:		2.295978
  validation loss:		2.244476
  validation accuracy:		13.04 %
Epoch 1785 of 2000 took 0.096s
  training loss:		2.294351
  validation loss:		2.244327
  validation accuracy:		13.04 %
Epoch 1786 of 2000 took 0.096s
  training loss:		2.294005
  validation loss:		2.244298
  validation accuracy:		13.04 %
Epoch 1787 of 2000 took 0.096s
  training loss:		2.295246
  validation loss:		2.244396
  validation accuracy:		13.04 %
Epoch 1788 of 2000 took 0.097s
  training loss:		2.295449
  validation loss:		2.244553
  validation accuracy:		13.04 %
Epoch 1789 of 2000 took 0.096s
  training loss:		2.294971
  validation loss:		2.244436
  validation accuracy:		17.07 %
Epoch 1790 of 2000 took 0.096s
  training loss:		2.294508
  validation loss:		2.244285
  validation accuracy:		12.93 %
Epoch 1791 of 2000 took 0.096s
  training loss:		2.294782
  validation loss:		2.244201
  validation accuracy:		12.93 %
Epoch 1792 of 2000 took 0.096s
  training loss:		2.295247
  validation loss:		2.244492
  validation accuracy:		12.93 %
Epoch 1793 of 2000 took 0.096s
  training loss:		2.294803
  validation loss:		2.244575
  validation accuracy:		18.59 %
Epoch 1794 of 2000 took 0.096s
  training loss:		2.295409
  validation loss:		2.244798
  validation accuracy:		15.00 %
Epoch 1795 of 2000 took 0.096s
  training loss:		2.294234
  validation loss:		2.244684
  validation accuracy:		12.93 %
Epoch 1796 of 2000 took 0.096s
  training loss:		2.294732
  validation loss:		2.244365
  validation accuracy:		12.93 %
Epoch 1797 of 2000 took 0.096s
  training loss:		2.295176
  validation loss:		2.244355
  validation accuracy:		12.93 %
Epoch 1798 of 2000 took 0.096s
  training loss:		2.294592
  validation loss:		2.244477
  validation accuracy:		13.04 %
Epoch 1799 of 2000 took 0.096s
  training loss:		2.295000
  validation loss:		2.244218
  validation accuracy:		13.04 %
Epoch 1800 of 2000 took 0.097s
  training loss:		2.294448
  validation loss:		2.244181
  validation accuracy:		12.83 %
Epoch 1801 of 2000 took 0.096s
  training loss:		2.294047
  validation loss:		2.244070
  validation accuracy:		13.04 %
Epoch 1802 of 2000 took 0.096s
  training loss:		2.295010
  validation loss:		2.243843
  validation accuracy:		13.37 %
Epoch 1803 of 2000 took 0.096s
  training loss:		2.295107
  validation loss:		2.243889
  validation accuracy:		13.48 %
Epoch 1804 of 2000 took 0.096s
  training loss:		2.294862
  validation loss:		2.243845
  validation accuracy:		13.04 %
Epoch 1805 of 2000 took 0.096s
  training loss:		2.294864
  validation loss:		2.244095
  validation accuracy:		16.09 %
Epoch 1806 of 2000 took 0.096s
  training loss:		2.295442
  validation loss:		2.244045
  validation accuracy:		12.93 %
Epoch 1807 of 2000 took 0.096s
  training loss:		2.295415
  validation loss:		2.244176
  validation accuracy:		17.39 %
Epoch 1808 of 2000 took 0.096s
  training loss:		2.295842
  validation loss:		2.244253
  validation accuracy:		13.04 %
Epoch 1809 of 2000 took 0.096s
  training loss:		2.295077
  validation loss:		2.244222
  validation accuracy:		13.04 %
Epoch 1810 of 2000 took 0.096s
  training loss:		2.295289
  validation loss:		2.244437
  validation accuracy:		13.04 %
Epoch 1811 of 2000 took 0.096s
  training loss:		2.294425
  validation loss:		2.244523
  validation accuracy:		13.04 %
Epoch 1812 of 2000 took 0.097s
  training loss:		2.295124
  validation loss:		2.244636
  validation accuracy:		13.04 %
Epoch 1813 of 2000 took 0.096s
  training loss:		2.294531
  validation loss:		2.244790
  validation accuracy:		13.04 %
Epoch 1814 of 2000 took 0.096s
  training loss:		2.295165
  validation loss:		2.244712
  validation accuracy:		12.83 %
Epoch 1815 of 2000 took 0.096s
  training loss:		2.295192
  validation loss:		2.244456
  validation accuracy:		14.67 %
Epoch 1816 of 2000 took 0.096s
  training loss:		2.294872
  validation loss:		2.244543
  validation accuracy:		13.04 %
Epoch 1817 of 2000 took 0.096s
  training loss:		2.294757
  validation loss:		2.244549
  validation accuracy:		13.04 %
Epoch 1818 of 2000 took 0.096s
  training loss:		2.295337
  validation loss:		2.244395
  validation accuracy:		13.04 %
Epoch 1819 of 2000 took 0.096s
  training loss:		2.294846
  validation loss:		2.244303
  validation accuracy:		13.59 %
Epoch 1820 of 2000 took 0.096s
  training loss:		2.294563
  validation loss:		2.244418
  validation accuracy:		12.93 %
Epoch 1821 of 2000 took 0.096s
  training loss:		2.295324
  validation loss:		2.244343
  validation accuracy:		16.74 %
Epoch 1822 of 2000 took 0.096s
  training loss:		2.295862
  validation loss:		2.244351
  validation accuracy:		18.59 %
Epoch 1823 of 2000 took 0.096s
  training loss:		2.295364
  validation loss:		2.244652
  validation accuracy:		12.83 %
Epoch 1824 of 2000 took 0.096s
  training loss:		2.294842
  validation loss:		2.244891
  validation accuracy:		16.85 %
Epoch 1825 of 2000 took 0.096s
  training loss:		2.294367
  validation loss:		2.244993
  validation accuracy:		12.83 %
Epoch 1826 of 2000 took 0.096s
  training loss:		2.295938
  validation loss:		2.245103
  validation accuracy:		12.83 %
Epoch 1827 of 2000 took 0.096s
  training loss:		2.295391
  validation loss:		2.245210
  validation accuracy:		13.04 %
Epoch 1828 of 2000 took 0.096s
  training loss:		2.295287
  validation loss:		2.244992
  validation accuracy:		13.04 %
Epoch 1829 of 2000 took 0.096s
  training loss:		2.295430
  validation loss:		2.245134
  validation accuracy:		13.04 %
Epoch 1830 of 2000 took 0.096s
  training loss:		2.295545
  validation loss:		2.245332
  validation accuracy:		13.15 %
Epoch 1831 of 2000 took 0.096s
  training loss:		2.294800
  validation loss:		2.245338
  validation accuracy:		13.37 %
Epoch 1832 of 2000 took 0.096s
  training loss:		2.294943
  validation loss:		2.245392
  validation accuracy:		16.74 %
Epoch 1833 of 2000 took 0.096s
  training loss:		2.294462
  validation loss:		2.245340
  validation accuracy:		12.93 %
Epoch 1834 of 2000 took 0.096s
  training loss:		2.295173
  validation loss:		2.245244
  validation accuracy:		12.93 %
Epoch 1835 of 2000 took 0.096s
  training loss:		2.295286
  validation loss:		2.245388
  validation accuracy:		12.93 %
Epoch 1836 of 2000 took 0.096s
  training loss:		2.295280
  validation loss:		2.245407
  validation accuracy:		12.93 %
Epoch 1837 of 2000 took 0.096s
  training loss:		2.293725
  validation loss:		2.245218
  validation accuracy:		12.93 %
Epoch 1838 of 2000 took 0.096s
  training loss:		2.294447
  validation loss:		2.244924
  validation accuracy:		12.93 %
Epoch 1839 of 2000 took 0.096s
  training loss:		2.294935
  validation loss:		2.244688
  validation accuracy:		12.93 %
Epoch 1840 of 2000 took 0.096s
  training loss:		2.295890
  validation loss:		2.244977
  validation accuracy:		12.93 %
Epoch 1841 of 2000 took 0.096s
  training loss:		2.295536
  validation loss:		2.244974
  validation accuracy:		12.93 %
Epoch 1842 of 2000 took 0.096s
  training loss:		2.295363
  validation loss:		2.245141
  validation accuracy:		12.93 %
Epoch 1843 of 2000 took 0.097s
  training loss:		2.295140
  validation loss:		2.245130
  validation accuracy:		12.93 %
Epoch 1844 of 2000 took 0.096s
  training loss:		2.294749
  validation loss:		2.245060
  validation accuracy:		12.93 %
Epoch 1845 of 2000 took 0.096s
  training loss:		2.294767
  validation loss:		2.245065
  validation accuracy:		12.93 %
Epoch 1846 of 2000 took 0.096s
  training loss:		2.294930
  validation loss:		2.244812
  validation accuracy:		13.15 %
Epoch 1847 of 2000 took 0.096s
  training loss:		2.294013
  validation loss:		2.244675
  validation accuracy:		12.93 %
Epoch 1848 of 2000 took 0.096s
  training loss:		2.295039
  validation loss:		2.244394
  validation accuracy:		12.93 %
Epoch 1849 of 2000 took 0.096s
  training loss:		2.294579
  validation loss:		2.244493
  validation accuracy:		12.93 %
Epoch 1850 of 2000 took 0.096s
  training loss:		2.294707
  validation loss:		2.244330
  validation accuracy:		12.93 %
Epoch 1851 of 2000 took 0.096s
  training loss:		2.294599
  validation loss:		2.244275
  validation accuracy:		13.04 %
Epoch 1852 of 2000 took 0.096s
  training loss:		2.295673
  validation loss:		2.244455
  validation accuracy:		12.93 %
Epoch 1853 of 2000 took 0.096s
  training loss:		2.294849
  validation loss:		2.244309
  validation accuracy:		12.93 %
Epoch 1854 of 2000 took 0.096s
  training loss:		2.295663
  validation loss:		2.244411
  validation accuracy:		13.59 %
Epoch 1855 of 2000 took 0.096s
  training loss:		2.295132
  validation loss:		2.244806
  validation accuracy:		14.13 %
Epoch 1856 of 2000 took 0.096s
  training loss:		2.295122
  validation loss:		2.244846
  validation accuracy:		12.93 %
Epoch 1857 of 2000 took 0.096s
  training loss:		2.295840
  validation loss:		2.245001
  validation accuracy:		12.93 %
Epoch 1858 of 2000 took 0.096s
  training loss:		2.294745
  validation loss:		2.245180
  validation accuracy:		15.43 %
Epoch 1859 of 2000 took 0.096s
  training loss:		2.294514
  validation loss:		2.245132
  validation accuracy:		13.48 %
Epoch 1860 of 2000 took 0.096s
  training loss:		2.294721
  validation loss:		2.245033
  validation accuracy:		12.93 %
Epoch 1861 of 2000 took 0.096s
  training loss:		2.295365
  validation loss:		2.244996
  validation accuracy:		18.91 %
Epoch 1862 of 2000 took 0.096s
  training loss:		2.295207
  validation loss:		2.245109
  validation accuracy:		17.50 %
Epoch 1863 of 2000 took 0.096s
  training loss:		2.294646
  validation loss:		2.245063
  validation accuracy:		13.80 %
Epoch 1864 of 2000 took 0.096s
  training loss:		2.295430
  validation loss:		2.245040
  validation accuracy:		13.37 %
Epoch 1865 of 2000 took 0.096s
  training loss:		2.295245
  validation loss:		2.244972
  validation accuracy:		16.85 %
Epoch 1866 of 2000 took 0.096s
  training loss:		2.295641
  validation loss:		2.245003
  validation accuracy:		21.09 %
Epoch 1867 of 2000 took 0.096s
  training loss:		2.295648
  validation loss:		2.245285
  validation accuracy:		14.13 %
Epoch 1868 of 2000 took 0.096s
  training loss:		2.293931
  validation loss:		2.245298
  validation accuracy:		14.35 %
Epoch 1869 of 2000 took 0.096s
  training loss:		2.295425
  validation loss:		2.245291
  validation accuracy:		12.83 %
Epoch 1870 of 2000 took 0.096s
  training loss:		2.295716
  validation loss:		2.245440
  validation accuracy:		13.04 %
Epoch 1871 of 2000 took 0.096s
  training loss:		2.295216
  validation loss:		2.245506
  validation accuracy:		15.87 %
Epoch 1872 of 2000 took 0.096s
  training loss:		2.295653
  validation loss:		2.245591
  validation accuracy:		13.15 %
Epoch 1873 of 2000 took 0.096s
  training loss:		2.294563
  validation loss:		2.245485
  validation accuracy:		13.37 %
Epoch 1874 of 2000 took 0.096s
  training loss:		2.295471
  validation loss:		2.245607
  validation accuracy:		20.22 %
Epoch 1875 of 2000 took 0.097s
  training loss:		2.294966
  validation loss:		2.245594
  validation accuracy:		12.83 %
Epoch 1876 of 2000 took 0.096s
  training loss:		2.294446
  validation loss:		2.245388
  validation accuracy:		15.65 %
Epoch 1877 of 2000 took 0.096s
  training loss:		2.295512
  validation loss:		2.245263
  validation accuracy:		16.74 %
Epoch 1878 of 2000 took 0.097s
  training loss:		2.295208
  validation loss:		2.245500
  validation accuracy:		13.04 %
Epoch 1879 of 2000 took 0.096s
  training loss:		2.294736
  validation loss:		2.245485
  validation accuracy:		13.04 %
Epoch 1880 of 2000 took 0.096s
  training loss:		2.294584
  validation loss:		2.245312
  validation accuracy:		12.93 %
Epoch 1881 of 2000 took 0.096s
  training loss:		2.294711
  validation loss:		2.245212
  validation accuracy:		12.93 %
Epoch 1882 of 2000 took 0.096s
  training loss:		2.294339
  validation loss:		2.245220
  validation accuracy:		12.93 %
Epoch 1883 of 2000 took 0.096s
  training loss:		2.293788
  validation loss:		2.245002
  validation accuracy:		12.93 %
Epoch 1884 of 2000 took 0.096s
  training loss:		2.294574
  validation loss:		2.244526
  validation accuracy:		12.93 %
Epoch 1885 of 2000 took 0.096s
  training loss:		2.294062
  validation loss:		2.244131
  validation accuracy:		12.93 %
Epoch 1886 of 2000 took 0.097s
  training loss:		2.294229
  validation loss:		2.243896
  validation accuracy:		12.93 %
Epoch 1887 of 2000 took 0.099s
  training loss:		2.294811
  validation loss:		2.243825
  validation accuracy:		12.93 %
Epoch 1888 of 2000 took 0.099s
  training loss:		2.295471
  validation loss:		2.244027
  validation accuracy:		12.93 %
Epoch 1889 of 2000 took 0.099s
  training loss:		2.293970
  validation loss:		2.243859
  validation accuracy:		12.93 %
Epoch 1890 of 2000 took 0.099s
  training loss:		2.294917
  validation loss:		2.243823
  validation accuracy:		17.50 %
Epoch 1891 of 2000 took 0.099s
  training loss:		2.294300
  validation loss:		2.243857
  validation accuracy:		17.72 %
Epoch 1892 of 2000 took 0.099s
  training loss:		2.294291
  validation loss:		2.243665
  validation accuracy:		17.28 %
Epoch 1893 of 2000 took 0.099s
  training loss:		2.295014
  validation loss:		2.243732
  validation accuracy:		13.04 %
Epoch 1894 of 2000 took 0.099s
  training loss:		2.293690
  validation loss:		2.243577
  validation accuracy:		12.83 %
Epoch 1895 of 2000 took 0.099s
  training loss:		2.295089
  validation loss:		2.243551
  validation accuracy:		12.93 %
Epoch 1896 of 2000 took 0.099s
  training loss:		2.293780
  validation loss:		2.243551
  validation accuracy:		12.93 %
Epoch 1897 of 2000 took 0.099s
  training loss:		2.295410
  validation loss:		2.243382
  validation accuracy:		12.93 %
Epoch 1898 of 2000 took 0.099s
  training loss:		2.294484
  validation loss:		2.243412
  validation accuracy:		12.93 %
Epoch 1899 of 2000 took 0.099s
  training loss:		2.295301
  validation loss:		2.243629
  validation accuracy:		12.93 %
Epoch 1900 of 2000 took 0.099s
  training loss:		2.294765
  validation loss:		2.243927
  validation accuracy:		12.93 %
Epoch 1901 of 2000 took 0.099s
  training loss:		2.294327
  validation loss:		2.243918
  validation accuracy:		12.93 %
Epoch 1902 of 2000 took 0.099s
  training loss:		2.293725
  validation loss:		2.243798
  validation accuracy:		12.93 %
Epoch 1903 of 2000 took 0.099s
  training loss:		2.295479
  validation loss:		2.243713
  validation accuracy:		12.93 %
Epoch 1904 of 2000 took 0.099s
  training loss:		2.294829
  validation loss:		2.243805
  validation accuracy:		12.93 %
Epoch 1905 of 2000 took 0.100s
  training loss:		2.294424
  validation loss:		2.243820
  validation accuracy:		12.93 %
Epoch 1906 of 2000 took 0.099s
  training loss:		2.294797
  validation loss:		2.243691
  validation accuracy:		12.93 %
Epoch 1907 of 2000 took 0.099s
  training loss:		2.295790
  validation loss:		2.244041
  validation accuracy:		12.93 %
Epoch 1908 of 2000 took 0.099s
  training loss:		2.294758
  validation loss:		2.244252
  validation accuracy:		12.93 %
Epoch 1909 of 2000 took 0.099s
  training loss:		2.295288
  validation loss:		2.244267
  validation accuracy:		13.59 %
Epoch 1910 of 2000 took 0.099s
  training loss:		2.295169
  validation loss:		2.244553
  validation accuracy:		12.93 %
Epoch 1911 of 2000 took 0.099s
  training loss:		2.294777
  validation loss:		2.244386
  validation accuracy:		15.00 %
Epoch 1912 of 2000 took 0.099s
  training loss:		2.295015
  validation loss:		2.244361
  validation accuracy:		15.65 %
Epoch 1913 of 2000 took 0.099s
  training loss:		2.294277
  validation loss:		2.244648
  validation accuracy:		18.48 %
Epoch 1914 of 2000 took 0.099s
  training loss:		2.294447
  validation loss:		2.244432
  validation accuracy:		18.48 %
Epoch 1915 of 2000 took 0.099s
  training loss:		2.295202
  validation loss:		2.244512
  validation accuracy:		17.17 %
Epoch 1916 of 2000 took 0.099s
  training loss:		2.294760
  validation loss:		2.244444
  validation accuracy:		12.83 %
Epoch 1917 of 2000 took 0.099s
  training loss:		2.294362
  validation loss:		2.244662
  validation accuracy:		12.83 %
Epoch 1918 of 2000 took 0.099s
  training loss:		2.294831
  validation loss:		2.244540
  validation accuracy:		12.83 %
Epoch 1919 of 2000 took 0.099s
  training loss:		2.294740
  validation loss:		2.244646
  validation accuracy:		13.04 %
Epoch 1920 of 2000 took 0.099s
  training loss:		2.294720
  validation loss:		2.244581
  validation accuracy:		12.93 %
Epoch 1921 of 2000 took 0.099s
  training loss:		2.293785
  validation loss:		2.244198
  validation accuracy:		12.83 %
Epoch 1922 of 2000 took 0.099s
  training loss:		2.294152
  validation loss:		2.244282
  validation accuracy:		12.93 %
Epoch 1923 of 2000 took 0.099s
  training loss:		2.295079
  validation loss:		2.243904
  validation accuracy:		12.93 %
Epoch 1924 of 2000 took 0.099s
  training loss:		2.295331
  validation loss:		2.243933
  validation accuracy:		13.80 %
Epoch 1925 of 2000 took 0.100s
  training loss:		2.295203
  validation loss:		2.244303
  validation accuracy:		17.93 %
Epoch 1926 of 2000 took 0.099s
  training loss:		2.294843
  validation loss:		2.244518
  validation accuracy:		14.13 %
Epoch 1927 of 2000 took 0.097s
  training loss:		2.295457
  validation loss:		2.244798
  validation accuracy:		12.93 %
Epoch 1928 of 2000 took 0.096s
  training loss:		2.293028
  validation loss:		2.244241
  validation accuracy:		12.93 %
Epoch 1929 of 2000 took 0.096s
  training loss:		2.294302
  validation loss:		2.244212
  validation accuracy:		12.93 %
Epoch 1930 of 2000 took 0.096s
  training loss:		2.295129
  validation loss:		2.244162
  validation accuracy:		16.85 %
Epoch 1931 of 2000 took 0.096s
  training loss:		2.295483
  validation loss:		2.244359
  validation accuracy:		15.33 %
Epoch 1932 of 2000 took 0.096s
  training loss:		2.294798
  validation loss:		2.244375
  validation accuracy:		12.93 %
Epoch 1933 of 2000 took 0.096s
  training loss:		2.294759
  validation loss:		2.244431
  validation accuracy:		12.93 %
Epoch 1934 of 2000 took 0.096s
  training loss:		2.294947
  validation loss:		2.244323
  validation accuracy:		12.93 %
Epoch 1935 of 2000 took 0.096s
  training loss:		2.294471
  validation loss:		2.244453
  validation accuracy:		13.37 %
Epoch 1936 of 2000 took 0.097s
  training loss:		2.294351
  validation loss:		2.244260
  validation accuracy:		15.54 %
Epoch 1937 of 2000 took 0.096s
  training loss:		2.294448
  validation loss:		2.244181
  validation accuracy:		12.93 %
Epoch 1938 of 2000 took 0.096s
  training loss:		2.294556
  validation loss:		2.244255
  validation accuracy:		12.93 %
Epoch 1939 of 2000 took 0.096s
  training loss:		2.295592
  validation loss:		2.244313
  validation accuracy:		12.93 %
Epoch 1940 of 2000 took 0.096s
  training loss:		2.294792
  validation loss:		2.244341
  validation accuracy:		12.93 %
Epoch 1941 of 2000 took 0.096s
  training loss:		2.295277
  validation loss:		2.244418
  validation accuracy:		12.93 %
Epoch 1942 of 2000 took 0.097s
  training loss:		2.295162
  validation loss:		2.244544
  validation accuracy:		13.80 %
Epoch 1943 of 2000 took 0.096s
  training loss:		2.293978
  validation loss:		2.244570
  validation accuracy:		12.93 %
Epoch 1944 of 2000 took 0.096s
  training loss:		2.295043
  validation loss:		2.244617
  validation accuracy:		12.93 %
Epoch 1945 of 2000 took 0.096s
  training loss:		2.293856
  validation loss:		2.244306
  validation accuracy:		13.37 %
Epoch 1946 of 2000 took 0.096s
  training loss:		2.294794
  validation loss:		2.244111
  validation accuracy:		12.93 %
Epoch 1947 of 2000 took 0.096s
  training loss:		2.295582
  validation loss:		2.244355
  validation accuracy:		12.93 %
Epoch 1948 of 2000 took 0.096s
  training loss:		2.294498
  validation loss:		2.244551
  validation accuracy:		12.93 %
Epoch 1949 of 2000 took 0.096s
  training loss:		2.295505
  validation loss:		2.244472
  validation accuracy:		12.93 %
Epoch 1950 of 2000 took 0.096s
  training loss:		2.294667
  validation loss:		2.244616
  validation accuracy:		12.93 %
Epoch 1951 of 2000 took 0.096s
  training loss:		2.294484
  validation loss:		2.244485
  validation accuracy:		12.93 %
Epoch 1952 of 2000 took 0.096s
  training loss:		2.295175
  validation loss:		2.244572
  validation accuracy:		12.93 %
Epoch 1953 of 2000 took 0.096s
  training loss:		2.295223
  validation loss:		2.244925
  validation accuracy:		12.93 %
Epoch 1954 of 2000 took 0.096s
  training loss:		2.295614
  validation loss:		2.244902
  validation accuracy:		12.93 %
Epoch 1955 of 2000 took 0.096s
  training loss:		2.293804
  validation loss:		2.244838
  validation accuracy:		12.93 %
Epoch 1956 of 2000 took 0.096s
  training loss:		2.295185
  validation loss:		2.244687
  validation accuracy:		14.02 %
Epoch 1957 of 2000 took 0.096s
  training loss:		2.294618
  validation loss:		2.244687
  validation accuracy:		17.50 %
Epoch 1958 of 2000 took 0.096s
  training loss:		2.294742
  validation loss:		2.244447
  validation accuracy:		14.67 %
Epoch 1959 of 2000 took 0.096s
  training loss:		2.294287
  validation loss:		2.244263
  validation accuracy:		17.39 %
Epoch 1960 of 2000 took 0.096s
  training loss:		2.294382
  validation loss:		2.244180
  validation accuracy:		17.28 %
Epoch 1961 of 2000 took 0.096s
  training loss:		2.294849
  validation loss:		2.244045
  validation accuracy:		13.04 %
Epoch 1962 of 2000 took 0.096s
  training loss:		2.295149
  validation loss:		2.244137
  validation accuracy:		13.04 %
Epoch 1963 of 2000 took 0.096s
  training loss:		2.295644
  validation loss:		2.244136
  validation accuracy:		13.48 %
Epoch 1964 of 2000 took 0.096s
  training loss:		2.295033
  validation loss:		2.244543
  validation accuracy:		17.17 %
Epoch 1965 of 2000 took 0.097s
  training loss:		2.295469
  validation loss:		2.244627
  validation accuracy:		13.26 %
Epoch 1966 of 2000 took 0.096s
  training loss:		2.294366
  validation loss:		2.244791
  validation accuracy:		13.37 %
Epoch 1967 of 2000 took 0.097s
  training loss:		2.293950
  validation loss:		2.244561
  validation accuracy:		12.83 %
Epoch 1968 of 2000 took 0.099s
  training loss:		2.295073
  validation loss:		2.244653
  validation accuracy:		13.04 %
Epoch 1969 of 2000 took 0.099s
  training loss:		2.296163
  validation loss:		2.244739
  validation accuracy:		13.04 %
Epoch 1970 of 2000 took 0.099s
  training loss:		2.294981
  validation loss:		2.245023
  validation accuracy:		13.04 %
Epoch 1971 of 2000 took 0.099s
  training loss:		2.295686
  validation loss:		2.245413
  validation accuracy:		13.04 %
Epoch 1972 of 2000 took 0.099s
  training loss:		2.293748
  validation loss:		2.245459
  validation accuracy:		13.04 %
Epoch 1973 of 2000 took 0.099s
  training loss:		2.295034
  validation loss:		2.245355
  validation accuracy:		13.04 %
Epoch 1974 of 2000 took 0.099s
  training loss:		2.294490
  validation loss:		2.245477
  validation accuracy:		13.04 %
Epoch 1975 of 2000 took 0.099s
  training loss:		2.293995
  validation loss:		2.245109
  validation accuracy:		13.04 %
Epoch 1976 of 2000 took 0.099s
  training loss:		2.294860
  validation loss:		2.245152
  validation accuracy:		13.04 %
Epoch 1977 of 2000 took 0.099s
  training loss:		2.294902
  validation loss:		2.244951
  validation accuracy:		13.04 %
Epoch 1978 of 2000 took 0.099s
  training loss:		2.295415
  validation loss:		2.244878
  validation accuracy:		13.04 %
Epoch 1979 of 2000 took 0.099s
  training loss:		2.294772
  validation loss:		2.244815
  validation accuracy:		12.83 %
Epoch 1980 of 2000 took 0.099s
  training loss:		2.295180
  validation loss:		2.244948
  validation accuracy:		12.83 %
Epoch 1981 of 2000 took 0.099s
  training loss:		2.294977
  validation loss:		2.245025
  validation accuracy:		13.04 %
Epoch 1982 of 2000 took 0.099s
  training loss:		2.294963
  validation loss:		2.244915
  validation accuracy:		13.04 %
Epoch 1983 of 2000 took 0.099s
  training loss:		2.294601
  validation loss:		2.245083
  validation accuracy:		13.04 %
Epoch 1984 of 2000 took 0.099s
  training loss:		2.295129
  validation loss:		2.245081
  validation accuracy:		13.04 %
Epoch 1985 of 2000 took 0.099s
  training loss:		2.294313
  validation loss:		2.244732
  validation accuracy:		12.83 %
Epoch 1986 of 2000 took 0.099s
  training loss:		2.293504
  validation loss:		2.244643
  validation accuracy:		15.98 %
Epoch 1987 of 2000 took 0.099s
  training loss:		2.294514
  validation loss:		2.244401
  validation accuracy:		13.37 %
Epoch 1988 of 2000 took 0.099s
  training loss:		2.295262
  validation loss:		2.244373
  validation accuracy:		12.93 %
Epoch 1989 of 2000 took 0.099s
  training loss:		2.294396
  validation loss:		2.244445
  validation accuracy:		12.93 %
Epoch 1990 of 2000 took 0.099s
  training loss:		2.294741
  validation loss:		2.244657
  validation accuracy:		12.93 %
Epoch 1991 of 2000 took 0.099s
  training loss:		2.294596
  validation loss:		2.244549
  validation accuracy:		12.93 %
Epoch 1992 of 2000 took 0.099s
  training loss:		2.295704
  validation loss:		2.244701
  validation accuracy:		12.93 %
Epoch 1993 of 2000 took 0.099s
  training loss:		2.294429
  validation loss:		2.244669
  validation accuracy:		12.93 %
Epoch 1994 of 2000 took 0.099s
  training loss:		2.295305
  validation loss:		2.244748
  validation accuracy:		12.93 %
Epoch 1995 of 2000 took 0.099s
  training loss:		2.294101
  validation loss:		2.244528
  validation accuracy:		14.24 %
Epoch 1996 of 2000 took 0.099s
  training loss:		2.294738
  validation loss:		2.244353
  validation accuracy:		21.63 %
Epoch 1997 of 2000 took 0.099s
  training loss:		2.294229
  validation loss:		2.244302
  validation accuracy:		13.48 %
Epoch 1998 of 2000 took 0.100s
  training loss:		2.295194
  validation loss:		2.244158
  validation accuracy:		16.20 %
Epoch 1999 of 2000 took 0.099s
  training loss:		2.294490
  validation loss:		2.244290
  validation accuracy:		13.91 %
Epoch 2000 of 2000 took 0.099s
  training loss:		2.295526
  validation loss:		2.244546
  validation accuracy:		13.59 %
Final results:
  test loss:			2.252671
  test accuracy:		12.42 %
