Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(200, 200),b=(200,)
w=(200, 200),b=(200,)
w=(200, 200),b=(200,)
decomposing tensor W of shape (3, 200, 200)...
decomposing tensor B of shape (3, 200)...
Starting training...
Epoch 1 of 2000 took 0.066s
  training loss:		2.967833
  validation loss:		2.884418
  validation accuracy:		12.83 %
Epoch 2 of 2000 took 0.055s
  training loss:		2.829586
  validation loss:		2.705446
  validation accuracy:		12.93 %
Epoch 3 of 2000 took 0.048s
  training loss:		2.662404
  validation loss:		2.515599
  validation accuracy:		12.83 %
Epoch 4 of 2000 took 0.046s
  training loss:		2.504672
  validation loss:		2.361407
  validation accuracy:		12.93 %
Epoch 5 of 2000 took 0.047s
  training loss:		2.399813
  validation loss:		2.284353
  validation accuracy:		12.93 %
Epoch 6 of 2000 took 0.049s
  training loss:		2.344453
  validation loss:		2.269143
  validation accuracy:		12.83 %
Epoch 7 of 2000 took 0.047s
  training loss:		2.322195
  validation loss:		2.272744
  validation accuracy:		12.93 %
Epoch 8 of 2000 took 0.046s
  training loss:		2.311303
  validation loss:		2.257634
  validation accuracy:		13.04 %
Epoch 9 of 2000 took 0.048s
  training loss:		2.306939
  validation loss:		2.253721
  validation accuracy:		12.83 %
Epoch 10 of 2000 took 0.061s
  training loss:		2.305100
  validation loss:		2.255901
  validation accuracy:		12.93 %
Epoch 11 of 2000 took 0.051s
  training loss:		2.303267
  validation loss:		2.249442
  validation accuracy:		12.93 %
Epoch 12 of 2000 took 0.044s
  training loss:		2.302335
  validation loss:		2.247466
  validation accuracy:		13.04 %
Epoch 13 of 2000 took 0.039s
  training loss:		2.301965
  validation loss:		2.252106
  validation accuracy:		12.83 %
Epoch 14 of 2000 took 0.036s
  training loss:		2.301232
  validation loss:		2.253785
  validation accuracy:		12.83 %
Epoch 15 of 2000 took 0.035s
  training loss:		2.300746
  validation loss:		2.247919
  validation accuracy:		12.93 %
Epoch 16 of 2000 took 0.035s
  training loss:		2.300285
  validation loss:		2.252626
  validation accuracy:		12.93 %
Epoch 17 of 2000 took 0.035s
  training loss:		2.299376
  validation loss:		2.244348
  validation accuracy:		12.83 %
Epoch 18 of 2000 took 0.035s
  training loss:		2.299702
  validation loss:		2.247467
  validation accuracy:		12.93 %
Epoch 19 of 2000 took 0.035s
  training loss:		2.299320
  validation loss:		2.249934
  validation accuracy:		12.93 %
Epoch 20 of 2000 took 0.035s
  training loss:		2.297881
  validation loss:		2.242860
  validation accuracy:		12.93 %
Epoch 21 of 2000 took 0.035s
  training loss:		2.298684
  validation loss:		2.245398
  validation accuracy:		12.93 %
Epoch 22 of 2000 took 0.038s
  training loss:		2.299184
  validation loss:		2.246564
  validation accuracy:		13.04 %
Epoch 23 of 2000 took 0.035s
  training loss:		2.298991
  validation loss:		2.241761
  validation accuracy:		12.93 %
Epoch 24 of 2000 took 0.035s
  training loss:		2.296757
  validation loss:		2.245436
  validation accuracy:		12.93 %
Epoch 25 of 2000 took 0.035s
  training loss:		2.298452
  validation loss:		2.241055
  validation accuracy:		12.83 %
Epoch 26 of 2000 took 0.035s
  training loss:		2.297680
  validation loss:		2.246747
  validation accuracy:		12.83 %
Epoch 27 of 2000 took 0.035s
  training loss:		2.297470
  validation loss:		2.247611
  validation accuracy:		12.93 %
Epoch 28 of 2000 took 0.035s
  training loss:		2.296855
  validation loss:		2.245899
  validation accuracy:		12.83 %
Epoch 29 of 2000 took 0.035s
  training loss:		2.297662
  validation loss:		2.248414
  validation accuracy:		12.93 %
Epoch 30 of 2000 took 0.035s
  training loss:		2.296412
  validation loss:		2.244468
  validation accuracy:		12.93 %
Epoch 31 of 2000 took 0.035s
  training loss:		2.297873
  validation loss:		2.245618
  validation accuracy:		12.93 %
Epoch 32 of 2000 took 0.035s
  training loss:		2.297277
  validation loss:		2.241704
  validation accuracy:		13.04 %
Epoch 33 of 2000 took 0.035s
  training loss:		2.298229
  validation loss:		2.246411
  validation accuracy:		13.04 %
Epoch 34 of 2000 took 0.035s
  training loss:		2.296995
  validation loss:		2.243504
  validation accuracy:		12.93 %
Epoch 35 of 2000 took 0.035s
  training loss:		2.296763
  validation loss:		2.243458
  validation accuracy:		12.93 %
Epoch 36 of 2000 took 0.035s
  training loss:		2.298019
  validation loss:		2.249370
  validation accuracy:		12.83 %
Epoch 37 of 2000 took 0.036s
  training loss:		2.297863
  validation loss:		2.249064
  validation accuracy:		12.93 %
Epoch 38 of 2000 took 0.035s
  training loss:		2.296761
  validation loss:		2.242152
  validation accuracy:		12.93 %
Epoch 39 of 2000 took 0.035s
  training loss:		2.297573
  validation loss:		2.247628
  validation accuracy:		12.83 %
Epoch 40 of 2000 took 0.036s
  training loss:		2.297298
  validation loss:		2.245372
  validation accuracy:		12.93 %
Epoch 41 of 2000 took 0.035s
  training loss:		2.296492
  validation loss:		2.243963
  validation accuracy:		13.04 %
Epoch 42 of 2000 took 0.035s
  training loss:		2.297923
  validation loss:		2.250091
  validation accuracy:		12.83 %
Epoch 43 of 2000 took 0.035s
  training loss:		2.297493
  validation loss:		2.238734
  validation accuracy:		12.83 %
Epoch 44 of 2000 took 0.035s
  training loss:		2.296558
  validation loss:		2.246978
  validation accuracy:		12.93 %
Epoch 45 of 2000 took 0.035s
  training loss:		2.296918
  validation loss:		2.254135
  validation accuracy:		12.93 %
Epoch 46 of 2000 took 0.035s
  training loss:		2.297585
  validation loss:		2.244378
  validation accuracy:		12.83 %
Epoch 47 of 2000 took 0.035s
  training loss:		2.296633
  validation loss:		2.242401
  validation accuracy:		12.93 %
Epoch 48 of 2000 took 0.035s
  training loss:		2.297362
  validation loss:		2.245957
  validation accuracy:		13.04 %
Epoch 49 of 2000 took 0.035s
  training loss:		2.296656
  validation loss:		2.247132
  validation accuracy:		12.83 %
Epoch 50 of 2000 took 0.035s
  training loss:		2.296031
  validation loss:		2.245745
  validation accuracy:		13.04 %
Epoch 51 of 2000 took 0.035s
  training loss:		2.296599
  validation loss:		2.245059
  validation accuracy:		12.93 %
Epoch 52 of 2000 took 0.035s
  training loss:		2.296304
  validation loss:		2.240730
  validation accuracy:		12.93 %
Epoch 53 of 2000 took 0.035s
  training loss:		2.296959
  validation loss:		2.252593
  validation accuracy:		12.83 %
Epoch 54 of 2000 took 0.036s
  training loss:		2.296844
  validation loss:		2.243190
  validation accuracy:		12.83 %
Epoch 55 of 2000 took 0.035s
  training loss:		2.296352
  validation loss:		2.246114
  validation accuracy:		13.04 %
Epoch 56 of 2000 took 0.035s
  training loss:		2.295829
  validation loss:		2.243631
  validation accuracy:		12.93 %
Epoch 57 of 2000 took 0.035s
  training loss:		2.298028
  validation loss:		2.244715
  validation accuracy:		12.93 %
Epoch 58 of 2000 took 0.035s
  training loss:		2.297323
  validation loss:		2.254920
  validation accuracy:		12.93 %
Epoch 59 of 2000 took 0.035s
  training loss:		2.296034
  validation loss:		2.238803
  validation accuracy:		13.04 %
Epoch 60 of 2000 took 0.035s
  training loss:		2.297902
  validation loss:		2.239254
  validation accuracy:		12.83 %
Epoch 61 of 2000 took 0.035s
  training loss:		2.296690
  validation loss:		2.248798
  validation accuracy:		12.83 %
Epoch 62 of 2000 took 0.035s
  training loss:		2.297510
  validation loss:		2.241549
  validation accuracy:		12.83 %
Epoch 63 of 2000 took 0.035s
  training loss:		2.297226
  validation loss:		2.244585
  validation accuracy:		12.93 %
Epoch 64 of 2000 took 0.035s
  training loss:		2.294893
  validation loss:		2.243145
  validation accuracy:		12.93 %
Epoch 65 of 2000 took 0.036s
  training loss:		2.298460
  validation loss:		2.244977
  validation accuracy:		12.83 %
Epoch 66 of 2000 took 0.036s
  training loss:		2.297503
  validation loss:		2.248466
  validation accuracy:		12.93 %
Epoch 67 of 2000 took 0.035s
  training loss:		2.296646
  validation loss:		2.243233
  validation accuracy:		12.93 %
Epoch 68 of 2000 took 0.035s
  training loss:		2.297236
  validation loss:		2.245507
  validation accuracy:		13.04 %
Epoch 69 of 2000 took 0.035s
  training loss:		2.297404
  validation loss:		2.250758
  validation accuracy:		13.04 %
Epoch 70 of 2000 took 0.035s
  training loss:		2.296594
  validation loss:		2.242378
  validation accuracy:		12.83 %
Epoch 71 of 2000 took 0.035s
  training loss:		2.297855
  validation loss:		2.249163
  validation accuracy:		13.04 %
Epoch 72 of 2000 took 0.035s
  training loss:		2.297895
  validation loss:		2.246966
  validation accuracy:		12.93 %
Epoch 73 of 2000 took 0.035s
  training loss:		2.298966
  validation loss:		2.250972
  validation accuracy:		12.83 %
Epoch 74 of 2000 took 0.035s
  training loss:		2.297735
  validation loss:		2.239893
  validation accuracy:		12.83 %
Epoch 75 of 2000 took 0.035s
  training loss:		2.295449
  validation loss:		2.245063
  validation accuracy:		12.83 %
Epoch 76 of 2000 took 0.035s
  training loss:		2.297108
  validation loss:		2.250961
  validation accuracy:		12.83 %
Epoch 77 of 2000 took 0.036s
  training loss:		2.295936
  validation loss:		2.247174
  validation accuracy:		12.93 %
Epoch 78 of 2000 took 0.035s
  training loss:		2.295968
  validation loss:		2.242764
  validation accuracy:		12.83 %
Epoch 79 of 2000 took 0.035s
  training loss:		2.296296
  validation loss:		2.241242
  validation accuracy:		12.93 %
Epoch 80 of 2000 took 0.035s
  training loss:		2.296907
  validation loss:		2.249891
  validation accuracy:		12.93 %
Epoch 81 of 2000 took 0.035s
  training loss:		2.295653
  validation loss:		2.244320
  validation accuracy:		12.83 %
Epoch 82 of 2000 took 0.035s
  training loss:		2.297780
  validation loss:		2.240778
  validation accuracy:		12.83 %
Epoch 83 of 2000 took 0.035s
  training loss:		2.296425
  validation loss:		2.243246
  validation accuracy:		13.04 %
Epoch 84 of 2000 took 0.035s
  training loss:		2.295841
  validation loss:		2.243432
  validation accuracy:		12.83 %
Epoch 85 of 2000 took 0.035s
  training loss:		2.298309
  validation loss:		2.243107
  validation accuracy:		12.83 %
Epoch 86 of 2000 took 0.035s
  training loss:		2.296504
  validation loss:		2.250312
  validation accuracy:		12.93 %
Epoch 87 of 2000 took 0.035s
  training loss:		2.295684
  validation loss:		2.242785
  validation accuracy:		12.93 %
Epoch 88 of 2000 took 0.035s
  training loss:		2.295840
  validation loss:		2.246177
  validation accuracy:		12.83 %
Epoch 89 of 2000 took 0.035s
  training loss:		2.296661
  validation loss:		2.244745
  validation accuracy:		12.83 %
Epoch 90 of 2000 took 0.035s
  training loss:		2.297572
  validation loss:		2.244818
  validation accuracy:		12.83 %
Epoch 91 of 2000 took 0.035s
  training loss:		2.297574
  validation loss:		2.248420
  validation accuracy:		12.93 %
Epoch 92 of 2000 took 0.035s
  training loss:		2.296178
  validation loss:		2.244121
  validation accuracy:		12.93 %
Epoch 93 of 2000 took 0.035s
  training loss:		2.296233
  validation loss:		2.242314
  validation accuracy:		12.83 %
Epoch 94 of 2000 took 0.037s
  training loss:		2.297205
  validation loss:		2.251169
  validation accuracy:		12.83 %
Epoch 95 of 2000 took 0.035s
  training loss:		2.298196
  validation loss:		2.247214
  validation accuracy:		12.93 %
Epoch 96 of 2000 took 0.035s
  training loss:		2.296879
  validation loss:		2.247996
  validation accuracy:		13.04 %
Epoch 97 of 2000 took 0.036s
  training loss:		2.296795
  validation loss:		2.245115
  validation accuracy:		12.83 %
Epoch 98 of 2000 took 0.035s
  training loss:		2.297070
  validation loss:		2.244258
  validation accuracy:		13.04 %
Epoch 99 of 2000 took 0.035s
  training loss:		2.296609
  validation loss:		2.237418
  validation accuracy:		12.83 %
Epoch 100 of 2000 took 0.035s
  training loss:		2.298434
  validation loss:		2.255565
  validation accuracy:		12.93 %
Epoch 101 of 2000 took 0.035s
  training loss:		2.296652
  validation loss:		2.237940
  validation accuracy:		12.83 %
Epoch 102 of 2000 took 0.035s
  training loss:		2.297493
  validation loss:		2.251828
  validation accuracy:		13.04 %
Epoch 103 of 2000 took 0.035s
  training loss:		2.295862
  validation loss:		2.238930
  validation accuracy:		13.04 %
Epoch 104 of 2000 took 0.035s
  training loss:		2.296127
  validation loss:		2.244188
  validation accuracy:		12.83 %
Epoch 105 of 2000 took 0.036s
  training loss:		2.296169
  validation loss:		2.240582
  validation accuracy:		12.93 %
Epoch 106 of 2000 took 0.035s
  training loss:		2.299050
  validation loss:		2.248999
  validation accuracy:		12.83 %
Epoch 107 of 2000 took 0.035s
  training loss:		2.297573
  validation loss:		2.244532
  validation accuracy:		12.93 %
Epoch 108 of 2000 took 0.035s
  training loss:		2.296879
  validation loss:		2.244153
  validation accuracy:		12.93 %
Epoch 109 of 2000 took 0.035s
  training loss:		2.296092
  validation loss:		2.241642
  validation accuracy:		12.93 %
Epoch 110 of 2000 took 0.035s
  training loss:		2.297174
  validation loss:		2.244817
  validation accuracy:		13.04 %
Epoch 111 of 2000 took 0.035s
  training loss:		2.296740
  validation loss:		2.249698
  validation accuracy:		12.83 %
Epoch 112 of 2000 took 0.035s
  training loss:		2.296054
  validation loss:		2.242504
  validation accuracy:		12.93 %
Epoch 113 of 2000 took 0.035s
  training loss:		2.296384
  validation loss:		2.240783
  validation accuracy:		12.83 %
Epoch 114 of 2000 took 0.035s
  training loss:		2.297303
  validation loss:		2.246412
  validation accuracy:		12.93 %
Epoch 115 of 2000 took 0.035s
  training loss:		2.296690
  validation loss:		2.252129
  validation accuracy:		12.83 %
Epoch 116 of 2000 took 0.035s
  training loss:		2.296888
  validation loss:		2.246787
  validation accuracy:		13.04 %
Epoch 117 of 2000 took 0.035s
  training loss:		2.296629
  validation loss:		2.240583
  validation accuracy:		12.83 %
Epoch 118 of 2000 took 0.035s
  training loss:		2.296443
  validation loss:		2.245016
  validation accuracy:		12.93 %
Epoch 119 of 2000 took 0.035s
  training loss:		2.296480
  validation loss:		2.249401
  validation accuracy:		12.93 %
Epoch 120 of 2000 took 0.035s
  training loss:		2.296265
  validation loss:		2.237482
  validation accuracy:		12.83 %
Epoch 121 of 2000 took 0.035s
  training loss:		2.296261
  validation loss:		2.244877
  validation accuracy:		13.04 %
Epoch 122 of 2000 took 0.036s
  training loss:		2.296800
  validation loss:		2.244204
  validation accuracy:		12.93 %
Epoch 123 of 2000 took 0.035s
  training loss:		2.297329
  validation loss:		2.254566
  validation accuracy:		12.93 %
Epoch 124 of 2000 took 0.035s
  training loss:		2.295334
  validation loss:		2.237818
  validation accuracy:		12.83 %
Epoch 125 of 2000 took 0.036s
  training loss:		2.294774
  validation loss:		2.242254
  validation accuracy:		12.83 %
Epoch 126 of 2000 took 0.035s
  training loss:		2.296707
  validation loss:		2.249280
  validation accuracy:		13.04 %
Epoch 127 of 2000 took 0.035s
  training loss:		2.297001
  validation loss:		2.244356
  validation accuracy:		13.04 %
Epoch 128 of 2000 took 0.035s
  training loss:		2.296795
  validation loss:		2.247386
  validation accuracy:		13.04 %
Epoch 129 of 2000 took 0.035s
  training loss:		2.296509
  validation loss:		2.240928
  validation accuracy:		13.04 %
Epoch 130 of 2000 took 0.035s
  training loss:		2.296477
  validation loss:		2.244774
  validation accuracy:		12.93 %
Epoch 131 of 2000 took 0.035s
  training loss:		2.295955
  validation loss:		2.238063
  validation accuracy:		13.04 %
Epoch 132 of 2000 took 0.035s
  training loss:		2.296407
  validation loss:		2.241859
  validation accuracy:		12.93 %
Epoch 133 of 2000 took 0.036s
  training loss:		2.295642
  validation loss:		2.250199
  validation accuracy:		12.93 %
Epoch 134 of 2000 took 0.035s
  training loss:		2.296588
  validation loss:		2.243008
  validation accuracy:		13.04 %
Epoch 135 of 2000 took 0.035s
  training loss:		2.295713
  validation loss:		2.239348
  validation accuracy:		12.93 %
Epoch 136 of 2000 took 0.035s
  training loss:		2.296088
  validation loss:		2.248729
  validation accuracy:		13.04 %
Epoch 137 of 2000 took 0.035s
  training loss:		2.296754
  validation loss:		2.242809
  validation accuracy:		12.83 %
Epoch 138 of 2000 took 0.035s
  training loss:		2.297844
  validation loss:		2.257037
  validation accuracy:		12.93 %
Epoch 139 of 2000 took 0.035s
  training loss:		2.296026
  validation loss:		2.237090
  validation accuracy:		12.83 %
Epoch 140 of 2000 took 0.035s
  training loss:		2.296114
  validation loss:		2.243256
  validation accuracy:		12.93 %
Epoch 141 of 2000 took 0.035s
  training loss:		2.295841
  validation loss:		2.243805
  validation accuracy:		12.83 %
Epoch 142 of 2000 took 0.035s
  training loss:		2.297351
  validation loss:		2.246767
  validation accuracy:		12.93 %
Epoch 143 of 2000 took 0.035s
  training loss:		2.296111
  validation loss:		2.240009
  validation accuracy:		12.93 %
Epoch 144 of 2000 took 0.035s
  training loss:		2.296130
  validation loss:		2.242872
  validation accuracy:		12.93 %
Epoch 145 of 2000 took 0.035s
  training loss:		2.295979
  validation loss:		2.240561
  validation accuracy:		12.83 %
Epoch 146 of 2000 took 0.035s
  training loss:		2.298209
  validation loss:		2.246584
  validation accuracy:		13.04 %
Epoch 147 of 2000 took 0.035s
  training loss:		2.296307
  validation loss:		2.240371
  validation accuracy:		12.93 %
Epoch 148 of 2000 took 0.035s
  training loss:		2.296392
  validation loss:		2.245284
  validation accuracy:		12.93 %
Epoch 149 of 2000 took 0.035s
  training loss:		2.296480
  validation loss:		2.244855
  validation accuracy:		12.93 %
Epoch 150 of 2000 took 0.036s
  training loss:		2.297078
  validation loss:		2.245013
  validation accuracy:		13.04 %
Epoch 151 of 2000 took 0.035s
  training loss:		2.296724
  validation loss:		2.251832
  validation accuracy:		13.04 %
Epoch 152 of 2000 took 0.036s
  training loss:		2.296327
  validation loss:		2.246973
  validation accuracy:		13.04 %
Epoch 153 of 2000 took 0.036s
  training loss:		2.295726
  validation loss:		2.239694
  validation accuracy:		12.83 %
Epoch 154 of 2000 took 0.035s
  training loss:		2.295052
  validation loss:		2.244323
  validation accuracy:		13.04 %
Epoch 155 of 2000 took 0.035s
  training loss:		2.296260
  validation loss:		2.244410
  validation accuracy:		12.83 %
Epoch 156 of 2000 took 0.035s
  training loss:		2.296781
  validation loss:		2.246923
  validation accuracy:		13.04 %
Epoch 157 of 2000 took 0.035s
  training loss:		2.297268
  validation loss:		2.240951
  validation accuracy:		12.83 %
Epoch 158 of 2000 took 0.035s
  training loss:		2.297000
  validation loss:		2.246158
  validation accuracy:		12.83 %
Epoch 159 of 2000 took 0.035s
  training loss:		2.295409
  validation loss:		2.247860
  validation accuracy:		13.04 %
Epoch 160 of 2000 took 0.035s
  training loss:		2.295311
  validation loss:		2.247473
  validation accuracy:		13.04 %
Epoch 161 of 2000 took 0.036s
  training loss:		2.297168
  validation loss:		2.241102
  validation accuracy:		12.83 %
Epoch 162 of 2000 took 0.035s
  training loss:		2.295023
  validation loss:		2.241412
  validation accuracy:		12.83 %
Epoch 163 of 2000 took 0.035s
  training loss:		2.295475
  validation loss:		2.248550
  validation accuracy:		12.93 %
Epoch 164 of 2000 took 0.035s
  training loss:		2.295465
  validation loss:		2.238632
  validation accuracy:		12.93 %
Epoch 165 of 2000 took 0.035s
  training loss:		2.295913
  validation loss:		2.242417
  validation accuracy:		12.93 %
Epoch 166 of 2000 took 0.035s
  training loss:		2.295176
  validation loss:		2.243213
  validation accuracy:		12.83 %
Epoch 167 of 2000 took 0.035s
  training loss:		2.295849
  validation loss:		2.247310
  validation accuracy:		12.83 %
Epoch 168 of 2000 took 0.035s
  training loss:		2.297159
  validation loss:		2.238912
  validation accuracy:		13.04 %
Epoch 169 of 2000 took 0.035s
  training loss:		2.297183
  validation loss:		2.247314
  validation accuracy:		12.93 %
Epoch 170 of 2000 took 0.035s
  training loss:		2.296170
  validation loss:		2.244519
  validation accuracy:		12.83 %
Epoch 171 of 2000 took 0.035s
  training loss:		2.296232
  validation loss:		2.240601
  validation accuracy:		12.93 %
Epoch 172 of 2000 took 0.035s
  training loss:		2.295620
  validation loss:		2.237120
  validation accuracy:		12.83 %
Epoch 173 of 2000 took 0.035s
  training loss:		2.295644
  validation loss:		2.246325
  validation accuracy:		13.04 %
Epoch 174 of 2000 took 0.035s
  training loss:		2.296579
  validation loss:		2.241562
  validation accuracy:		12.93 %
Epoch 175 of 2000 took 0.035s
  training loss:		2.296359
  validation loss:		2.241466
  validation accuracy:		12.93 %
Epoch 176 of 2000 took 0.035s
  training loss:		2.297385
  validation loss:		2.251132
  validation accuracy:		12.93 %
Epoch 177 of 2000 took 0.035s
  training loss:		2.296951
  validation loss:		2.241862
  validation accuracy:		13.04 %
Epoch 178 of 2000 took 0.035s
  training loss:		2.296384
  validation loss:		2.246617
  validation accuracy:		12.83 %
Epoch 179 of 2000 took 0.036s
  training loss:		2.295778
  validation loss:		2.241831
  validation accuracy:		12.83 %
Epoch 180 of 2000 took 0.035s
  training loss:		2.296336
  validation loss:		2.246326
  validation accuracy:		13.04 %
Epoch 181 of 2000 took 0.035s
  training loss:		2.296005
  validation loss:		2.241312
  validation accuracy:		12.83 %
Epoch 182 of 2000 took 0.035s
  training loss:		2.296485
  validation loss:		2.244610
  validation accuracy:		12.93 %
Epoch 183 of 2000 took 0.035s
  training loss:		2.295416
  validation loss:		2.250607
  validation accuracy:		13.04 %
Epoch 184 of 2000 took 0.035s
  training loss:		2.297407
  validation loss:		2.244758
  validation accuracy:		12.93 %
Epoch 185 of 2000 took 0.035s
  training loss:		2.298047
  validation loss:		2.254032
  validation accuracy:		12.83 %
Epoch 186 of 2000 took 0.035s
  training loss:		2.296075
  validation loss:		2.251679
  validation accuracy:		12.83 %
Epoch 187 of 2000 took 0.035s
  training loss:		2.296356
  validation loss:		2.235416
  validation accuracy:		12.83 %
Epoch 188 of 2000 took 0.035s
  training loss:		2.296466
  validation loss:		2.247055
  validation accuracy:		12.83 %
Epoch 189 of 2000 took 0.035s
  training loss:		2.296939
  validation loss:		2.249785
  validation accuracy:		13.04 %
Epoch 190 of 2000 took 0.036s
  training loss:		2.296225
  validation loss:		2.236569
  validation accuracy:		13.04 %
Epoch 191 of 2000 took 0.035s
  training loss:		2.296414
  validation loss:		2.252549
  validation accuracy:		12.83 %
Epoch 192 of 2000 took 0.035s
  training loss:		2.296744
  validation loss:		2.246300
  validation accuracy:		12.83 %
Epoch 193 of 2000 took 0.035s
  training loss:		2.296400
  validation loss:		2.240526
  validation accuracy:		13.04 %
Epoch 194 of 2000 took 0.035s
  training loss:		2.296257
  validation loss:		2.239407
  validation accuracy:		13.04 %
Epoch 195 of 2000 took 0.035s
  training loss:		2.297953
  validation loss:		2.243773
  validation accuracy:		12.93 %
Epoch 196 of 2000 took 0.035s
  training loss:		2.296574
  validation loss:		2.243800
  validation accuracy:		12.93 %
Epoch 197 of 2000 took 0.035s
  training loss:		2.296560
  validation loss:		2.250275
  validation accuracy:		12.93 %
Epoch 198 of 2000 took 0.035s
  training loss:		2.296181
  validation loss:		2.248018
  validation accuracy:		12.83 %
Epoch 199 of 2000 took 0.035s
  training loss:		2.295901
  validation loss:		2.247478
  validation accuracy:		12.93 %
Epoch 200 of 2000 took 0.035s
  training loss:		2.295740
  validation loss:		2.243436
  validation accuracy:		12.83 %
Epoch 201 of 2000 took 0.035s
  training loss:		2.296951
  validation loss:		2.251910
  validation accuracy:		12.93 %
Epoch 202 of 2000 took 0.035s
  training loss:		2.297448
  validation loss:		2.249307
  validation accuracy:		12.93 %
Epoch 203 of 2000 took 0.035s
  training loss:		2.295971
  validation loss:		2.235128
  validation accuracy:		12.93 %
Epoch 204 of 2000 took 0.035s
  training loss:		2.296619
  validation loss:		2.242747
  validation accuracy:		12.93 %
Epoch 205 of 2000 took 0.035s
  training loss:		2.297466
  validation loss:		2.247955
  validation accuracy:		12.93 %
Epoch 206 of 2000 took 0.035s
  training loss:		2.297431
  validation loss:		2.249253
  validation accuracy:		13.04 %
Epoch 207 of 2000 took 0.035s
  training loss:		2.296010
  validation loss:		2.243788
  validation accuracy:		12.93 %
Epoch 208 of 2000 took 0.035s
  training loss:		2.295969
  validation loss:		2.244565
  validation accuracy:		12.83 %
Epoch 209 of 2000 took 0.035s
  training loss:		2.295842
  validation loss:		2.250029
  validation accuracy:		13.04 %
Epoch 210 of 2000 took 0.036s
  training loss:		2.296909
  validation loss:		2.242296
  validation accuracy:		12.83 %
Epoch 211 of 2000 took 0.035s
  training loss:		2.295803
  validation loss:		2.240385
  validation accuracy:		12.93 %
Epoch 212 of 2000 took 0.035s
  training loss:		2.296612
  validation loss:		2.244162
  validation accuracy:		13.04 %
Epoch 213 of 2000 took 0.035s
  training loss:		2.296441
  validation loss:		2.245695
  validation accuracy:		12.93 %
Epoch 214 of 2000 took 0.035s
  training loss:		2.296380
  validation loss:		2.248666
  validation accuracy:		12.83 %
Epoch 215 of 2000 took 0.035s
  training loss:		2.297032
  validation loss:		2.244293
  validation accuracy:		12.83 %
Epoch 216 of 2000 took 0.035s
  training loss:		2.294866
  validation loss:		2.236255
  validation accuracy:		13.04 %
Epoch 217 of 2000 took 0.036s
  training loss:		2.296501
  validation loss:		2.246732
  validation accuracy:		12.83 %
Epoch 218 of 2000 took 0.036s
  training loss:		2.297235
  validation loss:		2.240343
  validation accuracy:		12.83 %
Epoch 219 of 2000 took 0.035s
  training loss:		2.297216
  validation loss:		2.251009
  validation accuracy:		13.04 %
Epoch 220 of 2000 took 0.035s
  training loss:		2.295728
  validation loss:		2.241410
  validation accuracy:		12.93 %
Epoch 221 of 2000 took 0.035s
  training loss:		2.295609
  validation loss:		2.242309
  validation accuracy:		12.93 %
Epoch 222 of 2000 took 0.035s
  training loss:		2.296886
  validation loss:		2.245290
  validation accuracy:		12.93 %
Epoch 223 of 2000 took 0.035s
  training loss:		2.296761
  validation loss:		2.244966
  validation accuracy:		12.93 %
Epoch 224 of 2000 took 0.035s
  training loss:		2.296597
  validation loss:		2.246278
  validation accuracy:		12.83 %
Epoch 225 of 2000 took 0.035s
  training loss:		2.296941
  validation loss:		2.243539
  validation accuracy:		12.83 %
Epoch 226 of 2000 took 0.035s
  training loss:		2.295557
  validation loss:		2.243850
  validation accuracy:		12.93 %
Epoch 227 of 2000 took 0.035s
  training loss:		2.296702
  validation loss:		2.240122
  validation accuracy:		13.04 %
Epoch 228 of 2000 took 0.035s
  training loss:		2.296073
  validation loss:		2.246422
  validation accuracy:		13.04 %
Epoch 229 of 2000 took 0.035s
  training loss:		2.296102
  validation loss:		2.243050
  validation accuracy:		12.83 %
Epoch 230 of 2000 took 0.035s
  training loss:		2.296197
  validation loss:		2.241296
  validation accuracy:		12.83 %
Epoch 231 of 2000 took 0.035s
  training loss:		2.296633
  validation loss:		2.244302
  validation accuracy:		13.04 %
Epoch 232 of 2000 took 0.035s
  training loss:		2.295457
  validation loss:		2.245357
  validation accuracy:		12.83 %
Epoch 233 of 2000 took 0.035s
  training loss:		2.297794
  validation loss:		2.240425
  validation accuracy:		12.93 %
Epoch 234 of 2000 took 0.035s
  training loss:		2.296816
  validation loss:		2.247771
  validation accuracy:		12.83 %
Epoch 235 of 2000 took 0.036s
  training loss:		2.296111
  validation loss:		2.239322
  validation accuracy:		12.93 %
Epoch 236 of 2000 took 0.035s
  training loss:		2.294914
  validation loss:		2.237800
  validation accuracy:		13.04 %
Epoch 237 of 2000 took 0.035s
  training loss:		2.298039
  validation loss:		2.252729
  validation accuracy:		12.83 %
Epoch 238 of 2000 took 0.035s
  training loss:		2.296747
  validation loss:		2.239781
  validation accuracy:		13.04 %
Epoch 239 of 2000 took 0.035s
  training loss:		2.296348
  validation loss:		2.243681
  validation accuracy:		12.83 %
Epoch 240 of 2000 took 0.035s
  training loss:		2.295943
  validation loss:		2.244130
  validation accuracy:		12.93 %
Epoch 241 of 2000 took 0.035s
  training loss:		2.296556
  validation loss:		2.245299
  validation accuracy:		12.83 %
Epoch 242 of 2000 took 0.035s
  training loss:		2.296708
  validation loss:		2.244723
  validation accuracy:		12.93 %
Epoch 243 of 2000 took 0.035s
  training loss:		2.296614
  validation loss:		2.245821
  validation accuracy:		12.83 %
Epoch 244 of 2000 took 0.035s
  training loss:		2.296589
  validation loss:		2.242993
  validation accuracy:		12.93 %
Epoch 245 of 2000 took 0.035s
  training loss:		2.296342
  validation loss:		2.242515
  validation accuracy:		12.93 %
Epoch 246 of 2000 took 0.036s
  training loss:		2.296791
  validation loss:		2.253114
  validation accuracy:		12.83 %
Epoch 247 of 2000 took 0.037s
  training loss:		2.296955
  validation loss:		2.244736
  validation accuracy:		12.93 %
Epoch 248 of 2000 took 0.036s
  training loss:		2.297237
  validation loss:		2.244022
  validation accuracy:		13.04 %
Epoch 249 of 2000 took 0.035s
  training loss:		2.296280
  validation loss:		2.245141
  validation accuracy:		12.83 %
Epoch 250 of 2000 took 0.035s
  training loss:		2.297949
  validation loss:		2.242838
  validation accuracy:		12.93 %
Epoch 251 of 2000 took 0.035s
  training loss:		2.296795
  validation loss:		2.249525
  validation accuracy:		12.83 %
Epoch 252 of 2000 took 0.035s
  training loss:		2.297704
  validation loss:		2.246017
  validation accuracy:		12.83 %
Epoch 253 of 2000 took 0.035s
  training loss:		2.296623
  validation loss:		2.238194
  validation accuracy:		12.83 %
Epoch 254 of 2000 took 0.035s
  training loss:		2.296115
  validation loss:		2.246388
  validation accuracy:		12.93 %
Epoch 255 of 2000 took 0.035s
  training loss:		2.297335
  validation loss:		2.240302
  validation accuracy:		12.83 %
Epoch 256 of 2000 took 0.035s
  training loss:		2.297051
  validation loss:		2.253424
  validation accuracy:		12.83 %
Epoch 257 of 2000 took 0.035s
  training loss:		2.296838
  validation loss:		2.245748
  validation accuracy:		12.93 %
Epoch 258 of 2000 took 0.035s
  training loss:		2.296624
  validation loss:		2.244711
  validation accuracy:		12.83 %
Epoch 259 of 2000 took 0.035s
  training loss:		2.296666
  validation loss:		2.250500
  validation accuracy:		12.83 %
Epoch 260 of 2000 took 0.035s
  training loss:		2.296092
  validation loss:		2.248267
  validation accuracy:		12.93 %
Epoch 261 of 2000 took 0.035s
  training loss:		2.296864
  validation loss:		2.242168
  validation accuracy:		12.93 %
Epoch 262 of 2000 took 0.035s
  training loss:		2.296455
  validation loss:		2.245268
  validation accuracy:		12.83 %
Epoch 263 of 2000 took 0.035s
  training loss:		2.296400
  validation loss:		2.251083
  validation accuracy:		13.04 %
Epoch 264 of 2000 took 0.036s
  training loss:		2.295664
  validation loss:		2.240110
  validation accuracy:		12.83 %
Epoch 265 of 2000 took 0.035s
  training loss:		2.297663
  validation loss:		2.254073
  validation accuracy:		12.83 %
Epoch 266 of 2000 took 0.036s
  training loss:		2.296068
  validation loss:		2.242771
  validation accuracy:		12.93 %
Epoch 267 of 2000 took 0.035s
  training loss:		2.296061
  validation loss:		2.240290
  validation accuracy:		13.04 %
Epoch 268 of 2000 took 0.035s
  training loss:		2.297542
  validation loss:		2.254734
  validation accuracy:		12.83 %
Epoch 269 of 2000 took 0.035s
  training loss:		2.295627
  validation loss:		2.238793
  validation accuracy:		12.93 %
Epoch 270 of 2000 took 0.035s
  training loss:		2.296382
  validation loss:		2.247331
  validation accuracy:		12.83 %
Epoch 271 of 2000 took 0.035s
  training loss:		2.296765
  validation loss:		2.247324
  validation accuracy:		12.83 %
Epoch 272 of 2000 took 0.035s
  training loss:		2.296035
  validation loss:		2.241391
  validation accuracy:		12.93 %
Epoch 273 of 2000 took 0.035s
  training loss:		2.295558
  validation loss:		2.250110
  validation accuracy:		12.93 %
Epoch 274 of 2000 took 0.036s
  training loss:		2.295598
  validation loss:		2.246246
  validation accuracy:		12.83 %
Epoch 275 of 2000 took 0.035s
  training loss:		2.296364
  validation loss:		2.243888
  validation accuracy:		12.93 %
Epoch 276 of 2000 took 0.035s
  training loss:		2.296095
  validation loss:		2.245247
  validation accuracy:		13.04 %
Epoch 277 of 2000 took 0.035s
  training loss:		2.297486
  validation loss:		2.247893
  validation accuracy:		13.04 %
Epoch 278 of 2000 took 0.035s
  training loss:		2.295349
  validation loss:		2.240525
  validation accuracy:		12.93 %
Epoch 279 of 2000 took 0.035s
  training loss:		2.296161
  validation loss:		2.249870
  validation accuracy:		12.83 %
Epoch 280 of 2000 took 0.035s
  training loss:		2.296149
  validation loss:		2.248245
  validation accuracy:		13.04 %
Epoch 281 of 2000 took 0.035s
  training loss:		2.297103
  validation loss:		2.240712
  validation accuracy:		12.83 %
Epoch 282 of 2000 took 0.035s
  training loss:		2.297089
  validation loss:		2.246124
  validation accuracy:		12.93 %
Epoch 283 of 2000 took 0.035s
  training loss:		2.295621
  validation loss:		2.246727
  validation accuracy:		13.04 %
Epoch 284 of 2000 took 0.035s
  training loss:		2.295903
  validation loss:		2.239057
  validation accuracy:		12.83 %
Epoch 285 of 2000 took 0.035s
  training loss:		2.296673
  validation loss:		2.243290
  validation accuracy:		12.93 %
Epoch 286 of 2000 took 0.035s
  training loss:		2.295908
  validation loss:		2.245522
  validation accuracy:		12.83 %
Epoch 287 of 2000 took 0.035s
  training loss:		2.297025
  validation loss:		2.244269
  validation accuracy:		13.04 %
Epoch 288 of 2000 took 0.035s
  training loss:		2.296483
  validation loss:		2.257066
  validation accuracy:		13.04 %
Epoch 289 of 2000 took 0.035s
  training loss:		2.296282
  validation loss:		2.243135
  validation accuracy:		12.83 %
Epoch 290 of 2000 took 0.035s
  training loss:		2.296155
  validation loss:		2.240440
  validation accuracy:		12.93 %
Epoch 291 of 2000 took 0.035s
  training loss:		2.297782
  validation loss:		2.258516
  validation accuracy:		13.04 %
Epoch 292 of 2000 took 0.035s
  training loss:		2.295799
  validation loss:		2.241331
  validation accuracy:		13.04 %
Epoch 293 of 2000 took 0.035s
  training loss:		2.296530
  validation loss:		2.238222
  validation accuracy:		12.83 %
Epoch 294 of 2000 took 0.035s
  training loss:		2.297541
  validation loss:		2.249708
  validation accuracy:		12.93 %
Epoch 295 of 2000 took 0.035s
  training loss:		2.294984
  validation loss:		2.243079
  validation accuracy:		12.83 %
Epoch 296 of 2000 took 0.035s
  training loss:		2.296762
  validation loss:		2.240327
  validation accuracy:		12.93 %
Epoch 297 of 2000 took 0.035s
  training loss:		2.296623
  validation loss:		2.247493
  validation accuracy:		12.93 %
Epoch 298 of 2000 took 0.035s
  training loss:		2.294707
  validation loss:		2.239352
  validation accuracy:		13.04 %
Epoch 299 of 2000 took 0.035s
  training loss:		2.295810
  validation loss:		2.244978
  validation accuracy:		12.93 %
Epoch 300 of 2000 took 0.035s
  training loss:		2.296292
  validation loss:		2.248136
  validation accuracy:		13.04 %
Epoch 301 of 2000 took 0.035s
  training loss:		2.295738
  validation loss:		2.236986
  validation accuracy:		12.93 %
Epoch 302 of 2000 took 0.035s
  training loss:		2.295581
  validation loss:		2.240079
  validation accuracy:		12.83 %
Epoch 303 of 2000 took 0.035s
  training loss:		2.296851
  validation loss:		2.250910
  validation accuracy:		13.04 %
Epoch 304 of 2000 took 0.035s
  training loss:		2.297806
  validation loss:		2.248226
  validation accuracy:		12.83 %
Epoch 305 of 2000 took 0.035s
  training loss:		2.296317
  validation loss:		2.239629
  validation accuracy:		12.83 %
Epoch 306 of 2000 took 0.035s
  training loss:		2.297261
  validation loss:		2.246177
  validation accuracy:		12.83 %
Epoch 307 of 2000 took 0.035s
  training loss:		2.297003
  validation loss:		2.249410
  validation accuracy:		13.04 %
Epoch 308 of 2000 took 0.035s
  training loss:		2.294766
  validation loss:		2.239966
  validation accuracy:		12.83 %
Epoch 309 of 2000 took 0.035s
  training loss:		2.297109
  validation loss:		2.242729
  validation accuracy:		12.93 %
Epoch 310 of 2000 took 0.035s
  training loss:		2.296835
  validation loss:		2.244842
  validation accuracy:		12.93 %
Epoch 311 of 2000 took 0.035s
  training loss:		2.296197
  validation loss:		2.240032
  validation accuracy:		12.93 %
Epoch 312 of 2000 took 0.035s
  training loss:		2.296642
  validation loss:		2.246702
  validation accuracy:		12.93 %
Epoch 313 of 2000 took 0.035s
  training loss:		2.295700
  validation loss:		2.241334
  validation accuracy:		13.04 %
Epoch 314 of 2000 took 0.035s
  training loss:		2.296447
  validation loss:		2.247951
  validation accuracy:		12.93 %
Epoch 315 of 2000 took 0.035s
  training loss:		2.296646
  validation loss:		2.243896
  validation accuracy:		13.04 %
Epoch 316 of 2000 took 0.035s
  training loss:		2.297557
  validation loss:		2.242980
  validation accuracy:		12.93 %
Epoch 317 of 2000 took 0.035s
  training loss:		2.296881
  validation loss:		2.243503
  validation accuracy:		13.04 %
Epoch 318 of 2000 took 0.035s
  training loss:		2.296585
  validation loss:		2.249818
  validation accuracy:		12.93 %
Epoch 319 of 2000 took 0.035s
  training loss:		2.296658
  validation loss:		2.247017
  validation accuracy:		12.93 %
Epoch 320 of 2000 took 0.035s
  training loss:		2.295565
  validation loss:		2.239332
  validation accuracy:		12.83 %
Epoch 321 of 2000 took 0.035s
  training loss:		2.296476
  validation loss:		2.245893
  validation accuracy:		12.93 %
Epoch 322 of 2000 took 0.035s
  training loss:		2.296440
  validation loss:		2.251574
  validation accuracy:		12.83 %
Epoch 323 of 2000 took 0.036s
  training loss:		2.295577
  validation loss:		2.236871
  validation accuracy:		12.93 %
Epoch 324 of 2000 took 0.035s
  training loss:		2.296328
  validation loss:		2.244703
  validation accuracy:		13.04 %
Epoch 325 of 2000 took 0.035s
  training loss:		2.295414
  validation loss:		2.235489
  validation accuracy:		12.93 %
Epoch 326 of 2000 took 0.035s
  training loss:		2.295621
  validation loss:		2.248000
  validation accuracy:		13.04 %
Epoch 327 of 2000 took 0.035s
  training loss:		2.295455
  validation loss:		2.242210
  validation accuracy:		12.93 %
Epoch 328 of 2000 took 0.035s
  training loss:		2.296424
  validation loss:		2.242400
  validation accuracy:		12.93 %
Epoch 329 of 2000 took 0.035s
  training loss:		2.296534
  validation loss:		2.241754
  validation accuracy:		12.83 %
Epoch 330 of 2000 took 0.035s
  training loss:		2.295285
  validation loss:		2.250154
  validation accuracy:		12.83 %
Epoch 331 of 2000 took 0.035s
  training loss:		2.296214
  validation loss:		2.240682
  validation accuracy:		12.83 %
Epoch 332 of 2000 took 0.035s
  training loss:		2.297319
  validation loss:		2.240423
  validation accuracy:		12.93 %
Epoch 333 of 2000 took 0.035s
  training loss:		2.296382
  validation loss:		2.251481
  validation accuracy:		12.93 %
Epoch 334 of 2000 took 0.035s
  training loss:		2.296870
  validation loss:		2.245228
  validation accuracy:		13.04 %
Epoch 335 of 2000 took 0.035s
  training loss:		2.294805
  validation loss:		2.235228
  validation accuracy:		12.93 %
Epoch 336 of 2000 took 0.035s
  training loss:		2.295870
  validation loss:		2.241717
  validation accuracy:		13.04 %
Epoch 337 of 2000 took 0.035s
  training loss:		2.296179
  validation loss:		2.242542
  validation accuracy:		12.93 %
Epoch 338 of 2000 took 0.035s
  training loss:		2.296435
  validation loss:		2.250039
  validation accuracy:		12.93 %
Epoch 339 of 2000 took 0.035s
  training loss:		2.295756
  validation loss:		2.245292
  validation accuracy:		13.04 %
Epoch 340 of 2000 took 0.035s
  training loss:		2.297179
  validation loss:		2.251395
  validation accuracy:		12.83 %
Epoch 341 of 2000 took 0.035s
  training loss:		2.296332
  validation loss:		2.241560
  validation accuracy:		12.83 %
Epoch 342 of 2000 took 0.035s
  training loss:		2.296473
  validation loss:		2.245030
  validation accuracy:		13.04 %
Epoch 343 of 2000 took 0.035s
  training loss:		2.295378
  validation loss:		2.239238
  validation accuracy:		12.93 %
Epoch 344 of 2000 took 0.035s
  training loss:		2.296744
  validation loss:		2.242756
  validation accuracy:		12.83 %
Epoch 345 of 2000 took 0.035s
  training loss:		2.295796
  validation loss:		2.243575
  validation accuracy:		12.83 %
Epoch 346 of 2000 took 0.035s
  training loss:		2.295069
  validation loss:		2.241831
  validation accuracy:		12.93 %
Epoch 347 of 2000 took 0.035s
  training loss:		2.296531
  validation loss:		2.246051
  validation accuracy:		12.93 %
Epoch 348 of 2000 took 0.036s
  training loss:		2.296683
  validation loss:		2.244016
  validation accuracy:		12.83 %
Epoch 349 of 2000 took 0.035s
  training loss:		2.295929
  validation loss:		2.238169
  validation accuracy:		12.93 %
Epoch 350 of 2000 took 0.035s
  training loss:		2.295159
  validation loss:		2.244526
  validation accuracy:		12.83 %
Epoch 351 of 2000 took 0.035s
  training loss:		2.296831
  validation loss:		2.247314
  validation accuracy:		12.93 %
Epoch 352 of 2000 took 0.035s
  training loss:		2.297340
  validation loss:		2.239849
  validation accuracy:		12.83 %
Epoch 353 of 2000 took 0.035s
  training loss:		2.296126
  validation loss:		2.254091
  validation accuracy:		13.04 %
Epoch 354 of 2000 took 0.035s
  training loss:		2.296119
  validation loss:		2.246543
  validation accuracy:		12.83 %
Epoch 355 of 2000 took 0.035s
  training loss:		2.295820
  validation loss:		2.241896
  validation accuracy:		12.83 %
Epoch 356 of 2000 took 0.035s
  training loss:		2.296056
  validation loss:		2.244077
  validation accuracy:		13.04 %
Epoch 357 of 2000 took 0.035s
  training loss:		2.295983
  validation loss:		2.236746
  validation accuracy:		12.93 %
Epoch 358 of 2000 took 0.035s
  training loss:		2.296419
  validation loss:		2.245679
  validation accuracy:		12.83 %
Epoch 359 of 2000 took 0.036s
  training loss:		2.295998
  validation loss:		2.247766
  validation accuracy:		12.93 %
Epoch 360 of 2000 took 0.035s
  training loss:		2.295730
  validation loss:		2.240389
  validation accuracy:		13.04 %
Epoch 361 of 2000 took 0.035s
  training loss:		2.296068
  validation loss:		2.243499
  validation accuracy:		12.93 %
Epoch 362 of 2000 took 0.035s
  training loss:		2.296037
  validation loss:		2.247254
  validation accuracy:		12.93 %
Epoch 363 of 2000 took 0.035s
  training loss:		2.296780
  validation loss:		2.243906
  validation accuracy:		12.83 %
Epoch 364 of 2000 took 0.035s
  training loss:		2.296590
  validation loss:		2.246789
  validation accuracy:		13.04 %
Epoch 365 of 2000 took 0.035s
  training loss:		2.296850
  validation loss:		2.243779
  validation accuracy:		13.04 %
Epoch 366 of 2000 took 0.035s
  training loss:		2.295074
  validation loss:		2.246611
  validation accuracy:		12.93 %
Epoch 367 of 2000 took 0.035s
  training loss:		2.296788
  validation loss:		2.247851
  validation accuracy:		13.04 %
Epoch 368 of 2000 took 0.035s
  training loss:		2.296534
  validation loss:		2.243044
  validation accuracy:		12.83 %
Epoch 369 of 2000 took 0.035s
  training loss:		2.296344
  validation loss:		2.237576
  validation accuracy:		13.04 %
Epoch 370 of 2000 took 0.035s
  training loss:		2.295937
  validation loss:		2.250621
  validation accuracy:		12.93 %
Epoch 371 of 2000 took 0.035s
  training loss:		2.296309
  validation loss:		2.244791
  validation accuracy:		12.83 %
Epoch 372 of 2000 took 0.035s
  training loss:		2.297823
  validation loss:		2.248470
  validation accuracy:		13.04 %
Epoch 373 of 2000 took 0.035s
  training loss:		2.295666
  validation loss:		2.247879
  validation accuracy:		13.04 %
Epoch 374 of 2000 took 0.035s
  training loss:		2.296288
  validation loss:		2.240884
  validation accuracy:		12.83 %
Epoch 375 of 2000 took 0.035s
  training loss:		2.296065
  validation loss:		2.249226
  validation accuracy:		13.04 %
Epoch 376 of 2000 took 0.035s
  training loss:		2.295768
  validation loss:		2.246693
  validation accuracy:		12.83 %
Epoch 377 of 2000 took 0.035s
  training loss:		2.294837
  validation loss:		2.247123
  validation accuracy:		12.93 %
Epoch 378 of 2000 took 0.035s
  training loss:		2.296758
  validation loss:		2.244073
  validation accuracy:		13.04 %
Epoch 379 of 2000 took 0.036s
  training loss:		2.297036
  validation loss:		2.238783
  validation accuracy:		12.83 %
Epoch 380 of 2000 took 0.035s
  training loss:		2.295579
  validation loss:		2.243454
  validation accuracy:		12.83 %
Epoch 381 of 2000 took 0.035s
  training loss:		2.296923
  validation loss:		2.250644
  validation accuracy:		12.83 %
Epoch 382 of 2000 took 0.035s
  training loss:		2.297084
  validation loss:		2.245764
  validation accuracy:		12.93 %
Epoch 383 of 2000 took 0.035s
  training loss:		2.296413
  validation loss:		2.245920
  validation accuracy:		12.93 %
Epoch 384 of 2000 took 0.035s
  training loss:		2.295351
  validation loss:		2.243449
  validation accuracy:		13.04 %
Epoch 385 of 2000 took 0.035s
  training loss:		2.297596
  validation loss:		2.247610
  validation accuracy:		13.04 %
Epoch 386 of 2000 took 0.035s
  training loss:		2.294687
  validation loss:		2.244838
  validation accuracy:		12.93 %
Epoch 387 of 2000 took 0.035s
  training loss:		2.295356
  validation loss:		2.247270
  validation accuracy:		12.93 %
Epoch 388 of 2000 took 0.035s
  training loss:		2.295939
  validation loss:		2.243257
  validation accuracy:		12.93 %
Epoch 389 of 2000 took 0.035s
  training loss:		2.296461
  validation loss:		2.246101
  validation accuracy:		13.04 %
Epoch 390 of 2000 took 0.035s
  training loss:		2.296413
  validation loss:		2.239156
  validation accuracy:		12.83 %
Epoch 391 of 2000 took 0.035s
  training loss:		2.296129
  validation loss:		2.247854
  validation accuracy:		12.93 %
Epoch 392 of 2000 took 0.035s
  training loss:		2.296481
  validation loss:		2.242165
  validation accuracy:		12.83 %
Epoch 393 of 2000 took 0.035s
  training loss:		2.296424
  validation loss:		2.245472
  validation accuracy:		12.93 %
Epoch 394 of 2000 took 0.035s
  training loss:		2.295874
  validation loss:		2.244424
  validation accuracy:		12.83 %
Epoch 395 of 2000 took 0.035s
  training loss:		2.294699
  validation loss:		2.244111
  validation accuracy:		13.04 %
Epoch 396 of 2000 took 0.035s
  training loss:		2.296618
  validation loss:		2.249299
  validation accuracy:		12.83 %
Epoch 397 of 2000 took 0.035s
  training loss:		2.296253
  validation loss:		2.245024
  validation accuracy:		12.83 %
Epoch 398 of 2000 took 0.035s
  training loss:		2.296572
  validation loss:		2.243310
  validation accuracy:		12.93 %
Epoch 399 of 2000 took 0.035s
  training loss:		2.296597
  validation loss:		2.239880
  validation accuracy:		12.83 %
Epoch 400 of 2000 took 0.035s
  training loss:		2.294482
  validation loss:		2.246902
  validation accuracy:		13.04 %
Epoch 401 of 2000 took 0.035s
  training loss:		2.295978
  validation loss:		2.246386
  validation accuracy:		12.93 %
Epoch 402 of 2000 took 0.035s
  training loss:		2.296366
  validation loss:		2.244403
  validation accuracy:		12.93 %
Epoch 403 of 2000 took 0.035s
  training loss:		2.296081
  validation loss:		2.241774
  validation accuracy:		13.04 %
Epoch 404 of 2000 took 0.035s
  training loss:		2.296044
  validation loss:		2.245127
  validation accuracy:		12.83 %
Epoch 405 of 2000 took 0.035s
  training loss:		2.295567
  validation loss:		2.244981
  validation accuracy:		12.83 %
Epoch 406 of 2000 took 0.035s
  training loss:		2.296088
  validation loss:		2.243912
  validation accuracy:		12.93 %
Epoch 407 of 2000 took 0.035s
  training loss:		2.296355
  validation loss:		2.239275
  validation accuracy:		12.83 %
Epoch 408 of 2000 took 0.035s
  training loss:		2.297576
  validation loss:		2.242307
  validation accuracy:		12.93 %
Epoch 409 of 2000 took 0.035s
  training loss:		2.295701
  validation loss:		2.252536
  validation accuracy:		13.04 %
Epoch 410 of 2000 took 0.035s
  training loss:		2.294797
  validation loss:		2.242580
  validation accuracy:		12.83 %
Epoch 411 of 2000 took 0.035s
  training loss:		2.296913
  validation loss:		2.240215
  validation accuracy:		12.93 %
Epoch 412 of 2000 took 0.035s
  training loss:		2.295469
  validation loss:		2.250713
  validation accuracy:		12.83 %
Epoch 413 of 2000 took 0.035s
  training loss:		2.295526
  validation loss:		2.248170
  validation accuracy:		12.83 %
Epoch 414 of 2000 took 0.035s
  training loss:		2.296832
  validation loss:		2.246353
  validation accuracy:		12.93 %
Epoch 415 of 2000 took 0.036s
  training loss:		2.295952
  validation loss:		2.248126
  validation accuracy:		12.83 %
Epoch 416 of 2000 took 0.035s
  training loss:		2.296081
  validation loss:		2.235723
  validation accuracy:		12.83 %
Epoch 417 of 2000 took 0.035s
  training loss:		2.296638
  validation loss:		2.240435
  validation accuracy:		13.04 %
Epoch 418 of 2000 took 0.035s
  training loss:		2.296358
  validation loss:		2.249197
  validation accuracy:		12.93 %
Epoch 419 of 2000 took 0.035s
  training loss:		2.296060
  validation loss:		2.245375
  validation accuracy:		12.93 %
Epoch 420 of 2000 took 0.035s
  training loss:		2.296042
  validation loss:		2.246221
  validation accuracy:		12.83 %
Epoch 421 of 2000 took 0.035s
  training loss:		2.296328
  validation loss:		2.239887
  validation accuracy:		12.93 %
Epoch 422 of 2000 took 0.035s
  training loss:		2.297095
  validation loss:		2.245142
  validation accuracy:		13.04 %
Epoch 423 of 2000 took 0.035s
  training loss:		2.297087
  validation loss:		2.246128
  validation accuracy:		12.93 %
Epoch 424 of 2000 took 0.035s
  training loss:		2.295994
  validation loss:		2.246524
  validation accuracy:		20.98 %
Epoch 425 of 2000 took 0.035s
  training loss:		2.294453
  validation loss:		2.238482
  validation accuracy:		12.93 %
Epoch 426 of 2000 took 0.036s
  training loss:		2.295129
  validation loss:		2.241150
  validation accuracy:		12.83 %
Epoch 427 of 2000 took 0.035s
  training loss:		2.295392
  validation loss:		2.246387
  validation accuracy:		12.83 %
Epoch 428 of 2000 took 0.035s
  training loss:		2.295493
  validation loss:		2.238671
  validation accuracy:		12.93 %
Epoch 429 of 2000 took 0.035s
  training loss:		2.296617
  validation loss:		2.245210
  validation accuracy:		12.83 %
Epoch 430 of 2000 took 0.035s
  training loss:		2.295147
  validation loss:		2.244965
  validation accuracy:		12.83 %
Epoch 431 of 2000 took 0.035s
  training loss:		2.297180
  validation loss:		2.243333
  validation accuracy:		12.93 %
Epoch 432 of 2000 took 0.035s
  training loss:		2.295863
  validation loss:		2.243482
  validation accuracy:		12.83 %
Epoch 433 of 2000 took 0.035s
  training loss:		2.295442
  validation loss:		2.244966
  validation accuracy:		13.04 %
Epoch 434 of 2000 took 0.035s
  training loss:		2.295509
  validation loss:		2.243519
  validation accuracy:		13.37 %
Epoch 435 of 2000 took 0.035s
  training loss:		2.295985
  validation loss:		2.246879
  validation accuracy:		12.83 %
Epoch 436 of 2000 took 0.036s
  training loss:		2.295252
  validation loss:		2.241758
  validation accuracy:		12.83 %
Epoch 437 of 2000 took 0.035s
  training loss:		2.295547
  validation loss:		2.246315
  validation accuracy:		12.93 %
Epoch 438 of 2000 took 0.035s
  training loss:		2.296065
  validation loss:		2.245236
  validation accuracy:		13.04 %
Epoch 439 of 2000 took 0.035s
  training loss:		2.296198
  validation loss:		2.248213
  validation accuracy:		12.83 %
Epoch 440 of 2000 took 0.035s
  training loss:		2.296419
  validation loss:		2.250357
  validation accuracy:		12.93 %
Epoch 441 of 2000 took 0.035s
  training loss:		2.296427
  validation loss:		2.239723
  validation accuracy:		12.83 %
Epoch 442 of 2000 took 0.035s
  training loss:		2.296621
  validation loss:		2.250495
  validation accuracy:		12.93 %
Epoch 443 of 2000 took 0.035s
  training loss:		2.296929
  validation loss:		2.245488
  validation accuracy:		12.93 %
Epoch 444 of 2000 took 0.035s
  training loss:		2.295090
  validation loss:		2.241327
  validation accuracy:		12.93 %
Epoch 445 of 2000 took 0.035s
  training loss:		2.296495
  validation loss:		2.240983
  validation accuracy:		12.83 %
Epoch 446 of 2000 took 0.036s
  training loss:		2.295734
  validation loss:		2.244825
  validation accuracy:		12.83 %
Epoch 447 of 2000 took 0.035s
  training loss:		2.296761
  validation loss:		2.244329
  validation accuracy:		12.93 %
Epoch 448 of 2000 took 0.035s
  training loss:		2.295619
  validation loss:		2.245258
  validation accuracy:		12.83 %
Epoch 449 of 2000 took 0.035s
  training loss:		2.295832
  validation loss:		2.243974
  validation accuracy:		12.93 %
Epoch 450 of 2000 took 0.035s
  training loss:		2.295554
  validation loss:		2.242458
  validation accuracy:		13.04 %
Epoch 451 of 2000 took 0.035s
  training loss:		2.296290
  validation loss:		2.245759
  validation accuracy:		12.93 %
Epoch 452 of 2000 took 0.035s
  training loss:		2.295794
  validation loss:		2.245701
  validation accuracy:		12.83 %
Epoch 453 of 2000 took 0.035s
  training loss:		2.295627
  validation loss:		2.246540
  validation accuracy:		14.13 %
Epoch 454 of 2000 took 0.035s
  training loss:		2.296231
  validation loss:		2.242738
  validation accuracy:		12.83 %
Epoch 455 of 2000 took 0.035s
  training loss:		2.296257
  validation loss:		2.243849
  validation accuracy:		12.83 %
Epoch 456 of 2000 took 0.035s
  training loss:		2.295115
  validation loss:		2.245400
  validation accuracy:		12.83 %
Epoch 457 of 2000 took 0.035s
  training loss:		2.294936
  validation loss:		2.245596
  validation accuracy:		12.93 %
Epoch 458 of 2000 took 0.035s
  training loss:		2.295915
  validation loss:		2.247659
  validation accuracy:		12.83 %
Epoch 459 of 2000 took 0.035s
  training loss:		2.294055
  validation loss:		2.235234
  validation accuracy:		13.04 %
Epoch 460 of 2000 took 0.035s
  training loss:		2.295159
  validation loss:		2.247403
  validation accuracy:		12.93 %
Epoch 461 of 2000 took 0.035s
  training loss:		2.295371
  validation loss:		2.241415
  validation accuracy:		12.83 %
Epoch 462 of 2000 took 0.035s
  training loss:		2.295863
  validation loss:		2.248189
  validation accuracy:		12.83 %
Epoch 463 of 2000 took 0.035s
  training loss:		2.296927
  validation loss:		2.244995
  validation accuracy:		12.93 %
Epoch 464 of 2000 took 0.035s
  training loss:		2.296357
  validation loss:		2.247103
  validation accuracy:		12.93 %
Epoch 465 of 2000 took 0.035s
  training loss:		2.294925
  validation loss:		2.242910
  validation accuracy:		12.93 %
Epoch 466 of 2000 took 0.035s
  training loss:		2.295630
  validation loss:		2.246931
  validation accuracy:		13.04 %
Epoch 467 of 2000 took 0.035s
  training loss:		2.295516
  validation loss:		2.246011
  validation accuracy:		12.93 %
Epoch 468 of 2000 took 0.035s
  training loss:		2.296548
  validation loss:		2.246068
  validation accuracy:		17.61 %
Epoch 469 of 2000 took 0.035s
  training loss:		2.295594
  validation loss:		2.247732
  validation accuracy:		12.93 %
Epoch 470 of 2000 took 0.035s
  training loss:		2.295727
  validation loss:		2.242967
  validation accuracy:		12.93 %
Epoch 471 of 2000 took 0.035s
  training loss:		2.295877
  validation loss:		2.247392
  validation accuracy:		12.83 %
Epoch 472 of 2000 took 0.036s
  training loss:		2.295846
  validation loss:		2.242628
  validation accuracy:		12.93 %
Epoch 473 of 2000 took 0.035s
  training loss:		2.296425
  validation loss:		2.241724
  validation accuracy:		12.83 %
Epoch 474 of 2000 took 0.035s
  training loss:		2.296285
  validation loss:		2.247284
  validation accuracy:		13.04 %
Epoch 475 of 2000 took 0.035s
  training loss:		2.296114
  validation loss:		2.250957
  validation accuracy:		12.93 %
Epoch 476 of 2000 took 0.035s
  training loss:		2.295453
  validation loss:		2.240453
  validation accuracy:		13.04 %
Epoch 477 of 2000 took 0.036s
  training loss:		2.295354
  validation loss:		2.246520
  validation accuracy:		12.83 %
Epoch 478 of 2000 took 0.035s
  training loss:		2.295115
  validation loss:		2.237221
  validation accuracy:		13.04 %
Epoch 479 of 2000 took 0.035s
  training loss:		2.294629
  validation loss:		2.244829
  validation accuracy:		12.83 %
Epoch 480 of 2000 took 0.035s
  training loss:		2.296232
  validation loss:		2.244402
  validation accuracy:		12.93 %
Epoch 481 of 2000 took 0.035s
  training loss:		2.295254
  validation loss:		2.247860
  validation accuracy:		12.93 %
Epoch 482 of 2000 took 0.035s
  training loss:		2.295675
  validation loss:		2.245504
  validation accuracy:		12.93 %
Epoch 483 of 2000 took 0.035s
  training loss:		2.296678
  validation loss:		2.240448
  validation accuracy:		12.83 %
Epoch 484 of 2000 took 0.035s
  training loss:		2.295638
  validation loss:		2.243629
  validation accuracy:		13.04 %
Epoch 485 of 2000 took 0.035s
  training loss:		2.295423
  validation loss:		2.244069
  validation accuracy:		12.83 %
Epoch 486 of 2000 took 0.035s
  training loss:		2.294916
  validation loss:		2.242476
  validation accuracy:		13.04 %
Epoch 487 of 2000 took 0.035s
  training loss:		2.294864
  validation loss:		2.243161
  validation accuracy:		12.83 %
Epoch 488 of 2000 took 0.035s
  training loss:		2.295473
  validation loss:		2.247566
  validation accuracy:		13.91 %
Epoch 489 of 2000 took 0.035s
  training loss:		2.294558
  validation loss:		2.244369
  validation accuracy:		13.04 %
Epoch 490 of 2000 took 0.035s
  training loss:		2.296980
  validation loss:		2.244015
  validation accuracy:		12.93 %
Epoch 491 of 2000 took 0.035s
  training loss:		2.294826
  validation loss:		2.240388
  validation accuracy:		12.83 %
Epoch 492 of 2000 took 0.035s
  training loss:		2.295535
  validation loss:		2.245256
  validation accuracy:		12.83 %
Epoch 493 of 2000 took 0.037s
  training loss:		2.295688
  validation loss:		2.246582
  validation accuracy:		13.04 %
Epoch 494 of 2000 took 0.036s
  training loss:		2.295628
  validation loss:		2.246168
  validation accuracy:		12.93 %
Epoch 495 of 2000 took 0.035s
  training loss:		2.295949
  validation loss:		2.242727
  validation accuracy:		12.83 %
Epoch 496 of 2000 took 0.035s
  training loss:		2.296351
  validation loss:		2.243133
  validation accuracy:		12.93 %
Epoch 497 of 2000 took 0.035s
  training loss:		2.295685
  validation loss:		2.245944
  validation accuracy:		12.83 %
Epoch 498 of 2000 took 0.035s
  training loss:		2.294872
  validation loss:		2.241404
  validation accuracy:		12.83 %
Epoch 499 of 2000 took 0.035s
  training loss:		2.294841
  validation loss:		2.235380
  validation accuracy:		12.93 %
Epoch 500 of 2000 took 0.036s
  training loss:		2.294253
  validation loss:		2.244754
  validation accuracy:		13.04 %
Epoch 501 of 2000 took 0.035s
  training loss:		2.295401
  validation loss:		2.245681
  validation accuracy:		12.83 %
Epoch 502 of 2000 took 0.035s
  training loss:		2.295471
  validation loss:		2.245811
  validation accuracy:		13.04 %
Epoch 503 of 2000 took 0.035s
  training loss:		2.295283
  validation loss:		2.237588
  validation accuracy:		12.83 %
Epoch 504 of 2000 took 0.035s
  training loss:		2.295168
  validation loss:		2.243703
  validation accuracy:		12.93 %
Epoch 505 of 2000 took 0.036s
  training loss:		2.295623
  validation loss:		2.252084
  validation accuracy:		12.83 %
Epoch 506 of 2000 took 0.035s
  training loss:		2.294941
  validation loss:		2.244405
  validation accuracy:		12.93 %
Epoch 507 of 2000 took 0.035s
  training loss:		2.295402
  validation loss:		2.241447
  validation accuracy:		12.83 %
Epoch 508 of 2000 took 0.035s
  training loss:		2.294576
  validation loss:		2.245808
  validation accuracy:		12.83 %
Epoch 509 of 2000 took 0.035s
  training loss:		2.294793
  validation loss:		2.244305
  validation accuracy:		12.83 %
Epoch 510 of 2000 took 0.035s
  training loss:		2.296080
  validation loss:		2.242843
  validation accuracy:		12.93 %
Epoch 511 of 2000 took 0.035s
  training loss:		2.294209
  validation loss:		2.246648
  validation accuracy:		12.93 %
Epoch 512 of 2000 took 0.035s
  training loss:		2.294965
  validation loss:		2.244538
  validation accuracy:		13.04 %
Epoch 513 of 2000 took 0.035s
  training loss:		2.294356
  validation loss:		2.246581
  validation accuracy:		12.93 %
Epoch 514 of 2000 took 0.035s
  training loss:		2.295072
  validation loss:		2.241460
  validation accuracy:		12.83 %
Epoch 515 of 2000 took 0.035s
  training loss:		2.294891
  validation loss:		2.240312
  validation accuracy:		12.93 %
Epoch 516 of 2000 took 0.035s
  training loss:		2.294699
  validation loss:		2.247404
  validation accuracy:		12.83 %
Epoch 517 of 2000 took 0.035s
  training loss:		2.293387
  validation loss:		2.233337
  validation accuracy:		13.04 %
Epoch 518 of 2000 took 0.035s
  training loss:		2.295397
  validation loss:		2.243129
  validation accuracy:		12.93 %
Epoch 519 of 2000 took 0.035s
  training loss:		2.295598
  validation loss:		2.252515
  validation accuracy:		12.83 %
Epoch 520 of 2000 took 0.035s
  training loss:		2.295203
  validation loss:		2.247515
  validation accuracy:		12.83 %
Epoch 521 of 2000 took 0.035s
  training loss:		2.294486
  validation loss:		2.241460
  validation accuracy:		12.93 %
Epoch 522 of 2000 took 0.035s
  training loss:		2.294154
  validation loss:		2.241400
  validation accuracy:		12.93 %
Epoch 523 of 2000 took 0.035s
  training loss:		2.294417
  validation loss:		2.248605
  validation accuracy:		12.93 %
Epoch 524 of 2000 took 0.035s
  training loss:		2.295744
  validation loss:		2.243560
  validation accuracy:		13.04 %
Epoch 525 of 2000 took 0.035s
  training loss:		2.293327
  validation loss:		2.238553
  validation accuracy:		12.83 %
Epoch 526 of 2000 took 0.035s
  training loss:		2.295018
  validation loss:		2.246891
  validation accuracy:		12.93 %
Epoch 527 of 2000 took 0.035s
  training loss:		2.295010
  validation loss:		2.249205
  validation accuracy:		12.83 %
Epoch 528 of 2000 took 0.035s
  training loss:		2.293424
  validation loss:		2.241025
  validation accuracy:		13.04 %
Epoch 529 of 2000 took 0.035s
  training loss:		2.293313
  validation loss:		2.239652
  validation accuracy:		13.04 %
Epoch 530 of 2000 took 0.035s
  training loss:		2.293563
  validation loss:		2.238183
  validation accuracy:		12.93 %
Epoch 531 of 2000 took 0.035s
  training loss:		2.293372
  validation loss:		2.237053
  validation accuracy:		12.83 %
Epoch 532 of 2000 took 0.035s
  training loss:		2.294859
  validation loss:		2.242027
  validation accuracy:		13.04 %
Epoch 533 of 2000 took 0.035s
  training loss:		2.293139
  validation loss:		2.236717
  validation accuracy:		12.93 %
Epoch 534 of 2000 took 0.035s
  training loss:		2.293570
  validation loss:		2.239854
  validation accuracy:		13.04 %
Epoch 535 of 2000 took 0.035s
  training loss:		2.293639
  validation loss:		2.249094
  validation accuracy:		13.04 %
Epoch 536 of 2000 took 0.035s
  training loss:		2.292955
  validation loss:		2.240595
  validation accuracy:		12.83 %
Epoch 537 of 2000 took 0.035s
  training loss:		2.292750
  validation loss:		2.243672
  validation accuracy:		12.93 %
Epoch 538 of 2000 took 0.035s
  training loss:		2.293558
  validation loss:		2.237163
  validation accuracy:		13.04 %
Epoch 539 of 2000 took 0.035s
  training loss:		2.294062
  validation loss:		2.240971
  validation accuracy:		12.83 %
Epoch 540 of 2000 took 0.035s
  training loss:		2.293523
  validation loss:		2.244626
  validation accuracy:		12.83 %
Epoch 541 of 2000 took 0.035s
  training loss:		2.292131
  validation loss:		2.236819
  validation accuracy:		12.83 %
Epoch 542 of 2000 took 0.035s
  training loss:		2.292818
  validation loss:		2.239250
  validation accuracy:		12.93 %
Epoch 543 of 2000 took 0.035s
  training loss:		2.291457
  validation loss:		2.238781
  validation accuracy:		12.93 %
Epoch 544 of 2000 took 0.035s
  training loss:		2.292658
  validation loss:		2.244709
  validation accuracy:		12.93 %
Epoch 545 of 2000 took 0.035s
  training loss:		2.291431
  validation loss:		2.234988
  validation accuracy:		12.93 %
Epoch 546 of 2000 took 0.035s
  training loss:		2.290574
  validation loss:		2.238096
  validation accuracy:		18.80 %
Epoch 547 of 2000 took 0.035s
  training loss:		2.291418
  validation loss:		2.237602
  validation accuracy:		25.22 %
Epoch 548 of 2000 took 0.035s
  training loss:		2.290856
  validation loss:		2.239438
  validation accuracy:		23.26 %
Epoch 549 of 2000 took 0.036s
  training loss:		2.290730
  validation loss:		2.243393
  validation accuracy:		18.70 %
Epoch 550 of 2000 took 0.035s
  training loss:		2.289904
  validation loss:		2.234046
  validation accuracy:		12.93 %
Epoch 551 of 2000 took 0.035s
  training loss:		2.288350
  validation loss:		2.241758
  validation accuracy:		15.87 %
Epoch 552 of 2000 took 0.035s
  training loss:		2.288701
  validation loss:		2.233053
  validation accuracy:		24.89 %
Epoch 553 of 2000 took 0.035s
  training loss:		2.288784
  validation loss:		2.235971
  validation accuracy:		22.72 %
Epoch 554 of 2000 took 0.035s
  training loss:		2.288075
  validation loss:		2.234312
  validation accuracy:		22.17 %
Epoch 555 of 2000 took 0.035s
  training loss:		2.287823
  validation loss:		2.232480
  validation accuracy:		26.41 %
Epoch 556 of 2000 took 0.035s
  training loss:		2.287728
  validation loss:		2.243615
  validation accuracy:		19.02 %
Epoch 557 of 2000 took 0.036s
  training loss:		2.287478
  validation loss:		2.238582
  validation accuracy:		29.24 %
Epoch 558 of 2000 took 0.035s
  training loss:		2.287092
  validation loss:		2.231570
  validation accuracy:		14.78 %
Epoch 559 of 2000 took 0.035s
  training loss:		2.284872
  validation loss:		2.230089
  validation accuracy:		29.46 %
Epoch 560 of 2000 took 0.035s
  training loss:		2.282969
  validation loss:		2.230022
  validation accuracy:		22.83 %
Epoch 561 of 2000 took 0.035s
  training loss:		2.282747
  validation loss:		2.228666
  validation accuracy:		12.93 %
Epoch 562 of 2000 took 0.035s
  training loss:		2.282263
  validation loss:		2.227967
  validation accuracy:		22.50 %
Epoch 563 of 2000 took 0.035s
  training loss:		2.281765
  validation loss:		2.230479
  validation accuracy:		27.50 %
Epoch 564 of 2000 took 0.035s
  training loss:		2.279499
  validation loss:		2.225451
  validation accuracy:		17.28 %
Epoch 565 of 2000 took 0.035s
  training loss:		2.277884
  validation loss:		2.222140
  validation accuracy:		28.37 %
Epoch 566 of 2000 took 0.036s
  training loss:		2.276257
  validation loss:		2.223778
  validation accuracy:		30.65 %
Epoch 567 of 2000 took 0.035s
  training loss:		2.272294
  validation loss:		2.219749
  validation accuracy:		21.85 %
Epoch 568 of 2000 took 0.035s
  training loss:		2.271297
  validation loss:		2.216532
  validation accuracy:		24.35 %
Epoch 569 of 2000 took 0.035s
  training loss:		2.266736
  validation loss:		2.212341
  validation accuracy:		30.98 %
Epoch 570 of 2000 took 0.035s
  training loss:		2.265286
  validation loss:		2.207047
  validation accuracy:		33.15 %
Epoch 571 of 2000 took 0.035s
  training loss:		2.262589
  validation loss:		2.205091
  validation accuracy:		20.76 %
Epoch 572 of 2000 took 0.035s
  training loss:		2.255525
  validation loss:		2.201835
  validation accuracy:		29.24 %
Epoch 573 of 2000 took 0.035s
  training loss:		2.247917
  validation loss:		2.188363
  validation accuracy:		24.89 %
Epoch 574 of 2000 took 0.035s
  training loss:		2.242203
  validation loss:		2.175527
  validation accuracy:		29.67 %
Epoch 575 of 2000 took 0.035s
  training loss:		2.232309
  validation loss:		2.170958
  validation accuracy:		32.39 %
Epoch 576 of 2000 took 0.035s
  training loss:		2.221879
  validation loss:		2.153080
  validation accuracy:		24.35 %
Epoch 577 of 2000 took 0.035s
  training loss:		2.208007
  validation loss:		2.138223
  validation accuracy:		28.91 %
Epoch 578 of 2000 took 0.035s
  training loss:		2.192525
  validation loss:		2.123568
  validation accuracy:		28.15 %
Epoch 579 of 2000 took 0.035s
  training loss:		2.173320
  validation loss:		2.096567
  validation accuracy:		31.63 %
Epoch 580 of 2000 took 0.035s
  training loss:		2.149825
  validation loss:		2.068352
  validation accuracy:		26.96 %
Epoch 581 of 2000 took 0.035s
  training loss:		2.116426
  validation loss:		2.031127
  validation accuracy:		30.22 %
Epoch 582 of 2000 took 0.035s
  training loss:		2.086696
  validation loss:		1.994814
  validation accuracy:		28.70 %
Epoch 583 of 2000 took 0.035s
  training loss:		2.052106
  validation loss:		1.958300
  validation accuracy:		33.48 %
Epoch 584 of 2000 took 0.035s
  training loss:		2.014014
  validation loss:		1.915991
  validation accuracy:		33.37 %
Epoch 585 of 2000 took 0.035s
  training loss:		1.971154
  validation loss:		1.868761
  validation accuracy:		35.33 %
Epoch 586 of 2000 took 0.035s
  training loss:		1.933880
  validation loss:		1.824773
  validation accuracy:		37.07 %
Epoch 587 of 2000 took 0.035s
  training loss:		1.895602
  validation loss:		1.787171
  validation accuracy:		39.57 %
Epoch 588 of 2000 took 0.035s
  training loss:		1.851988
  validation loss:		1.745775
  validation accuracy:		40.43 %
Epoch 589 of 2000 took 0.035s
  training loss:		1.814094
  validation loss:		1.702451
  validation accuracy:		40.43 %
Epoch 590 of 2000 took 0.035s
  training loss:		1.776869
  validation loss:		1.674511
  validation accuracy:		43.04 %
Epoch 591 of 2000 took 0.035s
  training loss:		1.741728
  validation loss:		1.634816
  validation accuracy:		43.15 %
Epoch 592 of 2000 took 0.035s
  training loss:		1.708207
  validation loss:		1.604145
  validation accuracy:		41.96 %
Epoch 593 of 2000 took 0.035s
  training loss:		1.676234
  validation loss:		1.575571
  validation accuracy:		44.35 %
Epoch 594 of 2000 took 0.035s
  training loss:		1.645630
  validation loss:		1.543104
  validation accuracy:		44.89 %
Epoch 595 of 2000 took 0.035s
  training loss:		1.624935
  validation loss:		1.525571
  validation accuracy:		45.98 %
Epoch 596 of 2000 took 0.035s
  training loss:		1.608720
  validation loss:		1.494031
  validation accuracy:		46.96 %
Epoch 597 of 2000 took 0.035s
  training loss:		1.586097
  validation loss:		1.476242
  validation accuracy:		46.63 %
Epoch 598 of 2000 took 0.035s
  training loss:		1.564652
  validation loss:		1.464053
  validation accuracy:		47.61 %
Epoch 599 of 2000 took 0.035s
  training loss:		1.550427
  validation loss:		1.448099
  validation accuracy:		47.50 %
Epoch 600 of 2000 took 0.035s
  training loss:		1.532211
  validation loss:		1.425746
  validation accuracy:		49.46 %
Epoch 601 of 2000 took 0.035s
  training loss:		1.521177
  validation loss:		1.412849
  validation accuracy:		48.37 %
Epoch 602 of 2000 took 0.036s
  training loss:		1.502009
  validation loss:		1.408251
  validation accuracy:		49.46 %
Epoch 603 of 2000 took 0.035s
  training loss:		1.494508
  validation loss:		1.400338
  validation accuracy:		48.26 %
Epoch 604 of 2000 took 0.035s
  training loss:		1.484689
  validation loss:		1.396768
  validation accuracy:		47.39 %
Epoch 605 of 2000 took 0.036s
  training loss:		1.491236
  validation loss:		1.375101
  validation accuracy:		51.30 %
Epoch 606 of 2000 took 0.035s
  training loss:		1.470130
  validation loss:		1.397399
  validation accuracy:		47.07 %
Epoch 607 of 2000 took 0.035s
  training loss:		1.465356
  validation loss:		1.369709
  validation accuracy:		49.57 %
Epoch 608 of 2000 took 0.035s
  training loss:		1.472814
  validation loss:		1.347305
  validation accuracy:		50.87 %
Epoch 609 of 2000 took 0.035s
  training loss:		1.478331
  validation loss:		1.502950
  validation accuracy:		42.61 %
Epoch 610 of 2000 took 0.035s
  training loss:		1.482463
  validation loss:		1.469562
  validation accuracy:		40.65 %
Epoch 611 of 2000 took 0.035s
  training loss:		1.718839
  validation loss:		1.696910
  validation accuracy:		32.83 %
Epoch 612 of 2000 took 0.035s
  training loss:		1.489522
  validation loss:		1.346590
  validation accuracy:		51.09 %
Epoch 613 of 2000 took 0.035s
  training loss:		1.425940
  validation loss:		1.337418
  validation accuracy:		51.20 %
Epoch 614 of 2000 took 0.035s
  training loss:		1.427753
  validation loss:		1.347006
  validation accuracy:		49.02 %
Epoch 615 of 2000 took 0.035s
  training loss:		1.556911
  validation loss:		1.358103
  validation accuracy:		49.46 %
Epoch 616 of 2000 took 0.036s
  training loss:		1.416705
  validation loss:		1.337295
  validation accuracy:		50.54 %
Epoch 617 of 2000 took 0.035s
  training loss:		1.441846
  validation loss:		1.373494
  validation accuracy:		47.17 %
Epoch 618 of 2000 took 0.035s
  training loss:		1.429000
  validation loss:		1.519265
  validation accuracy:		40.33 %
Epoch 619 of 2000 took 0.036s
  training loss:		1.617103
  validation loss:		1.645658
  validation accuracy:		34.35 %
Epoch 620 of 2000 took 0.035s
  training loss:		1.545242
  validation loss:		1.323089
  validation accuracy:		52.83 %
Epoch 621 of 2000 took 0.035s
  training loss:		1.406971
  validation loss:		1.313720
  validation accuracy:		51.74 %
Epoch 622 of 2000 took 0.035s
  training loss:		1.413454
  validation loss:		1.318026
  validation accuracy:		50.00 %
Epoch 623 of 2000 took 0.035s
  training loss:		1.522606
  validation loss:		1.642907
  validation accuracy:		35.98 %
Epoch 624 of 2000 took 0.035s
  training loss:		1.484314
  validation loss:		1.348519
  validation accuracy:		49.57 %
Epoch 625 of 2000 took 0.035s
  training loss:		1.422517
  validation loss:		1.313983
  validation accuracy:		51.20 %
Epoch 626 of 2000 took 0.035s
  training loss:		1.415072
  validation loss:		1.351383
  validation accuracy:		48.37 %
Epoch 627 of 2000 took 0.035s
  training loss:		1.391505
  validation loss:		1.317722
  validation accuracy:		49.57 %
Epoch 628 of 2000 took 0.035s
  training loss:		1.466360
  validation loss:		1.449084
  validation accuracy:		43.48 %
Epoch 629 of 2000 took 0.035s
  training loss:		1.408622
  validation loss:		1.315420
  validation accuracy:		50.76 %
Epoch 630 of 2000 took 0.035s
  training loss:		1.377783
  validation loss:		1.294785
  validation accuracy:		51.30 %
Epoch 631 of 2000 took 0.035s
  training loss:		1.402815
  validation loss:		1.342395
  validation accuracy:		49.13 %
Epoch 632 of 2000 took 0.035s
  training loss:		1.617389
  validation loss:		1.412712
  validation accuracy:		45.11 %
Epoch 633 of 2000 took 0.035s
  training loss:		1.394127
  validation loss:		1.329245
  validation accuracy:		50.00 %
Epoch 634 of 2000 took 0.035s
  training loss:		1.415068
  validation loss:		1.304446
  validation accuracy:		50.76 %
Epoch 635 of 2000 took 0.035s
  training loss:		1.373048
  validation loss:		1.300348
  validation accuracy:		51.52 %
Epoch 636 of 2000 took 0.035s
  training loss:		1.498315
  validation loss:		1.746443
  validation accuracy:		34.57 %
Epoch 637 of 2000 took 0.035s
  training loss:		1.613828
  validation loss:		1.394636
  validation accuracy:		46.52 %
Epoch 638 of 2000 took 0.035s
  training loss:		1.407521
  validation loss:		1.303502
  validation accuracy:		50.54 %
Epoch 639 of 2000 took 0.035s
  training loss:		1.381966
  validation loss:		1.297330
  validation accuracy:		50.54 %
Epoch 640 of 2000 took 0.035s
  training loss:		1.462932
  validation loss:		1.324589
  validation accuracy:		49.24 %
Epoch 641 of 2000 took 0.035s
  training loss:		1.397904
  validation loss:		1.327026
  validation accuracy:		49.24 %
Epoch 642 of 2000 took 0.035s
  training loss:		1.426292
  validation loss:		1.298460
  validation accuracy:		50.43 %
Epoch 643 of 2000 took 0.035s
  training loss:		1.382115
  validation loss:		1.326910
  validation accuracy:		49.24 %
Epoch 644 of 2000 took 0.035s
  training loss:		1.429957
  validation loss:		1.305514
  validation accuracy:		50.87 %
Epoch 645 of 2000 took 0.035s
  training loss:		1.372899
  validation loss:		1.320311
  validation accuracy:		49.13 %
Epoch 646 of 2000 took 0.035s
  training loss:		1.375232
  validation loss:		1.301683
  validation accuracy:		50.76 %
Epoch 647 of 2000 took 0.035s
  training loss:		1.372147
  validation loss:		1.326011
  validation accuracy:		50.11 %
Epoch 648 of 2000 took 0.035s
  training loss:		1.386554
  validation loss:		1.316354
  validation accuracy:		49.89 %
Epoch 649 of 2000 took 0.035s
  training loss:		1.611390
  validation loss:		1.457225
  validation accuracy:		44.13 %
Epoch 650 of 2000 took 0.035s
  training loss:		1.398948
  validation loss:		1.288362
  validation accuracy:		52.72 %
Epoch 651 of 2000 took 0.035s
  training loss:		1.364822
  validation loss:		1.328364
  validation accuracy:		48.48 %
Epoch 652 of 2000 took 0.035s
  training loss:		1.386493
  validation loss:		1.297352
  validation accuracy:		51.74 %
Epoch 653 of 2000 took 0.035s
  training loss:		1.378751
  validation loss:		1.283349
  validation accuracy:		50.43 %
Epoch 654 of 2000 took 0.035s
  training loss:		1.422602
  validation loss:		1.287155
  validation accuracy:		52.07 %
Epoch 655 of 2000 took 0.035s
  training loss:		1.399900
  validation loss:		1.299747
  validation accuracy:		49.57 %
Epoch 656 of 2000 took 0.035s
  training loss:		1.363628
  validation loss:		1.300907
  validation accuracy:		49.89 %
Epoch 657 of 2000 took 0.035s
  training loss:		1.368047
  validation loss:		1.316535
  validation accuracy:		48.80 %
Epoch 658 of 2000 took 0.035s
  training loss:		1.360737
  validation loss:		1.286785
  validation accuracy:		51.74 %
Epoch 659 of 2000 took 0.035s
  training loss:		1.426023
  validation loss:		1.347996
  validation accuracy:		48.59 %
Epoch 660 of 2000 took 0.035s
  training loss:		1.526100
  validation loss:		1.339835
  validation accuracy:		49.13 %
Epoch 661 of 2000 took 0.035s
  training loss:		1.390566
  validation loss:		1.366665
  validation accuracy:		46.96 %
Epoch 662 of 2000 took 0.036s
  training loss:		1.425185
  validation loss:		1.331946
  validation accuracy:		48.91 %
Epoch 663 of 2000 took 0.035s
  training loss:		1.415416
  validation loss:		1.321328
  validation accuracy:		48.04 %
Epoch 664 of 2000 took 0.035s
  training loss:		1.398230
  validation loss:		1.287018
  validation accuracy:		51.41 %
Epoch 665 of 2000 took 0.035s
  training loss:		1.370594
  validation loss:		1.299051
  validation accuracy:		51.41 %
Epoch 666 of 2000 took 0.035s
  training loss:		1.376303
  validation loss:		1.341575
  validation accuracy:		47.61 %
Epoch 667 of 2000 took 0.035s
  training loss:		1.425514
  validation loss:		1.491621
  validation accuracy:		42.72 %
Epoch 668 of 2000 took 0.035s
  training loss:		1.437149
  validation loss:		1.289724
  validation accuracy:		49.67 %
Epoch 669 of 2000 took 0.035s
  training loss:		1.351275
  validation loss:		1.275660
  validation accuracy:		51.30 %
Epoch 670 of 2000 took 0.035s
  training loss:		1.363668
  validation loss:		1.294782
  validation accuracy:		49.67 %
Epoch 671 of 2000 took 0.035s
  training loss:		1.366803
  validation loss:		1.283743
  validation accuracy:		51.20 %
Epoch 672 of 2000 took 0.035s
  training loss:		1.362121
  validation loss:		1.294401
  validation accuracy:		49.89 %
Epoch 673 of 2000 took 0.035s
  training loss:		1.399341
  validation loss:		1.477621
  validation accuracy:		43.80 %
Epoch 674 of 2000 took 0.035s
  training loss:		1.393877
  validation loss:		1.303933
  validation accuracy:		49.78 %
Epoch 675 of 2000 took 0.035s
  training loss:		1.366928
  validation loss:		1.293350
  validation accuracy:		50.76 %
Epoch 676 of 2000 took 0.035s
  training loss:		1.360904
  validation loss:		1.287714
  validation accuracy:		52.07 %
Epoch 677 of 2000 took 0.035s
  training loss:		1.400470
  validation loss:		1.381641
  validation accuracy:		46.52 %
Epoch 678 of 2000 took 0.035s
  training loss:		1.385694
  validation loss:		1.328578
  validation accuracy:		50.00 %
Epoch 679 of 2000 took 0.035s
  training loss:		1.388465
  validation loss:		1.278509
  validation accuracy:		51.41 %
Epoch 680 of 2000 took 0.035s
  training loss:		1.353993
  validation loss:		1.292032
  validation accuracy:		51.20 %
Epoch 681 of 2000 took 0.035s
  training loss:		1.376285
  validation loss:		1.315998
  validation accuracy:		50.11 %
Epoch 682 of 2000 took 0.035s
  training loss:		1.400063
  validation loss:		1.320016
  validation accuracy:		49.57 %
Epoch 683 of 2000 took 0.035s
  training loss:		1.452266
  validation loss:		1.296497
  validation accuracy:		51.30 %
Epoch 684 of 2000 took 0.035s
  training loss:		1.387370
  validation loss:		1.356019
  validation accuracy:		46.63 %
Epoch 685 of 2000 took 0.035s
  training loss:		1.363974
  validation loss:		1.276993
  validation accuracy:		51.30 %
Epoch 686 of 2000 took 0.035s
  training loss:		1.351744
  validation loss:		1.274722
  validation accuracy:		51.41 %
Epoch 687 of 2000 took 0.035s
  training loss:		1.345197
  validation loss:		1.300970
  validation accuracy:		49.67 %
Epoch 688 of 2000 took 0.035s
  training loss:		1.348958
  validation loss:		1.275688
  validation accuracy:		50.98 %
Epoch 689 of 2000 took 0.035s
  training loss:		1.358781
  validation loss:		1.275272
  validation accuracy:		51.09 %
Epoch 690 of 2000 took 0.035s
  training loss:		1.346955
  validation loss:		1.291392
  validation accuracy:		50.22 %
Epoch 691 of 2000 took 0.035s
  training loss:		1.351490
  validation loss:		1.305256
  validation accuracy:		49.67 %
Epoch 692 of 2000 took 0.035s
  training loss:		1.362943
  validation loss:		1.283491
  validation accuracy:		49.78 %
Epoch 693 of 2000 took 0.035s
  training loss:		1.350290
  validation loss:		1.284359
  validation accuracy:		52.61 %
Epoch 694 of 2000 took 0.035s
  training loss:		1.370372
  validation loss:		1.295005
  validation accuracy:		50.00 %
Epoch 695 of 2000 took 0.035s
  training loss:		1.360051
  validation loss:		1.274344
  validation accuracy:		51.41 %
Epoch 696 of 2000 took 0.035s
  training loss:		1.476770
  validation loss:		1.498082
  validation accuracy:		42.50 %
Epoch 697 of 2000 took 0.035s
  training loss:		1.557840
  validation loss:		1.334267
  validation accuracy:		49.35 %
Epoch 698 of 2000 took 0.036s
  training loss:		1.377155
  validation loss:		1.296600
  validation accuracy:		50.22 %
Epoch 699 of 2000 took 0.035s
  training loss:		1.366612
  validation loss:		1.274613
  validation accuracy:		51.41 %
Epoch 700 of 2000 took 0.035s
  training loss:		1.351285
  validation loss:		1.276722
  validation accuracy:		51.41 %
Epoch 701 of 2000 took 0.035s
  training loss:		1.341235
  validation loss:		1.285508
  validation accuracy:		50.43 %
Epoch 702 of 2000 took 0.035s
  training loss:		1.358954
  validation loss:		1.291588
  validation accuracy:		50.43 %
Epoch 703 of 2000 took 0.035s
  training loss:		1.379256
  validation loss:		1.310365
  validation accuracy:		50.33 %
Epoch 704 of 2000 took 0.035s
  training loss:		1.362147
  validation loss:		1.319301
  validation accuracy:		49.57 %
Epoch 705 of 2000 took 0.035s
  training loss:		1.364000
  validation loss:		1.287435
  validation accuracy:		50.76 %
Epoch 706 of 2000 took 0.035s
  training loss:		1.368929
  validation loss:		1.317147
  validation accuracy:		49.78 %
Epoch 707 of 2000 took 0.035s
  training loss:		1.427702
  validation loss:		1.307551
  validation accuracy:		49.02 %
Epoch 708 of 2000 took 0.035s
  training loss:		1.387749
  validation loss:		1.310267
  validation accuracy:		49.24 %
Epoch 709 of 2000 took 0.035s
  training loss:		1.436560
  validation loss:		1.338499
  validation accuracy:		48.26 %
Epoch 710 of 2000 took 0.035s
  training loss:		1.398800
  validation loss:		1.280458
  validation accuracy:		51.96 %
Epoch 711 of 2000 took 0.035s
  training loss:		1.359120
  validation loss:		1.280569
  validation accuracy:		50.87 %
Epoch 712 of 2000 took 0.035s
  training loss:		1.348481
  validation loss:		1.280446
  validation accuracy:		50.43 %
Epoch 713 of 2000 took 0.035s
  training loss:		1.379620
  validation loss:		1.315511
  validation accuracy:		49.46 %
Epoch 714 of 2000 took 0.035s
  training loss:		1.349799
  validation loss:		1.274723
  validation accuracy:		51.52 %
Epoch 715 of 2000 took 0.036s
  training loss:		1.356218
  validation loss:		1.283910
  validation accuracy:		50.43 %
Epoch 716 of 2000 took 0.035s
  training loss:		1.346847
  validation loss:		1.303975
  validation accuracy:		50.65 %
Epoch 717 of 2000 took 0.035s
  training loss:		1.347965
  validation loss:		1.275675
  validation accuracy:		51.63 %
Epoch 718 of 2000 took 0.036s
  training loss:		1.349435
  validation loss:		1.271445
  validation accuracy:		51.96 %
Epoch 719 of 2000 took 0.035s
  training loss:		1.360901
  validation loss:		1.315123
  validation accuracy:		49.46 %
Epoch 720 of 2000 took 0.035s
  training loss:		1.353274
  validation loss:		1.276641
  validation accuracy:		51.52 %
Epoch 721 of 2000 took 0.035s
  training loss:		1.360555
  validation loss:		1.278141
  validation accuracy:		51.74 %
Epoch 722 of 2000 took 0.035s
  training loss:		1.337370
  validation loss:		1.286747
  validation accuracy:		49.46 %
Epoch 723 of 2000 took 0.035s
  training loss:		1.446804
  validation loss:		1.504662
  validation accuracy:		43.37 %
Epoch 724 of 2000 took 0.035s
  training loss:		1.432270
  validation loss:		1.285003
  validation accuracy:		50.33 %
Epoch 725 of 2000 took 0.035s
  training loss:		1.356262
  validation loss:		1.278867
  validation accuracy:		50.00 %
Epoch 726 of 2000 took 0.036s
  training loss:		1.398517
  validation loss:		1.312811
  validation accuracy:		50.33 %
Epoch 727 of 2000 took 0.035s
  training loss:		1.345991
  validation loss:		1.273538
  validation accuracy:		51.20 %
Epoch 728 of 2000 took 0.035s
  training loss:		1.346724
  validation loss:		1.280308
  validation accuracy:		50.54 %
Epoch 729 of 2000 took 0.035s
  training loss:		1.347863
  validation loss:		1.278665
  validation accuracy:		52.50 %
Epoch 730 of 2000 took 0.035s
  training loss:		1.360368
  validation loss:		1.273829
  validation accuracy:		51.85 %
Epoch 731 of 2000 took 0.035s
  training loss:		1.359866
  validation loss:		1.299576
  validation accuracy:		49.57 %
Epoch 732 of 2000 took 0.035s
  training loss:		1.365101
  validation loss:		1.271066
  validation accuracy:		52.83 %
Epoch 733 of 2000 took 0.035s
  training loss:		1.358327
  validation loss:		1.366458
  validation accuracy:		46.52 %
Epoch 734 of 2000 took 0.035s
  training loss:		1.365393
  validation loss:		1.328883
  validation accuracy:		50.00 %
Epoch 735 of 2000 took 0.035s
  training loss:		1.344851
  validation loss:		1.275639
  validation accuracy:		51.09 %
Epoch 736 of 2000 took 0.035s
  training loss:		1.386136
  validation loss:		1.273567
  validation accuracy:		52.17 %
Epoch 737 of 2000 took 0.035s
  training loss:		1.354921
  validation loss:		1.277904
  validation accuracy:		51.96 %
Epoch 738 of 2000 took 0.035s
  training loss:		1.349063
  validation loss:		1.291681
  validation accuracy:		50.87 %
Epoch 739 of 2000 took 0.035s
  training loss:		1.367315
  validation loss:		1.271977
  validation accuracy:		50.98 %
Epoch 740 of 2000 took 0.035s
  training loss:		1.348923
  validation loss:		1.295528
  validation accuracy:		49.89 %
Epoch 741 of 2000 took 0.035s
  training loss:		1.352152
  validation loss:		1.293991
  validation accuracy:		50.98 %
Epoch 742 of 2000 took 0.035s
  training loss:		1.349766
  validation loss:		1.275479
  validation accuracy:		52.17 %
Epoch 743 of 2000 took 0.035s
  training loss:		1.343255
  validation loss:		1.277307
  validation accuracy:		51.63 %
Epoch 744 of 2000 took 0.036s
  training loss:		1.388428
  validation loss:		1.399350
  validation accuracy:		46.74 %
Epoch 745 of 2000 took 0.035s
  training loss:		1.352690
  validation loss:		1.276814
  validation accuracy:		50.43 %
Epoch 746 of 2000 took 0.035s
  training loss:		1.348059
  validation loss:		1.295544
  validation accuracy:		49.67 %
Epoch 747 of 2000 took 0.035s
  training loss:		1.392841
  validation loss:		1.285555
  validation accuracy:		52.50 %
Epoch 748 of 2000 took 0.035s
  training loss:		1.348469
  validation loss:		1.273499
  validation accuracy:		51.20 %
Epoch 749 of 2000 took 0.035s
  training loss:		1.351467
  validation loss:		1.310211
  validation accuracy:		49.67 %
Epoch 750 of 2000 took 0.035s
  training loss:		1.374364
  validation loss:		1.322771
  validation accuracy:		49.78 %
Epoch 751 of 2000 took 0.035s
  training loss:		1.400685
  validation loss:		1.375306
  validation accuracy:		47.39 %
Epoch 752 of 2000 took 0.035s
  training loss:		1.377008
  validation loss:		1.295946
  validation accuracy:		50.54 %
Epoch 753 of 2000 took 0.035s
  training loss:		1.353775
  validation loss:		1.274485
  validation accuracy:		51.85 %
Epoch 754 of 2000 took 0.035s
  training loss:		1.345363
  validation loss:		1.291870
  validation accuracy:		48.91 %
Epoch 755 of 2000 took 0.035s
  training loss:		1.360922
  validation loss:		1.276558
  validation accuracy:		51.63 %
Epoch 756 of 2000 took 0.035s
  training loss:		1.378557
  validation loss:		1.280861
  validation accuracy:		50.76 %
Epoch 757 of 2000 took 0.035s
  training loss:		1.348939
  validation loss:		1.288792
  validation accuracy:		49.78 %
Epoch 758 of 2000 took 0.035s
  training loss:		1.352910
  validation loss:		1.279526
  validation accuracy:		50.11 %
Epoch 759 of 2000 took 0.035s
  training loss:		1.398837
  validation loss:		1.353629
  validation accuracy:		47.93 %
Epoch 760 of 2000 took 0.035s
  training loss:		1.374593
  validation loss:		1.316962
  validation accuracy:		49.89 %
Epoch 761 of 2000 took 0.035s
  training loss:		1.350292
  validation loss:		1.275133
  validation accuracy:		51.41 %
Epoch 762 of 2000 took 0.035s
  training loss:		1.373161
  validation loss:		1.283865
  validation accuracy:		50.00 %
Epoch 763 of 2000 took 0.035s
  training loss:		1.365287
  validation loss:		1.298366
  validation accuracy:		50.65 %
Epoch 764 of 2000 took 0.035s
  training loss:		1.358276
  validation loss:		1.272874
  validation accuracy:		51.52 %
Epoch 765 of 2000 took 0.035s
  training loss:		1.356867
  validation loss:		1.280540
  validation accuracy:		51.41 %
Epoch 766 of 2000 took 0.035s
  training loss:		1.338533
  validation loss:		1.272613
  validation accuracy:		52.83 %
Epoch 767 of 2000 took 0.035s
  training loss:		1.355139
  validation loss:		1.286249
  validation accuracy:		51.20 %
Epoch 768 of 2000 took 0.035s
  training loss:		1.377603
  validation loss:		1.328000
  validation accuracy:		47.93 %
Epoch 769 of 2000 took 0.035s
  training loss:		1.351028
  validation loss:		1.280258
  validation accuracy:		51.20 %
Epoch 770 of 2000 took 0.035s
  training loss:		1.347282
  validation loss:		1.287253
  validation accuracy:		51.09 %
Epoch 771 of 2000 took 0.035s
  training loss:		1.340174
  validation loss:		1.273266
  validation accuracy:		51.20 %
Epoch 772 of 2000 took 0.035s
  training loss:		1.362186
  validation loss:		1.351790
  validation accuracy:		47.28 %
Epoch 773 of 2000 took 0.035s
  training loss:		1.364915
  validation loss:		1.286280
  validation accuracy:		51.09 %
Epoch 774 of 2000 took 0.035s
  training loss:		1.352092
  validation loss:		1.288229
  validation accuracy:		50.33 %
Epoch 775 of 2000 took 0.035s
  training loss:		1.339065
  validation loss:		1.271065
  validation accuracy:		51.20 %
Epoch 776 of 2000 took 0.035s
  training loss:		1.340046
  validation loss:		1.283482
  validation accuracy:		51.30 %
Epoch 777 of 2000 took 0.035s
  training loss:		1.379604
  validation loss:		1.357187
  validation accuracy:		48.59 %
Epoch 778 of 2000 took 0.035s
  training loss:		1.363991
  validation loss:		1.302503
  validation accuracy:		50.43 %
Epoch 779 of 2000 took 0.035s
  training loss:		1.363761
  validation loss:		1.301115
  validation accuracy:		49.67 %
Epoch 780 of 2000 took 0.035s
  training loss:		1.341972
  validation loss:		1.268057
  validation accuracy:		51.85 %
Epoch 781 of 2000 took 0.035s
  training loss:		1.357013
  validation loss:		1.320156
  validation accuracy:		50.22 %
Epoch 782 of 2000 took 0.035s
  training loss:		1.345998
  validation loss:		1.280033
  validation accuracy:		50.76 %
Epoch 783 of 2000 took 0.035s
  training loss:		1.340877
  validation loss:		1.276077
  validation accuracy:		51.85 %
Epoch 784 of 2000 took 0.035s
  training loss:		1.341344
  validation loss:		1.267267
  validation accuracy:		51.85 %
Epoch 785 of 2000 took 0.035s
  training loss:		1.344151
  validation loss:		1.268709
  validation accuracy:		51.63 %
Epoch 786 of 2000 took 0.035s
  training loss:		1.353773
  validation loss:		1.300302
  validation accuracy:		49.57 %
Epoch 787 of 2000 took 0.035s
  training loss:		1.353182
  validation loss:		1.306773
  validation accuracy:		49.46 %
Epoch 788 of 2000 took 0.035s
  training loss:		1.434194
  validation loss:		1.305332
  validation accuracy:		50.43 %
Epoch 789 of 2000 took 0.035s
  training loss:		1.377184
  validation loss:		1.306472
  validation accuracy:		50.00 %
Epoch 790 of 2000 took 0.035s
  training loss:		1.380129
  validation loss:		1.299875
  validation accuracy:		50.43 %
Epoch 791 of 2000 took 0.035s
  training loss:		1.367034
  validation loss:		1.282833
  validation accuracy:		52.17 %
Epoch 792 of 2000 took 0.035s
  training loss:		1.378809
  validation loss:		1.297314
  validation accuracy:		51.09 %
Epoch 793 of 2000 took 0.035s
  training loss:		1.355031
  validation loss:		1.277353
  validation accuracy:		51.20 %
Epoch 794 of 2000 took 0.035s
  training loss:		1.346120
  validation loss:		1.286040
  validation accuracy:		51.52 %
Epoch 795 of 2000 took 0.035s
  training loss:		1.347844
  validation loss:		1.296998
  validation accuracy:		49.24 %
Epoch 796 of 2000 took 0.035s
  training loss:		1.342845
  validation loss:		1.272935
  validation accuracy:		51.09 %
Epoch 797 of 2000 took 0.035s
  training loss:		1.342257
  validation loss:		1.269913
  validation accuracy:		51.30 %
Epoch 798 of 2000 took 0.035s
  training loss:		1.333715
  validation loss:		1.278266
  validation accuracy:		50.98 %
Epoch 799 of 2000 took 0.035s
  training loss:		1.360325
  validation loss:		1.279901
  validation accuracy:		52.07 %
Epoch 800 of 2000 took 0.036s
  training loss:		1.349282
  validation loss:		1.267871
  validation accuracy:		52.07 %
Epoch 801 of 2000 took 0.035s
  training loss:		1.343860
  validation loss:		1.271948
  validation accuracy:		51.30 %
Epoch 802 of 2000 took 0.035s
  training loss:		1.349095
  validation loss:		1.275463
  validation accuracy:		51.96 %
Epoch 803 of 2000 took 0.035s
  training loss:		1.351534
  validation loss:		1.299650
  validation accuracy:		50.76 %
Epoch 804 of 2000 took 0.035s
  training loss:		1.358281
  validation loss:		1.272017
  validation accuracy:		51.20 %
Epoch 805 of 2000 took 0.035s
  training loss:		1.344745
  validation loss:		1.273398
  validation accuracy:		51.09 %
Epoch 806 of 2000 took 0.035s
  training loss:		1.347609
  validation loss:		1.284224
  validation accuracy:		50.22 %
Epoch 807 of 2000 took 0.035s
  training loss:		1.356237
  validation loss:		1.334637
  validation accuracy:		49.24 %
Epoch 808 of 2000 took 0.035s
  training loss:		1.389788
  validation loss:		1.277322
  validation accuracy:		50.33 %
Epoch 809 of 2000 took 0.035s
  training loss:		1.349562
  validation loss:		1.348898
  validation accuracy:		48.37 %
Epoch 810 of 2000 took 0.035s
  training loss:		1.368287
  validation loss:		1.274873
  validation accuracy:		52.28 %
Epoch 811 of 2000 took 0.035s
  training loss:		1.348357
  validation loss:		1.326751
  validation accuracy:		49.46 %
Epoch 812 of 2000 took 0.035s
  training loss:		1.352291
  validation loss:		1.304720
  validation accuracy:		50.65 %
Epoch 813 of 2000 took 0.035s
  training loss:		1.354873
  validation loss:		1.299896
  validation accuracy:		49.46 %
Epoch 814 of 2000 took 0.035s
  training loss:		1.347216
  validation loss:		1.267840
  validation accuracy:		51.30 %
Epoch 815 of 2000 took 0.035s
  training loss:		1.341935
  validation loss:		1.290658
  validation accuracy:		51.09 %
Epoch 816 of 2000 took 0.035s
  training loss:		1.363610
  validation loss:		1.296361
  validation accuracy:		50.54 %
Epoch 817 of 2000 took 0.035s
  training loss:		1.359782
  validation loss:		1.271155
  validation accuracy:		52.28 %
Epoch 818 of 2000 took 0.035s
  training loss:		1.347304
  validation loss:		1.336717
  validation accuracy:		48.59 %
Epoch 819 of 2000 took 0.035s
  training loss:		1.362674
  validation loss:		1.278879
  validation accuracy:		52.07 %
Epoch 820 of 2000 took 0.035s
  training loss:		1.376970
  validation loss:		1.313770
  validation accuracy:		50.11 %
Epoch 821 of 2000 took 0.035s
  training loss:		1.348816
  validation loss:		1.271675
  validation accuracy:		51.20 %
Epoch 822 of 2000 took 0.035s
  training loss:		1.346320
  validation loss:		1.270375
  validation accuracy:		51.63 %
Epoch 823 of 2000 took 0.035s
  training loss:		1.351735
  validation loss:		1.272636
  validation accuracy:		51.63 %
Epoch 824 of 2000 took 0.035s
  training loss:		1.343321
  validation loss:		1.278068
  validation accuracy:		51.09 %
Epoch 825 of 2000 took 0.035s
  training loss:		1.344861
  validation loss:		1.269966
  validation accuracy:		50.98 %
Epoch 826 of 2000 took 0.035s
  training loss:		1.344973
  validation loss:		1.271629
  validation accuracy:		52.17 %
Epoch 827 of 2000 took 0.035s
  training loss:		1.349708
  validation loss:		1.288615
  validation accuracy:		50.65 %
Epoch 828 of 2000 took 0.035s
  training loss:		1.349824
  validation loss:		1.268311
  validation accuracy:		51.20 %
Epoch 829 of 2000 took 0.035s
  training loss:		1.358863
  validation loss:		1.316045
  validation accuracy:		50.43 %
Epoch 830 of 2000 took 0.035s
  training loss:		1.361665
  validation loss:		1.293826
  validation accuracy:		49.89 %
Epoch 831 of 2000 took 0.036s
  training loss:		1.351133
  validation loss:		1.279229
  validation accuracy:		51.20 %
Epoch 832 of 2000 took 0.035s
  training loss:		1.355907
  validation loss:		1.269244
  validation accuracy:		52.83 %
Epoch 833 of 2000 took 0.035s
  training loss:		1.348251
  validation loss:		1.279950
  validation accuracy:		50.76 %
Epoch 834 of 2000 took 0.035s
  training loss:		1.343264
  validation loss:		1.298174
  validation accuracy:		50.43 %
Epoch 835 of 2000 took 0.035s
  training loss:		1.339495
  validation loss:		1.276978
  validation accuracy:		52.28 %
Epoch 836 of 2000 took 0.035s
  training loss:		1.376365
  validation loss:		1.286822
  validation accuracy:		52.07 %
Epoch 837 of 2000 took 0.035s
  training loss:		1.349040
  validation loss:		1.272860
  validation accuracy:		50.33 %
Epoch 838 of 2000 took 0.035s
  training loss:		1.352381
  validation loss:		1.287061
  validation accuracy:		50.54 %
Epoch 839 of 2000 took 0.036s
  training loss:		1.348087
  validation loss:		1.270202
  validation accuracy:		51.63 %
Epoch 840 of 2000 took 0.035s
  training loss:		1.348421
  validation loss:		1.301881
  validation accuracy:		50.43 %
Epoch 841 of 2000 took 0.035s
  training loss:		1.354500
  validation loss:		1.307871
  validation accuracy:		49.78 %
Epoch 842 of 2000 took 0.035s
  training loss:		1.354717
  validation loss:		1.275248
  validation accuracy:		51.41 %
Epoch 843 of 2000 took 0.035s
  training loss:		1.351215
  validation loss:		1.281080
  validation accuracy:		51.20 %
Epoch 844 of 2000 took 0.035s
  training loss:		1.348145
  validation loss:		1.293276
  validation accuracy:		51.09 %
Epoch 845 of 2000 took 0.035s
  training loss:		1.366310
  validation loss:		1.334290
  validation accuracy:		49.57 %
Epoch 846 of 2000 took 0.035s
  training loss:		1.348188
  validation loss:		1.313474
  validation accuracy:		50.00 %
Epoch 847 of 2000 took 0.035s
  training loss:		1.347776
  validation loss:		1.272552
  validation accuracy:		51.96 %
Epoch 848 of 2000 took 0.036s
  training loss:		1.359651
  validation loss:		1.275338
  validation accuracy:		51.52 %
Epoch 849 of 2000 took 0.035s
  training loss:		1.344754
  validation loss:		1.287329
  validation accuracy:		50.87 %
Epoch 850 of 2000 took 0.035s
  training loss:		1.340521
  validation loss:		1.277064
  validation accuracy:		51.74 %
Epoch 851 of 2000 took 0.035s
  training loss:		1.346720
  validation loss:		1.307470
  validation accuracy:		49.57 %
Epoch 852 of 2000 took 0.035s
  training loss:		1.356332
  validation loss:		1.282099
  validation accuracy:		51.63 %
Epoch 853 of 2000 took 0.035s
  training loss:		1.352432
  validation loss:		1.282444
  validation accuracy:		51.96 %
Epoch 854 of 2000 took 0.035s
  training loss:		1.351535
  validation loss:		1.318401
  validation accuracy:		48.48 %
Epoch 855 of 2000 took 0.035s
  training loss:		1.361851
  validation loss:		1.293838
  validation accuracy:		50.87 %
Epoch 856 of 2000 took 0.035s
  training loss:		1.340081
  validation loss:		1.284991
  validation accuracy:		50.76 %
Epoch 857 of 2000 took 0.036s
  training loss:		1.360226
  validation loss:		1.268883
  validation accuracy:		52.72 %
Epoch 858 of 2000 took 0.035s
  training loss:		1.346831
  validation loss:		1.274209
  validation accuracy:		51.41 %
Epoch 859 of 2000 took 0.035s
  training loss:		1.337691
  validation loss:		1.275709
  validation accuracy:		50.98 %
Epoch 860 of 2000 took 0.035s
  training loss:		1.344897
  validation loss:		1.320103
  validation accuracy:		50.22 %
Epoch 861 of 2000 took 0.035s
  training loss:		1.363448
  validation loss:		1.274915
  validation accuracy:		51.52 %
Epoch 862 of 2000 took 0.035s
  training loss:		1.349027
  validation loss:		1.269202
  validation accuracy:		51.74 %
Epoch 863 of 2000 took 0.035s
  training loss:		1.348075
  validation loss:		1.332462
  validation accuracy:		49.67 %
Epoch 864 of 2000 took 0.035s
  training loss:		1.355380
  validation loss:		1.283275
  validation accuracy:		50.98 %
Epoch 865 of 2000 took 0.035s
  training loss:		1.346761
  validation loss:		1.267954
  validation accuracy:		52.17 %
Epoch 866 of 2000 took 0.035s
  training loss:		1.344500
  validation loss:		1.268014
  validation accuracy:		51.20 %
Epoch 867 of 2000 took 0.035s
  training loss:		1.349323
  validation loss:		1.303191
  validation accuracy:		49.89 %
Epoch 868 of 2000 took 0.036s
  training loss:		1.358029
  validation loss:		1.283592
  validation accuracy:		50.76 %
Epoch 869 of 2000 took 0.035s
  training loss:		1.349865
  validation loss:		1.280101
  validation accuracy:		51.41 %
Epoch 870 of 2000 took 0.035s
  training loss:		1.350147
  validation loss:		1.273364
  validation accuracy:		52.50 %
Epoch 871 of 2000 took 0.035s
  training loss:		1.350715
  validation loss:		1.287263
  validation accuracy:		50.76 %
Epoch 872 of 2000 took 0.035s
  training loss:		1.353356
  validation loss:		1.277116
  validation accuracy:		50.98 %
Epoch 873 of 2000 took 0.035s
  training loss:		1.349159
  validation loss:		1.275662
  validation accuracy:		51.09 %
Epoch 874 of 2000 took 0.035s
  training loss:		1.352278
  validation loss:		1.272199
  validation accuracy:		52.28 %
Epoch 875 of 2000 took 0.035s
  training loss:		1.340917
  validation loss:		1.277442
  validation accuracy:		52.17 %
Epoch 876 of 2000 took 0.035s
  training loss:		1.337841
  validation loss:		1.269841
  validation accuracy:		52.17 %
Epoch 877 of 2000 took 0.035s
  training loss:		1.351924
  validation loss:		1.277781
  validation accuracy:		51.09 %
Epoch 878 of 2000 took 0.035s
  training loss:		1.355209
  validation loss:		1.277116
  validation accuracy:		51.52 %
Epoch 879 of 2000 took 0.035s
  training loss:		1.346532
  validation loss:		1.276026
  validation accuracy:		51.41 %
Epoch 880 of 2000 took 0.035s
  training loss:		1.371860
  validation loss:		1.271223
  validation accuracy:		51.52 %
Epoch 881 of 2000 took 0.035s
  training loss:		1.346216
  validation loss:		1.274149
  validation accuracy:		51.63 %
Epoch 882 of 2000 took 0.035s
  training loss:		1.356757
  validation loss:		1.290333
  validation accuracy:		51.30 %
Epoch 883 of 2000 took 0.035s
  training loss:		1.386336
  validation loss:		1.278365
  validation accuracy:		51.20 %
Epoch 884 of 2000 took 0.035s
  training loss:		1.347662
  validation loss:		1.286512
  validation accuracy:		51.63 %
Epoch 885 of 2000 took 0.035s
  training loss:		1.348487
  validation loss:		1.270643
  validation accuracy:		51.41 %
Epoch 886 of 2000 took 0.035s
  training loss:		1.337874
  validation loss:		1.280503
  validation accuracy:		51.30 %
Epoch 887 of 2000 took 0.037s
  training loss:		1.343723
  validation loss:		1.277745
  validation accuracy:		51.63 %
Epoch 888 of 2000 took 0.036s
  training loss:		1.348899
  validation loss:		1.266540
  validation accuracy:		52.72 %
Epoch 889 of 2000 took 0.035s
  training loss:		1.337990
  validation loss:		1.274437
  validation accuracy:		50.22 %
Epoch 890 of 2000 took 0.035s
  training loss:		1.347660
  validation loss:		1.274226
  validation accuracy:		51.63 %
Epoch 891 of 2000 took 0.035s
  training loss:		1.360708
  validation loss:		1.270382
  validation accuracy:		52.28 %
Epoch 892 of 2000 took 0.035s
  training loss:		1.363251
  validation loss:		1.272509
  validation accuracy:		51.74 %
Epoch 893 of 2000 took 0.035s
  training loss:		1.351435
  validation loss:		1.297206
  validation accuracy:		50.98 %
Epoch 894 of 2000 took 0.035s
  training loss:		1.357451
  validation loss:		1.275332
  validation accuracy:		53.70 %
Epoch 895 of 2000 took 0.035s
  training loss:		1.350809
  validation loss:		1.270457
  validation accuracy:		51.52 %
Epoch 896 of 2000 took 0.036s
  training loss:		1.347934
  validation loss:		1.275157
  validation accuracy:		51.30 %
Epoch 897 of 2000 took 0.035s
  training loss:		1.339978
  validation loss:		1.285986
  validation accuracy:		52.50 %
Epoch 898 of 2000 took 0.035s
  training loss:		1.351089
  validation loss:		1.276457
  validation accuracy:		50.87 %
Epoch 899 of 2000 took 0.035s
  training loss:		1.345787
  validation loss:		1.273168
  validation accuracy:		51.52 %
Epoch 900 of 2000 took 0.035s
  training loss:		1.338439
  validation loss:		1.266864
  validation accuracy:		51.74 %
Epoch 901 of 2000 took 0.035s
  training loss:		1.349359
  validation loss:		1.280648
  validation accuracy:		50.87 %
Epoch 902 of 2000 took 0.035s
  training loss:		1.354410
  validation loss:		1.289439
  validation accuracy:		50.98 %
Epoch 903 of 2000 took 0.035s
  training loss:		1.358332
  validation loss:		1.312917
  validation accuracy:		50.00 %
Epoch 904 of 2000 took 0.035s
  training loss:		1.349601
  validation loss:		1.278468
  validation accuracy:		51.20 %
Epoch 905 of 2000 took 0.035s
  training loss:		1.354380
  validation loss:		1.288020
  validation accuracy:		50.76 %
Epoch 906 of 2000 took 0.035s
  training loss:		1.343891
  validation loss:		1.300165
  validation accuracy:		51.20 %
Epoch 907 of 2000 took 0.035s
  training loss:		1.351926
  validation loss:		1.296146
  validation accuracy:		51.52 %
Epoch 908 of 2000 took 0.035s
  training loss:		1.349794
  validation loss:		1.279935
  validation accuracy:		51.96 %
Epoch 909 of 2000 took 0.035s
  training loss:		1.353501
  validation loss:		1.283292
  validation accuracy:		51.30 %
Epoch 910 of 2000 took 0.035s
  training loss:		1.378397
  validation loss:		1.336852
  validation accuracy:		49.02 %
Epoch 911 of 2000 took 0.035s
  training loss:		1.347688
  validation loss:		1.286232
  validation accuracy:		51.52 %
Epoch 912 of 2000 took 0.035s
  training loss:		1.343000
  validation loss:		1.269518
  validation accuracy:		51.30 %
Epoch 913 of 2000 took 0.036s
  training loss:		1.348939
  validation loss:		1.278563
  validation accuracy:		51.85 %
Epoch 914 of 2000 took 0.035s
  training loss:		1.345141
  validation loss:		1.293023
  validation accuracy:		51.41 %
Epoch 915 of 2000 took 0.035s
  training loss:		1.353515
  validation loss:		1.274376
  validation accuracy:		52.17 %
Epoch 916 of 2000 took 0.035s
  training loss:		1.368558
  validation loss:		1.313380
  validation accuracy:		50.76 %
Epoch 917 of 2000 took 0.035s
  training loss:		1.360562
  validation loss:		1.281520
  validation accuracy:		50.87 %
Epoch 918 of 2000 took 0.035s
  training loss:		1.340426
  validation loss:		1.275649
  validation accuracy:		51.96 %
Epoch 919 of 2000 took 0.035s
  training loss:		1.369529
  validation loss:		1.288641
  validation accuracy:		50.76 %
Epoch 920 of 2000 took 0.035s
  training loss:		1.356179
  validation loss:		1.276742
  validation accuracy:		51.41 %
Epoch 921 of 2000 took 0.035s
  training loss:		1.351045
  validation loss:		1.303312
  validation accuracy:		49.78 %
Epoch 922 of 2000 took 0.035s
  training loss:		1.349561
  validation loss:		1.281329
  validation accuracy:		51.20 %
Epoch 923 of 2000 took 0.035s
  training loss:		1.354135
  validation loss:		1.277222
  validation accuracy:		52.28 %
Epoch 924 of 2000 took 0.035s
  training loss:		1.341150
  validation loss:		1.294029
  validation accuracy:		51.09 %
Epoch 925 of 2000 took 0.035s
  training loss:		1.356109
  validation loss:		1.297367
  validation accuracy:		50.11 %
Epoch 926 of 2000 took 0.035s
  training loss:		1.368790
  validation loss:		1.308917
  validation accuracy:		50.54 %
Epoch 927 of 2000 took 0.035s
  training loss:		1.365302
  validation loss:		1.299415
  validation accuracy:		50.33 %
Epoch 928 of 2000 took 0.035s
  training loss:		1.348853
  validation loss:		1.276436
  validation accuracy:		51.52 %
Epoch 929 of 2000 took 0.035s
  training loss:		1.340375
  validation loss:		1.303884
  validation accuracy:		50.43 %
Epoch 930 of 2000 took 0.035s
  training loss:		1.349258
  validation loss:		1.279106
  validation accuracy:		51.74 %
Epoch 931 of 2000 took 0.035s
  training loss:		1.347852
  validation loss:		1.274066
  validation accuracy:		52.07 %
Epoch 932 of 2000 took 0.035s
  training loss:		1.344589
  validation loss:		1.274842
  validation accuracy:		51.52 %
Epoch 933 of 2000 took 0.035s
  training loss:		1.342879
  validation loss:		1.272450
  validation accuracy:		51.63 %
Epoch 934 of 2000 took 0.035s
  training loss:		1.349717
  validation loss:		1.279485
  validation accuracy:		50.65 %
Epoch 935 of 2000 took 0.035s
  training loss:		1.353061
  validation loss:		1.265799
  validation accuracy:		52.28 %
Epoch 936 of 2000 took 0.035s
  training loss:		1.336677
  validation loss:		1.274240
  validation accuracy:		51.30 %
Epoch 937 of 2000 took 0.035s
  training loss:		1.359693
  validation loss:		1.313261
  validation accuracy:		49.24 %
Epoch 938 of 2000 took 0.035s
  training loss:		1.347746
  validation loss:		1.281932
  validation accuracy:		50.65 %
Epoch 939 of 2000 took 0.035s
  training loss:		1.344144
  validation loss:		1.270370
  validation accuracy:		51.41 %
Epoch 940 of 2000 took 0.035s
  training loss:		1.353034
  validation loss:		1.274105
  validation accuracy:		51.96 %
Epoch 941 of 2000 took 0.035s
  training loss:		1.348840
  validation loss:		1.273894
  validation accuracy:		50.43 %
Epoch 942 of 2000 took 0.035s
  training loss:		1.343369
  validation loss:		1.275615
  validation accuracy:		52.39 %
Epoch 943 of 2000 took 0.035s
  training loss:		1.362760
  validation loss:		1.300523
  validation accuracy:		51.30 %
Epoch 944 of 2000 took 0.036s
  training loss:		1.346704
  validation loss:		1.295058
  validation accuracy:		51.41 %
Epoch 945 of 2000 took 0.035s
  training loss:		1.346632
  validation loss:		1.326134
  validation accuracy:		49.13 %
Epoch 946 of 2000 took 0.035s
  training loss:		1.352971
  validation loss:		1.292373
  validation accuracy:		51.09 %
Epoch 947 of 2000 took 0.035s
  training loss:		1.352075
  validation loss:		1.271515
  validation accuracy:		51.74 %
Epoch 948 of 2000 took 0.035s
  training loss:		1.344828
  validation loss:		1.279590
  validation accuracy:		50.87 %
Epoch 949 of 2000 took 0.035s
  training loss:		1.347900
  validation loss:		1.280148
  validation accuracy:		51.20 %
Epoch 950 of 2000 took 0.035s
  training loss:		1.353122
  validation loss:		1.279908
  validation accuracy:		51.63 %
Epoch 951 of 2000 took 0.035s
  training loss:		1.371315
  validation loss:		1.274387
  validation accuracy:		51.96 %
Epoch 952 of 2000 took 0.035s
  training loss:		1.350102
  validation loss:		1.270995
  validation accuracy:		50.98 %
Epoch 953 of 2000 took 0.035s
  training loss:		1.346195
  validation loss:		1.272204
  validation accuracy:		51.30 %
Epoch 954 of 2000 took 0.035s
  training loss:		1.350062
  validation loss:		1.278774
  validation accuracy:		51.30 %
Epoch 955 of 2000 took 0.035s
  training loss:		1.343409
  validation loss:		1.275329
  validation accuracy:		51.09 %
Epoch 956 of 2000 took 0.035s
  training loss:		1.351855
  validation loss:		1.286586
  validation accuracy:		52.39 %
Epoch 957 of 2000 took 0.035s
  training loss:		1.353784
  validation loss:		1.271699
  validation accuracy:		50.98 %
Epoch 958 of 2000 took 0.035s
  training loss:		1.356711
  validation loss:		1.273021
  validation accuracy:		51.96 %
Epoch 959 of 2000 took 0.035s
  training loss:		1.353839
  validation loss:		1.275318
  validation accuracy:		52.72 %
Epoch 960 of 2000 took 0.035s
  training loss:		1.344888
  validation loss:		1.275906
  validation accuracy:		51.41 %
Epoch 961 of 2000 took 0.035s
  training loss:		1.357559
  validation loss:		1.278783
  validation accuracy:		51.20 %
Epoch 962 of 2000 took 0.035s
  training loss:		1.337896
  validation loss:		1.267876
  validation accuracy:		51.74 %
Epoch 963 of 2000 took 0.035s
  training loss:		1.340702
  validation loss:		1.271821
  validation accuracy:		51.30 %
Epoch 964 of 2000 took 0.035s
  training loss:		1.336009
  validation loss:		1.273037
  validation accuracy:		52.28 %
Epoch 965 of 2000 took 0.035s
  training loss:		1.344332
  validation loss:		1.291652
  validation accuracy:		51.63 %
Epoch 966 of 2000 took 0.035s
  training loss:		1.346364
  validation loss:		1.277238
  validation accuracy:		51.85 %
Epoch 967 of 2000 took 0.035s
  training loss:		1.344146
  validation loss:		1.273680
  validation accuracy:		50.65 %
Epoch 968 of 2000 took 0.036s
  training loss:		1.338290
  validation loss:		1.296579
  validation accuracy:		51.74 %
Epoch 969 of 2000 took 0.035s
  training loss:		1.353730
  validation loss:		1.315865
  validation accuracy:		50.00 %
Epoch 970 of 2000 took 0.035s
  training loss:		1.350917
  validation loss:		1.275482
  validation accuracy:		51.96 %
Epoch 971 of 2000 took 0.035s
  training loss:		1.345187
  validation loss:		1.270958
  validation accuracy:		50.00 %
Epoch 972 of 2000 took 0.035s
  training loss:		1.353329
  validation loss:		1.292960
  validation accuracy:		51.63 %
Epoch 973 of 2000 took 0.035s
  training loss:		1.355003
  validation loss:		1.287385
  validation accuracy:		51.20 %
Epoch 974 of 2000 took 0.035s
  training loss:		1.351044
  validation loss:		1.288707
  validation accuracy:		51.30 %
Epoch 975 of 2000 took 0.035s
  training loss:		1.358806
  validation loss:		1.286845
  validation accuracy:		51.52 %
Epoch 976 of 2000 took 0.035s
  training loss:		1.346345
  validation loss:		1.273285
  validation accuracy:		51.09 %
Epoch 977 of 2000 took 0.035s
  training loss:		1.357832
  validation loss:		1.321340
  validation accuracy:		49.24 %
Epoch 978 of 2000 took 0.035s
  training loss:		1.369631
  validation loss:		1.275410
  validation accuracy:		51.63 %
Epoch 979 of 2000 took 0.035s
  training loss:		1.345543
  validation loss:		1.270689
  validation accuracy:		51.63 %
Epoch 980 of 2000 took 0.035s
  training loss:		1.362635
  validation loss:		1.283779
  validation accuracy:		52.28 %
Epoch 981 of 2000 took 0.035s
  training loss:		1.345907
  validation loss:		1.272251
  validation accuracy:		51.09 %
Epoch 982 of 2000 took 0.035s
  training loss:		1.344914
  validation loss:		1.290413
  validation accuracy:		50.22 %
Epoch 983 of 2000 took 0.035s
  training loss:		1.345197
  validation loss:		1.270283
  validation accuracy:		51.41 %
Epoch 984 of 2000 took 0.035s
  training loss:		1.345730
  validation loss:		1.273141
  validation accuracy:		53.15 %
Epoch 985 of 2000 took 0.035s
  training loss:		1.333261
  validation loss:		1.276523
  validation accuracy:		51.96 %
Epoch 986 of 2000 took 0.035s
  training loss:		1.343008
  validation loss:		1.302991
  validation accuracy:		51.09 %
Epoch 987 of 2000 took 0.035s
  training loss:		1.352693
  validation loss:		1.272304
  validation accuracy:		51.52 %
Epoch 988 of 2000 took 0.035s
  training loss:		1.386030
  validation loss:		1.274912
  validation accuracy:		51.20 %
Epoch 989 of 2000 took 0.035s
  training loss:		1.346781
  validation loss:		1.292477
  validation accuracy:		51.41 %
Epoch 990 of 2000 took 0.035s
  training loss:		1.346975
  validation loss:		1.283943
  validation accuracy:		51.63 %
Epoch 991 of 2000 took 0.035s
  training loss:		1.337654
  validation loss:		1.271342
  validation accuracy:		51.30 %
Epoch 992 of 2000 took 0.035s
  training loss:		1.362264
  validation loss:		1.295827
  validation accuracy:		51.09 %
Epoch 993 of 2000 took 0.035s
  training loss:		1.368509
  validation loss:		1.272390
  validation accuracy:		51.41 %
Epoch 994 of 2000 took 0.035s
  training loss:		1.347975
  validation loss:		1.271677
  validation accuracy:		50.98 %
Epoch 995 of 2000 took 0.035s
  training loss:		1.339079
  validation loss:		1.267352
  validation accuracy:		51.63 %
Epoch 996 of 2000 took 0.035s
  training loss:		1.341594
  validation loss:		1.297446
  validation accuracy:		49.78 %
Epoch 997 of 2000 took 0.035s
  training loss:		1.343942
  validation loss:		1.280015
  validation accuracy:		50.98 %
Epoch 998 of 2000 took 0.036s
  training loss:		1.362666
  validation loss:		1.280366
  validation accuracy:		52.72 %
Epoch 999 of 2000 took 0.036s
  training loss:		1.346657
  validation loss:		1.276054
  validation accuracy:		52.17 %
Epoch 1000 of 2000 took 0.035s
  training loss:		1.348385
  validation loss:		1.271262
  validation accuracy:		51.41 %
Epoch 1001 of 2000 took 0.036s
  training loss:		1.341323
  validation loss:		1.279179
  validation accuracy:		50.11 %
Epoch 1002 of 2000 took 0.035s
  training loss:		1.348735
  validation loss:		1.269450
  validation accuracy:		51.74 %
Epoch 1003 of 2000 took 0.035s
  training loss:		1.343578
  validation loss:		1.296362
  validation accuracy:		50.11 %
Epoch 1004 of 2000 took 0.035s
  training loss:		1.362658
  validation loss:		1.278189
  validation accuracy:		51.30 %
Epoch 1005 of 2000 took 0.035s
  training loss:		1.337425
  validation loss:		1.269628
  validation accuracy:		51.41 %
Epoch 1006 of 2000 took 0.035s
  training loss:		1.343689
  validation loss:		1.266661
  validation accuracy:		51.74 %
Epoch 1007 of 2000 took 0.035s
  training loss:		1.342318
  validation loss:		1.279501
  validation accuracy:		50.11 %
Epoch 1008 of 2000 took 0.035s
  training loss:		1.344217
  validation loss:		1.266125
  validation accuracy:		52.17 %
Epoch 1009 of 2000 took 0.035s
  training loss:		1.336808
  validation loss:		1.285734
  validation accuracy:		50.76 %
Epoch 1010 of 2000 took 0.035s
  training loss:		1.341610
  validation loss:		1.274114
  validation accuracy:		52.83 %
Epoch 1011 of 2000 took 0.035s
  training loss:		1.337315
  validation loss:		1.274297
  validation accuracy:		51.09 %
Epoch 1012 of 2000 took 0.035s
  training loss:		1.347048
  validation loss:		1.268369
  validation accuracy:		51.52 %
Epoch 1013 of 2000 took 0.035s
  training loss:		1.351662
  validation loss:		1.288351
  validation accuracy:		50.87 %
Epoch 1014 of 2000 took 0.035s
  training loss:		1.353509
  validation loss:		1.278988
  validation accuracy:		51.85 %
Epoch 1015 of 2000 took 0.035s
  training loss:		1.347850
  validation loss:		1.284825
  validation accuracy:		50.43 %
Epoch 1016 of 2000 took 0.035s
  training loss:		1.345291
  validation loss:		1.274513
  validation accuracy:		51.20 %
Epoch 1017 of 2000 took 0.035s
  training loss:		1.337621
  validation loss:		1.288100
  validation accuracy:		50.54 %
Epoch 1018 of 2000 took 0.035s
  training loss:		1.345403
  validation loss:		1.277128
  validation accuracy:		50.43 %
Epoch 1019 of 2000 took 0.035s
  training loss:		1.354949
  validation loss:		1.277026
  validation accuracy:		52.72 %
Epoch 1020 of 2000 took 0.035s
  training loss:		1.348419
  validation loss:		1.271629
  validation accuracy:		52.28 %
Epoch 1021 of 2000 took 0.035s
  training loss:		1.345930
  validation loss:		1.334369
  validation accuracy:		49.57 %
Epoch 1022 of 2000 took 0.035s
  training loss:		1.346092
  validation loss:		1.282035
  validation accuracy:		50.76 %
Epoch 1023 of 2000 took 0.035s
  training loss:		1.345513
  validation loss:		1.275742
  validation accuracy:		51.85 %
Epoch 1024 of 2000 took 0.035s
  training loss:		1.344692
  validation loss:		1.268970
  validation accuracy:		51.96 %
Epoch 1025 of 2000 took 0.035s
  training loss:		1.346135
  validation loss:		1.277709
  validation accuracy:		51.63 %
Epoch 1026 of 2000 took 0.036s
  training loss:		1.344095
  validation loss:		1.280834
  validation accuracy:		51.20 %
Epoch 1027 of 2000 took 0.035s
  training loss:		1.337254
  validation loss:		1.302753
  validation accuracy:		51.41 %
Epoch 1028 of 2000 took 0.035s
  training loss:		1.346099
  validation loss:		1.275593
  validation accuracy:		50.98 %
Epoch 1029 of 2000 took 0.035s
  training loss:		1.340690
  validation loss:		1.270799
  validation accuracy:		51.85 %
Epoch 1030 of 2000 took 0.035s
  training loss:		1.352056
  validation loss:		1.269725
  validation accuracy:		51.63 %
Epoch 1031 of 2000 took 0.035s
  training loss:		1.336108
  validation loss:		1.270606
  validation accuracy:		51.41 %
Epoch 1032 of 2000 took 0.035s
  training loss:		1.349556
  validation loss:		1.283553
  validation accuracy:		51.30 %
Epoch 1033 of 2000 took 0.035s
  training loss:		1.344265
  validation loss:		1.288061
  validation accuracy:		51.52 %
Epoch 1034 of 2000 took 0.035s
  training loss:		1.339206
  validation loss:		1.289153
  validation accuracy:		52.28 %
Epoch 1035 of 2000 took 0.035s
  training loss:		1.345190
  validation loss:		1.271200
  validation accuracy:		51.30 %
Epoch 1036 of 2000 took 0.035s
  training loss:		1.338126
  validation loss:		1.264076
  validation accuracy:		51.41 %
Epoch 1037 of 2000 took 0.035s
  training loss:		1.330756
  validation loss:		1.273432
  validation accuracy:		51.85 %
Epoch 1038 of 2000 took 0.035s
  training loss:		1.342693
  validation loss:		1.281435
  validation accuracy:		50.87 %
Epoch 1039 of 2000 took 0.035s
  training loss:		1.344228
  validation loss:		1.271716
  validation accuracy:		51.63 %
Epoch 1040 of 2000 took 0.035s
  training loss:		1.337771
  validation loss:		1.274611
  validation accuracy:		51.85 %
Epoch 1041 of 2000 took 0.035s
  training loss:		1.342804
  validation loss:		1.270682
  validation accuracy:		51.74 %
Epoch 1042 of 2000 took 0.035s
  training loss:		1.346081
  validation loss:		1.273402
  validation accuracy:		50.54 %
Epoch 1043 of 2000 took 0.035s
  training loss:		1.339173
  validation loss:		1.273013
  validation accuracy:		52.07 %
Epoch 1044 of 2000 took 0.035s
  training loss:		1.340934
  validation loss:		1.271660
  validation accuracy:		52.28 %
Epoch 1045 of 2000 took 0.035s
  training loss:		1.343144
  validation loss:		1.271138
  validation accuracy:		51.20 %
Epoch 1046 of 2000 took 0.035s
  training loss:		1.334421
  validation loss:		1.268626
  validation accuracy:		50.98 %
Epoch 1047 of 2000 took 0.035s
  training loss:		1.340291
  validation loss:		1.333642
  validation accuracy:		49.46 %
Epoch 1048 of 2000 took 0.035s
  training loss:		1.348822
  validation loss:		1.277095
  validation accuracy:		52.17 %
Epoch 1049 of 2000 took 0.035s
  training loss:		1.358796
  validation loss:		1.296160
  validation accuracy:		50.22 %
Epoch 1050 of 2000 took 0.035s
  training loss:		1.332013
  validation loss:		1.287307
  validation accuracy:		51.85 %
Epoch 1051 of 2000 took 0.035s
  training loss:		1.339998
  validation loss:		1.273661
  validation accuracy:		52.50 %
Epoch 1052 of 2000 took 0.035s
  training loss:		1.345232
  validation loss:		1.276619
  validation accuracy:		50.98 %
Epoch 1053 of 2000 took 0.035s
  training loss:		1.347757
  validation loss:		1.270335
  validation accuracy:		52.17 %
Epoch 1054 of 2000 took 0.036s
  training loss:		1.371363
  validation loss:		1.369819
  validation accuracy:		48.37 %
Epoch 1055 of 2000 took 0.035s
  training loss:		1.350848
  validation loss:		1.280217
  validation accuracy:		51.85 %
Epoch 1056 of 2000 took 0.035s
  training loss:		1.341633
  validation loss:		1.270497
  validation accuracy:		50.65 %
Epoch 1057 of 2000 took 0.036s
  training loss:		1.342021
  validation loss:		1.271125
  validation accuracy:		51.52 %
Epoch 1058 of 2000 took 0.035s
  training loss:		1.345077
  validation loss:		1.279229
  validation accuracy:		52.39 %
Epoch 1059 of 2000 took 0.035s
  training loss:		1.343777
  validation loss:		1.279076
  validation accuracy:		50.98 %
Epoch 1060 of 2000 took 0.035s
  training loss:		1.341477
  validation loss:		1.291813
  validation accuracy:		50.76 %
Epoch 1061 of 2000 took 0.035s
  training loss:		1.338189
  validation loss:		1.283554
  validation accuracy:		50.98 %
Epoch 1062 of 2000 took 0.035s
  training loss:		1.340324
  validation loss:		1.290749
  validation accuracy:		52.17 %
Epoch 1063 of 2000 took 0.035s
  training loss:		1.339088
  validation loss:		1.270955
  validation accuracy:		51.41 %
Epoch 1064 of 2000 took 0.035s
  training loss:		1.339111
  validation loss:		1.269220
  validation accuracy:		52.07 %
Epoch 1065 of 2000 took 0.035s
  training loss:		1.345737
  validation loss:		1.265222
  validation accuracy:		52.17 %
Epoch 1066 of 2000 took 0.035s
  training loss:		1.339795
  validation loss:		1.273726
  validation accuracy:		51.09 %
Epoch 1067 of 2000 took 0.035s
  training loss:		1.341872
  validation loss:		1.278162
  validation accuracy:		51.52 %
Epoch 1068 of 2000 took 0.035s
  training loss:		1.343301
  validation loss:		1.281145
  validation accuracy:		51.74 %
Epoch 1069 of 2000 took 0.035s
  training loss:		1.339047
  validation loss:		1.267515
  validation accuracy:		50.65 %
Epoch 1070 of 2000 took 0.035s
  training loss:		1.336371
  validation loss:		1.266128
  validation accuracy:		50.98 %
Epoch 1071 of 2000 took 0.036s
  training loss:		1.341286
  validation loss:		1.280790
  validation accuracy:		51.63 %
Epoch 1072 of 2000 took 0.035s
  training loss:		1.336256
  validation loss:		1.264643
  validation accuracy:		52.39 %
Epoch 1073 of 2000 took 0.035s
  training loss:		1.336106
  validation loss:		1.288257
  validation accuracy:		51.63 %
Epoch 1074 of 2000 took 0.035s
  training loss:		1.349423
  validation loss:		1.265222
  validation accuracy:		52.28 %
Epoch 1075 of 2000 took 0.036s
  training loss:		1.339686
  validation loss:		1.271926
  validation accuracy:		51.63 %
Epoch 1076 of 2000 took 0.035s
  training loss:		1.339896
  validation loss:		1.273694
  validation accuracy:		51.09 %
Epoch 1077 of 2000 took 0.035s
  training loss:		1.335679
  validation loss:		1.265878
  validation accuracy:		52.17 %
Epoch 1078 of 2000 took 0.035s
  training loss:		1.343292
  validation loss:		1.276729
  validation accuracy:		50.54 %
Epoch 1079 of 2000 took 0.035s
  training loss:		1.341722
  validation loss:		1.270055
  validation accuracy:		51.52 %
Epoch 1080 of 2000 took 0.035s
  training loss:		1.343643
  validation loss:		1.269427
  validation accuracy:		52.61 %
Epoch 1081 of 2000 took 0.035s
  training loss:		1.345116
  validation loss:		1.269878
  validation accuracy:		51.96 %
Epoch 1082 of 2000 took 0.035s
  training loss:		1.339985
  validation loss:		1.265415
  validation accuracy:		51.52 %
Epoch 1083 of 2000 took 0.036s
  training loss:		1.335670
  validation loss:		1.274413
  validation accuracy:		51.30 %
Epoch 1084 of 2000 took 0.035s
  training loss:		1.340058
  validation loss:		1.277650
  validation accuracy:		51.30 %
Epoch 1085 of 2000 took 0.035s
  training loss:		1.347433
  validation loss:		1.277145
  validation accuracy:		51.52 %
Epoch 1086 of 2000 took 0.035s
  training loss:		1.337702
  validation loss:		1.261663
  validation accuracy:		51.52 %
Epoch 1087 of 2000 took 0.035s
  training loss:		1.339296
  validation loss:		1.273329
  validation accuracy:		51.52 %
Epoch 1088 of 2000 took 0.035s
  training loss:		1.330682
  validation loss:		1.263814
  validation accuracy:		51.85 %
Epoch 1089 of 2000 took 0.035s
  training loss:		1.342538
  validation loss:		1.262702
  validation accuracy:		52.17 %
Epoch 1090 of 2000 took 0.035s
  training loss:		1.332395
  validation loss:		1.269209
  validation accuracy:		51.74 %
Epoch 1091 of 2000 took 0.035s
  training loss:		1.342728
  validation loss:		1.262847
  validation accuracy:		51.85 %
Epoch 1092 of 2000 took 0.035s
  training loss:		1.335202
  validation loss:		1.273533
  validation accuracy:		51.09 %
Epoch 1093 of 2000 took 0.035s
  training loss:		1.334883
  validation loss:		1.282430
  validation accuracy:		51.30 %
Epoch 1094 of 2000 took 0.036s
  training loss:		1.329532
  validation loss:		1.270187
  validation accuracy:		53.59 %
Epoch 1095 of 2000 took 0.035s
  training loss:		1.338525
  validation loss:		1.271006
  validation accuracy:		51.20 %
Epoch 1096 of 2000 took 0.035s
  training loss:		1.345240
  validation loss:		1.271050
  validation accuracy:		51.96 %
Epoch 1097 of 2000 took 0.035s
  training loss:		1.348711
  validation loss:		1.266196
  validation accuracy:		52.72 %
Epoch 1098 of 2000 took 0.035s
  training loss:		1.341897
  validation loss:		1.283216
  validation accuracy:		51.30 %
Epoch 1099 of 2000 took 0.035s
  training loss:		1.329957
  validation loss:		1.267858
  validation accuracy:		51.52 %
Epoch 1100 of 2000 took 0.035s
  training loss:		1.338149
  validation loss:		1.262220
  validation accuracy:		52.50 %
Epoch 1101 of 2000 took 0.035s
  training loss:		1.332124
  validation loss:		1.268409
  validation accuracy:		51.85 %
Epoch 1102 of 2000 took 0.035s
  training loss:		1.341976
  validation loss:		1.270976
  validation accuracy:		52.39 %
Epoch 1103 of 2000 took 0.035s
  training loss:		1.341607
  validation loss:		1.268753
  validation accuracy:		51.96 %
Epoch 1104 of 2000 took 0.035s
  training loss:		1.337172
  validation loss:		1.259711
  validation accuracy:		52.93 %
Epoch 1105 of 2000 took 0.035s
  training loss:		1.334357
  validation loss:		1.269980
  validation accuracy:		52.50 %
Epoch 1106 of 2000 took 0.035s
  training loss:		1.332894
  validation loss:		1.262615
  validation accuracy:		52.50 %
Epoch 1107 of 2000 took 0.035s
  training loss:		1.343688
  validation loss:		1.258354
  validation accuracy:		52.50 %
Epoch 1108 of 2000 took 0.036s
  training loss:		1.326158
  validation loss:		1.263282
  validation accuracy:		51.30 %
Epoch 1109 of 2000 took 0.035s
  training loss:		1.327807
  validation loss:		1.263294
  validation accuracy:		51.41 %
Epoch 1110 of 2000 took 0.035s
  training loss:		1.324365
  validation loss:		1.264009
  validation accuracy:		52.07 %
Epoch 1111 of 2000 took 0.035s
  training loss:		1.333457
  validation loss:		1.255937
  validation accuracy:		52.61 %
Epoch 1112 of 2000 took 0.035s
  training loss:		1.326532
  validation loss:		1.287260
  validation accuracy:		51.20 %
Epoch 1113 of 2000 took 0.035s
  training loss:		1.333105
  validation loss:		1.254169
  validation accuracy:		52.61 %
Epoch 1114 of 2000 took 0.036s
  training loss:		1.340435
  validation loss:		1.258213
  validation accuracy:		53.37 %
Epoch 1115 of 2000 took 0.035s
  training loss:		1.333470
  validation loss:		1.258921
  validation accuracy:		52.07 %
Epoch 1116 of 2000 took 0.035s
  training loss:		1.332129
  validation loss:		1.254383
  validation accuracy:		52.28 %
Epoch 1117 of 2000 took 0.035s
  training loss:		1.327784
  validation loss:		1.295278
  validation accuracy:		50.98 %
Epoch 1118 of 2000 took 0.035s
  training loss:		1.328680
  validation loss:		1.248770
  validation accuracy:		52.61 %
Epoch 1119 of 2000 took 0.035s
  training loss:		1.332676
  validation loss:		1.272318
  validation accuracy:		52.07 %
Epoch 1120 of 2000 took 0.035s
  training loss:		1.333041
  validation loss:		1.251946
  validation accuracy:		52.61 %
Epoch 1121 of 2000 took 0.035s
  training loss:		1.318384
  validation loss:		1.256682
  validation accuracy:		51.85 %
Epoch 1122 of 2000 took 0.035s
  training loss:		1.318959
  validation loss:		1.273312
  validation accuracy:		52.61 %
Epoch 1123 of 2000 took 0.035s
  training loss:		1.318673
  validation loss:		1.258178
  validation accuracy:		53.70 %
Epoch 1124 of 2000 took 0.035s
  training loss:		1.322308
  validation loss:		1.261772
  validation accuracy:		53.15 %
Epoch 1125 of 2000 took 0.035s
  training loss:		1.327813
  validation loss:		1.245099
  validation accuracy:		52.72 %
Epoch 1126 of 2000 took 0.035s
  training loss:		1.311105
  validation loss:		1.247569
  validation accuracy:		52.39 %
Epoch 1127 of 2000 took 0.035s
  training loss:		1.323142
  validation loss:		1.266087
  validation accuracy:		52.17 %
Epoch 1128 of 2000 took 0.035s
  training loss:		1.309607
  validation loss:		1.242125
  validation accuracy:		53.04 %
Epoch 1129 of 2000 took 0.035s
  training loss:		1.311313
  validation loss:		1.252338
  validation accuracy:		52.83 %
Epoch 1130 of 2000 took 0.035s
  training loss:		1.328786
  validation loss:		1.245952
  validation accuracy:		52.93 %
Epoch 1131 of 2000 took 0.035s
  training loss:		1.313020
  validation loss:		1.248481
  validation accuracy:		53.59 %
Epoch 1132 of 2000 took 0.035s
  training loss:		1.311142
  validation loss:		1.241045
  validation accuracy:		53.59 %
Epoch 1133 of 2000 took 0.035s
  training loss:		1.315051
  validation loss:		1.241603
  validation accuracy:		52.28 %
Epoch 1134 of 2000 took 0.035s
  training loss:		1.311198
  validation loss:		1.235314
  validation accuracy:		54.67 %
Epoch 1135 of 2000 took 0.035s
  training loss:		1.312761
  validation loss:		1.237974
  validation accuracy:		52.83 %
Epoch 1136 of 2000 took 0.035s
  training loss:		1.313166
  validation loss:		1.231878
  validation accuracy:		53.04 %
Epoch 1137 of 2000 took 0.035s
  training loss:		1.300698
  validation loss:		1.251884
  validation accuracy:		53.80 %
Epoch 1138 of 2000 took 0.035s
  training loss:		1.297438
  validation loss:		1.231450
  validation accuracy:		54.13 %
Epoch 1139 of 2000 took 0.035s
  training loss:		1.298233
  validation loss:		1.227123
  validation accuracy:		53.37 %
Epoch 1140 of 2000 took 0.035s
  training loss:		1.313742
  validation loss:		1.230759
  validation accuracy:		53.91 %
Epoch 1141 of 2000 took 0.035s
  training loss:		1.294817
  validation loss:		1.268644
  validation accuracy:		52.17 %
Epoch 1142 of 2000 took 0.035s
  training loss:		1.300335
  validation loss:		1.215478
  validation accuracy:		55.11 %
Epoch 1143 of 2000 took 0.035s
  training loss:		1.298268
  validation loss:		1.235870
  validation accuracy:		53.80 %
Epoch 1144 of 2000 took 0.036s
  training loss:		1.291249
  validation loss:		1.221266
  validation accuracy:		55.87 %
Epoch 1145 of 2000 took 0.035s
  training loss:		1.301349
  validation loss:		1.216342
  validation accuracy:		56.09 %
Epoch 1146 of 2000 took 0.035s
  training loss:		1.280031
  validation loss:		1.201332
  validation accuracy:		55.11 %
Epoch 1147 of 2000 took 0.035s
  training loss:		1.280897
  validation loss:		1.207383
  validation accuracy:		55.76 %
Epoch 1148 of 2000 took 0.035s
  training loss:		1.271763
  validation loss:		1.216027
  validation accuracy:		55.87 %
Epoch 1149 of 2000 took 0.035s
  training loss:		1.277199
  validation loss:		1.209573
  validation accuracy:		56.52 %
Epoch 1150 of 2000 took 0.035s
  training loss:		1.263847
  validation loss:		1.188749
  validation accuracy:		56.52 %
Epoch 1151 of 2000 took 0.035s
  training loss:		1.264159
  validation loss:		1.201283
  validation accuracy:		57.72 %
Epoch 1152 of 2000 took 0.035s
  training loss:		1.265756
  validation loss:		1.220322
  validation accuracy:		56.09 %
Epoch 1153 of 2000 took 0.035s
  training loss:		1.255857
  validation loss:		1.173697
  validation accuracy:		57.61 %
Epoch 1154 of 2000 took 0.035s
  training loss:		1.249551
  validation loss:		1.165796
  validation accuracy:		57.50 %
Epoch 1155 of 2000 took 0.035s
  training loss:		1.238446
  validation loss:		1.148038
  validation accuracy:		58.48 %
Epoch 1156 of 2000 took 0.035s
  training loss:		1.221047
  validation loss:		1.146386
  validation accuracy:		58.48 %
Epoch 1157 of 2000 took 0.035s
  training loss:		1.217320
  validation loss:		1.139041
  validation accuracy:		59.78 %
Epoch 1158 of 2000 took 0.035s
  training loss:		1.208897
  validation loss:		1.134660
  validation accuracy:		60.33 %
Epoch 1159 of 2000 took 0.035s
  training loss:		1.194022
  validation loss:		1.117822
  validation accuracy:		61.20 %
Epoch 1160 of 2000 took 0.035s
  training loss:		1.188584
  validation loss:		1.117859
  validation accuracy:		61.52 %
Epoch 1161 of 2000 took 0.035s
  training loss:		1.180640
  validation loss:		1.095508
  validation accuracy:		61.20 %
Epoch 1162 of 2000 took 0.035s
  training loss:		1.168338
  validation loss:		1.090807
  validation accuracy:		62.39 %
Epoch 1163 of 2000 took 0.035s
  training loss:		1.159308
  validation loss:		1.077663
  validation accuracy:		62.61 %
Epoch 1164 of 2000 took 0.035s
  training loss:		1.142365
  validation loss:		1.070123
  validation accuracy:		62.39 %
Epoch 1165 of 2000 took 0.035s
  training loss:		1.129258
  validation loss:		1.054114
  validation accuracy:		63.70 %
Epoch 1166 of 2000 took 0.035s
  training loss:		1.120596
  validation loss:		1.039574
  validation accuracy:		64.02 %
Epoch 1167 of 2000 took 0.035s
  training loss:		1.106062
  validation loss:		1.032128
  validation accuracy:		64.13 %
Epoch 1168 of 2000 took 0.035s
  training loss:		1.096771
  validation loss:		1.018891
  validation accuracy:		64.24 %
Epoch 1169 of 2000 took 0.035s
  training loss:		1.086897
  validation loss:		1.010432
  validation accuracy:		64.57 %
Epoch 1170 of 2000 took 0.036s
  training loss:		1.075307
  validation loss:		0.994063
  validation accuracy:		65.43 %
Epoch 1171 of 2000 took 0.035s
  training loss:		1.059014
  validation loss:		0.989171
  validation accuracy:		65.43 %
Epoch 1172 of 2000 took 0.035s
  training loss:		1.048604
  validation loss:		0.976842
  validation accuracy:		66.96 %
Epoch 1173 of 2000 took 0.035s
  training loss:		1.034418
  validation loss:		0.970056
  validation accuracy:		66.30 %
Epoch 1174 of 2000 took 0.035s
  training loss:		1.022113
  validation loss:		0.956291
  validation accuracy:		66.85 %
Epoch 1175 of 2000 took 0.035s
  training loss:		1.002154
  validation loss:		0.945863
  validation accuracy:		67.39 %
Epoch 1176 of 2000 took 0.035s
  training loss:		0.989724
  validation loss:		0.926980
  validation accuracy:		68.48 %
Epoch 1177 of 2000 took 0.035s
  training loss:		0.982889
  validation loss:		0.928499
  validation accuracy:		68.37 %
Epoch 1178 of 2000 took 0.036s
  training loss:		0.979945
  validation loss:		0.907770
  validation accuracy:		68.80 %
Epoch 1179 of 2000 took 0.035s
  training loss:		0.958467
  validation loss:		0.896856
  validation accuracy:		69.46 %
Epoch 1180 of 2000 took 0.035s
  training loss:		0.957194
  validation loss:		0.901689
  validation accuracy:		69.89 %
Epoch 1181 of 2000 took 0.035s
  training loss:		0.948974
  validation loss:		0.880473
  validation accuracy:		70.87 %
Epoch 1182 of 2000 took 0.035s
  training loss:		0.937980
  validation loss:		0.871251
  validation accuracy:		71.20 %
Epoch 1183 of 2000 took 0.036s
  training loss:		0.927854
  validation loss:		0.858171
  validation accuracy:		71.52 %
Epoch 1184 of 2000 took 0.035s
  training loss:		0.917747
  validation loss:		0.858802
  validation accuracy:		71.41 %
Epoch 1185 of 2000 took 0.035s
  training loss:		0.903847
  validation loss:		0.849897
  validation accuracy:		72.17 %
Epoch 1186 of 2000 took 0.035s
  training loss:		0.894038
  validation loss:		0.842960
  validation accuracy:		72.39 %
Epoch 1187 of 2000 took 0.035s
  training loss:		0.886895
  validation loss:		0.825510
  validation accuracy:		73.15 %
Epoch 1188 of 2000 took 0.035s
  training loss:		0.873303
  validation loss:		0.813621
  validation accuracy:		72.93 %
Epoch 1189 of 2000 took 0.035s
  training loss:		0.870920
  validation loss:		0.803051
  validation accuracy:		73.48 %
Epoch 1190 of 2000 took 0.035s
  training loss:		0.858622
  validation loss:		0.798999
  validation accuracy:		73.80 %
Epoch 1191 of 2000 took 0.035s
  training loss:		0.853563
  validation loss:		0.798138
  validation accuracy:		73.15 %
Epoch 1192 of 2000 took 0.035s
  training loss:		0.836734
  validation loss:		0.778497
  validation accuracy:		74.46 %
Epoch 1193 of 2000 took 0.035s
  training loss:		0.836891
  validation loss:		0.776018
  validation accuracy:		73.70 %
Epoch 1194 of 2000 took 0.035s
  training loss:		0.824122
  validation loss:		0.773375
  validation accuracy:		74.35 %
Epoch 1195 of 2000 took 0.035s
  training loss:		0.817178
  validation loss:		0.762594
  validation accuracy:		74.35 %
Epoch 1196 of 2000 took 0.035s
  training loss:		0.803499
  validation loss:		0.749775
  validation accuracy:		75.00 %
Epoch 1197 of 2000 took 0.035s
  training loss:		0.808306
  validation loss:		0.761365
  validation accuracy:		75.43 %
Epoch 1198 of 2000 took 0.035s
  training loss:		0.794219
  validation loss:		0.748856
  validation accuracy:		74.57 %
Epoch 1199 of 2000 took 0.035s
  training loss:		0.790694
  validation loss:		0.744547
  validation accuracy:		75.65 %
Epoch 1200 of 2000 took 0.035s
  training loss:		0.775364
  validation loss:		0.732501
  validation accuracy:		75.65 %
Epoch 1201 of 2000 took 0.035s
  training loss:		0.778566
  validation loss:		0.733114
  validation accuracy:		75.76 %
Epoch 1202 of 2000 took 0.035s
  training loss:		0.774893
  validation loss:		0.735556
  validation accuracy:		75.65 %
Epoch 1203 of 2000 took 0.035s
  training loss:		0.767655
  validation loss:		0.716775
  validation accuracy:		76.20 %
Epoch 1204 of 2000 took 0.035s
  training loss:		0.758678
  validation loss:		0.725579
  validation accuracy:		75.98 %
Epoch 1205 of 2000 took 0.035s
  training loss:		0.759981
  validation loss:		0.718577
  validation accuracy:		75.76 %
Epoch 1206 of 2000 took 0.035s
  training loss:		0.745203
  validation loss:		0.710677
  validation accuracy:		76.20 %
Epoch 1207 of 2000 took 0.036s
  training loss:		0.746064
  validation loss:		0.701262
  validation accuracy:		76.63 %
Epoch 1208 of 2000 took 0.035s
  training loss:		0.741812
  validation loss:		0.695524
  validation accuracy:		76.85 %
Epoch 1209 of 2000 took 0.035s
  training loss:		0.739910
  validation loss:		0.691099
  validation accuracy:		76.85 %
Epoch 1210 of 2000 took 0.035s
  training loss:		0.727158
  validation loss:		0.690982
  validation accuracy:		76.74 %
Epoch 1211 of 2000 took 0.035s
  training loss:		0.723932
  validation loss:		0.688290
  validation accuracy:		76.52 %
Epoch 1212 of 2000 took 0.035s
  training loss:		0.719758
  validation loss:		0.685669
  validation accuracy:		77.07 %
Epoch 1213 of 2000 took 0.035s
  training loss:		0.713061
  validation loss:		0.681974
  validation accuracy:		77.39 %
Epoch 1214 of 2000 took 0.035s
  training loss:		0.717657
  validation loss:		0.674688
  validation accuracy:		77.17 %
Epoch 1215 of 2000 took 0.035s
  training loss:		0.708207
  validation loss:		0.680635
  validation accuracy:		77.07 %
Epoch 1216 of 2000 took 0.035s
  training loss:		0.706304
  validation loss:		0.672497
  validation accuracy:		76.96 %
Epoch 1217 of 2000 took 0.035s
  training loss:		0.702316
  validation loss:		0.682584
  validation accuracy:		76.85 %
Epoch 1218 of 2000 took 0.035s
  training loss:		0.697611
  validation loss:		0.682319
  validation accuracy:		76.85 %
Epoch 1219 of 2000 took 0.035s
  training loss:		0.702920
  validation loss:		0.656897
  validation accuracy:		77.83 %
Epoch 1220 of 2000 took 0.035s
  training loss:		0.691954
  validation loss:		0.668058
  validation accuracy:		77.93 %
Epoch 1221 of 2000 took 0.035s
  training loss:		0.692558
  validation loss:		0.651746
  validation accuracy:		77.83 %
Epoch 1222 of 2000 took 0.035s
  training loss:		0.692180
  validation loss:		0.662962
  validation accuracy:		77.72 %
Epoch 1223 of 2000 took 0.035s
  training loss:		0.677941
  validation loss:		0.652904
  validation accuracy:		77.93 %
Epoch 1224 of 2000 took 0.035s
  training loss:		0.679768
  validation loss:		0.661085
  validation accuracy:		77.93 %
Epoch 1225 of 2000 took 0.035s
  training loss:		0.678849
  validation loss:		0.653207
  validation accuracy:		78.15 %
Epoch 1226 of 2000 took 0.035s
  training loss:		0.682027
  validation loss:		0.684082
  validation accuracy:		77.07 %
Epoch 1227 of 2000 took 0.036s
  training loss:		0.659075
  validation loss:		0.650960
  validation accuracy:		79.02 %
Epoch 1228 of 2000 took 0.035s
  training loss:		0.686092
  validation loss:		0.656666
  validation accuracy:		77.93 %
Epoch 1229 of 2000 took 0.035s
  training loss:		0.683954
  validation loss:		0.642574
  validation accuracy:		78.80 %
Epoch 1230 of 2000 took 0.035s
  training loss:		0.680935
  validation loss:		0.652798
  validation accuracy:		78.48 %
Epoch 1231 of 2000 took 0.035s
  training loss:		0.677767
  validation loss:		0.639909
  validation accuracy:		79.35 %
Epoch 1232 of 2000 took 0.035s
  training loss:		0.662287
  validation loss:		0.662913
  validation accuracy:		78.04 %
Epoch 1233 of 2000 took 0.035s
  training loss:		0.651199
  validation loss:		0.641260
  validation accuracy:		78.70 %
Epoch 1234 of 2000 took 0.035s
  training loss:		0.658554
  validation loss:		0.652380
  validation accuracy:		79.02 %
Epoch 1235 of 2000 took 0.035s
  training loss:		0.655718
  validation loss:		0.638500
  validation accuracy:		78.91 %
Epoch 1236 of 2000 took 0.035s
  training loss:		0.657670
  validation loss:		0.637920
  validation accuracy:		80.11 %
Epoch 1237 of 2000 took 0.035s
  training loss:		0.651888
  validation loss:		0.635560
  validation accuracy:		79.89 %
Epoch 1238 of 2000 took 0.035s
  training loss:		0.646972
  validation loss:		0.634560
  validation accuracy:		79.67 %
Epoch 1239 of 2000 took 0.035s
  training loss:		0.646610
  validation loss:		0.634963
  validation accuracy:		79.13 %
Epoch 1240 of 2000 took 0.035s
  training loss:		0.649796
  validation loss:		0.639741
  validation accuracy:		80.22 %
Epoch 1241 of 2000 took 0.035s
  training loss:		0.669636
  validation loss:		0.645743
  validation accuracy:		78.80 %
Epoch 1242 of 2000 took 0.035s
  training loss:		0.641151
  validation loss:		0.646100
  validation accuracy:		78.59 %
Epoch 1243 of 2000 took 0.035s
  training loss:		0.665068
  validation loss:		0.755297
  validation accuracy:		75.54 %
Epoch 1244 of 2000 took 0.035s
  training loss:		0.653794
  validation loss:		0.631388
  validation accuracy:		80.87 %
Epoch 1245 of 2000 took 0.035s
  training loss:		0.645068
  validation loss:		0.682756
  validation accuracy:		78.04 %
Epoch 1246 of 2000 took 0.035s
  training loss:		0.639632
  validation loss:		0.637077
  validation accuracy:		78.91 %
Epoch 1247 of 2000 took 0.035s
  training loss:		0.642918
  validation loss:		0.633767
  validation accuracy:		80.11 %
Epoch 1248 of 2000 took 0.035s
  training loss:		0.625986
  validation loss:		0.699009
  validation accuracy:		78.04 %
Epoch 1249 of 2000 took 0.035s
  training loss:		0.640481
  validation loss:		0.630485
  validation accuracy:		79.57 %
Epoch 1250 of 2000 took 0.035s
  training loss:		0.642873
  validation loss:		0.660276
  validation accuracy:		80.43 %
Epoch 1251 of 2000 took 0.035s
  training loss:		0.633818
  validation loss:		0.639010
  validation accuracy:		79.46 %
Epoch 1252 of 2000 took 0.035s
  training loss:		0.634118
  validation loss:		0.636479
  validation accuracy:		79.57 %
Epoch 1253 of 2000 took 0.036s
  training loss:		0.635184
  validation loss:		0.643544
  validation accuracy:		78.91 %
Epoch 1254 of 2000 took 0.035s
  training loss:		0.643538
  validation loss:		0.645169
  validation accuracy:		78.70 %
Epoch 1255 of 2000 took 0.035s
  training loss:		0.636554
  validation loss:		0.649157
  validation accuracy:		79.02 %
Epoch 1256 of 2000 took 0.035s
  training loss:		0.636733
  validation loss:		0.632498
  validation accuracy:		80.87 %
Epoch 1257 of 2000 took 0.035s
  training loss:		0.632180
  validation loss:		0.632092
  validation accuracy:		80.43 %
Epoch 1258 of 2000 took 0.035s
  training loss:		0.635576
  validation loss:		0.630621
  validation accuracy:		81.52 %
Epoch 1259 of 2000 took 0.035s
  training loss:		0.648757
  validation loss:		0.661970
  validation accuracy:		78.80 %
Epoch 1260 of 2000 took 0.035s
  training loss:		0.629906
  validation loss:		0.636859
  validation accuracy:		79.35 %
Epoch 1261 of 2000 took 0.035s
  training loss:		0.624897
  validation loss:		0.631593
  validation accuracy:		81.41 %
Epoch 1262 of 2000 took 0.035s
  training loss:		0.619419
  validation loss:		0.682506
  validation accuracy:		78.37 %
Epoch 1263 of 2000 took 0.035s
  training loss:		0.628721
  validation loss:		0.627940
  validation accuracy:		81.52 %
Epoch 1264 of 2000 took 0.035s
  training loss:		0.618802
  validation loss:		0.638198
  validation accuracy:		81.09 %
Epoch 1265 of 2000 took 0.035s
  training loss:		0.686942
  validation loss:		0.701694
  validation accuracy:		78.26 %
Epoch 1266 of 2000 took 0.035s
  training loss:		0.661963
  validation loss:		0.636898
  validation accuracy:		81.20 %
Epoch 1267 of 2000 took 0.035s
  training loss:		0.622661
  validation loss:		0.659274
  validation accuracy:		79.02 %
Epoch 1268 of 2000 took 0.035s
  training loss:		0.628508
  validation loss:		0.631584
  validation accuracy:		80.87 %
Epoch 1269 of 2000 took 0.035s
  training loss:		0.617452
  validation loss:		0.643132
  validation accuracy:		80.22 %
Epoch 1270 of 2000 took 0.035s
  training loss:		0.611544
  validation loss:		0.625288
  validation accuracy:		81.41 %
Epoch 1271 of 2000 took 0.035s
  training loss:		0.630924
  validation loss:		0.633598
  validation accuracy:		80.65 %
Epoch 1272 of 2000 took 0.035s
  training loss:		0.640061
  validation loss:		0.629894
  validation accuracy:		81.41 %
Epoch 1273 of 2000 took 0.035s
  training loss:		0.617205
  validation loss:		0.621343
  validation accuracy:		80.54 %
Epoch 1274 of 2000 took 0.035s
  training loss:		0.613570
  validation loss:		0.631468
  validation accuracy:		81.41 %
Epoch 1275 of 2000 took 0.035s
  training loss:		0.599898
  validation loss:		0.646450
  validation accuracy:		78.70 %
Epoch 1276 of 2000 took 0.035s
  training loss:		0.618291
  validation loss:		0.631537
  validation accuracy:		80.98 %
Epoch 1277 of 2000 took 0.035s
  training loss:		0.619061
  validation loss:		0.619694
  validation accuracy:		81.63 %
Epoch 1278 of 2000 took 0.035s
  training loss:		0.613980
  validation loss:		0.624283
  validation accuracy:		81.52 %
Epoch 1279 of 2000 took 0.035s
  training loss:		0.618834
  validation loss:		0.614961
  validation accuracy:		81.52 %
Epoch 1280 of 2000 took 0.036s
  training loss:		0.615928
  validation loss:		0.643611
  validation accuracy:		78.80 %
Epoch 1281 of 2000 took 0.035s
  training loss:		0.605614
  validation loss:		0.631818
  validation accuracy:		79.57 %
Epoch 1282 of 2000 took 0.035s
  training loss:		0.626885
  validation loss:		0.627283
  validation accuracy:		79.67 %
Epoch 1283 of 2000 took 0.036s
  training loss:		0.617246
  validation loss:		0.639266
  validation accuracy:		80.98 %
Epoch 1284 of 2000 took 0.035s
  training loss:		0.616164
  validation loss:		0.623900
  validation accuracy:		81.30 %
Epoch 1285 of 2000 took 0.035s
  training loss:		0.632459
  validation loss:		0.654734
  validation accuracy:		78.91 %
Epoch 1286 of 2000 took 0.035s
  training loss:		0.627524
  validation loss:		0.653814
  validation accuracy:		78.70 %
Epoch 1287 of 2000 took 0.035s
  training loss:		0.612585
  validation loss:		0.616840
  validation accuracy:		81.52 %
Epoch 1288 of 2000 took 0.035s
  training loss:		0.602159
  validation loss:		0.626700
  validation accuracy:		79.89 %
Epoch 1289 of 2000 took 0.035s
  training loss:		0.610044
  validation loss:		0.628915
  validation accuracy:		79.89 %
Epoch 1290 of 2000 took 0.035s
  training loss:		0.596317
  validation loss:		0.620504
  validation accuracy:		81.41 %
Epoch 1291 of 2000 took 0.036s
  training loss:		0.606863
  validation loss:		0.622402
  validation accuracy:		80.87 %
Epoch 1292 of 2000 took 0.036s
  training loss:		0.607295
  validation loss:		0.681900
  validation accuracy:		78.70 %
Epoch 1293 of 2000 took 0.035s
  training loss:		0.605476
  validation loss:		0.632767
  validation accuracy:		79.78 %
Epoch 1294 of 2000 took 0.035s
  training loss:		0.596105
  validation loss:		0.625633
  validation accuracy:		81.96 %
Epoch 1295 of 2000 took 0.035s
  training loss:		0.604345
  validation loss:		0.637325
  validation accuracy:		79.89 %
Epoch 1296 of 2000 took 0.036s
  training loss:		0.619569
  validation loss:		0.624815
  validation accuracy:		80.11 %
Epoch 1297 of 2000 took 0.036s
  training loss:		0.599155
  validation loss:		0.632711
  validation accuracy:		79.57 %
Epoch 1298 of 2000 took 0.035s
  training loss:		0.604517
  validation loss:		0.666574
  validation accuracy:		80.11 %
Epoch 1299 of 2000 took 0.035s
  training loss:		0.617756
  validation loss:		0.617038
  validation accuracy:		80.54 %
Epoch 1300 of 2000 took 0.035s
  training loss:		0.604257
  validation loss:		0.666374
  validation accuracy:		78.48 %
Epoch 1301 of 2000 took 0.035s
  training loss:		0.597388
  validation loss:		0.634997
  validation accuracy:		79.89 %
Epoch 1302 of 2000 took 0.035s
  training loss:		0.605410
  validation loss:		0.612652
  validation accuracy:		80.98 %
Epoch 1303 of 2000 took 0.035s
  training loss:		0.602260
  validation loss:		0.625592
  validation accuracy:		80.76 %
Epoch 1304 of 2000 took 0.035s
  training loss:		0.580288
  validation loss:		0.618945
  validation accuracy:		80.54 %
Epoch 1305 of 2000 took 0.035s
  training loss:		0.636682
  validation loss:		0.631233
  validation accuracy:		80.65 %
Epoch 1306 of 2000 took 0.036s
  training loss:		0.597210
  validation loss:		0.618621
  validation accuracy:		80.43 %
Epoch 1307 of 2000 took 0.035s
  training loss:		0.597109
  validation loss:		0.607824
  validation accuracy:		81.63 %
Epoch 1308 of 2000 took 0.035s
  training loss:		0.587055
  validation loss:		0.613005
  validation accuracy:		80.76 %
Epoch 1309 of 2000 took 0.036s
  training loss:		0.600748
  validation loss:		0.623072
  validation accuracy:		80.43 %
Epoch 1310 of 2000 took 0.035s
  training loss:		0.609351
  validation loss:		0.618581
  validation accuracy:		80.33 %
Epoch 1311 of 2000 took 0.035s
  training loss:		0.584473
  validation loss:		0.619451
  validation accuracy:		81.30 %
Epoch 1312 of 2000 took 0.035s
  training loss:		0.658035
  validation loss:		0.616417
  validation accuracy:		80.43 %
Epoch 1313 of 2000 took 0.035s
  training loss:		0.601080
  validation loss:		0.626165
  validation accuracy:		80.76 %
Epoch 1314 of 2000 took 0.035s
  training loss:		0.611536
  validation loss:		0.627575
  validation accuracy:		80.65 %
Epoch 1315 of 2000 took 0.035s
  training loss:		0.591787
  validation loss:		0.618408
  validation accuracy:		80.65 %
Epoch 1316 of 2000 took 0.035s
  training loss:		0.584219
  validation loss:		0.619557
  validation accuracy:		81.20 %
Epoch 1317 of 2000 took 0.035s
  training loss:		0.583546
  validation loss:		0.627692
  validation accuracy:		80.11 %
Epoch 1318 of 2000 took 0.035s
  training loss:		0.581933
  validation loss:		0.611511
  validation accuracy:		80.76 %
Epoch 1319 of 2000 took 0.035s
  training loss:		0.762542
  validation loss:		0.736214
  validation accuracy:		77.17 %
Epoch 1320 of 2000 took 0.036s
  training loss:		0.609533
  validation loss:		0.630769
  validation accuracy:		80.43 %
Epoch 1321 of 2000 took 0.035s
  training loss:		0.599387
  validation loss:		0.635422
  validation accuracy:		79.67 %
Epoch 1322 of 2000 took 0.035s
  training loss:		0.589437
  validation loss:		0.610207
  validation accuracy:		81.30 %
Epoch 1323 of 2000 took 0.035s
  training loss:		0.585817
  validation loss:		0.627945
  validation accuracy:		80.33 %
Epoch 1324 of 2000 took 0.035s
  training loss:		0.595654
  validation loss:		0.632576
  validation accuracy:		80.22 %
Epoch 1325 of 2000 took 0.035s
  training loss:		0.595324
  validation loss:		0.617378
  validation accuracy:		80.87 %
Epoch 1326 of 2000 took 0.035s
  training loss:		0.597425
  validation loss:		0.606907
  validation accuracy:		80.87 %
Epoch 1327 of 2000 took 0.035s
  training loss:		0.599555
  validation loss:		0.609454
  validation accuracy:		81.09 %
Epoch 1328 of 2000 took 0.035s
  training loss:		0.586893
  validation loss:		0.712418
  validation accuracy:		76.96 %
Epoch 1329 of 2000 took 0.035s
  training loss:		0.594841
  validation loss:		0.622734
  validation accuracy:		80.65 %
Epoch 1330 of 2000 took 0.036s
  training loss:		0.592235
  validation loss:		0.613991
  validation accuracy:		80.87 %
Epoch 1331 of 2000 took 0.035s
  training loss:		0.584803
  validation loss:		0.613588
  validation accuracy:		80.43 %
Epoch 1332 of 2000 took 0.035s
  training loss:		0.573544
  validation loss:		0.614580
  validation accuracy:		80.54 %
Epoch 1333 of 2000 took 0.035s
  training loss:		0.603473
  validation loss:		0.649735
  validation accuracy:		79.57 %
Epoch 1334 of 2000 took 0.035s
  training loss:		0.579868
  validation loss:		0.611144
  validation accuracy:		80.98 %
Epoch 1335 of 2000 took 0.035s
  training loss:		0.580886
  validation loss:		0.628976
  validation accuracy:		80.43 %
Epoch 1336 of 2000 took 0.035s
  training loss:		0.576992
  validation loss:		0.616288
  validation accuracy:		80.54 %
Epoch 1337 of 2000 took 0.036s
  training loss:		0.582755
  validation loss:		0.604470
  validation accuracy:		81.20 %
Epoch 1338 of 2000 took 0.035s
  training loss:		0.587376
  validation loss:		0.642611
  validation accuracy:		79.89 %
Epoch 1339 of 2000 took 0.035s
  training loss:		0.576006
  validation loss:		0.605268
  validation accuracy:		80.98 %
Epoch 1340 of 2000 took 0.036s
  training loss:		0.579480
  validation loss:		0.609592
  validation accuracy:		80.65 %
Epoch 1341 of 2000 took 0.035s
  training loss:		0.585827
  validation loss:		0.629573
  validation accuracy:		80.54 %
Epoch 1342 of 2000 took 0.035s
  training loss:		0.599666
  validation loss:		0.753538
  validation accuracy:		76.96 %
Epoch 1343 of 2000 took 0.035s
  training loss:		0.621943
  validation loss:		0.618934
  validation accuracy:		80.65 %
Epoch 1344 of 2000 took 0.035s
  training loss:		0.592906
  validation loss:		0.644400
  validation accuracy:		80.22 %
Epoch 1345 of 2000 took 0.035s
  training loss:		0.578367
  validation loss:		0.606839
  validation accuracy:		81.52 %
Epoch 1346 of 2000 took 0.035s
  training loss:		0.575165
  validation loss:		0.619734
  validation accuracy:		79.89 %
Epoch 1347 of 2000 took 0.035s
  training loss:		0.581766
  validation loss:		0.603433
  validation accuracy:		81.09 %
Epoch 1348 of 2000 took 0.035s
  training loss:		0.573387
  validation loss:		0.605246
  validation accuracy:		81.30 %
Epoch 1349 of 2000 took 0.035s
  training loss:		0.576637
  validation loss:		0.610769
  validation accuracy:		81.09 %
Epoch 1350 of 2000 took 0.035s
  training loss:		0.577852
  validation loss:		0.646743
  validation accuracy:		78.59 %
Epoch 1351 of 2000 took 0.035s
  training loss:		0.589843
  validation loss:		0.620176
  validation accuracy:		80.43 %
Epoch 1352 of 2000 took 0.035s
  training loss:		0.569069
  validation loss:		0.601232
  validation accuracy:		81.30 %
Epoch 1353 of 2000 took 0.035s
  training loss:		0.579478
  validation loss:		0.619250
  validation accuracy:		80.43 %
Epoch 1354 of 2000 took 0.035s
  training loss:		0.578488
  validation loss:		0.605099
  validation accuracy:		80.87 %
Epoch 1355 of 2000 took 0.035s
  training loss:		0.574007
  validation loss:		0.601569
  validation accuracy:		80.76 %
Epoch 1356 of 2000 took 0.035s
  training loss:		0.588453
  validation loss:		0.634321
  validation accuracy:		79.57 %
Epoch 1357 of 2000 took 0.035s
  training loss:		0.621548
  validation loss:		0.622306
  validation accuracy:		80.65 %
Epoch 1358 of 2000 took 0.035s
  training loss:		0.576099
  validation loss:		0.608317
  validation accuracy:		80.87 %
Epoch 1359 of 2000 took 0.035s
  training loss:		0.571609
  validation loss:		0.602467
  validation accuracy:		81.09 %
Epoch 1360 of 2000 took 0.035s
  training loss:		0.562699
  validation loss:		0.619187
  validation accuracy:		80.22 %
Epoch 1361 of 2000 took 0.035s
  training loss:		0.573434
  validation loss:		0.599600
  validation accuracy:		81.52 %
Epoch 1362 of 2000 took 0.035s
  training loss:		0.569006
  validation loss:		0.617112
  validation accuracy:		80.98 %
Epoch 1363 of 2000 took 0.035s
  training loss:		0.557378
  validation loss:		0.622434
  validation accuracy:		80.33 %
Epoch 1364 of 2000 took 0.035s
  training loss:		0.579468
  validation loss:		0.639982
  validation accuracy:		79.78 %
Epoch 1365 of 2000 took 0.035s
  training loss:		0.572694
  validation loss:		0.617214
  validation accuracy:		80.65 %
Epoch 1366 of 2000 took 0.035s
  training loss:		0.578557
  validation loss:		0.611650
  validation accuracy:		80.76 %
Epoch 1367 of 2000 took 0.035s
  training loss:		0.563924
  validation loss:		0.608586
  validation accuracy:		81.20 %
Epoch 1368 of 2000 took 0.035s
  training loss:		0.560194
  validation loss:		0.620811
  validation accuracy:		80.54 %
Epoch 1369 of 2000 took 0.035s
  training loss:		0.564884
  validation loss:		0.613589
  validation accuracy:		80.76 %
Epoch 1370 of 2000 took 0.035s
  training loss:		0.574746
  validation loss:		0.620433
  validation accuracy:		80.54 %
Epoch 1371 of 2000 took 0.035s
  training loss:		0.587336
  validation loss:		0.605092
  validation accuracy:		81.09 %
Epoch 1372 of 2000 took 0.035s
  training loss:		0.570569
  validation loss:		0.610130
  validation accuracy:		80.54 %
Epoch 1373 of 2000 took 0.035s
  training loss:		0.568019
  validation loss:		0.601433
  validation accuracy:		80.98 %
Epoch 1374 of 2000 took 0.035s
  training loss:		0.576701
  validation loss:		0.602393
  validation accuracy:		81.20 %
Epoch 1375 of 2000 took 0.035s
  training loss:		0.576284
  validation loss:		0.609543
  validation accuracy:		81.09 %
Epoch 1376 of 2000 took 0.035s
  training loss:		0.572873
  validation loss:		0.613786
  validation accuracy:		80.65 %
Epoch 1377 of 2000 took 0.035s
  training loss:		0.575483
  validation loss:		0.634150
  validation accuracy:		80.11 %
Epoch 1378 of 2000 took 0.035s
  training loss:		0.581715
  validation loss:		0.592394
  validation accuracy:		81.63 %
Epoch 1379 of 2000 took 0.035s
  training loss:		0.555325
  validation loss:		0.632317
  validation accuracy:		80.33 %
Epoch 1380 of 2000 took 0.035s
  training loss:		0.577694
  validation loss:		0.598741
  validation accuracy:		81.09 %
Epoch 1381 of 2000 took 0.036s
  training loss:		0.558129
  validation loss:		0.597638
  validation accuracy:		80.98 %
Epoch 1382 of 2000 took 0.035s
  training loss:		0.569989
  validation loss:		0.633271
  validation accuracy:		80.43 %
Epoch 1383 of 2000 took 0.035s
  training loss:		0.579027
  validation loss:		0.615986
  validation accuracy:		81.09 %
Epoch 1384 of 2000 took 0.035s
  training loss:		0.564634
  validation loss:		0.626679
  validation accuracy:		80.65 %
Epoch 1385 of 2000 took 0.035s
  training loss:		0.562822
  validation loss:		0.625121
  validation accuracy:		80.65 %
Epoch 1386 of 2000 took 0.035s
  training loss:		0.550742
  validation loss:		0.623594
  validation accuracy:		80.76 %
Epoch 1387 of 2000 took 0.035s
  training loss:		0.569660
  validation loss:		0.601130
  validation accuracy:		80.76 %
Epoch 1388 of 2000 took 0.035s
  training loss:		0.563459
  validation loss:		0.638912
  validation accuracy:		79.78 %
Epoch 1389 of 2000 took 0.035s
  training loss:		0.608286
  validation loss:		0.638171
  validation accuracy:		80.11 %
Epoch 1390 of 2000 took 0.035s
  training loss:		0.567454
  validation loss:		0.595525
  validation accuracy:		81.09 %
Epoch 1391 of 2000 took 0.035s
  training loss:		0.576104
  validation loss:		0.688045
  validation accuracy:		77.93 %
Epoch 1392 of 2000 took 0.035s
  training loss:		0.581112
  validation loss:		0.603219
  validation accuracy:		80.76 %
Epoch 1393 of 2000 took 0.035s
  training loss:		0.564443
  validation loss:		0.606240
  validation accuracy:		81.09 %
Epoch 1394 of 2000 took 0.035s
  training loss:		0.584323
  validation loss:		0.600732
  validation accuracy:		81.63 %
Epoch 1395 of 2000 took 0.035s
  training loss:		0.552394
  validation loss:		0.618296
  validation accuracy:		80.33 %
Epoch 1396 of 2000 took 0.036s
  training loss:		0.584088
  validation loss:		0.591664
  validation accuracy:		81.52 %
Epoch 1397 of 2000 took 0.035s
  training loss:		0.572840
  validation loss:		0.636920
  validation accuracy:		80.22 %
Epoch 1398 of 2000 took 0.035s
  training loss:		0.548303
  validation loss:		0.596614
  validation accuracy:		81.30 %
Epoch 1399 of 2000 took 0.035s
  training loss:		0.558899
  validation loss:		0.595749
  validation accuracy:		80.76 %
Epoch 1400 of 2000 took 0.035s
  training loss:		0.562517
  validation loss:		0.603890
  validation accuracy:		81.20 %
Epoch 1401 of 2000 took 0.035s
  training loss:		0.575044
  validation loss:		0.596226
  validation accuracy:		81.63 %
Epoch 1402 of 2000 took 0.035s
  training loss:		0.586015
  validation loss:		0.601064
  validation accuracy:		81.52 %
Epoch 1403 of 2000 took 0.035s
  training loss:		0.564778
  validation loss:		0.594192
  validation accuracy:		82.07 %
Epoch 1404 of 2000 took 0.036s
  training loss:		0.560836
  validation loss:		0.587447
  validation accuracy:		81.85 %
Epoch 1405 of 2000 took 0.035s
  training loss:		0.564681
  validation loss:		0.613111
  validation accuracy:		80.98 %
Epoch 1406 of 2000 took 0.035s
  training loss:		0.556592
  validation loss:		0.595099
  validation accuracy:		81.20 %
Epoch 1407 of 2000 took 0.035s
  training loss:		0.552740
  validation loss:		0.596873
  validation accuracy:		81.74 %
Epoch 1408 of 2000 took 0.035s
  training loss:		0.572694
  validation loss:		0.590609
  validation accuracy:		81.63 %
Epoch 1409 of 2000 took 0.036s
  training loss:		0.559240
  validation loss:		0.614667
  validation accuracy:		80.22 %
Epoch 1410 of 2000 took 0.035s
  training loss:		0.575995
  validation loss:		0.630543
  validation accuracy:		80.00 %
Epoch 1411 of 2000 took 0.035s
  training loss:		0.551267
  validation loss:		0.593750
  validation accuracy:		81.96 %
Epoch 1412 of 2000 took 0.035s
  training loss:		0.567263
  validation loss:		0.591665
  validation accuracy:		81.74 %
Epoch 1413 of 2000 took 0.036s
  training loss:		0.570227
  validation loss:		0.589804
  validation accuracy:		82.28 %
Epoch 1414 of 2000 took 0.035s
  training loss:		0.556138
  validation loss:		0.594525
  validation accuracy:		82.07 %
Epoch 1415 of 2000 took 0.035s
  training loss:		0.556100
  validation loss:		0.627150
  validation accuracy:		80.11 %
Epoch 1416 of 2000 took 0.035s
  training loss:		0.565261
  validation loss:		0.588343
  validation accuracy:		82.28 %
Epoch 1417 of 2000 took 0.035s
  training loss:		0.568938
  validation loss:		0.606445
  validation accuracy:		80.65 %
Epoch 1418 of 2000 took 0.035s
  training loss:		0.570207
  validation loss:		0.592834
  validation accuracy:		82.07 %
Epoch 1419 of 2000 took 0.035s
  training loss:		0.551801
  validation loss:		0.596284
  validation accuracy:		81.74 %
Epoch 1420 of 2000 took 0.035s
  training loss:		0.553319
  validation loss:		0.585216
  validation accuracy:		82.17 %
Epoch 1421 of 2000 took 0.035s
  training loss:		0.547683
  validation loss:		0.633362
  validation accuracy:		79.35 %
Epoch 1422 of 2000 took 0.036s
  training loss:		0.551854
  validation loss:		0.591022
  validation accuracy:		81.09 %
Epoch 1423 of 2000 took 0.035s
  training loss:		0.554893
  validation loss:		0.610557
  validation accuracy:		81.63 %
Epoch 1424 of 2000 took 0.035s
  training loss:		0.555490
  validation loss:		0.588106
  validation accuracy:		81.96 %
Epoch 1425 of 2000 took 0.035s
  training loss:		0.559474
  validation loss:		0.599083
  validation accuracy:		81.41 %
Epoch 1426 of 2000 took 0.035s
  training loss:		0.560900
  validation loss:		0.602297
  validation accuracy:		81.52 %
Epoch 1427 of 2000 took 0.035s
  training loss:		0.559662
  validation loss:		0.614754
  validation accuracy:		80.43 %
Epoch 1428 of 2000 took 0.035s
  training loss:		0.562167
  validation loss:		0.598258
  validation accuracy:		81.41 %
Epoch 1429 of 2000 took 0.035s
  training loss:		0.564090
  validation loss:		0.588989
  validation accuracy:		82.28 %
Epoch 1430 of 2000 took 0.035s
  training loss:		0.545757
  validation loss:		0.582089
  validation accuracy:		82.07 %
Epoch 1431 of 2000 took 0.035s
  training loss:		0.554796
  validation loss:		0.593806
  validation accuracy:		82.17 %
Epoch 1432 of 2000 took 0.036s
  training loss:		0.555382
  validation loss:		0.616402
  validation accuracy:		80.22 %
Epoch 1433 of 2000 took 0.035s
  training loss:		0.561985
  validation loss:		0.596638
  validation accuracy:		81.63 %
Epoch 1434 of 2000 took 0.035s
  training loss:		0.554626
  validation loss:		0.581781
  validation accuracy:		82.07 %
Epoch 1435 of 2000 took 0.035s
  training loss:		0.553991
  validation loss:		0.592269
  validation accuracy:		82.39 %
Epoch 1436 of 2000 took 0.035s
  training loss:		0.568775
  validation loss:		0.600934
  validation accuracy:		81.63 %
Epoch 1437 of 2000 took 0.035s
  training loss:		0.588259
  validation loss:		0.595200
  validation accuracy:		81.52 %
Epoch 1438 of 2000 took 0.035s
  training loss:		0.553565
  validation loss:		0.600665
  validation accuracy:		81.74 %
Epoch 1439 of 2000 took 0.035s
  training loss:		0.553600
  validation loss:		0.588769
  validation accuracy:		81.41 %
Epoch 1440 of 2000 took 0.035s
  training loss:		0.546219
  validation loss:		0.594714
  validation accuracy:		81.30 %
Epoch 1441 of 2000 took 0.035s
  training loss:		0.550119
  validation loss:		0.599460
  validation accuracy:		81.52 %
Epoch 1442 of 2000 took 0.035s
  training loss:		0.561336
  validation loss:		0.587583
  validation accuracy:		81.74 %
Epoch 1443 of 2000 took 0.035s
  training loss:		0.569209
  validation loss:		0.593664
  validation accuracy:		81.74 %
Epoch 1444 of 2000 took 0.035s
  training loss:		0.550594
  validation loss:		0.592046
  validation accuracy:		81.85 %
Epoch 1445 of 2000 took 0.035s
  training loss:		0.557491
  validation loss:		0.588484
  validation accuracy:		82.50 %
Epoch 1446 of 2000 took 0.035s
  training loss:		0.547425
  validation loss:		0.655526
  validation accuracy:		78.26 %
Epoch 1447 of 2000 took 0.035s
  training loss:		0.549904
  validation loss:		0.597744
  validation accuracy:		81.41 %
Epoch 1448 of 2000 took 0.035s
  training loss:		0.540658
  validation loss:		0.602011
  validation accuracy:		81.30 %
Epoch 1449 of 2000 took 0.035s
  training loss:		0.556616
  validation loss:		0.585194
  validation accuracy:		81.63 %
Epoch 1450 of 2000 took 0.036s
  training loss:		0.549814
  validation loss:		0.607534
  validation accuracy:		80.98 %
Epoch 1451 of 2000 took 0.035s
  training loss:		0.554698
  validation loss:		0.589960
  validation accuracy:		81.96 %
Epoch 1452 of 2000 took 0.035s
  training loss:		0.544275
  validation loss:		0.583018
  validation accuracy:		81.41 %
Epoch 1453 of 2000 took 0.036s
  training loss:		0.571996
  validation loss:		0.617259
  validation accuracy:		80.43 %
Epoch 1454 of 2000 took 0.035s
  training loss:		0.552797
  validation loss:		0.592584
  validation accuracy:		81.52 %
Epoch 1455 of 2000 took 0.035s
  training loss:		0.547938
  validation loss:		0.580758
  validation accuracy:		82.39 %
Epoch 1456 of 2000 took 0.035s
  training loss:		0.550304
  validation loss:		0.605182
  validation accuracy:		81.09 %
Epoch 1457 of 2000 took 0.035s
  training loss:		0.558541
  validation loss:		0.585436
  validation accuracy:		81.74 %
Epoch 1458 of 2000 took 0.035s
  training loss:		0.549455
  validation loss:		0.601194
  validation accuracy:		82.17 %
Epoch 1459 of 2000 took 0.035s
  training loss:		0.544050
  validation loss:		0.592073
  validation accuracy:		81.52 %
Epoch 1460 of 2000 took 0.035s
  training loss:		0.545959
  validation loss:		0.581735
  validation accuracy:		81.85 %
Epoch 1461 of 2000 took 0.035s
  training loss:		0.560300
  validation loss:		0.598369
  validation accuracy:		81.74 %
Epoch 1462 of 2000 took 0.035s
  training loss:		0.549691
  validation loss:		0.619073
  validation accuracy:		80.65 %
Epoch 1463 of 2000 took 0.035s
  training loss:		0.560998
  validation loss:		0.589771
  validation accuracy:		81.20 %
Epoch 1464 of 2000 took 0.035s
  training loss:		0.541909
  validation loss:		0.604387
  validation accuracy:		81.30 %
Epoch 1465 of 2000 took 0.035s
  training loss:		0.544542
  validation loss:		0.581277
  validation accuracy:		81.96 %
Epoch 1466 of 2000 took 0.035s
  training loss:		0.552511
  validation loss:		0.597287
  validation accuracy:		81.09 %
Epoch 1467 of 2000 took 0.035s
  training loss:		0.539121
  validation loss:		0.579636
  validation accuracy:		82.39 %
Epoch 1468 of 2000 took 0.035s
  training loss:		0.561957
  validation loss:		0.583515
  validation accuracy:		81.63 %
Epoch 1469 of 2000 took 0.035s
  training loss:		0.545958
  validation loss:		0.583583
  validation accuracy:		81.96 %
Epoch 1470 of 2000 took 0.035s
  training loss:		0.540295
  validation loss:		0.592019
  validation accuracy:		81.74 %
Epoch 1471 of 2000 took 0.035s
  training loss:		0.551163
  validation loss:		0.577930
  validation accuracy:		82.50 %
Epoch 1472 of 2000 took 0.035s
  training loss:		0.555241
  validation loss:		0.585386
  validation accuracy:		81.74 %
Epoch 1473 of 2000 took 0.035s
  training loss:		0.550257
  validation loss:		0.584634
  validation accuracy:		82.39 %
Epoch 1474 of 2000 took 0.035s
  training loss:		0.558087
  validation loss:		0.590447
  validation accuracy:		81.63 %
Epoch 1475 of 2000 took 0.035s
  training loss:		0.543511
  validation loss:		0.585558
  validation accuracy:		82.50 %
Epoch 1476 of 2000 took 0.035s
  training loss:		0.542822
  validation loss:		0.591079
  validation accuracy:		81.96 %
Epoch 1477 of 2000 took 0.035s
  training loss:		0.548517
  validation loss:		0.584435
  validation accuracy:		82.61 %
Epoch 1478 of 2000 took 0.035s
  training loss:		0.541727
  validation loss:		0.581465
  validation accuracy:		82.50 %
Epoch 1479 of 2000 took 0.036s
  training loss:		0.543106
  validation loss:		0.593418
  validation accuracy:		81.20 %
Epoch 1480 of 2000 took 0.035s
  training loss:		0.547558
  validation loss:		0.580678
  validation accuracy:		82.28 %
Epoch 1481 of 2000 took 0.035s
  training loss:		0.538605
  validation loss:		0.605052
  validation accuracy:		81.30 %
Epoch 1482 of 2000 took 0.035s
  training loss:		0.544593
  validation loss:		0.589434
  validation accuracy:		81.85 %
Epoch 1483 of 2000 took 0.035s
  training loss:		0.540640
  validation loss:		0.582371
  validation accuracy:		81.30 %
Epoch 1484 of 2000 took 0.035s
  training loss:		0.546564
  validation loss:		0.585284
  validation accuracy:		81.52 %
Epoch 1485 of 2000 took 0.035s
  training loss:		0.553994
  validation loss:		0.586839
  validation accuracy:		82.50 %
Epoch 1486 of 2000 took 0.035s
  training loss:		0.542121
  validation loss:		0.580439
  validation accuracy:		82.72 %
Epoch 1487 of 2000 took 0.035s
  training loss:		0.549273
  validation loss:		0.588823
  validation accuracy:		81.96 %
Epoch 1488 of 2000 took 0.035s
  training loss:		0.543995
  validation loss:		0.591768
  validation accuracy:		81.74 %
Epoch 1489 of 2000 took 0.035s
  training loss:		0.548255
  validation loss:		0.593017
  validation accuracy:		81.52 %
Epoch 1490 of 2000 took 0.035s
  training loss:		0.552374
  validation loss:		0.595723
  validation accuracy:		82.17 %
Epoch 1491 of 2000 took 0.035s
  training loss:		0.545626
  validation loss:		0.594940
  validation accuracy:		81.52 %
Epoch 1492 of 2000 took 0.035s
  training loss:		0.549750
  validation loss:		0.587662
  validation accuracy:		82.17 %
Epoch 1493 of 2000 took 0.035s
  training loss:		0.545680
  validation loss:		0.576895
  validation accuracy:		82.28 %
Epoch 1494 of 2000 took 0.035s
  training loss:		0.532579
  validation loss:		0.583944
  validation accuracy:		82.39 %
Epoch 1495 of 2000 took 0.035s
  training loss:		0.545658
  validation loss:		0.594568
  validation accuracy:		81.96 %
Epoch 1496 of 2000 took 0.035s
  training loss:		0.542188
  validation loss:		0.585205
  validation accuracy:		82.17 %
Epoch 1497 of 2000 took 0.035s
  training loss:		0.543090
  validation loss:		0.604291
  validation accuracy:		81.41 %
Epoch 1498 of 2000 took 0.035s
  training loss:		0.554708
  validation loss:		0.609688
  validation accuracy:		80.65 %
Epoch 1499 of 2000 took 0.035s
  training loss:		0.539812
  validation loss:		0.601548
  validation accuracy:		81.30 %
Epoch 1500 of 2000 took 0.035s
  training loss:		0.537755
  validation loss:		0.592724
  validation accuracy:		82.17 %
Epoch 1501 of 2000 took 0.035s
  training loss:		0.551864
  validation loss:		0.615354
  validation accuracy:		80.43 %
Epoch 1502 of 2000 took 0.035s
  training loss:		0.546243
  validation loss:		0.582733
  validation accuracy:		82.39 %
Epoch 1503 of 2000 took 0.035s
  training loss:		0.538309
  validation loss:		0.578800
  validation accuracy:		82.50 %
Epoch 1504 of 2000 took 0.035s
  training loss:		0.552409
  validation loss:		0.578110
  validation accuracy:		82.17 %
Epoch 1505 of 2000 took 0.035s
  training loss:		0.544826
  validation loss:		0.582451
  validation accuracy:		82.28 %
Epoch 1506 of 2000 took 0.035s
  training loss:		0.546773
  validation loss:		0.589017
  validation accuracy:		82.17 %
Epoch 1507 of 2000 took 0.035s
  training loss:		0.551829
  validation loss:		0.578973
  validation accuracy:		81.85 %
Epoch 1508 of 2000 took 0.035s
  training loss:		0.534616
  validation loss:		0.593842
  validation accuracy:		82.39 %
Epoch 1509 of 2000 took 0.036s
  training loss:		0.531831
  validation loss:		0.576657
  validation accuracy:		82.07 %
Epoch 1510 of 2000 took 0.035s
  training loss:		0.532606
  validation loss:		0.615102
  validation accuracy:		80.33 %
Epoch 1511 of 2000 took 0.035s
  training loss:		0.541421
  validation loss:		0.582312
  validation accuracy:		82.07 %
Epoch 1512 of 2000 took 0.035s
  training loss:		0.528781
  validation loss:		0.586161
  validation accuracy:		81.96 %
Epoch 1513 of 2000 took 0.036s
  training loss:		0.544712
  validation loss:		0.588563
  validation accuracy:		82.39 %
Epoch 1514 of 2000 took 0.035s
  training loss:		0.523914
  validation loss:		0.587051
  validation accuracy:		82.17 %
Epoch 1515 of 2000 took 0.035s
  training loss:		0.545922
  validation loss:		0.589250
  validation accuracy:		81.96 %
Epoch 1516 of 2000 took 0.037s
  training loss:		0.539358
  validation loss:		0.581629
  validation accuracy:		81.74 %
Epoch 1517 of 2000 took 0.036s
  training loss:		0.535432
  validation loss:		0.577216
  validation accuracy:		82.83 %
Epoch 1518 of 2000 took 0.035s
  training loss:		0.537436
  validation loss:		0.594163
  validation accuracy:		81.96 %
Epoch 1519 of 2000 took 0.035s
  training loss:		0.537494
  validation loss:		0.589521
  validation accuracy:		82.28 %
Epoch 1520 of 2000 took 0.035s
  training loss:		0.543339
  validation loss:		0.578417
  validation accuracy:		82.93 %
Epoch 1521 of 2000 took 0.035s
  training loss:		0.541675
  validation loss:		0.579993
  validation accuracy:		82.28 %
Epoch 1522 of 2000 took 0.036s
  training loss:		0.547001
  validation loss:		0.593742
  validation accuracy:		82.07 %
Epoch 1523 of 2000 took 0.035s
  training loss:		0.539124
  validation loss:		0.577939
  validation accuracy:		82.17 %
Epoch 1524 of 2000 took 0.035s
  training loss:		0.544815
  validation loss:		0.590870
  validation accuracy:		81.85 %
Epoch 1525 of 2000 took 0.035s
  training loss:		0.544471
  validation loss:		0.578972
  validation accuracy:		83.15 %
Epoch 1526 of 2000 took 0.035s
  training loss:		0.532433
  validation loss:		0.592570
  validation accuracy:		82.07 %
Epoch 1527 of 2000 took 0.035s
  training loss:		0.541299
  validation loss:		0.576431
  validation accuracy:		83.04 %
Epoch 1528 of 2000 took 0.035s
  training loss:		0.533588
  validation loss:		0.597863
  validation accuracy:		81.74 %
Epoch 1529 of 2000 took 0.035s
  training loss:		0.546258
  validation loss:		0.586611
  validation accuracy:		82.39 %
Epoch 1530 of 2000 took 0.035s
  training loss:		0.535762
  validation loss:		0.581527
  validation accuracy:		81.85 %
Epoch 1531 of 2000 took 0.035s
  training loss:		0.532668
  validation loss:		0.582336
  validation accuracy:		83.04 %
Epoch 1532 of 2000 took 0.035s
  training loss:		0.537120
  validation loss:		0.587787
  validation accuracy:		82.28 %
Epoch 1533 of 2000 took 0.035s
  training loss:		0.535667
  validation loss:		0.597509
  validation accuracy:		81.09 %
Epoch 1534 of 2000 took 0.035s
  training loss:		0.542953
  validation loss:		0.577937
  validation accuracy:		83.04 %
Epoch 1535 of 2000 took 0.035s
  training loss:		0.551836
  validation loss:		0.574278
  validation accuracy:		83.26 %
Epoch 1536 of 2000 took 0.036s
  training loss:		0.530190
  validation loss:		0.584111
  validation accuracy:		82.50 %
Epoch 1537 of 2000 took 0.035s
  training loss:		0.542206
  validation loss:		0.577941
  validation accuracy:		82.17 %
Epoch 1538 of 2000 took 0.035s
  training loss:		0.541043
  validation loss:		0.585882
  validation accuracy:		81.09 %
Epoch 1539 of 2000 took 0.035s
  training loss:		0.537178
  validation loss:		0.587527
  validation accuracy:		82.61 %
Epoch 1540 of 2000 took 0.035s
  training loss:		0.551453
  validation loss:		0.571987
  validation accuracy:		83.04 %
Epoch 1541 of 2000 took 0.035s
  training loss:		0.537623
  validation loss:		0.584004
  validation accuracy:		82.17 %
Epoch 1542 of 2000 took 0.035s
  training loss:		0.544196
  validation loss:		0.576941
  validation accuracy:		83.26 %
Epoch 1543 of 2000 took 0.035s
  training loss:		0.552603
  validation loss:		0.583657
  validation accuracy:		82.83 %
Epoch 1544 of 2000 took 0.035s
  training loss:		0.542648
  validation loss:		0.585514
  validation accuracy:		82.17 %
Epoch 1545 of 2000 took 0.036s
  training loss:		0.540911
  validation loss:		0.580839
  validation accuracy:		83.04 %
Epoch 1546 of 2000 took 0.035s
  training loss:		0.541465
  validation loss:		0.577212
  validation accuracy:		82.17 %
Epoch 1547 of 2000 took 0.035s
  training loss:		0.543865
  validation loss:		0.581231
  validation accuracy:		83.04 %
Epoch 1548 of 2000 took 0.035s
  training loss:		0.531451
  validation loss:		0.625589
  validation accuracy:		80.54 %
Epoch 1549 of 2000 took 0.035s
  training loss:		0.543771
  validation loss:		0.588211
  validation accuracy:		82.17 %
Epoch 1550 of 2000 took 0.035s
  training loss:		0.550722
  validation loss:		0.609560
  validation accuracy:		80.65 %
Epoch 1551 of 2000 took 0.035s
  training loss:		0.546048
  validation loss:		0.586786
  validation accuracy:		82.83 %
Epoch 1552 of 2000 took 0.035s
  training loss:		0.541548
  validation loss:		0.585621
  validation accuracy:		82.17 %
Epoch 1553 of 2000 took 0.035s
  training loss:		0.542116
  validation loss:		0.609550
  validation accuracy:		80.76 %
Epoch 1554 of 2000 took 0.035s
  training loss:		0.531838
  validation loss:		0.581221
  validation accuracy:		81.96 %
Epoch 1555 of 2000 took 0.035s
  training loss:		0.543293
  validation loss:		0.597347
  validation accuracy:		82.07 %
Epoch 1556 of 2000 took 0.035s
  training loss:		0.545375
  validation loss:		0.572672
  validation accuracy:		82.72 %
Epoch 1557 of 2000 took 0.035s
  training loss:		0.535183
  validation loss:		0.578062
  validation accuracy:		82.83 %
Epoch 1558 of 2000 took 0.035s
  training loss:		0.537635
  validation loss:		0.575357
  validation accuracy:		82.72 %
Epoch 1559 of 2000 took 0.035s
  training loss:		0.548536
  validation loss:		0.573006
  validation accuracy:		83.15 %
Epoch 1560 of 2000 took 0.035s
  training loss:		0.539419
  validation loss:		0.582245
  validation accuracy:		81.96 %
Epoch 1561 of 2000 took 0.035s
  training loss:		0.531537
  validation loss:		0.588658
  validation accuracy:		82.17 %
Epoch 1562 of 2000 took 0.035s
  training loss:		0.542703
  validation loss:		0.579110
  validation accuracy:		82.17 %
Epoch 1563 of 2000 took 0.035s
  training loss:		0.539992
  validation loss:		0.580203
  validation accuracy:		82.50 %
Epoch 1564 of 2000 took 0.035s
  training loss:		0.533650
  validation loss:		0.574275
  validation accuracy:		83.04 %
Epoch 1565 of 2000 took 0.035s
  training loss:		0.540260
  validation loss:		0.588452
  validation accuracy:		81.63 %
Epoch 1566 of 2000 took 0.036s
  training loss:		0.537388
  validation loss:		0.586528
  validation accuracy:		82.17 %
Epoch 1567 of 2000 took 0.035s
  training loss:		0.531543
  validation loss:		0.595487
  validation accuracy:		81.63 %
Epoch 1568 of 2000 took 0.035s
  training loss:		0.543444
  validation loss:		0.604210
  validation accuracy:		81.20 %
Epoch 1569 of 2000 took 0.035s
  training loss:		0.533629
  validation loss:		0.571664
  validation accuracy:		83.15 %
Epoch 1570 of 2000 took 0.035s
  training loss:		0.536583
  validation loss:		0.573105
  validation accuracy:		83.04 %
Epoch 1571 of 2000 took 0.035s
  training loss:		0.539819
  validation loss:		0.570516
  validation accuracy:		83.15 %
Epoch 1572 of 2000 took 0.035s
  training loss:		0.532204
  validation loss:		0.571693
  validation accuracy:		82.72 %
Epoch 1573 of 2000 took 0.035s
  training loss:		0.536338
  validation loss:		0.586703
  validation accuracy:		82.50 %
Epoch 1574 of 2000 took 0.035s
  training loss:		0.536774
  validation loss:		0.581899
  validation accuracy:		81.96 %
Epoch 1575 of 2000 took 0.036s
  training loss:		0.536233
  validation loss:		0.579600
  validation accuracy:		82.17 %
Epoch 1576 of 2000 took 0.036s
  training loss:		0.528232
  validation loss:		0.586799
  validation accuracy:		82.83 %
Epoch 1577 of 2000 took 0.035s
  training loss:		0.534997
  validation loss:		0.614363
  validation accuracy:		80.87 %
Epoch 1578 of 2000 took 0.035s
  training loss:		0.533989
  validation loss:		0.604958
  validation accuracy:		81.41 %
Epoch 1579 of 2000 took 0.035s
  training loss:		0.533756
  validation loss:		0.580180
  validation accuracy:		82.17 %
Epoch 1580 of 2000 took 0.035s
  training loss:		0.533083
  validation loss:		0.592538
  validation accuracy:		81.85 %
Epoch 1581 of 2000 took 0.035s
  training loss:		0.542924
  validation loss:		0.602526
  validation accuracy:		80.98 %
Epoch 1582 of 2000 took 0.035s
  training loss:		0.537990
  validation loss:		0.595882
  validation accuracy:		81.74 %
Epoch 1583 of 2000 took 0.035s
  training loss:		0.547704
  validation loss:		0.585439
  validation accuracy:		82.07 %
Epoch 1584 of 2000 took 0.035s
  training loss:		0.541597
  validation loss:		0.569541
  validation accuracy:		83.04 %
Epoch 1585 of 2000 took 0.035s
  training loss:		0.537397
  validation loss:		0.570896
  validation accuracy:		82.93 %
Epoch 1586 of 2000 took 0.035s
  training loss:		0.529982
  validation loss:		0.586109
  validation accuracy:		81.96 %
Epoch 1587 of 2000 took 0.035s
  training loss:		0.535599
  validation loss:		0.574968
  validation accuracy:		83.04 %
Epoch 1588 of 2000 took 0.035s
  training loss:		0.542263
  validation loss:		0.574576
  validation accuracy:		82.17 %
Epoch 1589 of 2000 took 0.035s
  training loss:		0.544259
  validation loss:		0.574095
  validation accuracy:		82.72 %
Epoch 1590 of 2000 took 0.035s
  training loss:		0.528221
  validation loss:		0.583030
  validation accuracy:		82.39 %
Epoch 1591 of 2000 took 0.035s
  training loss:		0.530149
  validation loss:		0.584600
  validation accuracy:		82.72 %
Epoch 1592 of 2000 took 0.035s
  training loss:		0.525055
  validation loss:		0.577016
  validation accuracy:		83.15 %
Epoch 1593 of 2000 took 0.035s
  training loss:		0.531576
  validation loss:		0.572258
  validation accuracy:		82.61 %
Epoch 1594 of 2000 took 0.035s
  training loss:		0.531433
  validation loss:		0.572287
  validation accuracy:		82.83 %
Epoch 1595 of 2000 took 0.035s
  training loss:		0.530003
  validation loss:		0.572746
  validation accuracy:		82.50 %
Epoch 1596 of 2000 took 0.035s
  training loss:		0.534688
  validation loss:		0.582723
  validation accuracy:		83.04 %
Epoch 1597 of 2000 took 0.035s
  training loss:		0.539199
  validation loss:		0.619784
  validation accuracy:		80.54 %
Epoch 1598 of 2000 took 0.035s
  training loss:		0.535800
  validation loss:		0.573837
  validation accuracy:		82.50 %
Epoch 1599 of 2000 took 0.035s
  training loss:		0.525103
  validation loss:		0.572871
  validation accuracy:		81.96 %
Epoch 1600 of 2000 took 0.035s
  training loss:		0.520017
  validation loss:		0.578856
  validation accuracy:		83.26 %
Epoch 1601 of 2000 took 0.035s
  training loss:		0.534839
  validation loss:		0.588640
  validation accuracy:		81.63 %
Epoch 1602 of 2000 took 0.036s
  training loss:		0.530157
  validation loss:		0.588966
  validation accuracy:		82.28 %
Epoch 1603 of 2000 took 0.035s
  training loss:		0.542287
  validation loss:		0.567611
  validation accuracy:		82.93 %
Epoch 1604 of 2000 took 0.035s
  training loss:		0.531283
  validation loss:		0.577180
  validation accuracy:		82.17 %
Epoch 1605 of 2000 took 0.035s
  training loss:		0.529908
  validation loss:		0.575777
  validation accuracy:		82.72 %
Epoch 1606 of 2000 took 0.035s
  training loss:		0.532469
  validation loss:		0.577323
  validation accuracy:		82.61 %
Epoch 1607 of 2000 took 0.035s
  training loss:		0.531629
  validation loss:		0.575546
  validation accuracy:		83.26 %
Epoch 1608 of 2000 took 0.035s
  training loss:		0.526444
  validation loss:		0.567877
  validation accuracy:		82.72 %
Epoch 1609 of 2000 took 0.035s
  training loss:		0.533189
  validation loss:		0.566588
  validation accuracy:		82.50 %
Epoch 1610 of 2000 took 0.035s
  training loss:		0.529011
  validation loss:		0.579255
  validation accuracy:		83.15 %
Epoch 1611 of 2000 took 0.035s
  training loss:		0.531270
  validation loss:		0.600789
  validation accuracy:		81.20 %
Epoch 1612 of 2000 took 0.035s
  training loss:		0.544367
  validation loss:		0.595586
  validation accuracy:		82.07 %
Epoch 1613 of 2000 took 0.035s
  training loss:		0.535460
  validation loss:		0.593918
  validation accuracy:		82.39 %
Epoch 1614 of 2000 took 0.035s
  training loss:		0.540702
  validation loss:		0.578466
  validation accuracy:		83.26 %
Epoch 1615 of 2000 took 0.035s
  training loss:		0.526247
  validation loss:		0.584505
  validation accuracy:		82.93 %
Epoch 1616 of 2000 took 0.035s
  training loss:		0.531929
  validation loss:		0.562312
  validation accuracy:		82.50 %
Epoch 1617 of 2000 took 0.035s
  training loss:		0.530050
  validation loss:		0.572987
  validation accuracy:		82.39 %
Epoch 1618 of 2000 took 0.035s
  training loss:		0.545828
  validation loss:		0.590158
  validation accuracy:		80.98 %
Epoch 1619 of 2000 took 0.035s
  training loss:		0.539322
  validation loss:		0.572058
  validation accuracy:		81.96 %
Epoch 1620 of 2000 took 0.035s
  training loss:		0.533550
  validation loss:		0.594454
  validation accuracy:		82.07 %
Epoch 1621 of 2000 took 0.035s
  training loss:		0.520835
  validation loss:		0.575036
  validation accuracy:		82.83 %
Epoch 1622 of 2000 took 0.036s
  training loss:		0.533437
  validation loss:		0.567307
  validation accuracy:		83.04 %
Epoch 1623 of 2000 took 0.035s
  training loss:		0.537382
  validation loss:		0.584117
  validation accuracy:		82.07 %
Epoch 1624 of 2000 took 0.035s
  training loss:		0.527706
  validation loss:		0.577708
  validation accuracy:		82.07 %
Epoch 1625 of 2000 took 0.035s
  training loss:		0.551250
  validation loss:		0.586037
  validation accuracy:		81.85 %
Epoch 1626 of 2000 took 0.035s
  training loss:		0.549132
  validation loss:		0.577271
  validation accuracy:		82.28 %
Epoch 1627 of 2000 took 0.035s
  training loss:		0.528313
  validation loss:		0.583706
  validation accuracy:		82.61 %
Epoch 1628 of 2000 took 0.035s
  training loss:		0.534772
  validation loss:		0.585593
  validation accuracy:		82.50 %
Epoch 1629 of 2000 took 0.035s
  training loss:		0.532342
  validation loss:		0.593736
  validation accuracy:		82.07 %
Epoch 1630 of 2000 took 0.035s
  training loss:		0.532237
  validation loss:		0.601206
  validation accuracy:		81.41 %
Epoch 1631 of 2000 took 0.035s
  training loss:		0.539741
  validation loss:		0.584410
  validation accuracy:		82.07 %
Epoch 1632 of 2000 took 0.035s
  training loss:		0.535645
  validation loss:		0.594308
  validation accuracy:		81.74 %
Epoch 1633 of 2000 took 0.035s
  training loss:		0.527572
  validation loss:		0.589002
  validation accuracy:		82.28 %
Epoch 1634 of 2000 took 0.035s
  training loss:		0.546613
  validation loss:		0.573459
  validation accuracy:		82.93 %
Epoch 1635 of 2000 took 0.036s
  training loss:		0.540414
  validation loss:		0.598399
  validation accuracy:		81.52 %
Epoch 1636 of 2000 took 0.035s
  training loss:		0.532786
  validation loss:		0.571921
  validation accuracy:		82.50 %
Epoch 1637 of 2000 took 0.035s
  training loss:		0.530304
  validation loss:		0.573443
  validation accuracy:		82.39 %
Epoch 1638 of 2000 took 0.035s
  training loss:		0.536653
  validation loss:		0.604050
  validation accuracy:		81.41 %
Epoch 1639 of 2000 took 0.035s
  training loss:		0.535446
  validation loss:		0.569176
  validation accuracy:		82.93 %
Epoch 1640 of 2000 took 0.035s
  training loss:		0.544114
  validation loss:		0.577071
  validation accuracy:		82.28 %
Epoch 1641 of 2000 took 0.035s
  training loss:		0.539599
  validation loss:		0.574414
  validation accuracy:		82.39 %
Epoch 1642 of 2000 took 0.035s
  training loss:		0.531261
  validation loss:		0.590131
  validation accuracy:		82.17 %
Epoch 1643 of 2000 took 0.035s
  training loss:		0.529721
  validation loss:		0.592163
  validation accuracy:		82.17 %
Epoch 1644 of 2000 took 0.035s
  training loss:		0.542861
  validation loss:		0.573277
  validation accuracy:		83.15 %
Epoch 1645 of 2000 took 0.035s
  training loss:		0.531738
  validation loss:		0.583570
  validation accuracy:		82.17 %
Epoch 1646 of 2000 took 0.035s
  training loss:		0.533511
  validation loss:		0.568725
  validation accuracy:		83.37 %
Epoch 1647 of 2000 took 0.035s
  training loss:		0.527155
  validation loss:		0.573250
  validation accuracy:		82.39 %
Epoch 1648 of 2000 took 0.035s
  training loss:		0.530568
  validation loss:		0.573071
  validation accuracy:		83.37 %
Epoch 1649 of 2000 took 0.035s
  training loss:		0.532131
  validation loss:		0.566400
  validation accuracy:		83.04 %
Epoch 1650 of 2000 took 0.035s
  training loss:		0.538816
  validation loss:		0.587817
  validation accuracy:		81.96 %
Epoch 1651 of 2000 took 0.035s
  training loss:		0.526079
  validation loss:		0.588204
  validation accuracy:		82.61 %
Epoch 1652 of 2000 took 0.035s
  training loss:		0.532065
  validation loss:		0.578938
  validation accuracy:		82.61 %
Epoch 1653 of 2000 took 0.035s
  training loss:		0.530125
  validation loss:		0.577299
  validation accuracy:		82.28 %
Epoch 1654 of 2000 took 0.035s
  training loss:		0.536941
  validation loss:		0.575816
  validation accuracy:		83.04 %
Epoch 1655 of 2000 took 0.035s
  training loss:		0.530289
  validation loss:		0.570874
  validation accuracy:		83.04 %
Epoch 1656 of 2000 took 0.035s
  training loss:		0.528082
  validation loss:		0.592987
  validation accuracy:		81.85 %
Epoch 1657 of 2000 took 0.035s
  training loss:		0.535180
  validation loss:		0.564221
  validation accuracy:		82.83 %
Epoch 1658 of 2000 took 0.036s
  training loss:		0.529428
  validation loss:		0.569642
  validation accuracy:		82.50 %
Epoch 1659 of 2000 took 0.035s
  training loss:		0.533976
  validation loss:		0.574723
  validation accuracy:		83.26 %
Epoch 1660 of 2000 took 0.035s
  training loss:		0.533604
  validation loss:		0.566976
  validation accuracy:		83.37 %
Epoch 1661 of 2000 took 0.035s
  training loss:		0.531532
  validation loss:		0.570254
  validation accuracy:		83.26 %
Epoch 1662 of 2000 took 0.035s
  training loss:		0.531742
  validation loss:		0.571514
  validation accuracy:		83.37 %
Epoch 1663 of 2000 took 0.035s
  training loss:		0.533141
  validation loss:		0.573421
  validation accuracy:		81.96 %
Epoch 1664 of 2000 took 0.035s
  training loss:		0.530982
  validation loss:		0.569884
  validation accuracy:		83.37 %
Epoch 1665 of 2000 took 0.035s
  training loss:		0.517606
  validation loss:		0.575094
  validation accuracy:		83.04 %
Epoch 1666 of 2000 took 0.035s
  training loss:		0.524034
  validation loss:		0.573339
  validation accuracy:		82.83 %
Epoch 1667 of 2000 took 0.035s
  training loss:		0.528717
  validation loss:		0.567423
  validation accuracy:		82.93 %
Epoch 1668 of 2000 took 0.035s
  training loss:		0.535597
  validation loss:		0.576158
  validation accuracy:		82.61 %
Epoch 1669 of 2000 took 0.035s
  training loss:		0.537596
  validation loss:		0.579113
  validation accuracy:		82.83 %
Epoch 1670 of 2000 took 0.035s
  training loss:		0.531300
  validation loss:		0.575122
  validation accuracy:		82.93 %
Epoch 1671 of 2000 took 0.035s
  training loss:		0.528075
  validation loss:		0.584759
  validation accuracy:		82.39 %
Epoch 1672 of 2000 took 0.035s
  training loss:		0.529572
  validation loss:		0.571509
  validation accuracy:		83.15 %
Epoch 1673 of 2000 took 0.035s
  training loss:		0.524620
  validation loss:		0.579482
  validation accuracy:		82.50 %
Epoch 1674 of 2000 took 0.035s
  training loss:		0.520375
  validation loss:		0.568579
  validation accuracy:		82.83 %
Epoch 1675 of 2000 took 0.035s
  training loss:		0.526265
  validation loss:		0.577068
  validation accuracy:		82.72 %
Epoch 1676 of 2000 took 0.035s
  training loss:		0.526580
  validation loss:		0.569332
  validation accuracy:		82.93 %
Epoch 1677 of 2000 took 0.035s
  training loss:		0.534820
  validation loss:		0.571228
  validation accuracy:		82.61 %
Epoch 1678 of 2000 took 0.036s
  training loss:		0.526115
  validation loss:		0.574615
  validation accuracy:		82.93 %
Epoch 1679 of 2000 took 0.035s
  training loss:		0.535355
  validation loss:		0.580143
  validation accuracy:		82.61 %
Epoch 1680 of 2000 took 0.036s
  training loss:		0.533503
  validation loss:		0.568440
  validation accuracy:		82.72 %
Epoch 1681 of 2000 took 0.035s
  training loss:		0.531924
  validation loss:		0.562219
  validation accuracy:		83.15 %
Epoch 1682 of 2000 took 0.035s
  training loss:		0.529265
  validation loss:		0.566515
  validation accuracy:		83.26 %
Epoch 1683 of 2000 took 0.035s
  training loss:		0.525112
  validation loss:		0.580057
  validation accuracy:		82.39 %
Epoch 1684 of 2000 took 0.035s
  training loss:		0.529830
  validation loss:		0.571106
  validation accuracy:		82.93 %
Epoch 1685 of 2000 took 0.035s
  training loss:		0.525637
  validation loss:		0.571594
  validation accuracy:		82.83 %
Epoch 1686 of 2000 took 0.036s
  training loss:		0.529658
  validation loss:		0.572887
  validation accuracy:		82.61 %
Epoch 1687 of 2000 took 0.036s
  training loss:		0.537229
  validation loss:		0.582257
  validation accuracy:		82.72 %
Epoch 1688 of 2000 took 0.035s
  training loss:		0.526457
  validation loss:		0.585303
  validation accuracy:		82.07 %
Epoch 1689 of 2000 took 0.035s
  training loss:		0.527079
  validation loss:		0.583713
  validation accuracy:		82.93 %
Epoch 1690 of 2000 took 0.035s
  training loss:		0.536693
  validation loss:		0.576258
  validation accuracy:		82.72 %
Epoch 1691 of 2000 took 0.035s
  training loss:		0.525397
  validation loss:		0.579760
  validation accuracy:		82.50 %
Epoch 1692 of 2000 took 0.035s
  training loss:		0.531506
  validation loss:		0.564478
  validation accuracy:		83.04 %
Epoch 1693 of 2000 took 0.035s
  training loss:		0.536117
  validation loss:		0.564037
  validation accuracy:		83.26 %
Epoch 1694 of 2000 took 0.035s
  training loss:		0.526641
  validation loss:		0.574399
  validation accuracy:		82.72 %
Epoch 1695 of 2000 took 0.035s
  training loss:		0.532960
  validation loss:		0.576213
  validation accuracy:		82.50 %
Epoch 1696 of 2000 took 0.035s
  training loss:		0.533507
  validation loss:		0.575776
  validation accuracy:		82.72 %
Epoch 1697 of 2000 took 0.035s
  training loss:		0.533013
  validation loss:		0.589275
  validation accuracy:		82.39 %
Epoch 1698 of 2000 took 0.035s
  training loss:		0.536041
  validation loss:		0.565522
  validation accuracy:		82.93 %
Epoch 1699 of 2000 took 0.035s
  training loss:		0.523343
  validation loss:		0.573707
  validation accuracy:		82.50 %
Epoch 1700 of 2000 took 0.035s
  training loss:		0.524454
  validation loss:		0.582706
  validation accuracy:		82.50 %
Epoch 1701 of 2000 took 0.035s
  training loss:		0.531800
  validation loss:		0.564646
  validation accuracy:		83.15 %
Epoch 1702 of 2000 took 0.035s
  training loss:		0.535746
  validation loss:		0.565641
  validation accuracy:		83.04 %
Epoch 1703 of 2000 took 0.035s
  training loss:		0.533309
  validation loss:		0.581410
  validation accuracy:		82.28 %
Epoch 1704 of 2000 took 0.036s
  training loss:		0.530984
  validation loss:		0.569836
  validation accuracy:		83.04 %
Epoch 1705 of 2000 took 0.035s
  training loss:		0.529611
  validation loss:		0.563237
  validation accuracy:		83.26 %
Epoch 1706 of 2000 took 0.035s
  training loss:		0.526263
  validation loss:		0.585821
  validation accuracy:		82.72 %
Epoch 1707 of 2000 took 0.035s
  training loss:		0.525584
  validation loss:		0.565644
  validation accuracy:		82.93 %
Epoch 1708 of 2000 took 0.035s
  training loss:		0.531328
  validation loss:		0.572001
  validation accuracy:		82.50 %
Epoch 1709 of 2000 took 0.035s
  training loss:		0.530477
  validation loss:		0.579723
  validation accuracy:		82.72 %
Epoch 1710 of 2000 took 0.035s
  training loss:		0.527428
  validation loss:		0.573074
  validation accuracy:		82.50 %
Epoch 1711 of 2000 took 0.035s
  training loss:		0.526821
  validation loss:		0.562321
  validation accuracy:		83.04 %
Epoch 1712 of 2000 took 0.035s
  training loss:		0.526823
  validation loss:		0.560581
  validation accuracy:		82.83 %
Epoch 1713 of 2000 took 0.035s
  training loss:		0.526738
  validation loss:		0.578189
  validation accuracy:		82.72 %
Epoch 1714 of 2000 took 0.035s
  training loss:		0.526022
  validation loss:		0.580434
  validation accuracy:		82.72 %
Epoch 1715 of 2000 took 0.035s
  training loss:		0.534476
  validation loss:		0.571935
  validation accuracy:		82.61 %
Epoch 1716 of 2000 took 0.035s
  training loss:		0.532603
  validation loss:		0.571482
  validation accuracy:		82.50 %
Epoch 1717 of 2000 took 0.035s
  training loss:		0.527845
  validation loss:		0.587222
  validation accuracy:		82.07 %
Epoch 1718 of 2000 took 0.035s
  training loss:		0.531627
  validation loss:		0.566901
  validation accuracy:		82.72 %
Epoch 1719 of 2000 took 0.035s
  training loss:		0.529609
  validation loss:		0.571095
  validation accuracy:		82.72 %
Epoch 1720 of 2000 took 0.035s
  training loss:		0.525696
  validation loss:		0.568332
  validation accuracy:		82.83 %
Epoch 1721 of 2000 took 0.035s
  training loss:		0.525197
  validation loss:		0.577509
  validation accuracy:		82.61 %
Epoch 1722 of 2000 took 0.035s
  training loss:		0.521917
  validation loss:		0.573611
  validation accuracy:		82.28 %
Epoch 1723 of 2000 took 0.035s
  training loss:		0.523781
  validation loss:		0.565678
  validation accuracy:		82.93 %
Epoch 1724 of 2000 took 0.035s
  training loss:		0.526213
  validation loss:		0.584825
  validation accuracy:		82.17 %
Epoch 1725 of 2000 took 0.035s
  training loss:		0.531241
  validation loss:		0.561645
  validation accuracy:		83.15 %
Epoch 1726 of 2000 took 0.035s
  training loss:		0.541190
  validation loss:		0.589355
  validation accuracy:		81.85 %
Epoch 1727 of 2000 took 0.035s
  training loss:		0.532638
  validation loss:		0.561917
  validation accuracy:		82.93 %
Epoch 1728 of 2000 took 0.035s
  training loss:		0.525590
  validation loss:		0.572124
  validation accuracy:		82.93 %
Epoch 1729 of 2000 took 0.035s
  training loss:		0.524856
  validation loss:		0.565949
  validation accuracy:		82.93 %
Epoch 1730 of 2000 took 0.035s
  training loss:		0.527712
  validation loss:		0.570375
  validation accuracy:		82.39 %
Epoch 1731 of 2000 took 0.035s
  training loss:		0.528897
  validation loss:		0.600723
  validation accuracy:		81.74 %
Epoch 1732 of 2000 took 0.035s
  training loss:		0.537481
  validation loss:		0.563197
  validation accuracy:		83.04 %
Epoch 1733 of 2000 took 0.035s
  training loss:		0.530010
  validation loss:		0.575450
  validation accuracy:		82.50 %
Epoch 1734 of 2000 took 0.035s
  training loss:		0.531448
  validation loss:		0.571288
  validation accuracy:		82.83 %
Epoch 1735 of 2000 took 0.036s
  training loss:		0.537087
  validation loss:		0.568086
  validation accuracy:		82.61 %
Epoch 1736 of 2000 took 0.035s
  training loss:		0.534152
  validation loss:		0.564782
  validation accuracy:		82.93 %
Epoch 1737 of 2000 took 0.035s
  training loss:		0.524668
  validation loss:		0.578192
  validation accuracy:		82.17 %
Epoch 1738 of 2000 took 0.035s
  training loss:		0.531343
  validation loss:		0.573322
  validation accuracy:		82.61 %
Epoch 1739 of 2000 took 0.035s
  training loss:		0.522148
  validation loss:		0.586921
  validation accuracy:		81.96 %
Epoch 1740 of 2000 took 0.035s
  training loss:		0.536721
  validation loss:		0.562085
  validation accuracy:		83.04 %
Epoch 1741 of 2000 took 0.035s
  training loss:		0.534794
  validation loss:		0.569577
  validation accuracy:		82.50 %
Epoch 1742 of 2000 took 0.035s
  training loss:		0.533426
  validation loss:		0.564095
  validation accuracy:		82.83 %
Epoch 1743 of 2000 took 0.035s
  training loss:		0.516850
  validation loss:		0.559092
  validation accuracy:		83.37 %
Epoch 1744 of 2000 took 0.035s
  training loss:		0.522853
  validation loss:		0.573781
  validation accuracy:		83.15 %
Epoch 1745 of 2000 took 0.035s
  training loss:		0.526717
  validation loss:		0.569025
  validation accuracy:		82.50 %
Epoch 1746 of 2000 took 0.035s
  training loss:		0.519413
  validation loss:		0.571368
  validation accuracy:		82.72 %
Epoch 1747 of 2000 took 0.035s
  training loss:		0.532795
  validation loss:		0.564811
  validation accuracy:		83.26 %
Epoch 1748 of 2000 took 0.035s
  training loss:		0.524434
  validation loss:		0.571061
  validation accuracy:		82.83 %
Epoch 1749 of 2000 took 0.036s
  training loss:		0.529279
  validation loss:		0.575763
  validation accuracy:		82.61 %
Epoch 1750 of 2000 took 0.035s
  training loss:		0.528035
  validation loss:		0.571091
  validation accuracy:		83.04 %
Epoch 1751 of 2000 took 0.035s
  training loss:		0.534858
  validation loss:		0.575829
  validation accuracy:		81.96 %
Epoch 1752 of 2000 took 0.035s
  training loss:		0.522208
  validation loss:		0.565086
  validation accuracy:		82.72 %
Epoch 1753 of 2000 took 0.035s
  training loss:		0.533367
  validation loss:		0.592185
  validation accuracy:		81.41 %
Epoch 1754 of 2000 took 0.035s
  training loss:		0.522214
  validation loss:		0.557932
  validation accuracy:		83.15 %
Epoch 1755 of 2000 took 0.035s
  training loss:		0.528879
  validation loss:		0.595592
  validation accuracy:		81.63 %
Epoch 1756 of 2000 took 0.035s
  training loss:		0.531576
  validation loss:		0.574649
  validation accuracy:		82.39 %
Epoch 1757 of 2000 took 0.035s
  training loss:		0.520346
  validation loss:		0.560844
  validation accuracy:		82.93 %
Epoch 1758 of 2000 took 0.035s
  training loss:		0.529939
  validation loss:		0.576808
  validation accuracy:		82.50 %
Epoch 1759 of 2000 took 0.036s
  training loss:		0.518336
  validation loss:		0.561495
  validation accuracy:		82.83 %
Epoch 1760 of 2000 took 0.035s
  training loss:		0.527362
  validation loss:		0.582756
  validation accuracy:		82.39 %
Epoch 1761 of 2000 took 0.035s
  training loss:		0.535167
  validation loss:		0.572925
  validation accuracy:		82.72 %
Epoch 1762 of 2000 took 0.035s
  training loss:		0.529177
  validation loss:		0.574297
  validation accuracy:		82.17 %
Epoch 1763 of 2000 took 0.035s
  training loss:		0.528330
  validation loss:		0.585998
  validation accuracy:		81.63 %
Epoch 1764 of 2000 took 0.035s
  training loss:		0.530207
  validation loss:		0.564847
  validation accuracy:		82.83 %
Epoch 1765 of 2000 took 0.035s
  training loss:		0.527566
  validation loss:		0.564715
  validation accuracy:		82.93 %
Epoch 1766 of 2000 took 0.035s
  training loss:		0.536228
  validation loss:		0.578264
  validation accuracy:		82.28 %
Epoch 1767 of 2000 took 0.035s
  training loss:		0.528294
  validation loss:		0.589813
  validation accuracy:		81.96 %
Epoch 1768 of 2000 took 0.035s
  training loss:		0.522184
  validation loss:		0.559784
  validation accuracy:		82.93 %
Epoch 1769 of 2000 took 0.035s
  training loss:		0.523269
  validation loss:		0.569990
  validation accuracy:		82.61 %
Epoch 1770 of 2000 took 0.035s
  training loss:		0.525044
  validation loss:		0.565333
  validation accuracy:		82.50 %
Epoch 1771 of 2000 took 0.036s
  training loss:		0.521248
  validation loss:		0.570589
  validation accuracy:		82.50 %
Epoch 1772 of 2000 took 0.035s
  training loss:		0.519856
  validation loss:		0.582715
  validation accuracy:		81.63 %
Epoch 1773 of 2000 took 0.035s
  training loss:		0.525243
  validation loss:		0.586312
  validation accuracy:		81.52 %
Epoch 1774 of 2000 took 0.035s
  training loss:		0.531164
  validation loss:		0.566216
  validation accuracy:		83.15 %
Epoch 1775 of 2000 took 0.035s
  training loss:		0.519036
  validation loss:		0.562124
  validation accuracy:		82.72 %
Epoch 1776 of 2000 took 0.035s
  training loss:		0.530848
  validation loss:		0.561128
  validation accuracy:		82.83 %
Epoch 1777 of 2000 took 0.035s
  training loss:		0.521624
  validation loss:		0.559066
  validation accuracy:		82.83 %
Epoch 1778 of 2000 took 0.035s
  training loss:		0.542407
  validation loss:		0.611811
  validation accuracy:		81.09 %
Epoch 1779 of 2000 took 0.035s
  training loss:		0.532659
  validation loss:		0.565191
  validation accuracy:		82.72 %
Epoch 1780 of 2000 took 0.035s
  training loss:		0.525315
  validation loss:		0.585072
  validation accuracy:		82.17 %
Epoch 1781 of 2000 took 0.035s
  training loss:		0.532086
  validation loss:		0.566521
  validation accuracy:		82.72 %
Epoch 1782 of 2000 took 0.035s
  training loss:		0.524490
  validation loss:		0.568460
  validation accuracy:		82.39 %
Epoch 1783 of 2000 took 0.035s
  training loss:		0.530145
  validation loss:		0.576640
  validation accuracy:		82.50 %
Epoch 1784 of 2000 took 0.035s
  training loss:		0.523734
  validation loss:		0.582103
  validation accuracy:		81.96 %
Epoch 1785 of 2000 took 0.035s
  training loss:		0.521629
  validation loss:		0.570978
  validation accuracy:		82.50 %
Epoch 1786 of 2000 took 0.035s
  training loss:		0.525526
  validation loss:		0.572190
  validation accuracy:		82.28 %
Epoch 1787 of 2000 took 0.035s
  training loss:		0.524334
  validation loss:		0.572286
  validation accuracy:		82.39 %
Epoch 1788 of 2000 took 0.036s
  training loss:		0.531164
  validation loss:		0.562559
  validation accuracy:		83.37 %
Epoch 1789 of 2000 took 0.035s
  training loss:		0.527357
  validation loss:		0.570000
  validation accuracy:		82.83 %
Epoch 1790 of 2000 took 0.035s
  training loss:		0.525123
  validation loss:		0.564821
  validation accuracy:		83.04 %
Epoch 1791 of 2000 took 0.036s
  training loss:		0.523944
  validation loss:		0.562663
  validation accuracy:		82.93 %
Epoch 1792 of 2000 took 0.035s
  training loss:		0.528537
  validation loss:		0.560454
  validation accuracy:		82.93 %
Epoch 1793 of 2000 took 0.035s
  training loss:		0.523097
  validation loss:		0.571709
  validation accuracy:		82.39 %
Epoch 1794 of 2000 took 0.035s
  training loss:		0.515172
  validation loss:		0.572817
  validation accuracy:		82.50 %
Epoch 1795 of 2000 took 0.035s
  training loss:		0.521864
  validation loss:		0.566392
  validation accuracy:		82.83 %
Epoch 1796 of 2000 took 0.035s
  training loss:		0.527251
  validation loss:		0.562764
  validation accuracy:		82.61 %
Epoch 1797 of 2000 took 0.035s
  training loss:		0.524832
  validation loss:		0.556029
  validation accuracy:		82.93 %
Epoch 1798 of 2000 took 0.035s
  training loss:		0.531565
  validation loss:		0.571270
  validation accuracy:		82.39 %
Epoch 1799 of 2000 took 0.036s
  training loss:		0.525623
  validation loss:		0.563884
  validation accuracy:		83.15 %
Epoch 1800 of 2000 took 0.035s
  training loss:		0.522183
  validation loss:		0.568573
  validation accuracy:		82.72 %
Epoch 1801 of 2000 took 0.035s
  training loss:		0.523588
  validation loss:		0.592146
  validation accuracy:		81.74 %
Epoch 1802 of 2000 took 0.035s
  training loss:		0.541481
  validation loss:		0.566918
  validation accuracy:		83.04 %
Epoch 1803 of 2000 took 0.035s
  training loss:		0.525093
  validation loss:		0.573301
  validation accuracy:		82.61 %
Epoch 1804 of 2000 took 0.035s
  training loss:		0.521606
  validation loss:		0.558555
  validation accuracy:		82.83 %
Epoch 1805 of 2000 took 0.035s
  training loss:		0.525499
  validation loss:		0.568850
  validation accuracy:		82.39 %
Epoch 1806 of 2000 took 0.035s
  training loss:		0.528130
  validation loss:		0.591522
  validation accuracy:		81.96 %
Epoch 1807 of 2000 took 0.035s
  training loss:		0.524038
  validation loss:		0.564532
  validation accuracy:		82.72 %
Epoch 1808 of 2000 took 0.035s
  training loss:		0.520541
  validation loss:		0.563319
  validation accuracy:		82.72 %
Epoch 1809 of 2000 took 0.035s
  training loss:		0.523854
  validation loss:		0.561501
  validation accuracy:		83.04 %
Epoch 1810 of 2000 took 0.035s
  training loss:		0.537962
  validation loss:		0.568793
  validation accuracy:		82.83 %
Epoch 1811 of 2000 took 0.035s
  training loss:		0.534030
  validation loss:		0.558225
  validation accuracy:		82.83 %
Epoch 1812 of 2000 took 0.035s
  training loss:		0.523281
  validation loss:		0.558924
  validation accuracy:		82.83 %
Epoch 1813 of 2000 took 0.035s
  training loss:		0.524106
  validation loss:		0.595775
  validation accuracy:		81.52 %
Epoch 1814 of 2000 took 0.035s
  training loss:		0.518444
  validation loss:		0.566057
  validation accuracy:		82.83 %
Epoch 1815 of 2000 took 0.035s
  training loss:		0.531952
  validation loss:		0.556467
  validation accuracy:		83.26 %
Epoch 1816 of 2000 took 0.036s
  training loss:		0.531868
  validation loss:		0.558401
  validation accuracy:		82.93 %
Epoch 1817 of 2000 took 0.037s
  training loss:		0.527508
  validation loss:		0.578099
  validation accuracy:		81.63 %
Epoch 1818 of 2000 took 0.037s
  training loss:		0.531622
  validation loss:		0.570820
  validation accuracy:		82.07 %
Epoch 1819 of 2000 took 0.037s
  training loss:		0.529776
  validation loss:		0.564065
  validation accuracy:		83.15 %
Epoch 1820 of 2000 took 0.037s
  training loss:		0.528288
  validation loss:		0.566036
  validation accuracy:		82.72 %
Epoch 1821 of 2000 took 0.037s
  training loss:		0.524429
  validation loss:		0.561812
  validation accuracy:		82.93 %
Epoch 1822 of 2000 took 0.037s
  training loss:		0.527716
  validation loss:		0.575257
  validation accuracy:		81.85 %
Epoch 1823 of 2000 took 0.036s
  training loss:		0.523446
  validation loss:		0.558210
  validation accuracy:		82.83 %
Epoch 1824 of 2000 took 0.036s
  training loss:		0.515190
  validation loss:		0.569511
  validation accuracy:		83.37 %
Epoch 1825 of 2000 took 0.036s
  training loss:		0.520252
  validation loss:		0.564919
  validation accuracy:		82.61 %
Epoch 1826 of 2000 took 0.036s
  training loss:		0.521099
  validation loss:		0.573355
  validation accuracy:		82.17 %
Epoch 1827 of 2000 took 0.036s
  training loss:		0.530336
  validation loss:		0.600767
  validation accuracy:		81.09 %
Epoch 1828 of 2000 took 0.036s
  training loss:		0.522858
  validation loss:		0.564719
  validation accuracy:		82.72 %
Epoch 1829 of 2000 took 0.036s
  training loss:		0.518004
  validation loss:		0.565130
  validation accuracy:		83.04 %
Epoch 1830 of 2000 took 0.036s
  training loss:		0.526909
  validation loss:		0.559557
  validation accuracy:		83.15 %
Epoch 1831 of 2000 took 0.036s
  training loss:		0.524259
  validation loss:		0.572679
  validation accuracy:		82.50 %
Epoch 1832 of 2000 took 0.036s
  training loss:		0.525512
  validation loss:		0.581186
  validation accuracy:		81.30 %
Epoch 1833 of 2000 took 0.036s
  training loss:		0.535708
  validation loss:		0.579042
  validation accuracy:		81.74 %
Epoch 1834 of 2000 took 0.036s
  training loss:		0.529939
  validation loss:		0.559215
  validation accuracy:		82.61 %
Epoch 1835 of 2000 took 0.036s
  training loss:		0.523209
  validation loss:		0.579084
  validation accuracy:		81.85 %
Epoch 1836 of 2000 took 0.036s
  training loss:		0.522615
  validation loss:		0.563429
  validation accuracy:		82.72 %
Epoch 1837 of 2000 took 0.036s
  training loss:		0.527081
  validation loss:		0.563563
  validation accuracy:		82.72 %
Epoch 1838 of 2000 took 0.036s
  training loss:		0.528225
  validation loss:		0.571229
  validation accuracy:		82.61 %
Epoch 1839 of 2000 took 0.036s
  training loss:		0.529394
  validation loss:		0.583796
  validation accuracy:		81.74 %
Epoch 1840 of 2000 took 0.035s
  training loss:		0.527506
  validation loss:		0.574042
  validation accuracy:		82.07 %
Epoch 1841 of 2000 took 0.035s
  training loss:		0.529797
  validation loss:		0.577231
  validation accuracy:		82.07 %
Epoch 1842 of 2000 took 0.035s
  training loss:		0.525668
  validation loss:		0.567447
  validation accuracy:		82.61 %
Epoch 1843 of 2000 took 0.035s
  training loss:		0.513900
  validation loss:		0.563589
  validation accuracy:		82.72 %
Epoch 1844 of 2000 took 0.035s
  training loss:		0.530454
  validation loss:		0.565958
  validation accuracy:		83.15 %
Epoch 1845 of 2000 took 0.035s
  training loss:		0.526986
  validation loss:		0.572804
  validation accuracy:		82.39 %
Epoch 1846 of 2000 took 0.036s
  training loss:		0.517285
  validation loss:		0.562919
  validation accuracy:		82.72 %
Epoch 1847 of 2000 took 0.036s
  training loss:		0.523876
  validation loss:		0.563491
  validation accuracy:		82.72 %
Epoch 1848 of 2000 took 0.035s
  training loss:		0.528019
  validation loss:		0.568323
  validation accuracy:		82.61 %
Epoch 1849 of 2000 took 0.035s
  training loss:		0.521148
  validation loss:		0.556176
  validation accuracy:		83.15 %
Epoch 1850 of 2000 took 0.035s
  training loss:		0.526295
  validation loss:		0.558812
  validation accuracy:		82.93 %
Epoch 1851 of 2000 took 0.035s
  training loss:		0.527588
  validation loss:		0.554938
  validation accuracy:		83.15 %
Epoch 1852 of 2000 took 0.035s
  training loss:		0.528271
  validation loss:		0.578792
  validation accuracy:		82.28 %
Epoch 1853 of 2000 took 0.035s
  training loss:		0.525356
  validation loss:		0.572728
  validation accuracy:		82.07 %
Epoch 1854 of 2000 took 0.035s
  training loss:		0.528359
  validation loss:		0.565835
  validation accuracy:		82.50 %
Epoch 1855 of 2000 took 0.035s
  training loss:		0.525573
  validation loss:		0.566520
  validation accuracy:		82.39 %
Epoch 1856 of 2000 took 0.035s
  training loss:		0.534046
  validation loss:		0.567282
  validation accuracy:		82.17 %
Epoch 1857 of 2000 took 0.035s
  training loss:		0.516024
  validation loss:		0.563002
  validation accuracy:		82.61 %
Epoch 1858 of 2000 took 0.035s
  training loss:		0.524663
  validation loss:		0.561788
  validation accuracy:		82.50 %
Epoch 1859 of 2000 took 0.035s
  training loss:		0.520947
  validation loss:		0.565080
  validation accuracy:		82.50 %
Epoch 1860 of 2000 took 0.035s
  training loss:		0.521434
  validation loss:		0.574144
  validation accuracy:		82.07 %
Epoch 1861 of 2000 took 0.035s
  training loss:		0.533341
  validation loss:		0.569607
  validation accuracy:		82.72 %
Epoch 1862 of 2000 took 0.035s
  training loss:		0.525571
  validation loss:		0.577048
  validation accuracy:		82.39 %
Epoch 1863 of 2000 took 0.035s
  training loss:		0.531294
  validation loss:		0.559073
  validation accuracy:		82.93 %
Epoch 1864 of 2000 took 0.035s
  training loss:		0.532720
  validation loss:		0.561960
  validation accuracy:		82.50 %
Epoch 1865 of 2000 took 0.035s
  training loss:		0.521935
  validation loss:		0.554245
  validation accuracy:		82.72 %
Epoch 1866 of 2000 took 0.035s
  training loss:		0.542151
  validation loss:		0.584860
  validation accuracy:		81.74 %
Epoch 1867 of 2000 took 0.035s
  training loss:		0.521590
  validation loss:		0.568429
  validation accuracy:		82.50 %
Epoch 1868 of 2000 took 0.035s
  training loss:		0.518734
  validation loss:		0.570587
  validation accuracy:		82.28 %
Epoch 1869 of 2000 took 0.035s
  training loss:		0.515570
  validation loss:		0.558229
  validation accuracy:		82.72 %
Epoch 1870 of 2000 took 0.035s
  training loss:		0.528729
  validation loss:		0.565210
  validation accuracy:		82.72 %
Epoch 1871 of 2000 took 0.035s
  training loss:		0.526000
  validation loss:		0.572617
  validation accuracy:		81.96 %
Epoch 1872 of 2000 took 0.035s
  training loss:		0.524161
  validation loss:		0.586441
  validation accuracy:		81.63 %
Epoch 1873 of 2000 took 0.035s
  training loss:		0.547950
  validation loss:		0.570514
  validation accuracy:		82.39 %
Epoch 1874 of 2000 took 0.035s
  training loss:		0.513127
  validation loss:		0.559904
  validation accuracy:		82.50 %
Epoch 1875 of 2000 took 0.035s
  training loss:		0.524088
  validation loss:		0.577440
  validation accuracy:		81.96 %
Epoch 1876 of 2000 took 0.035s
  training loss:		0.526426
  validation loss:		0.565807
  validation accuracy:		82.61 %
Epoch 1877 of 2000 took 0.036s
  training loss:		0.520920
  validation loss:		0.578319
  validation accuracy:		81.96 %
Epoch 1878 of 2000 took 0.035s
  training loss:		0.532212
  validation loss:		0.554751
  validation accuracy:		82.61 %
Epoch 1879 of 2000 took 0.035s
  training loss:		0.530137
  validation loss:		0.560559
  validation accuracy:		82.61 %
Epoch 1880 of 2000 took 0.035s
  training loss:		0.523095
  validation loss:		0.566352
  validation accuracy:		82.50 %
Epoch 1881 of 2000 took 0.035s
  training loss:		0.526601
  validation loss:		0.572544
  validation accuracy:		82.61 %
Epoch 1882 of 2000 took 0.035s
  training loss:		0.525698
  validation loss:		0.582994
  validation accuracy:		81.74 %
Epoch 1883 of 2000 took 0.036s
  training loss:		0.531082
  validation loss:		0.557309
  validation accuracy:		82.83 %
Epoch 1884 of 2000 took 0.035s
  training loss:		0.527425
  validation loss:		0.618589
  validation accuracy:		80.76 %
Epoch 1885 of 2000 took 0.035s
  training loss:		0.533833
  validation loss:		0.557613
  validation accuracy:		83.04 %
Epoch 1886 of 2000 took 0.035s
  training loss:		0.522149
  validation loss:		0.558454
  validation accuracy:		82.72 %
Epoch 1887 of 2000 took 0.035s
  training loss:		0.522763
  validation loss:		0.583000
  validation accuracy:		81.63 %
Epoch 1888 of 2000 took 0.035s
  training loss:		0.527731
  validation loss:		0.563282
  validation accuracy:		82.50 %
Epoch 1889 of 2000 took 0.035s
  training loss:		0.512678
  validation loss:		0.591071
  validation accuracy:		81.30 %
Epoch 1890 of 2000 took 0.035s
  training loss:		0.524006
  validation loss:		0.563608
  validation accuracy:		82.61 %
Epoch 1891 of 2000 took 0.035s
  training loss:		0.521824
  validation loss:		0.569655
  validation accuracy:		82.50 %
Epoch 1892 of 2000 took 0.035s
  training loss:		0.522375
  validation loss:		0.571458
  validation accuracy:		82.28 %
Epoch 1893 of 2000 took 0.035s
  training loss:		0.523161
  validation loss:		0.572477
  validation accuracy:		82.39 %
Epoch 1894 of 2000 took 0.036s
  training loss:		0.517147
  validation loss:		0.580087
  validation accuracy:		81.63 %
Epoch 1895 of 2000 took 0.035s
  training loss:		0.523188
  validation loss:		0.553558
  validation accuracy:		83.04 %
Epoch 1896 of 2000 took 0.035s
  training loss:		0.522383
  validation loss:		0.582038
  validation accuracy:		81.63 %
Epoch 1897 of 2000 took 0.035s
  training loss:		0.519412
  validation loss:		0.573860
  validation accuracy:		82.61 %
Epoch 1898 of 2000 took 0.035s
  training loss:		0.523209
  validation loss:		0.564905
  validation accuracy:		82.50 %
Epoch 1899 of 2000 took 0.035s
  training loss:		0.517317
  validation loss:		0.578624
  validation accuracy:		82.28 %
Epoch 1900 of 2000 took 0.036s
  training loss:		0.533128
  validation loss:		0.572130
  validation accuracy:		82.28 %
Epoch 1901 of 2000 took 0.036s
  training loss:		0.519687
  validation loss:		0.555584
  validation accuracy:		83.48 %
Epoch 1902 of 2000 took 0.035s
  training loss:		0.517939
  validation loss:		0.569104
  validation accuracy:		82.17 %
Epoch 1903 of 2000 took 0.035s
  training loss:		0.533455
  validation loss:		0.571582
  validation accuracy:		82.17 %
Epoch 1904 of 2000 took 0.036s
  training loss:		0.516827
  validation loss:		0.584877
  validation accuracy:		81.96 %
Epoch 1905 of 2000 took 0.035s
  training loss:		0.523531
  validation loss:		0.565076
  validation accuracy:		82.72 %
Epoch 1906 of 2000 took 0.035s
  training loss:		0.519604
  validation loss:		0.558540
  validation accuracy:		82.61 %
Epoch 1907 of 2000 took 0.035s
  training loss:		0.522547
  validation loss:		0.566335
  validation accuracy:		82.28 %
Epoch 1908 of 2000 took 0.035s
  training loss:		0.525373
  validation loss:		0.559539
  validation accuracy:		82.61 %
Epoch 1909 of 2000 took 0.035s
  training loss:		0.519431
  validation loss:		0.576484
  validation accuracy:		81.74 %
Epoch 1910 of 2000 took 0.035s
  training loss:		0.522720
  validation loss:		0.575154
  validation accuracy:		82.07 %
Epoch 1911 of 2000 took 0.036s
  training loss:		0.533319
  validation loss:		0.567949
  validation accuracy:		82.17 %
Epoch 1912 of 2000 took 0.035s
  training loss:		0.523067
  validation loss:		0.572675
  validation accuracy:		82.07 %
Epoch 1913 of 2000 took 0.035s
  training loss:		0.523104
  validation loss:		0.567271
  validation accuracy:		82.39 %
Epoch 1914 of 2000 took 0.035s
  training loss:		0.517142
  validation loss:		0.572335
  validation accuracy:		81.96 %
Epoch 1915 of 2000 took 0.035s
  training loss:		0.519543
  validation loss:		0.553506
  validation accuracy:		82.83 %
Epoch 1916 of 2000 took 0.035s
  training loss:		0.519143
  validation loss:		0.560073
  validation accuracy:		82.83 %
Epoch 1917 of 2000 took 0.036s
  training loss:		0.513446
  validation loss:		0.561146
  validation accuracy:		82.17 %
Epoch 1918 of 2000 took 0.035s
  training loss:		0.524253
  validation loss:		0.553934
  validation accuracy:		83.15 %
Epoch 1919 of 2000 took 0.035s
  training loss:		0.521593
  validation loss:		0.561418
  validation accuracy:		82.50 %
Epoch 1920 of 2000 took 0.035s
  training loss:		0.518847
  validation loss:		0.578115
  validation accuracy:		81.96 %
Epoch 1921 of 2000 took 0.035s
  training loss:		0.523431
  validation loss:		0.570242
  validation accuracy:		82.17 %
Epoch 1922 of 2000 took 0.035s
  training loss:		0.517257
  validation loss:		0.557886
  validation accuracy:		82.83 %
Epoch 1923 of 2000 took 0.035s
  training loss:		0.515584
  validation loss:		0.556949
  validation accuracy:		82.83 %
Epoch 1924 of 2000 took 0.035s
  training loss:		0.519438
  validation loss:		0.566546
  validation accuracy:		82.39 %
Epoch 1925 of 2000 took 0.035s
  training loss:		0.518845
  validation loss:		0.566015
  validation accuracy:		82.83 %
Epoch 1926 of 2000 took 0.035s
  training loss:		0.524559
  validation loss:		0.560871
  validation accuracy:		82.72 %
Epoch 1927 of 2000 took 0.035s
  training loss:		0.518354
  validation loss:		0.568006
  validation accuracy:		82.07 %
Epoch 1928 of 2000 took 0.036s
  training loss:		0.524147
  validation loss:		0.565605
  validation accuracy:		82.39 %
Epoch 1929 of 2000 took 0.035s
  training loss:		0.531179
  validation loss:		0.564527
  validation accuracy:		82.83 %
Epoch 1930 of 2000 took 0.035s
  training loss:		0.523148
  validation loss:		0.564564
  validation accuracy:		82.72 %
Epoch 1931 of 2000 took 0.035s
  training loss:		0.514788
  validation loss:		0.557072
  validation accuracy:		82.72 %
Epoch 1932 of 2000 took 0.035s
  training loss:		0.524969
  validation loss:		0.552089
  validation accuracy:		83.04 %
Epoch 1933 of 2000 took 0.035s
  training loss:		0.518851
  validation loss:		0.563803
  validation accuracy:		82.28 %
Epoch 1934 of 2000 took 0.035s
  training loss:		0.516272
  validation loss:		0.577151
  validation accuracy:		82.07 %
Epoch 1935 of 2000 took 0.035s
  training loss:		0.526405
  validation loss:		0.573330
  validation accuracy:		82.07 %
Epoch 1936 of 2000 took 0.035s
  training loss:		0.523855
  validation loss:		0.565122
  validation accuracy:		82.39 %
Epoch 1937 of 2000 took 0.035s
  training loss:		0.515589
  validation loss:		0.559811
  validation accuracy:		82.83 %
Epoch 1938 of 2000 took 0.035s
  training loss:		0.522311
  validation loss:		0.571616
  validation accuracy:		82.28 %
Epoch 1939 of 2000 took 0.035s
  training loss:		0.520017
  validation loss:		0.568688
  validation accuracy:		82.28 %
Epoch 1940 of 2000 took 0.036s
  training loss:		0.519335
  validation loss:		0.569543
  validation accuracy:		82.50 %
Epoch 1941 of 2000 took 0.035s
  training loss:		0.517855
  validation loss:		0.572202
  validation accuracy:		82.28 %
Epoch 1942 of 2000 took 0.035s
  training loss:		0.528623
  validation loss:		0.569947
  validation accuracy:		82.17 %
Epoch 1943 of 2000 took 0.035s
  training loss:		0.524470
  validation loss:		0.575183
  validation accuracy:		81.96 %
Epoch 1944 of 2000 took 0.035s
  training loss:		0.531541
  validation loss:		0.557631
  validation accuracy:		82.50 %
Epoch 1945 of 2000 took 0.035s
  training loss:		0.532571
  validation loss:		0.565213
  validation accuracy:		82.61 %
Epoch 1946 of 2000 took 0.035s
  training loss:		0.519858
  validation loss:		0.570704
  validation accuracy:		82.07 %
Epoch 1947 of 2000 took 0.035s
  training loss:		0.522644
  validation loss:		0.579252
  validation accuracy:		81.63 %
Epoch 1948 of 2000 took 0.035s
  training loss:		0.524621
  validation loss:		0.554618
  validation accuracy:		82.93 %
Epoch 1949 of 2000 took 0.035s
  training loss:		0.521193
  validation loss:		0.557516
  validation accuracy:		82.93 %
Epoch 1950 of 2000 took 0.035s
  training loss:		0.519076
  validation loss:		0.554831
  validation accuracy:		82.93 %
Epoch 1951 of 2000 took 0.035s
  training loss:		0.518500
  validation loss:		0.562059
  validation accuracy:		82.17 %
Epoch 1952 of 2000 took 0.035s
  training loss:		0.517834
  validation loss:		0.563494
  validation accuracy:		82.39 %
Epoch 1953 of 2000 took 0.035s
  training loss:		0.522095
  validation loss:		0.568327
  validation accuracy:		82.28 %
Epoch 1954 of 2000 took 0.035s
  training loss:		0.519551
  validation loss:		0.548224
  validation accuracy:		83.15 %
Epoch 1955 of 2000 took 0.035s
  training loss:		0.518684
  validation loss:		0.566529
  validation accuracy:		82.50 %
Epoch 1956 of 2000 took 0.035s
  training loss:		0.527399
  validation loss:		0.552199
  validation accuracy:		83.04 %
Epoch 1957 of 2000 took 0.035s
  training loss:		0.519304
  validation loss:		0.558322
  validation accuracy:		82.61 %
Epoch 1958 of 2000 took 0.035s
  training loss:		0.515244
  validation loss:		0.551390
  validation accuracy:		83.04 %
Epoch 1959 of 2000 took 0.035s
  training loss:		0.522520
  validation loss:		0.558433
  validation accuracy:		82.28 %
Epoch 1960 of 2000 took 0.036s
  training loss:		0.523431
  validation loss:		0.577531
  validation accuracy:		82.07 %
Epoch 1961 of 2000 took 0.035s
  training loss:		0.526583
  validation loss:		0.573794
  validation accuracy:		82.17 %
Epoch 1962 of 2000 took 0.035s
  training loss:		0.525899
  validation loss:		0.575607
  validation accuracy:		81.63 %
Epoch 1963 of 2000 took 0.035s
  training loss:		0.523794
  validation loss:		0.580610
  validation accuracy:		81.63 %
Epoch 1964 of 2000 took 0.035s
  training loss:		0.529858
  validation loss:		0.555907
  validation accuracy:		82.93 %
Epoch 1965 of 2000 took 0.035s
  training loss:		0.523015
  validation loss:		0.563579
  validation accuracy:		82.17 %
Epoch 1966 of 2000 took 0.035s
  training loss:		0.528943
  validation loss:		0.565405
  validation accuracy:		82.39 %
Epoch 1967 of 2000 took 0.035s
  training loss:		0.511363
  validation loss:		0.565123
  validation accuracy:		81.85 %
Epoch 1968 of 2000 took 0.035s
  training loss:		0.519596
  validation loss:		0.581450
  validation accuracy:		82.28 %
Epoch 1969 of 2000 took 0.035s
  training loss:		0.522901
  validation loss:		0.555902
  validation accuracy:		82.39 %
Epoch 1970 of 2000 took 0.035s
  training loss:		0.517752
  validation loss:		0.557656
  validation accuracy:		82.72 %
Epoch 1971 of 2000 took 0.035s
  training loss:		0.519464
  validation loss:		0.560660
  validation accuracy:		82.39 %
Epoch 1972 of 2000 took 0.035s
  training loss:		0.524615
  validation loss:		0.553871
  validation accuracy:		82.72 %
Epoch 1973 of 2000 took 0.035s
  training loss:		0.515315
  validation loss:		0.562011
  validation accuracy:		82.39 %
Epoch 1974 of 2000 took 0.036s
  training loss:		0.520537
  validation loss:		0.560167
  validation accuracy:		82.50 %
Epoch 1975 of 2000 took 0.035s
  training loss:		0.526239
  validation loss:		0.551754
  validation accuracy:		82.93 %
Epoch 1976 of 2000 took 0.035s
  training loss:		0.515376
  validation loss:		0.569610
  validation accuracy:		82.07 %
Epoch 1977 of 2000 took 0.035s
  training loss:		0.522280
  validation loss:		0.574704
  validation accuracy:		81.96 %
Epoch 1978 of 2000 took 0.035s
  training loss:		0.515843
  validation loss:		0.562120
  validation accuracy:		82.83 %
Epoch 1979 of 2000 took 0.035s
  training loss:		0.525576
  validation loss:		0.571672
  validation accuracy:		82.07 %
Epoch 1980 of 2000 took 0.035s
  training loss:		0.527722
  validation loss:		0.551615
  validation accuracy:		83.04 %
Epoch 1981 of 2000 took 0.035s
  training loss:		0.519718
  validation loss:		0.562851
  validation accuracy:		82.39 %
Epoch 1982 of 2000 took 0.035s
  training loss:		0.512022
  validation loss:		0.552471
  validation accuracy:		82.93 %
Epoch 1983 of 2000 took 0.035s
  training loss:		0.518217
  validation loss:		0.564138
  validation accuracy:		82.61 %
Epoch 1984 of 2000 took 0.035s
  training loss:		0.525829
  validation loss:		0.566462
  validation accuracy:		82.07 %
Epoch 1985 of 2000 took 0.035s
  training loss:		0.520276
  validation loss:		0.567764
  validation accuracy:		81.96 %
Epoch 1986 of 2000 took 0.035s
  training loss:		0.524739
  validation loss:		0.564751
  validation accuracy:		82.50 %
Epoch 1987 of 2000 took 0.036s
  training loss:		0.522656
  validation loss:		0.559157
  validation accuracy:		82.72 %
Epoch 1988 of 2000 took 0.035s
  training loss:		0.512660
  validation loss:		0.563998
  validation accuracy:		82.39 %
Epoch 1989 of 2000 took 0.035s
  training loss:		0.517099
  validation loss:		0.561014
  validation accuracy:		82.93 %
Epoch 1990 of 2000 took 0.035s
  training loss:		0.515889
  validation loss:		0.588123
  validation accuracy:		81.41 %
Epoch 1991 of 2000 took 0.035s
  training loss:		0.526179
  validation loss:		0.555952
  validation accuracy:		82.83 %
Epoch 1992 of 2000 took 0.035s
  training loss:		0.517786
  validation loss:		0.555497
  validation accuracy:		82.83 %
Epoch 1993 of 2000 took 0.035s
  training loss:		0.525238
  validation loss:		0.551443
  validation accuracy:		82.93 %
Epoch 1994 of 2000 took 0.035s
  training loss:		0.527881
  validation loss:		0.556241
  validation accuracy:		82.61 %
Epoch 1995 of 2000 took 0.035s
  training loss:		0.514098
  validation loss:		0.555344
  validation accuracy:		82.72 %
Epoch 1996 of 2000 took 0.036s
  training loss:		0.514490
  validation loss:		0.557448
  validation accuracy:		82.72 %
Epoch 1997 of 2000 took 0.035s
  training loss:		0.517744
  validation loss:		0.552813
  validation accuracy:		83.04 %
Epoch 1998 of 2000 took 0.035s
  training loss:		0.510958
  validation loss:		0.571571
  validation accuracy:		82.28 %
Epoch 1999 of 2000 took 0.035s
  training loss:		0.527815
  validation loss:		0.552238
  validation accuracy:		83.37 %
Epoch 2000 of 2000 took 0.035s
  training loss:		0.525803
  validation loss:		0.555469
  validation accuracy:		82.72 %
Final results:
  test loss:			0.871339
  test accuracy:		73.07 %
