Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (3, 50, 50)...
decomposing tensor B of shape (3, 50)...
Starting training...
Epoch 1 of 2000 took 0.100s
  training loss:		2.988380
  validation loss:		2.973458
  validation accuracy:		0.00 %
Epoch 2 of 2000 took 0.096s
  training loss:		2.964775
  validation loss:		2.937299
  validation accuracy:		12.17 %
Epoch 3 of 2000 took 0.096s
  training loss:		2.934684
  validation loss:		2.896497
  validation accuracy:		12.83 %
Epoch 4 of 2000 took 0.097s
  training loss:		2.902368
  validation loss:		2.853014
  validation accuracy:		13.04 %
Epoch 5 of 2000 took 0.097s
  training loss:		2.867080
  validation loss:		2.806486
  validation accuracy:		13.04 %
Epoch 6 of 2000 took 0.097s
  training loss:		2.830254
  validation loss:		2.756035
  validation accuracy:		12.72 %
Epoch 7 of 2000 took 0.097s
  training loss:		2.788436
  validation loss:		2.699151
  validation accuracy:		12.93 %
Epoch 8 of 2000 took 0.097s
  training loss:		2.742365
  validation loss:		2.636416
  validation accuracy:		12.93 %
Epoch 9 of 2000 took 0.098s
  training loss:		2.691283
  validation loss:		2.568480
  validation accuracy:		12.93 %
Epoch 10 of 2000 took 0.097s
  training loss:		2.637931
  validation loss:		2.499366
  validation accuracy:		12.93 %
Epoch 11 of 2000 took 0.098s
  training loss:		2.587038
  validation loss:		2.432970
  validation accuracy:		12.93 %
Epoch 12 of 2000 took 0.097s
  training loss:		2.537037
  validation loss:		2.375733
  validation accuracy:		12.93 %
Epoch 13 of 2000 took 0.097s
  training loss:		2.484849
  validation loss:		2.329418
  validation accuracy:		12.93 %
Epoch 14 of 2000 took 0.097s
  training loss:		2.435959
  validation loss:		2.294209
  validation accuracy:		14.13 %
Epoch 15 of 2000 took 0.097s
  training loss:		2.399592
  validation loss:		2.271937
  validation accuracy:		15.22 %
Epoch 16 of 2000 took 0.096s
  training loss:		2.366150
  validation loss:		2.263897
  validation accuracy:		17.83 %
Epoch 17 of 2000 took 0.097s
  training loss:		2.340661
  validation loss:		2.264593
  validation accuracy:		12.93 %
Epoch 18 of 2000 took 0.097s
  training loss:		2.326474
  validation loss:		2.265088
  validation accuracy:		12.93 %
Epoch 19 of 2000 took 0.097s
  training loss:		2.316183
  validation loss:		2.261648
  validation accuracy:		19.57 %
Epoch 20 of 2000 took 0.097s
  training loss:		2.309770
  validation loss:		2.254227
  validation accuracy:		13.37 %
Epoch 21 of 2000 took 0.096s
  training loss:		2.306206
  validation loss:		2.252247
  validation accuracy:		13.04 %
Epoch 22 of 2000 took 0.096s
  training loss:		2.302552
  validation loss:		2.248469
  validation accuracy:		13.26 %
Epoch 23 of 2000 took 0.096s
  training loss:		2.301555
  validation loss:		2.248106
  validation accuracy:		13.59 %
Epoch 24 of 2000 took 0.097s
  training loss:		2.299371
  validation loss:		2.246045
  validation accuracy:		13.15 %
Epoch 25 of 2000 took 0.097s
  training loss:		2.298182
  validation loss:		2.246309
  validation accuracy:		13.04 %
Epoch 26 of 2000 took 0.096s
  training loss:		2.297717
  validation loss:		2.244636
  validation accuracy:		13.48 %
Epoch 27 of 2000 took 0.097s
  training loss:		2.297751
  validation loss:		2.249866
  validation accuracy:		13.04 %
Epoch 28 of 2000 took 0.097s
  training loss:		2.296620
  validation loss:		2.246366
  validation accuracy:		21.74 %
Epoch 29 of 2000 took 0.096s
  training loss:		2.296003
  validation loss:		2.244673
  validation accuracy:		13.04 %
Epoch 30 of 2000 took 0.097s
  training loss:		2.296683
  validation loss:		2.245212
  validation accuracy:		18.91 %
Epoch 31 of 2000 took 0.097s
  training loss:		2.296347
  validation loss:		2.248101
  validation accuracy:		13.26 %
Epoch 32 of 2000 took 0.097s
  training loss:		2.294990
  validation loss:		2.242027
  validation accuracy:		13.04 %
Epoch 33 of 2000 took 0.100s
  training loss:		2.294244
  validation loss:		2.241555
  validation accuracy:		18.26 %
Epoch 34 of 2000 took 0.097s
  training loss:		2.294772
  validation loss:		2.244262
  validation accuracy:		17.83 %
Epoch 35 of 2000 took 0.097s
  training loss:		2.294466
  validation loss:		2.242133
  validation accuracy:		13.04 %
Epoch 36 of 2000 took 0.097s
  training loss:		2.293844
  validation loss:		2.242874
  validation accuracy:		19.35 %
Epoch 37 of 2000 took 0.097s
  training loss:		2.293036
  validation loss:		2.241808
  validation accuracy:		13.04 %
Epoch 38 of 2000 took 0.097s
  training loss:		2.294878
  validation loss:		2.249530
  validation accuracy:		20.76 %
Epoch 39 of 2000 took 0.097s
  training loss:		2.292589
  validation loss:		2.242832
  validation accuracy:		13.15 %
Epoch 40 of 2000 took 0.097s
  training loss:		2.293195
  validation loss:		2.240940
  validation accuracy:		17.72 %
Epoch 41 of 2000 took 0.097s
  training loss:		2.292241
  validation loss:		2.238504
  validation accuracy:		16.85 %
Epoch 42 of 2000 took 0.097s
  training loss:		2.291729
  validation loss:		2.241136
  validation accuracy:		15.54 %
Epoch 43 of 2000 took 0.097s
  training loss:		2.292887
  validation loss:		2.239759
  validation accuracy:		13.04 %
Epoch 44 of 2000 took 0.097s
  training loss:		2.293195
  validation loss:		2.240901
  validation accuracy:		16.96 %
Epoch 45 of 2000 took 0.097s
  training loss:		2.292735
  validation loss:		2.241907
  validation accuracy:		17.61 %
Epoch 46 of 2000 took 0.097s
  training loss:		2.292193
  validation loss:		2.237720
  validation accuracy:		14.46 %
Epoch 47 of 2000 took 0.097s
  training loss:		2.292698
  validation loss:		2.245617
  validation accuracy:		17.93 %
Epoch 48 of 2000 took 0.097s
  training loss:		2.291627
  validation loss:		2.242466
  validation accuracy:		15.87 %
Epoch 49 of 2000 took 0.096s
  training loss:		2.292344
  validation loss:		2.236945
  validation accuracy:		19.24 %
Epoch 50 of 2000 took 0.097s
  training loss:		2.292149
  validation loss:		2.242377
  validation accuracy:		13.04 %
Epoch 51 of 2000 took 0.097s
  training loss:		2.290591
  validation loss:		2.239499
  validation accuracy:		18.37 %
Epoch 52 of 2000 took 0.097s
  training loss:		2.291194
  validation loss:		2.237878
  validation accuracy:		18.70 %
Epoch 53 of 2000 took 0.097s
  training loss:		2.290953
  validation loss:		2.235638
  validation accuracy:		19.02 %
Epoch 54 of 2000 took 0.097s
  training loss:		2.290906
  validation loss:		2.238160
  validation accuracy:		15.22 %
Epoch 55 of 2000 took 0.097s
  training loss:		2.290926
  validation loss:		2.243214
  validation accuracy:		20.33 %
Epoch 56 of 2000 took 0.097s
  training loss:		2.291419
  validation loss:		2.242358
  validation accuracy:		13.26 %
Epoch 57 of 2000 took 0.097s
  training loss:		2.291240
  validation loss:		2.241581
  validation accuracy:		13.15 %
Epoch 58 of 2000 took 0.097s
  training loss:		2.290366
  validation loss:		2.231571
  validation accuracy:		13.04 %
Epoch 59 of 2000 took 0.097s
  training loss:		2.289728
  validation loss:		2.237817
  validation accuracy:		17.39 %
Epoch 60 of 2000 took 0.096s
  training loss:		2.290263
  validation loss:		2.242771
  validation accuracy:		16.30 %
Epoch 61 of 2000 took 0.097s
  training loss:		2.290274
  validation loss:		2.234988
  validation accuracy:		20.87 %
Epoch 62 of 2000 took 0.097s
  training loss:		2.288152
  validation loss:		2.237824
  validation accuracy:		23.04 %
Epoch 63 of 2000 took 0.097s
  training loss:		2.289195
  validation loss:		2.238734
  validation accuracy:		13.70 %
Epoch 64 of 2000 took 0.097s
  training loss:		2.290438
  validation loss:		2.233570
  validation accuracy:		18.59 %
Epoch 65 of 2000 took 0.097s
  training loss:		2.288709
  validation loss:		2.239127
  validation accuracy:		20.43 %
Epoch 66 of 2000 took 0.097s
  training loss:		2.289667
  validation loss:		2.239639
  validation accuracy:		19.57 %
Epoch 67 of 2000 took 0.097s
  training loss:		2.289378
  validation loss:		2.238165
  validation accuracy:		15.76 %
Epoch 68 of 2000 took 0.097s
  training loss:		2.287762
  validation loss:		2.234826
  validation accuracy:		17.17 %
Epoch 69 of 2000 took 0.097s
  training loss:		2.289203
  validation loss:		2.229621
  validation accuracy:		13.80 %
Epoch 70 of 2000 took 0.096s
  training loss:		2.287660
  validation loss:		2.237048
  validation accuracy:		15.76 %
Epoch 71 of 2000 took 0.097s
  training loss:		2.288048
  validation loss:		2.244010
  validation accuracy:		23.59 %
Epoch 72 of 2000 took 0.097s
  training loss:		2.286533
  validation loss:		2.229534
  validation accuracy:		16.85 %
Epoch 73 of 2000 took 0.097s
  training loss:		2.287672
  validation loss:		2.225702
  validation accuracy:		19.57 %
Epoch 74 of 2000 took 0.097s
  training loss:		2.285302
  validation loss:		2.235907
  validation accuracy:		13.15 %
Epoch 75 of 2000 took 0.097s
  training loss:		2.286358
  validation loss:		2.238286
  validation accuracy:		18.48 %
Epoch 76 of 2000 took 0.097s
  training loss:		2.286193
  validation loss:		2.229146
  validation accuracy:		18.48 %
Epoch 77 of 2000 took 0.097s
  training loss:		2.286273
  validation loss:		2.237369
  validation accuracy:		13.26 %
Epoch 78 of 2000 took 0.097s
  training loss:		2.283984
  validation loss:		2.234751
  validation accuracy:		23.48 %
Epoch 79 of 2000 took 0.097s
  training loss:		2.286478
  validation loss:		2.228871
  validation accuracy:		18.48 %
Epoch 80 of 2000 took 0.096s
  training loss:		2.285248
  validation loss:		2.237493
  validation accuracy:		18.91 %
Epoch 81 of 2000 took 0.097s
  training loss:		2.285004
  validation loss:		2.239840
  validation accuracy:		21.96 %
Epoch 82 of 2000 took 0.097s
  training loss:		2.284336
  validation loss:		2.228813
  validation accuracy:		13.15 %
Epoch 83 of 2000 took 0.097s
  training loss:		2.284542
  validation loss:		2.232634
  validation accuracy:		23.15 %
Epoch 84 of 2000 took 0.097s
  training loss:		2.284557
  validation loss:		2.234926
  validation accuracy:		14.13 %
Epoch 85 of 2000 took 0.097s
  training loss:		2.283905
  validation loss:		2.227448
  validation accuracy:		21.63 %
Epoch 86 of 2000 took 0.097s
  training loss:		2.283253
  validation loss:		2.234450
  validation accuracy:		14.13 %
Epoch 87 of 2000 took 0.097s
  training loss:		2.283608
  validation loss:		2.229491
  validation accuracy:		16.74 %
Epoch 88 of 2000 took 0.097s
  training loss:		2.283178
  validation loss:		2.225545
  validation accuracy:		15.00 %
Epoch 89 of 2000 took 0.099s
  training loss:		2.280957
  validation loss:		2.232260
  validation accuracy:		24.13 %
Epoch 90 of 2000 took 0.097s
  training loss:		2.282710
  validation loss:		2.230972
  validation accuracy:		17.28 %
Epoch 91 of 2000 took 0.097s
  training loss:		2.281819
  validation loss:		2.226039
  validation accuracy:		19.24 %
Epoch 92 of 2000 took 0.097s
  training loss:		2.280722
  validation loss:		2.233459
  validation accuracy:		18.04 %
Epoch 93 of 2000 took 0.097s
  training loss:		2.281734
  validation loss:		2.232667
  validation accuracy:		20.76 %
Epoch 94 of 2000 took 0.097s
  training loss:		2.280466
  validation loss:		2.227370
  validation accuracy:		19.67 %
Epoch 95 of 2000 took 0.097s
  training loss:		2.279568
  validation loss:		2.220505
  validation accuracy:		17.61 %
Epoch 96 of 2000 took 0.097s
  training loss:		2.279617
  validation loss:		2.230054
  validation accuracy:		23.04 %
Epoch 97 of 2000 took 0.097s
  training loss:		2.279724
  validation loss:		2.232952
  validation accuracy:		14.35 %
Epoch 98 of 2000 took 0.097s
  training loss:		2.278689
  validation loss:		2.222576
  validation accuracy:		22.61 %
Epoch 99 of 2000 took 0.097s
  training loss:		2.278801
  validation loss:		2.226131
  validation accuracy:		20.87 %
Epoch 100 of 2000 took 0.097s
  training loss:		2.277362
  validation loss:		2.219611
  validation accuracy:		13.48 %
Epoch 101 of 2000 took 0.096s
  training loss:		2.278424
  validation loss:		2.225855
  validation accuracy:		20.11 %
Epoch 102 of 2000 took 0.097s
  training loss:		2.275972
  validation loss:		2.224396
  validation accuracy:		20.22 %
Epoch 103 of 2000 took 0.097s
  training loss:		2.275474
  validation loss:		2.217231
  validation accuracy:		20.65 %
Epoch 104 of 2000 took 0.097s
  training loss:		2.275592
  validation loss:		2.223018
  validation accuracy:		23.37 %
Epoch 105 of 2000 took 0.097s
  training loss:		2.275307
  validation loss:		2.226146
  validation accuracy:		19.78 %
Epoch 106 of 2000 took 0.097s
  training loss:		2.273447
  validation loss:		2.212915
  validation accuracy:		25.22 %
Epoch 107 of 2000 took 0.097s
  training loss:		2.273267
  validation loss:		2.219510
  validation accuracy:		18.04 %
Epoch 108 of 2000 took 0.097s
  training loss:		2.272525
  validation loss:		2.227391
  validation accuracy:		21.09 %
Epoch 109 of 2000 took 0.097s
  training loss:		2.270722
  validation loss:		2.210556
  validation accuracy:		21.63 %
Epoch 110 of 2000 took 0.097s
  training loss:		2.270477
  validation loss:		2.215985
  validation accuracy:		20.22 %
Epoch 111 of 2000 took 0.097s
  training loss:		2.269720
  validation loss:		2.217953
  validation accuracy:		19.35 %
Epoch 112 of 2000 took 0.097s
  training loss:		2.267623
  validation loss:		2.212726
  validation accuracy:		19.24 %
Epoch 113 of 2000 took 0.097s
  training loss:		2.267872
  validation loss:		2.210655
  validation accuracy:		21.41 %
Epoch 114 of 2000 took 0.097s
  training loss:		2.267962
  validation loss:		2.219104
  validation accuracy:		21.09 %
Epoch 115 of 2000 took 0.097s
  training loss:		2.264593
  validation loss:		2.205329
  validation accuracy:		23.70 %
Epoch 116 of 2000 took 0.097s
  training loss:		2.262645
  validation loss:		2.205621
  validation accuracy:		17.72 %
Epoch 117 of 2000 took 0.097s
  training loss:		2.261697
  validation loss:		2.209342
  validation accuracy:		24.35 %
Epoch 118 of 2000 took 0.097s
  training loss:		2.260206
  validation loss:		2.203538
  validation accuracy:		23.15 %
Epoch 119 of 2000 took 0.097s
  training loss:		2.258616
  validation loss:		2.201699
  validation accuracy:		25.22 %
Epoch 120 of 2000 took 0.097s
  training loss:		2.257186
  validation loss:		2.194979
  validation accuracy:		23.26 %
Epoch 121 of 2000 took 0.097s
  training loss:		2.254969
  validation loss:		2.200196
  validation accuracy:		19.57 %
Epoch 122 of 2000 took 0.097s
  training loss:		2.252245
  validation loss:		2.194162
  validation accuracy:		23.04 %
Epoch 123 of 2000 took 0.097s
  training loss:		2.251410
  validation loss:		2.190904
  validation accuracy:		22.17 %
Epoch 124 of 2000 took 0.097s
  training loss:		2.247571
  validation loss:		2.189630
  validation accuracy:		23.26 %
Epoch 125 of 2000 took 0.097s
  training loss:		2.247225
  validation loss:		2.188120
  validation accuracy:		24.67 %
Epoch 126 of 2000 took 0.097s
  training loss:		2.243533
  validation loss:		2.183335
  validation accuracy:		23.15 %
Epoch 127 of 2000 took 0.097s
  training loss:		2.239729
  validation loss:		2.176445
  validation accuracy:		25.00 %
Epoch 128 of 2000 took 0.097s
  training loss:		2.236382
  validation loss:		2.177287
  validation accuracy:		24.46 %
Epoch 129 of 2000 took 0.097s
  training loss:		2.234148
  validation loss:		2.177764
  validation accuracy:		26.85 %
Epoch 130 of 2000 took 0.097s
  training loss:		2.227429
  validation loss:		2.159960
  validation accuracy:		25.22 %
Epoch 131 of 2000 took 0.097s
  training loss:		2.223252
  validation loss:		2.160616
  validation accuracy:		25.11 %
Epoch 132 of 2000 took 0.097s
  training loss:		2.217375
  validation loss:		2.155475
  validation accuracy:		24.67 %
Epoch 133 of 2000 took 0.097s
  training loss:		2.212578
  validation loss:		2.153050
  validation accuracy:		25.11 %
Epoch 134 of 2000 took 0.097s
  training loss:		2.205885
  validation loss:		2.138266
  validation accuracy:		28.80 %
Epoch 135 of 2000 took 0.098s
  training loss:		2.196883
  validation loss:		2.126921
  validation accuracy:		27.50 %
Epoch 136 of 2000 took 0.097s
  training loss:		2.188360
  validation loss:		2.119617
  validation accuracy:		27.17 %
Epoch 137 of 2000 took 0.097s
  training loss:		2.179140
  validation loss:		2.106827
  validation accuracy:		28.37 %
Epoch 138 of 2000 took 0.097s
  training loss:		2.165914
  validation loss:		2.092752
  validation accuracy:		28.26 %
Epoch 139 of 2000 took 0.097s
  training loss:		2.153587
  validation loss:		2.081391
  validation accuracy:		29.35 %
Epoch 140 of 2000 took 0.097s
  training loss:		2.138783
  validation loss:		2.066262
  validation accuracy:		30.22 %
Epoch 141 of 2000 took 0.097s
  training loss:		2.126184
  validation loss:		2.046182
  validation accuracy:		29.89 %
Epoch 142 of 2000 took 0.097s
  training loss:		2.111925
  validation loss:		2.030762
  validation accuracy:		31.96 %
Epoch 143 of 2000 took 0.097s
  training loss:		2.091394
  validation loss:		2.012578
  validation accuracy:		33.15 %
Epoch 144 of 2000 took 0.097s
  training loss:		2.077356
  validation loss:		1.992152
  validation accuracy:		32.93 %
Epoch 145 of 2000 took 0.097s
  training loss:		2.058771
  validation loss:		1.976851
  validation accuracy:		31.52 %
Epoch 146 of 2000 took 0.097s
  training loss:		2.041384
  validation loss:		1.959265
  validation accuracy:		32.93 %
Epoch 147 of 2000 took 0.097s
  training loss:		2.021657
  validation loss:		1.942994
  validation accuracy:		35.54 %
Epoch 148 of 2000 took 0.097s
  training loss:		2.001694
  validation loss:		1.924267
  validation accuracy:		34.78 %
Epoch 149 of 2000 took 0.097s
  training loss:		1.980944
  validation loss:		1.908021
  validation accuracy:		35.11 %
Epoch 150 of 2000 took 0.097s
  training loss:		1.966920
  validation loss:		1.888270
  validation accuracy:		35.87 %
Epoch 151 of 2000 took 0.097s
  training loss:		1.948704
  validation loss:		1.866620
  validation accuracy:		35.43 %
Epoch 152 of 2000 took 0.097s
  training loss:		1.925744
  validation loss:		1.846057
  validation accuracy:		35.11 %
Epoch 153 of 2000 took 0.097s
  training loss:		1.905036
  validation loss:		1.828302
  validation accuracy:		35.00 %
Epoch 154 of 2000 took 0.097s
  training loss:		1.888710
  validation loss:		1.812248
  validation accuracy:		38.15 %
Epoch 155 of 2000 took 0.097s
  training loss:		1.870070
  validation loss:		1.792420
  validation accuracy:		39.35 %
Epoch 156 of 2000 took 0.097s
  training loss:		1.845605
  validation loss:		1.765854
  validation accuracy:		37.93 %
Epoch 157 of 2000 took 0.097s
  training loss:		1.827664
  validation loss:		1.747867
  validation accuracy:		40.87 %
Epoch 158 of 2000 took 0.097s
  training loss:		1.807588
  validation loss:		1.734030
  validation accuracy:		41.09 %
Epoch 159 of 2000 took 0.097s
  training loss:		1.785582
  validation loss:		1.708362
  validation accuracy:		42.50 %
Epoch 160 of 2000 took 0.097s
  training loss:		1.762307
  validation loss:		1.681034
  validation accuracy:		41.63 %
Epoch 161 of 2000 took 0.097s
  training loss:		1.741337
  validation loss:		1.663323
  validation accuracy:		41.63 %
Epoch 162 of 2000 took 0.097s
  training loss:		1.718149
  validation loss:		1.640133
  validation accuracy:		43.59 %
Epoch 163 of 2000 took 0.100s
  training loss:		1.698258
  validation loss:		1.616629
  validation accuracy:		43.91 %
Epoch 164 of 2000 took 0.097s
  training loss:		1.677970
  validation loss:		1.601001
  validation accuracy:		45.43 %
Epoch 165 of 2000 took 0.097s
  training loss:		1.658420
  validation loss:		1.581196
  validation accuracy:		46.63 %
Epoch 166 of 2000 took 0.097s
  training loss:		1.635299
  validation loss:		1.563362
  validation accuracy:		47.07 %
Epoch 167 of 2000 took 0.097s
  training loss:		1.619006
  validation loss:		1.553580
  validation accuracy:		47.28 %
Epoch 168 of 2000 took 0.097s
  training loss:		1.592702
  validation loss:		1.526665
  validation accuracy:		48.37 %
Epoch 169 of 2000 took 0.097s
  training loss:		1.582853
  validation loss:		1.504188
  validation accuracy:		48.15 %
Epoch 170 of 2000 took 0.097s
  training loss:		1.561976
  validation loss:		1.495995
  validation accuracy:		49.78 %
Epoch 171 of 2000 took 0.097s
  training loss:		1.546079
  validation loss:		1.485967
  validation accuracy:		51.85 %
Epoch 172 of 2000 took 0.097s
  training loss:		1.540413
  validation loss:		1.469270
  validation accuracy:		52.39 %
Epoch 173 of 2000 took 0.097s
  training loss:		1.520563
  validation loss:		1.443531
  validation accuracy:		53.37 %
Epoch 174 of 2000 took 0.099s
  training loss:		1.503767
  validation loss:		1.438736
  validation accuracy:		54.24 %
Epoch 175 of 2000 took 0.109s
  training loss:		1.507265
  validation loss:		1.431890
  validation accuracy:		55.33 %
Epoch 176 of 2000 took 0.132s
  training loss:		1.477573
  validation loss:		1.412449
  validation accuracy:		55.22 %
Epoch 177 of 2000 took 0.117s
  training loss:		1.473542
  validation loss:		1.400666
  validation accuracy:		56.20 %
Epoch 178 of 2000 took 0.101s
  training loss:		1.457189
  validation loss:		1.395109
  validation accuracy:		57.17 %
Epoch 179 of 2000 took 0.102s
  training loss:		1.468155
  validation loss:		1.477983
  validation accuracy:		50.00 %
Epoch 180 of 2000 took 0.097s
  training loss:		1.507157
  validation loss:		1.449215
  validation accuracy:		53.59 %
Epoch 181 of 2000 took 0.097s
  training loss:		1.447026
  validation loss:		1.364824
  validation accuracy:		59.78 %
Epoch 182 of 2000 took 0.098s
  training loss:		1.441501
  validation loss:		1.346036
  validation accuracy:		60.54 %
Epoch 183 of 2000 took 0.099s
  training loss:		1.412688
  validation loss:		1.337865
  validation accuracy:		61.30 %
Epoch 184 of 2000 took 0.099s
  training loss:		1.511223
  validation loss:		1.395241
  validation accuracy:		59.13 %
Epoch 185 of 2000 took 0.099s
  training loss:		1.409876
  validation loss:		1.339657
  validation accuracy:		61.20 %
Epoch 186 of 2000 took 0.099s
  training loss:		1.422521
  validation loss:		1.345729
  validation accuracy:		60.33 %
Epoch 187 of 2000 took 0.099s
  training loss:		1.625110
  validation loss:		1.489345
  validation accuracy:		50.76 %
Epoch 188 of 2000 took 0.099s
  training loss:		1.426855
  validation loss:		1.312196
  validation accuracy:		62.17 %
Epoch 189 of 2000 took 0.097s
  training loss:		1.369051
  validation loss:		1.311668
  validation accuracy:		62.07 %
Epoch 190 of 2000 took 0.096s
  training loss:		1.372980
  validation loss:		1.326050
  validation accuracy:		61.85 %
Epoch 191 of 2000 took 0.096s
  training loss:		1.361764
  validation loss:		1.284750
  validation accuracy:		63.37 %
Epoch 192 of 2000 took 0.096s
  training loss:		1.357978
  validation loss:		1.311317
  validation accuracy:		60.87 %
Epoch 193 of 2000 took 0.095s
  training loss:		1.382422
  validation loss:		1.360370
  validation accuracy:		59.46 %
Epoch 194 of 2000 took 0.096s
  training loss:		1.424756
  validation loss:		1.277431
  validation accuracy:		64.02 %
Epoch 195 of 2000 took 0.095s
  training loss:		1.387781
  validation loss:		1.330769
  validation accuracy:		61.09 %
Epoch 196 of 2000 took 0.096s
  training loss:		1.357913
  validation loss:		1.242782
  validation accuracy:		65.00 %
Epoch 197 of 2000 took 0.096s
  training loss:		1.323918
  validation loss:		1.236376
  validation accuracy:		65.76 %
Epoch 198 of 2000 took 0.096s
  training loss:		1.301935
  validation loss:		1.228233
  validation accuracy:		66.41 %
Epoch 199 of 2000 took 0.096s
  training loss:		1.314761
  validation loss:		1.298130
  validation accuracy:		62.72 %
Epoch 200 of 2000 took 0.097s
  training loss:		1.491129
  validation loss:		1.459366
  validation accuracy:		51.30 %
Epoch 201 of 2000 took 0.097s
  training loss:		1.440806
  validation loss:		1.257322
  validation accuracy:		64.89 %
Epoch 202 of 2000 took 0.097s
  training loss:		1.281566
  validation loss:		1.196832
  validation accuracy:		67.07 %
Epoch 203 of 2000 took 0.097s
  training loss:		1.281805
  validation loss:		1.175711
  validation accuracy:		68.04 %
Epoch 204 of 2000 took 0.096s
  training loss:		1.265630
  validation loss:		1.185508
  validation accuracy:		67.50 %
Epoch 205 of 2000 took 0.096s
  training loss:		1.273206
  validation loss:		1.188816
  validation accuracy:		67.17 %
Epoch 206 of 2000 took 0.096s
  training loss:		1.224853
  validation loss:		1.133649
  validation accuracy:		68.15 %
Epoch 207 of 2000 took 0.096s
  training loss:		1.209359
  validation loss:		1.115653
  validation accuracy:		69.13 %
Epoch 208 of 2000 took 0.097s
  training loss:		1.201665
  validation loss:		1.109386
  validation accuracy:		69.13 %
Epoch 209 of 2000 took 0.097s
  training loss:		1.216608
  validation loss:		1.109009
  validation accuracy:		68.59 %
Epoch 210 of 2000 took 0.097s
  training loss:		1.213616
  validation loss:		1.126214
  validation accuracy:		67.28 %
Epoch 211 of 2000 took 0.097s
  training loss:		1.167559
  validation loss:		1.118997
  validation accuracy:		66.85 %
Epoch 212 of 2000 took 0.097s
  training loss:		1.267309
  validation loss:		1.096877
  validation accuracy:		68.26 %
Epoch 213 of 2000 took 0.097s
  training loss:		1.124993
  validation loss:		1.043348
  validation accuracy:		70.33 %
Epoch 214 of 2000 took 0.097s
  training loss:		1.103568
  validation loss:		0.992655
  validation accuracy:		71.20 %
Epoch 215 of 2000 took 0.097s
  training loss:		1.073383
  validation loss:		0.975191
  validation accuracy:		72.61 %
Epoch 216 of 2000 took 0.097s
  training loss:		1.057410
  validation loss:		0.950659
  validation accuracy:		72.61 %
Epoch 217 of 2000 took 0.097s
  training loss:		1.103108
  validation loss:		0.953246
  validation accuracy:		72.17 %
Epoch 218 of 2000 took 0.097s
  training loss:		1.020846
  validation loss:		0.928079
  validation accuracy:		73.80 %
Epoch 219 of 2000 took 0.097s
  training loss:		0.993546
  validation loss:		0.906796
  validation accuracy:		73.91 %
Epoch 220 of 2000 took 0.097s
  training loss:		0.993135
  validation loss:		0.915172
  validation accuracy:		73.04 %
Epoch 221 of 2000 took 0.097s
  training loss:		0.972576
  validation loss:		0.874901
  validation accuracy:		74.13 %
Epoch 222 of 2000 took 0.097s
  training loss:		0.975442
  validation loss:		0.862329
  validation accuracy:		74.13 %
Epoch 223 of 2000 took 0.097s
  training loss:		0.938868
  validation loss:		0.839609
  validation accuracy:		74.78 %
Epoch 224 of 2000 took 0.097s
  training loss:		0.923779
  validation loss:		0.831228
  validation accuracy:		74.57 %
Epoch 225 of 2000 took 0.097s
  training loss:		0.941054
  validation loss:		0.820272
  validation accuracy:		74.57 %
Epoch 226 of 2000 took 0.097s
  training loss:		0.911560
  validation loss:		0.838288
  validation accuracy:		73.59 %
Epoch 227 of 2000 took 0.097s
  training loss:		0.897947
  validation loss:		0.943945
  validation accuracy:		69.24 %
Epoch 228 of 2000 took 0.097s
  training loss:		0.945012
  validation loss:		0.795126
  validation accuracy:		75.65 %
Epoch 229 of 2000 took 0.097s
  training loss:		0.852288
  validation loss:		0.809748
  validation accuracy:		75.22 %
Epoch 230 of 2000 took 0.097s
  training loss:		0.837787
  validation loss:		0.749724
  validation accuracy:		76.09 %
Epoch 231 of 2000 took 0.097s
  training loss:		0.835336
  validation loss:		0.744900
  validation accuracy:		76.09 %
Epoch 232 of 2000 took 0.097s
  training loss:		0.814436
  validation loss:		0.732276
  validation accuracy:		77.28 %
Epoch 233 of 2000 took 0.097s
  training loss:		0.813349
  validation loss:		0.755845
  validation accuracy:		75.98 %
Epoch 234 of 2000 took 0.097s
  training loss:		0.801545
  validation loss:		0.715564
  validation accuracy:		77.17 %
Epoch 235 of 2000 took 0.098s
  training loss:		0.784243
  validation loss:		0.724952
  validation accuracy:		77.39 %
Epoch 236 of 2000 took 0.098s
  training loss:		0.813223
  validation loss:		0.713779
  validation accuracy:		75.98 %
Epoch 237 of 2000 took 0.099s
  training loss:		0.775741
  validation loss:		0.694749
  validation accuracy:		78.04 %
Epoch 238 of 2000 took 0.098s
  training loss:		0.779064
  validation loss:		0.683740
  validation accuracy:		78.48 %
Epoch 239 of 2000 took 0.097s
  training loss:		0.772996
  validation loss:		0.679484
  validation accuracy:		78.48 %
Epoch 240 of 2000 took 0.097s
  training loss:		0.765590
  validation loss:		0.668717
  validation accuracy:		78.70 %
Epoch 241 of 2000 took 0.097s
  training loss:		0.764753
  validation loss:		0.663024
  validation accuracy:		78.80 %
Epoch 242 of 2000 took 0.099s
  training loss:		0.760109
  validation loss:		0.715424
  validation accuracy:		76.30 %
Epoch 243 of 2000 took 0.098s
  training loss:		0.760514
  validation loss:		0.656747
  validation accuracy:		78.70 %
Epoch 244 of 2000 took 0.097s
  training loss:		0.720945
  validation loss:		0.691693
  validation accuracy:		77.17 %
Epoch 245 of 2000 took 0.097s
  training loss:		0.734226
  validation loss:		0.661736
  validation accuracy:		78.91 %
Epoch 246 of 2000 took 0.097s
  training loss:		0.722838
  validation loss:		0.645943
  validation accuracy:		78.70 %
Epoch 247 of 2000 took 0.097s
  training loss:		0.719240
  validation loss:		0.647350
  validation accuracy:		79.02 %
Epoch 248 of 2000 took 0.098s
  training loss:		0.715661
  validation loss:		0.646569
  validation accuracy:		79.13 %
Epoch 249 of 2000 took 0.098s
  training loss:		0.706587
  validation loss:		0.635001
  validation accuracy:		79.13 %
Epoch 250 of 2000 took 0.097s
  training loss:		0.734885
  validation loss:		0.770243
  validation accuracy:		73.59 %
Epoch 251 of 2000 took 0.097s
  training loss:		0.726169
  validation loss:		0.671940
  validation accuracy:		77.61 %
Epoch 252 of 2000 took 0.097s
  training loss:		0.714626
  validation loss:		0.640263
  validation accuracy:		78.91 %
Epoch 253 of 2000 took 0.097s
  training loss:		0.702751
  validation loss:		0.621846
  validation accuracy:		79.13 %
Epoch 254 of 2000 took 0.097s
  training loss:		0.693242
  validation loss:		0.638929
  validation accuracy:		79.57 %
Epoch 255 of 2000 took 0.097s
  training loss:		0.687785
  validation loss:		0.612258
  validation accuracy:		80.11 %
Epoch 256 of 2000 took 0.097s
  training loss:		0.684432
  validation loss:		0.620051
  validation accuracy:		79.46 %
Epoch 257 of 2000 took 0.097s
  training loss:		0.686449
  validation loss:		0.603846
  validation accuracy:		81.20 %
Epoch 258 of 2000 took 0.097s
  training loss:		0.686785
  validation loss:		0.611432
  validation accuracy:		80.11 %
Epoch 259 of 2000 took 0.097s
  training loss:		0.676905
  validation loss:		0.606536
  validation accuracy:		80.00 %
Epoch 260 of 2000 took 0.097s
  training loss:		0.682265
  validation loss:		0.595498
  validation accuracy:		80.43 %
Epoch 261 of 2000 took 0.097s
  training loss:		0.658431
  validation loss:		0.589118
  validation accuracy:		81.52 %
Epoch 262 of 2000 took 0.097s
  training loss:		0.668932
  validation loss:		0.603227
  validation accuracy:		80.00 %
Epoch 263 of 2000 took 0.097s
  training loss:		0.671794
  validation loss:		0.595068
  validation accuracy:		80.43 %
Epoch 264 of 2000 took 0.097s
  training loss:		0.660644
  validation loss:		0.615663
  validation accuracy:		79.57 %
Epoch 265 of 2000 took 0.097s
  training loss:		0.657031
  validation loss:		0.589626
  validation accuracy:		80.22 %
Epoch 266 of 2000 took 0.097s
  training loss:		0.660466
  validation loss:		0.583398
  validation accuracy:		80.11 %
Epoch 267 of 2000 took 0.097s
  training loss:		0.647643
  validation loss:		0.580457
  validation accuracy:		81.09 %
Epoch 268 of 2000 took 0.097s
  training loss:		0.654673
  validation loss:		0.592265
  validation accuracy:		80.87 %
Epoch 269 of 2000 took 0.097s
  training loss:		0.643492
  validation loss:		0.581746
  validation accuracy:		80.22 %
Epoch 270 of 2000 took 0.097s
  training loss:		0.643942
  validation loss:		0.576749
  validation accuracy:		80.65 %
Epoch 271 of 2000 took 0.097s
  training loss:		0.636396
  validation loss:		0.581587
  validation accuracy:		80.11 %
Epoch 272 of 2000 took 0.097s
  training loss:		0.641379
  validation loss:		0.584892
  validation accuracy:		80.98 %
Epoch 273 of 2000 took 0.097s
  training loss:		0.651005
  validation loss:		0.572261
  validation accuracy:		81.20 %
Epoch 274 of 2000 took 0.097s
  training loss:		0.635697
  validation loss:		0.591002
  validation accuracy:		80.33 %
Epoch 275 of 2000 took 0.097s
  training loss:		0.630717
  validation loss:		0.574480
  validation accuracy:		80.98 %
Epoch 276 of 2000 took 0.097s
  training loss:		0.632611
  validation loss:		0.568526
  validation accuracy:		81.52 %
Epoch 277 of 2000 took 0.097s
  training loss:		0.657306
  validation loss:		0.567993
  validation accuracy:		81.30 %
Epoch 278 of 2000 took 0.097s
  training loss:		0.630682
  validation loss:		0.576320
  validation accuracy:		81.20 %
Epoch 279 of 2000 took 0.097s
  training loss:		0.631507
  validation loss:		0.573361
  validation accuracy:		81.41 %
Epoch 280 of 2000 took 0.097s
  training loss:		0.636562
  validation loss:		0.566813
  validation accuracy:		81.63 %
Epoch 281 of 2000 took 0.097s
  training loss:		0.619398
  validation loss:		0.573945
  validation accuracy:		80.65 %
Epoch 282 of 2000 took 0.097s
  training loss:		0.634644
  validation loss:		0.563623
  validation accuracy:		81.20 %
Epoch 283 of 2000 took 0.097s
  training loss:		0.615794
  validation loss:		0.567458
  validation accuracy:		81.30 %
Epoch 284 of 2000 took 0.097s
  training loss:		0.609392
  validation loss:		0.565308
  validation accuracy:		81.74 %
Epoch 285 of 2000 took 0.097s
  training loss:		0.612008
  validation loss:		0.563593
  validation accuracy:		81.41 %
Epoch 286 of 2000 took 0.097s
  training loss:		0.615389
  validation loss:		0.597192
  validation accuracy:		80.00 %
Epoch 287 of 2000 took 0.097s
  training loss:		0.641692
  validation loss:		0.575916
  validation accuracy:		80.43 %
Epoch 288 of 2000 took 0.097s
  training loss:		0.630968
  validation loss:		0.553417
  validation accuracy:		81.85 %
Epoch 289 of 2000 took 0.097s
  training loss:		0.618409
  validation loss:		0.556847
  validation accuracy:		81.30 %
Epoch 290 of 2000 took 0.097s
  training loss:		0.611899
  validation loss:		0.558241
  validation accuracy:		81.63 %
Epoch 291 of 2000 took 0.097s
  training loss:		0.602147
  validation loss:		0.572975
  validation accuracy:		80.33 %
Epoch 292 of 2000 took 0.097s
  training loss:		0.612535
  validation loss:		0.570099
  validation accuracy:		80.33 %
Epoch 293 of 2000 took 0.097s
  training loss:		0.602259
  validation loss:		0.564336
  validation accuracy:		81.63 %
Epoch 294 of 2000 took 0.097s
  training loss:		0.609225
  validation loss:		0.548674
  validation accuracy:		81.63 %
Epoch 295 of 2000 took 0.097s
  training loss:		0.603747
  validation loss:		0.564502
  validation accuracy:		81.63 %
Epoch 296 of 2000 took 0.097s
  training loss:		0.605003
  validation loss:		0.604012
  validation accuracy:		79.57 %
Epoch 297 of 2000 took 0.111s
  training loss:		0.603256
  validation loss:		0.579189
  validation accuracy:		79.78 %
Epoch 298 of 2000 took 0.111s
  training loss:		0.605223
  validation loss:		0.697984
  validation accuracy:		74.35 %
Epoch 299 of 2000 took 0.105s
  training loss:		0.617907
  validation loss:		0.581204
  validation accuracy:		79.78 %
Epoch 300 of 2000 took 0.100s
  training loss:		0.603126
  validation loss:		0.546304
  validation accuracy:		81.41 %
Epoch 301 of 2000 took 0.100s
  training loss:		0.587821
  validation loss:		0.582596
  validation accuracy:		79.89 %
Epoch 302 of 2000 took 0.100s
  training loss:		0.610329
  validation loss:		0.541364
  validation accuracy:		82.61 %
Epoch 303 of 2000 took 0.100s
  training loss:		0.617497
  validation loss:		0.573201
  validation accuracy:		81.74 %
Epoch 304 of 2000 took 0.100s
  training loss:		0.613163
  validation loss:		0.595152
  validation accuracy:		79.67 %
Epoch 305 of 2000 took 0.100s
  training loss:		0.603699
  validation loss:		0.555709
  validation accuracy:		81.96 %
Epoch 306 of 2000 took 0.100s
  training loss:		0.596913
  validation loss:		0.549770
  validation accuracy:		81.63 %
Epoch 307 of 2000 took 0.100s
  training loss:		0.600437
  validation loss:		0.553676
  validation accuracy:		81.30 %
Epoch 308 of 2000 took 0.100s
  training loss:		0.617409
  validation loss:		0.542705
  validation accuracy:		82.28 %
Epoch 309 of 2000 took 0.100s
  training loss:		0.587437
  validation loss:		0.539686
  validation accuracy:		82.61 %
Epoch 310 of 2000 took 0.100s
  training loss:		0.592935
  validation loss:		0.559430
  validation accuracy:		81.20 %
Epoch 311 of 2000 took 0.100s
  training loss:		0.601878
  validation loss:		0.572475
  validation accuracy:		80.54 %
Epoch 312 of 2000 took 0.099s
  training loss:		0.588183
  validation loss:		0.602259
  validation accuracy:		79.46 %
Epoch 313 of 2000 took 0.097s
  training loss:		0.605261
  validation loss:		0.574934
  validation accuracy:		79.89 %
Epoch 314 of 2000 took 0.097s
  training loss:		0.602081
  validation loss:		0.529898
  validation accuracy:		82.83 %
Epoch 315 of 2000 took 0.097s
  training loss:		0.585197
  validation loss:		0.583594
  validation accuracy:		80.22 %
Epoch 316 of 2000 took 0.097s
  training loss:		0.591777
  validation loss:		0.534752
  validation accuracy:		82.17 %
Epoch 317 of 2000 took 0.097s
  training loss:		0.582503
  validation loss:		0.533335
  validation accuracy:		82.61 %
Epoch 318 of 2000 took 0.097s
  training loss:		0.581489
  validation loss:		0.535459
  validation accuracy:		82.83 %
Epoch 319 of 2000 took 0.097s
  training loss:		0.579209
  validation loss:		0.539384
  validation accuracy:		81.74 %
Epoch 320 of 2000 took 0.097s
  training loss:		0.574044
  validation loss:		0.561731
  validation accuracy:		81.41 %
Epoch 321 of 2000 took 0.097s
  training loss:		0.570028
  validation loss:		0.535080
  validation accuracy:		81.74 %
Epoch 322 of 2000 took 0.097s
  training loss:		0.570234
  validation loss:		0.517587
  validation accuracy:		83.37 %
Epoch 323 of 2000 took 0.097s
  training loss:		0.599667
  validation loss:		0.519195
  validation accuracy:		82.83 %
Epoch 324 of 2000 took 0.097s
  training loss:		0.560563
  validation loss:		0.554630
  validation accuracy:		81.41 %
Epoch 325 of 2000 took 0.097s
  training loss:		0.572603
  validation loss:		0.544962
  validation accuracy:		81.74 %
Epoch 326 of 2000 took 0.097s
  training loss:		0.572103
  validation loss:		0.518913
  validation accuracy:		83.48 %
Epoch 327 of 2000 took 0.097s
  training loss:		0.570164
  validation loss:		0.513938
  validation accuracy:		83.59 %
Epoch 328 of 2000 took 0.097s
  training loss:		0.562453
  validation loss:		0.512297
  validation accuracy:		83.59 %
Epoch 329 of 2000 took 0.097s
  training loss:		0.552104
  validation loss:		0.529375
  validation accuracy:		82.39 %
Epoch 330 of 2000 took 0.101s
  training loss:		0.552211
  validation loss:		0.532523
  validation accuracy:		82.17 %
Epoch 331 of 2000 took 0.102s
  training loss:		0.553455
  validation loss:		0.518377
  validation accuracy:		82.28 %
Epoch 332 of 2000 took 0.097s
  training loss:		0.549161
  validation loss:		0.507845
  validation accuracy:		83.59 %
Epoch 333 of 2000 took 0.096s
  training loss:		0.558380
  validation loss:		0.496571
  validation accuracy:		84.13 %
Epoch 334 of 2000 took 0.097s
  training loss:		0.543831
  validation loss:		0.519635
  validation accuracy:		82.72 %
Epoch 335 of 2000 took 0.098s
  training loss:		0.549320
  validation loss:		0.496267
  validation accuracy:		84.35 %
Epoch 336 of 2000 took 0.097s
  training loss:		0.553449
  validation loss:		0.497811
  validation accuracy:		84.02 %
Epoch 337 of 2000 took 0.099s
  training loss:		0.538102
  validation loss:		0.520844
  validation accuracy:		82.61 %
Epoch 338 of 2000 took 0.097s
  training loss:		0.550376
  validation loss:		0.510805
  validation accuracy:		83.80 %
Epoch 339 of 2000 took 0.097s
  training loss:		0.532640
  validation loss:		0.486175
  validation accuracy:		84.67 %
Epoch 340 of 2000 took 0.097s
  training loss:		0.533224
  validation loss:		0.486248
  validation accuracy:		84.67 %
Epoch 341 of 2000 took 0.097s
  training loss:		0.524355
  validation loss:		0.524137
  validation accuracy:		83.26 %
Epoch 342 of 2000 took 0.097s
  training loss:		0.526001
  validation loss:		0.491587
  validation accuracy:		84.57 %
Epoch 343 of 2000 took 0.097s
  training loss:		0.525038
  validation loss:		0.496826
  validation accuracy:		83.59 %
Epoch 344 of 2000 took 0.097s
  training loss:		0.522658
  validation loss:		0.469912
  validation accuracy:		85.33 %
Epoch 345 of 2000 took 0.097s
  training loss:		0.504825
  validation loss:		0.491436
  validation accuracy:		84.24 %
Epoch 346 of 2000 took 0.097s
  training loss:		0.511648
  validation loss:		0.466434
  validation accuracy:		85.54 %
Epoch 347 of 2000 took 0.097s
  training loss:		0.516812
  validation loss:		0.472259
  validation accuracy:		85.43 %
Epoch 348 of 2000 took 0.097s
  training loss:		0.515965
  validation loss:		0.481454
  validation accuracy:		84.24 %
Epoch 349 of 2000 took 0.097s
  training loss:		0.511207
  validation loss:		0.502389
  validation accuracy:		84.13 %
Epoch 350 of 2000 took 0.097s
  training loss:		0.543004
  validation loss:		0.475092
  validation accuracy:		85.00 %
Epoch 351 of 2000 took 0.098s
  training loss:		0.497559
  validation loss:		0.473619
  validation accuracy:		84.89 %
Epoch 352 of 2000 took 0.097s
  training loss:		0.526999
  validation loss:		0.461090
  validation accuracy:		85.22 %
Epoch 353 of 2000 took 0.097s
  training loss:		0.505359
  validation loss:		0.449285
  validation accuracy:		86.09 %
Epoch 354 of 2000 took 0.097s
  training loss:		0.497992
  validation loss:		0.504970
  validation accuracy:		83.48 %
Epoch 355 of 2000 took 0.097s
  training loss:		0.486677
  validation loss:		0.444732
  validation accuracy:		86.41 %
Epoch 356 of 2000 took 0.097s
  training loss:		0.490546
  validation loss:		0.478549
  validation accuracy:		84.67 %
Epoch 357 of 2000 took 0.097s
  training loss:		0.482742
  validation loss:		0.532531
  validation accuracy:		82.93 %
Epoch 358 of 2000 took 0.097s
  training loss:		0.484388
  validation loss:		0.445329
  validation accuracy:		85.87 %
Epoch 359 of 2000 took 0.098s
  training loss:		0.471814
  validation loss:		0.439546
  validation accuracy:		86.52 %
Epoch 360 of 2000 took 0.097s
  training loss:		0.492829
  validation loss:		0.490494
  validation accuracy:		84.78 %
Epoch 361 of 2000 took 0.097s
  training loss:		0.478889
  validation loss:		0.439374
  validation accuracy:		85.54 %
Epoch 362 of 2000 took 0.097s
  training loss:		0.472762
  validation loss:		0.455418
  validation accuracy:		86.09 %
Epoch 363 of 2000 took 0.098s
  training loss:		0.467257
  validation loss:		0.449084
  validation accuracy:		85.87 %
Epoch 364 of 2000 took 0.097s
  training loss:		0.463277
  validation loss:		0.433932
  validation accuracy:		85.98 %
Epoch 365 of 2000 took 0.097s
  training loss:		0.470426
  validation loss:		0.435298
  validation accuracy:		86.41 %
Epoch 366 of 2000 took 0.097s
  training loss:		0.466941
  validation loss:		0.436249
  validation accuracy:		85.87 %
Epoch 367 of 2000 took 0.097s
  training loss:		0.476141
  validation loss:		0.446227
  validation accuracy:		85.22 %
Epoch 368 of 2000 took 0.097s
  training loss:		0.476553
  validation loss:		0.432917
  validation accuracy:		86.52 %
Epoch 369 of 2000 took 0.097s
  training loss:		0.483048
  validation loss:		0.435391
  validation accuracy:		86.20 %
Epoch 370 of 2000 took 0.097s
  training loss:		0.474861
  validation loss:		0.444499
  validation accuracy:		86.20 %
Epoch 371 of 2000 took 0.097s
  training loss:		0.453659
  validation loss:		0.440762
  validation accuracy:		85.76 %
Epoch 372 of 2000 took 0.097s
  training loss:		0.458229
  validation loss:		0.433588
  validation accuracy:		85.98 %
Epoch 373 of 2000 took 0.097s
  training loss:		0.444107
  validation loss:		0.423770
  validation accuracy:		86.63 %
Epoch 374 of 2000 took 0.097s
  training loss:		0.455347
  validation loss:		0.440288
  validation accuracy:		85.65 %
Epoch 375 of 2000 took 0.097s
  training loss:		0.450485
  validation loss:		0.450171
  validation accuracy:		85.54 %
Epoch 376 of 2000 took 0.097s
  training loss:		0.445382
  validation loss:		0.441612
  validation accuracy:		85.98 %
Epoch 377 of 2000 took 0.097s
  training loss:		0.455028
  validation loss:		0.429064
  validation accuracy:		85.87 %
Epoch 378 of 2000 took 0.097s
  training loss:		0.446733
  validation loss:		0.455565
  validation accuracy:		84.78 %
Epoch 379 of 2000 took 0.097s
  training loss:		0.444398
  validation loss:		0.454826
  validation accuracy:		85.00 %
Epoch 380 of 2000 took 0.097s
  training loss:		0.443131
  validation loss:		0.416761
  validation accuracy:		86.20 %
Epoch 381 of 2000 took 0.097s
  training loss:		0.434616
  validation loss:		0.429116
  validation accuracy:		86.20 %
Epoch 382 of 2000 took 0.098s
  training loss:		0.430765
  validation loss:		0.410402
  validation accuracy:		86.85 %
Epoch 383 of 2000 took 0.097s
  training loss:		0.430684
  validation loss:		0.445081
  validation accuracy:		85.22 %
Epoch 384 of 2000 took 0.097s
  training loss:		0.434503
  validation loss:		0.445715
  validation accuracy:		85.76 %
Epoch 385 of 2000 took 0.097s
  training loss:		0.414436
  validation loss:		0.414151
  validation accuracy:		86.20 %
Epoch 386 of 2000 took 0.097s
  training loss:		0.430187
  validation loss:		0.438654
  validation accuracy:		85.11 %
Epoch 387 of 2000 took 0.097s
  training loss:		0.427511
  validation loss:		0.418638
  validation accuracy:		86.85 %
Epoch 388 of 2000 took 0.097s
  training loss:		0.423049
  validation loss:		0.430974
  validation accuracy:		86.30 %
Epoch 389 of 2000 took 0.097s
  training loss:		0.420730
  validation loss:		0.418641
  validation accuracy:		86.74 %
Epoch 390 of 2000 took 0.097s
  training loss:		0.423471
  validation loss:		0.425525
  validation accuracy:		85.98 %
Epoch 391 of 2000 took 0.097s
  training loss:		0.423221
  validation loss:		0.421525
  validation accuracy:		86.85 %
Epoch 392 of 2000 took 0.097s
  training loss:		0.419095
  validation loss:		0.426845
  validation accuracy:		86.85 %
Epoch 393 of 2000 took 0.097s
  training loss:		0.428220
  validation loss:		0.428774
  validation accuracy:		85.87 %
Epoch 394 of 2000 took 0.097s
  training loss:		0.412364
  validation loss:		0.415422
  validation accuracy:		87.07 %
Epoch 395 of 2000 took 0.097s
  training loss:		0.423516
  validation loss:		0.467917
  validation accuracy:		85.54 %
Epoch 396 of 2000 took 0.097s
  training loss:		0.422891
  validation loss:		0.405743
  validation accuracy:		87.07 %
Epoch 397 of 2000 took 0.097s
  training loss:		0.423681
  validation loss:		0.418124
  validation accuracy:		85.76 %
Epoch 398 of 2000 took 0.097s
  training loss:		0.411198
  validation loss:		0.431801
  validation accuracy:		85.54 %
Epoch 399 of 2000 took 0.097s
  training loss:		0.412736
  validation loss:		0.422759
  validation accuracy:		86.74 %
Epoch 400 of 2000 took 0.097s
  training loss:		0.430390
  validation loss:		0.427066
  validation accuracy:		86.63 %
Epoch 401 of 2000 took 0.097s
  training loss:		0.408071
  validation loss:		0.476284
  validation accuracy:		84.46 %
Epoch 402 of 2000 took 0.097s
  training loss:		0.421397
  validation loss:		0.409165
  validation accuracy:		86.41 %
Epoch 403 of 2000 took 0.097s
  training loss:		0.413352
  validation loss:		0.408504
  validation accuracy:		87.28 %
Epoch 404 of 2000 took 0.097s
  training loss:		0.418122
  validation loss:		0.444080
  validation accuracy:		86.09 %
Epoch 405 of 2000 took 0.097s
  training loss:		0.407116
  validation loss:		0.408622
  validation accuracy:		87.07 %
Epoch 406 of 2000 took 0.097s
  training loss:		0.401582
  validation loss:		0.433924
  validation accuracy:		86.20 %
Epoch 407 of 2000 took 0.097s
  training loss:		0.409635
  validation loss:		0.403832
  validation accuracy:		86.96 %
Epoch 408 of 2000 took 0.097s
  training loss:		0.398991
  validation loss:		0.412706
  validation accuracy:		87.83 %
Epoch 409 of 2000 took 0.097s
  training loss:		0.398260
  validation loss:		0.400833
  validation accuracy:		87.61 %
Epoch 410 of 2000 took 0.097s
  training loss:		0.405544
  validation loss:		0.458875
  validation accuracy:		85.54 %
Epoch 411 of 2000 took 0.097s
  training loss:		0.398168
  validation loss:		0.413493
  validation accuracy:		86.20 %
Epoch 412 of 2000 took 0.097s
  training loss:		0.403623
  validation loss:		0.421810
  validation accuracy:		86.63 %
Epoch 413 of 2000 took 0.098s
  training loss:		0.393718
  validation loss:		0.407493
  validation accuracy:		86.74 %
Epoch 414 of 2000 took 0.097s
  training loss:		0.396029
  validation loss:		0.406521
  validation accuracy:		87.50 %
Epoch 415 of 2000 took 0.097s
  training loss:		0.390197
  validation loss:		0.412389
  validation accuracy:		86.52 %
Epoch 416 of 2000 took 0.097s
  training loss:		0.398928
  validation loss:		0.408025
  validation accuracy:		86.96 %
Epoch 417 of 2000 took 0.097s
  training loss:		0.393888
  validation loss:		0.401174
  validation accuracy:		87.61 %
Epoch 418 of 2000 took 0.097s
  training loss:		0.389263
  validation loss:		0.443716
  validation accuracy:		85.98 %
Epoch 419 of 2000 took 0.097s
  training loss:		0.401650
  validation loss:		0.403955
  validation accuracy:		87.72 %
Epoch 420 of 2000 took 0.097s
  training loss:		0.387432
  validation loss:		0.409428
  validation accuracy:		86.96 %
Epoch 421 of 2000 took 0.097s
  training loss:		0.388977
  validation loss:		0.433099
  validation accuracy:		86.30 %
Epoch 422 of 2000 took 0.097s
  training loss:		0.385100
  validation loss:		0.401541
  validation accuracy:		86.74 %
Epoch 423 of 2000 took 0.097s
  training loss:		0.384081
  validation loss:		0.413015
  validation accuracy:		87.17 %
Epoch 424 of 2000 took 0.097s
  training loss:		0.391273
  validation loss:		0.395943
  validation accuracy:		87.07 %
Epoch 425 of 2000 took 0.097s
  training loss:		0.389148
  validation loss:		0.436184
  validation accuracy:		85.87 %
Epoch 426 of 2000 took 0.097s
  training loss:		0.385645
  validation loss:		0.414024
  validation accuracy:		86.96 %
Epoch 427 of 2000 took 0.098s
  training loss:		0.382099
  validation loss:		0.397808
  validation accuracy:		87.50 %
Epoch 428 of 2000 took 0.099s
  training loss:		0.389231
  validation loss:		0.403733
  validation accuracy:		87.39 %
Epoch 429 of 2000 took 0.097s
  training loss:		0.378726
  validation loss:		0.396569
  validation accuracy:		87.07 %
Epoch 430 of 2000 took 0.097s
  training loss:		0.381467
  validation loss:		0.408462
  validation accuracy:		87.39 %
Epoch 431 of 2000 took 0.097s
  training loss:		0.375944
  validation loss:		0.409136
  validation accuracy:		86.85 %
Epoch 432 of 2000 took 0.097s
  training loss:		0.379302
  validation loss:		0.394178
  validation accuracy:		87.17 %
Epoch 433 of 2000 took 0.097s
  training loss:		0.374645
  validation loss:		0.410484
  validation accuracy:		86.96 %
Epoch 434 of 2000 took 0.097s
  training loss:		0.379386
  validation loss:		0.401317
  validation accuracy:		86.85 %
Epoch 435 of 2000 took 0.097s
  training loss:		0.381906
  validation loss:		0.408815
  validation accuracy:		86.96 %
Epoch 436 of 2000 took 0.097s
  training loss:		0.371506
  validation loss:		0.422913
  validation accuracy:		86.30 %
Epoch 437 of 2000 took 0.097s
  training loss:		0.364282
  validation loss:		0.400241
  validation accuracy:		87.28 %
Epoch 438 of 2000 took 0.097s
  training loss:		0.369645
  validation loss:		0.435894
  validation accuracy:		86.30 %
Epoch 439 of 2000 took 0.097s
  training loss:		0.370803
  validation loss:		0.402027
  validation accuracy:		86.74 %
Epoch 440 of 2000 took 0.097s
  training loss:		0.373560
  validation loss:		0.412558
  validation accuracy:		86.85 %
Epoch 441 of 2000 took 0.097s
  training loss:		0.368530
  validation loss:		0.409515
  validation accuracy:		86.85 %
Epoch 442 of 2000 took 0.097s
  training loss:		0.372644
  validation loss:		0.401123
  validation accuracy:		87.28 %
Epoch 443 of 2000 took 0.097s
  training loss:		0.370948
  validation loss:		0.388646
  validation accuracy:		87.61 %
Epoch 444 of 2000 took 0.101s
  training loss:		0.363749
  validation loss:		0.392763
  validation accuracy:		86.96 %
Epoch 445 of 2000 took 0.101s
  training loss:		0.372792
  validation loss:		0.409694
  validation accuracy:		86.85 %
Epoch 446 of 2000 took 0.101s
  training loss:		0.365798
  validation loss:		0.434451
  validation accuracy:		86.09 %
Epoch 447 of 2000 took 0.100s
  training loss:		0.363896
  validation loss:		0.397455
  validation accuracy:		87.07 %
Epoch 448 of 2000 took 0.100s
  training loss:		0.360231
  validation loss:		0.419233
  validation accuracy:		86.41 %
Epoch 449 of 2000 took 0.100s
  training loss:		0.367165
  validation loss:		0.410057
  validation accuracy:		86.74 %
Epoch 450 of 2000 took 0.100s
  training loss:		0.364687
  validation loss:		0.434938
  validation accuracy:		85.87 %
Epoch 451 of 2000 took 0.100s
  training loss:		0.362437
  validation loss:		0.406242
  validation accuracy:		86.85 %
Epoch 452 of 2000 took 0.100s
  training loss:		0.350664
  validation loss:		0.388911
  validation accuracy:		86.96 %
Epoch 453 of 2000 took 0.100s
  training loss:		0.356519
  validation loss:		0.393501
  validation accuracy:		87.93 %
Epoch 454 of 2000 took 0.100s
  training loss:		0.356163
  validation loss:		0.412578
  validation accuracy:		87.07 %
Epoch 455 of 2000 took 0.100s
  training loss:		0.352894
  validation loss:		0.418473
  validation accuracy:		86.52 %
Epoch 456 of 2000 took 0.101s
  training loss:		0.350615
  validation loss:		0.406338
  validation accuracy:		86.85 %
Epoch 457 of 2000 took 0.100s
  training loss:		0.361650
  validation loss:		0.398217
  validation accuracy:		86.85 %
Epoch 458 of 2000 took 0.100s
  training loss:		0.360951
  validation loss:		0.404326
  validation accuracy:		87.28 %
Epoch 459 of 2000 took 0.100s
  training loss:		0.352020
  validation loss:		0.421869
  validation accuracy:		86.85 %
Epoch 460 of 2000 took 0.100s
  training loss:		0.357241
  validation loss:		0.412919
  validation accuracy:		86.74 %
Epoch 461 of 2000 took 0.100s
  training loss:		0.347206
  validation loss:		0.399309
  validation accuracy:		86.63 %
Epoch 462 of 2000 took 0.100s
  training loss:		0.357773
  validation loss:		0.409932
  validation accuracy:		86.41 %
Epoch 463 of 2000 took 0.100s
  training loss:		0.352091
  validation loss:		0.406788
  validation accuracy:		87.28 %
Epoch 464 of 2000 took 0.100s
  training loss:		0.353923
  validation loss:		0.387693
  validation accuracy:		87.61 %
Epoch 465 of 2000 took 0.100s
  training loss:		0.346572
  validation loss:		0.411160
  validation accuracy:		86.63 %
Epoch 466 of 2000 took 0.100s
  training loss:		0.350384
  validation loss:		0.400941
  validation accuracy:		86.96 %
Epoch 467 of 2000 took 0.104s
  training loss:		0.351275
  validation loss:		0.418780
  validation accuracy:		86.96 %
Epoch 468 of 2000 took 0.112s
  training loss:		0.343963
  validation loss:		0.391687
  validation accuracy:		87.07 %
Epoch 469 of 2000 took 0.132s
  training loss:		0.349426
  validation loss:		0.386971
  validation accuracy:		87.93 %
Epoch 470 of 2000 took 0.120s
  training loss:		0.349181
  validation loss:		0.402187
  validation accuracy:		87.17 %
Epoch 471 of 2000 took 0.103s
  training loss:		0.354088
  validation loss:		0.411368
  validation accuracy:		86.74 %
Epoch 472 of 2000 took 0.105s
  training loss:		0.348061
  validation loss:		0.402788
  validation accuracy:		87.50 %
Epoch 473 of 2000 took 0.099s
  training loss:		0.342777
  validation loss:		0.395904
  validation accuracy:		86.96 %
Epoch 474 of 2000 took 0.104s
  training loss:		0.337448
  validation loss:		0.393353
  validation accuracy:		87.93 %
Epoch 475 of 2000 took 0.103s
  training loss:		0.353798
  validation loss:		0.384619
  validation accuracy:		87.50 %
Epoch 476 of 2000 took 0.101s
  training loss:		0.345990
  validation loss:		0.399706
  validation accuracy:		87.50 %
Epoch 477 of 2000 took 0.102s
  training loss:		0.342765
  validation loss:		0.422085
  validation accuracy:		86.30 %
Epoch 478 of 2000 took 0.102s
  training loss:		0.342559
  validation loss:		0.390568
  validation accuracy:		87.61 %
Epoch 479 of 2000 took 0.101s
  training loss:		0.338070
  validation loss:		0.402197
  validation accuracy:		87.72 %
Epoch 480 of 2000 took 0.102s
  training loss:		0.352700
  validation loss:		0.401365
  validation accuracy:		86.52 %
Epoch 481 of 2000 took 0.101s
  training loss:		0.345810
  validation loss:		0.405272
  validation accuracy:		86.96 %
Epoch 482 of 2000 took 0.102s
  training loss:		0.337609
  validation loss:		0.398120
  validation accuracy:		87.72 %
Epoch 483 of 2000 took 0.101s
  training loss:		0.336728
  validation loss:		0.394254
  validation accuracy:		87.17 %
Epoch 484 of 2000 took 0.102s
  training loss:		0.336916
  validation loss:		0.400461
  validation accuracy:		87.17 %
Epoch 485 of 2000 took 0.102s
  training loss:		0.339658
  validation loss:		0.409375
  validation accuracy:		86.41 %
Epoch 486 of 2000 took 0.101s
  training loss:		0.340165
  validation loss:		0.418441
  validation accuracy:		86.63 %
Epoch 487 of 2000 took 0.103s
  training loss:		0.339936
  validation loss:		0.397863
  validation accuracy:		87.39 %
Epoch 488 of 2000 took 0.102s
  training loss:		0.328148
  validation loss:		0.391044
  validation accuracy:		87.83 %
Epoch 489 of 2000 took 0.102s
  training loss:		0.335177
  validation loss:		0.424811
  validation accuracy:		87.28 %
Epoch 490 of 2000 took 0.101s
  training loss:		0.344293
  validation loss:		0.380363
  validation accuracy:		88.04 %
Epoch 491 of 2000 took 0.101s
  training loss:		0.336049
  validation loss:		0.391547
  validation accuracy:		87.61 %
Epoch 492 of 2000 took 0.102s
  training loss:		0.332958
  validation loss:		0.404315
  validation accuracy:		87.28 %
Epoch 493 of 2000 took 0.102s
  training loss:		0.335979
  validation loss:		0.422087
  validation accuracy:		87.39 %
Epoch 494 of 2000 took 0.103s
  training loss:		0.334213
  validation loss:		0.399168
  validation accuracy:		87.61 %
Epoch 495 of 2000 took 0.103s
  training loss:		0.334233
  validation loss:		0.385241
  validation accuracy:		87.93 %
Epoch 496 of 2000 took 0.102s
  training loss:		0.341381
  validation loss:		0.405038
  validation accuracy:		87.61 %
Epoch 497 of 2000 took 0.101s
  training loss:		0.336006
  validation loss:		0.402712
  validation accuracy:		87.83 %
Epoch 498 of 2000 took 0.101s
  training loss:		0.340642
  validation loss:		0.395524
  validation accuracy:		87.50 %
Epoch 499 of 2000 took 0.102s
  training loss:		0.334677
  validation loss:		0.399415
  validation accuracy:		87.61 %
Epoch 500 of 2000 took 0.101s
  training loss:		0.331885
  validation loss:		0.399995
  validation accuracy:		87.39 %
Epoch 501 of 2000 took 0.102s
  training loss:		0.330063
  validation loss:		0.406126
  validation accuracy:		86.96 %
Epoch 502 of 2000 took 0.102s
  training loss:		0.329741
  validation loss:		0.423919
  validation accuracy:		87.28 %
Epoch 503 of 2000 took 0.103s
  training loss:		0.323155
  validation loss:		0.403524
  validation accuracy:		87.83 %
Epoch 504 of 2000 took 0.102s
  training loss:		0.329685
  validation loss:		0.405606
  validation accuracy:		87.28 %
Epoch 505 of 2000 took 0.102s
  training loss:		0.325224
  validation loss:		0.415148
  validation accuracy:		87.39 %
Epoch 506 of 2000 took 0.102s
  training loss:		0.330985
  validation loss:		0.396336
  validation accuracy:		87.83 %
Epoch 507 of 2000 took 0.102s
  training loss:		0.324399
  validation loss:		0.399029
  validation accuracy:		87.72 %
Epoch 508 of 2000 took 0.105s
  training loss:		0.326229
  validation loss:		0.391034
  validation accuracy:		88.04 %
Epoch 509 of 2000 took 0.142s
  training loss:		0.325188
  validation loss:		0.444620
  validation accuracy:		86.85 %
Epoch 510 of 2000 took 0.168s
  training loss:		0.321766
  validation loss:		0.403257
  validation accuracy:		87.50 %
Epoch 511 of 2000 took 0.168s
  training loss:		0.327221
  validation loss:		0.393234
  validation accuracy:		87.83 %
Epoch 512 of 2000 took 0.168s
  training loss:		0.326705
  validation loss:		0.413527
  validation accuracy:		87.72 %
Epoch 513 of 2000 took 0.166s
  training loss:		0.333141
  validation loss:		0.409154
  validation accuracy:		87.50 %
Epoch 514 of 2000 took 0.102s
  training loss:		0.324059
  validation loss:		0.407626
  validation accuracy:		87.93 %
Epoch 515 of 2000 took 0.101s
  training loss:		0.324746
  validation loss:		0.391232
  validation accuracy:		87.93 %
Epoch 516 of 2000 took 0.102s
  training loss:		0.321150
  validation loss:		0.389223
  validation accuracy:		87.50 %
Epoch 517 of 2000 took 0.108s
  training loss:		0.321423
  validation loss:		0.400968
  validation accuracy:		87.61 %
Epoch 518 of 2000 took 0.110s
  training loss:		0.319163
  validation loss:		0.418340
  validation accuracy:		87.28 %
Epoch 519 of 2000 took 0.101s
  training loss:		0.321943
  validation loss:		0.404171
  validation accuracy:		87.39 %
Epoch 520 of 2000 took 0.101s
  training loss:		0.327392
  validation loss:		0.406172
  validation accuracy:		87.72 %
Epoch 521 of 2000 took 0.102s
  training loss:		0.321158
  validation loss:		0.412888
  validation accuracy:		87.17 %
Epoch 522 of 2000 took 0.101s
  training loss:		0.330980
  validation loss:		0.424442
  validation accuracy:		87.72 %
Epoch 523 of 2000 took 0.101s
  training loss:		0.323181
  validation loss:		0.400897
  validation accuracy:		86.96 %
Epoch 524 of 2000 took 0.101s
  training loss:		0.318197
  validation loss:		0.398533
  validation accuracy:		87.83 %
Epoch 525 of 2000 took 0.101s
  training loss:		0.322775
  validation loss:		0.399855
  validation accuracy:		87.61 %
Epoch 526 of 2000 took 0.101s
  training loss:		0.315990
  validation loss:		0.403243
  validation accuracy:		87.61 %
Epoch 527 of 2000 took 0.101s
  training loss:		0.318976
  validation loss:		0.390128
  validation accuracy:		87.61 %
Epoch 528 of 2000 took 0.101s
  training loss:		0.309010
  validation loss:		0.417369
  validation accuracy:		87.72 %
Epoch 529 of 2000 took 0.106s
  training loss:		0.319210
  validation loss:		0.418744
  validation accuracy:		87.39 %
Epoch 530 of 2000 took 0.105s
  training loss:		0.314086
  validation loss:		0.391772
  validation accuracy:		88.37 %
Epoch 531 of 2000 took 0.102s
  training loss:		0.315908
  validation loss:		0.422829
  validation accuracy:		87.93 %
Epoch 532 of 2000 took 0.105s
  training loss:		0.329337
  validation loss:		0.403041
  validation accuracy:		87.93 %
Epoch 533 of 2000 took 0.106s
  training loss:		0.313043
  validation loss:		0.392977
  validation accuracy:		88.04 %
Epoch 534 of 2000 took 0.106s
  training loss:		0.316654
  validation loss:		0.422843
  validation accuracy:		87.50 %
Epoch 535 of 2000 took 0.106s
  training loss:		0.319197
  validation loss:		0.412642
  validation accuracy:		87.93 %
Epoch 536 of 2000 took 0.106s
  training loss:		0.316305
  validation loss:		0.386790
  validation accuracy:		88.04 %
Epoch 537 of 2000 took 0.106s
  training loss:		0.316726
  validation loss:		0.434720
  validation accuracy:		87.39 %
Epoch 538 of 2000 took 0.106s
  training loss:		0.309230
  validation loss:		0.414871
  validation accuracy:		87.72 %
Epoch 539 of 2000 took 0.106s
  training loss:		0.315876
  validation loss:		0.418012
  validation accuracy:		87.50 %
Epoch 540 of 2000 took 0.106s
  training loss:		0.317064
  validation loss:		0.417559
  validation accuracy:		87.72 %
Epoch 541 of 2000 took 0.106s
  training loss:		0.317483
  validation loss:		0.435155
  validation accuracy:		86.74 %
Epoch 542 of 2000 took 0.106s
  training loss:		0.310436
  validation loss:		0.405410
  validation accuracy:		88.04 %
Epoch 543 of 2000 took 0.106s
  training loss:		0.316807
  validation loss:		0.404337
  validation accuracy:		87.50 %
Epoch 544 of 2000 took 0.106s
  training loss:		0.318780
  validation loss:		0.404514
  validation accuracy:		88.37 %
Epoch 545 of 2000 took 0.111s
  training loss:		0.313349
  validation loss:		0.421631
  validation accuracy:		87.83 %
Epoch 546 of 2000 took 0.106s
  training loss:		0.319477
  validation loss:		0.400077
  validation accuracy:		88.37 %
Epoch 547 of 2000 took 0.106s
  training loss:		0.318708
  validation loss:		0.399555
  validation accuracy:		88.26 %
Epoch 548 of 2000 took 0.106s
  training loss:		0.309700
  validation loss:		0.403061
  validation accuracy:		88.04 %
Epoch 549 of 2000 took 0.106s
  training loss:		0.314029
  validation loss:		0.408456
  validation accuracy:		88.04 %
Epoch 550 of 2000 took 0.106s
  training loss:		0.315926
  validation loss:		0.400337
  validation accuracy:		88.26 %
Epoch 551 of 2000 took 0.106s
  training loss:		0.307569
  validation loss:		0.410158
  validation accuracy:		88.04 %
Epoch 552 of 2000 took 0.106s
  training loss:		0.314394
  validation loss:		0.391905
  validation accuracy:		87.61 %
Epoch 553 of 2000 took 0.106s
  training loss:		0.317827
  validation loss:		0.388373
  validation accuracy:		87.83 %
Epoch 554 of 2000 took 0.106s
  training loss:		0.308953
  validation loss:		0.407180
  validation accuracy:		88.15 %
Epoch 555 of 2000 took 0.106s
  training loss:		0.308459
  validation loss:		0.410170
  validation accuracy:		88.26 %
Epoch 556 of 2000 took 0.106s
  training loss:		0.318356
  validation loss:		0.397025
  validation accuracy:		87.61 %
Epoch 557 of 2000 took 0.106s
  training loss:		0.309276
  validation loss:		0.435350
  validation accuracy:		87.39 %
Epoch 558 of 2000 took 0.107s
  training loss:		0.308920
  validation loss:		0.424827
  validation accuracy:		87.93 %
Epoch 559 of 2000 took 0.106s
  training loss:		0.303596
  validation loss:		0.423461
  validation accuracy:		87.50 %
Epoch 560 of 2000 took 0.106s
  training loss:		0.313568
  validation loss:		0.430333
  validation accuracy:		87.50 %
Epoch 561 of 2000 took 0.106s
  training loss:		0.311433
  validation loss:		0.394896
  validation accuracy:		88.70 %
Epoch 562 of 2000 took 0.106s
  training loss:		0.304514
  validation loss:		0.402290
  validation accuracy:		88.37 %
Epoch 563 of 2000 took 0.106s
  training loss:		0.299971
  validation loss:		0.400682
  validation accuracy:		88.15 %
Epoch 564 of 2000 took 0.106s
  training loss:		0.306824
  validation loss:		0.428327
  validation accuracy:		87.50 %
Epoch 565 of 2000 took 0.106s
  training loss:		0.308294
  validation loss:		0.411919
  validation accuracy:		88.04 %
Epoch 566 of 2000 took 0.106s
  training loss:		0.317093
  validation loss:		0.404608
  validation accuracy:		88.15 %
Epoch 567 of 2000 took 0.106s
  training loss:		0.313906
  validation loss:		0.402367
  validation accuracy:		88.04 %
Epoch 568 of 2000 took 0.106s
  training loss:		0.307346
  validation loss:		0.423697
  validation accuracy:		87.61 %
Epoch 569 of 2000 took 0.106s
  training loss:		0.302960
  validation loss:		0.418506
  validation accuracy:		87.50 %
Epoch 570 of 2000 took 0.106s
  training loss:		0.314350
  validation loss:		0.411573
  validation accuracy:		87.72 %
Epoch 571 of 2000 took 0.106s
  training loss:		0.308492
  validation loss:		0.422143
  validation accuracy:		87.61 %
Epoch 572 of 2000 took 0.106s
  training loss:		0.305013
  validation loss:		0.409241
  validation accuracy:		88.04 %
Epoch 573 of 2000 took 0.106s
  training loss:		0.306862
  validation loss:		0.391785
  validation accuracy:		88.26 %
Epoch 574 of 2000 took 0.106s
  training loss:		0.299546
  validation loss:		0.407779
  validation accuracy:		88.04 %
Epoch 575 of 2000 took 0.106s
  training loss:		0.304549
  validation loss:		0.409053
  validation accuracy:		88.48 %
Epoch 576 of 2000 took 0.107s
  training loss:		0.304344
  validation loss:		0.421620
  validation accuracy:		88.04 %
Epoch 577 of 2000 took 0.112s
  training loss:		0.303452
  validation loss:		0.390204
  validation accuracy:		88.37 %
Epoch 578 of 2000 took 0.111s
  training loss:		0.311067
  validation loss:		0.418262
  validation accuracy:		87.83 %
Epoch 579 of 2000 took 0.109s
  training loss:		0.300947
  validation loss:		0.411073
  validation accuracy:		88.26 %
Epoch 580 of 2000 took 0.109s
  training loss:		0.301808
  validation loss:		0.406767
  validation accuracy:		88.48 %
Epoch 581 of 2000 took 0.109s
  training loss:		0.302909
  validation loss:		0.424300
  validation accuracy:		88.26 %
Epoch 582 of 2000 took 0.109s
  training loss:		0.310795
  validation loss:		0.414888
  validation accuracy:		87.93 %
Epoch 583 of 2000 took 0.106s
  training loss:		0.301929
  validation loss:		0.405587
  validation accuracy:		88.15 %
Epoch 584 of 2000 took 0.106s
  training loss:		0.306183
  validation loss:		0.393916
  validation accuracy:		88.26 %
Epoch 585 of 2000 took 0.115s
  training loss:		0.302034
  validation loss:		0.409972
  validation accuracy:		88.26 %
Epoch 586 of 2000 took 0.156s
  training loss:		0.306080
  validation loss:		0.421050
  validation accuracy:		87.72 %
Epoch 587 of 2000 took 0.104s
  training loss:		0.297132
  validation loss:		0.398133
  validation accuracy:		88.15 %
Epoch 588 of 2000 took 0.102s
  training loss:		0.294926
  validation loss:		0.417756
  validation accuracy:		88.48 %
Epoch 589 of 2000 took 0.104s
  training loss:		0.299230
  validation loss:		0.418565
  validation accuracy:		87.83 %
Epoch 590 of 2000 took 0.107s
  training loss:		0.303511
  validation loss:		0.437479
  validation accuracy:		87.61 %
Epoch 591 of 2000 took 0.104s
  training loss:		0.301438
  validation loss:		0.429848
  validation accuracy:		87.83 %
Epoch 592 of 2000 took 0.103s
  training loss:		0.301065
  validation loss:		0.408029
  validation accuracy:		88.15 %
Epoch 593 of 2000 took 0.102s
  training loss:		0.299045
  validation loss:		0.446492
  validation accuracy:		87.28 %
Epoch 594 of 2000 took 0.102s
  training loss:		0.301643
  validation loss:		0.432083
  validation accuracy:		88.15 %
Epoch 595 of 2000 took 0.102s
  training loss:		0.300480
  validation loss:		0.423487
  validation accuracy:		87.83 %
Epoch 596 of 2000 took 0.101s
  training loss:		0.294684
  validation loss:		0.416934
  validation accuracy:		88.48 %
Epoch 597 of 2000 took 0.102s
  training loss:		0.296765
  validation loss:		0.408484
  validation accuracy:		88.37 %
Epoch 598 of 2000 took 0.102s
  training loss:		0.299450
  validation loss:		0.431825
  validation accuracy:		87.50 %
Epoch 599 of 2000 took 0.102s
  training loss:		0.304054
  validation loss:		0.431191
  validation accuracy:		87.93 %
Epoch 600 of 2000 took 0.102s
  training loss:		0.302142
  validation loss:		0.442453
  validation accuracy:		87.83 %
Epoch 601 of 2000 took 0.101s
  training loss:		0.293239
  validation loss:		0.407979
  validation accuracy:		88.26 %
Epoch 602 of 2000 took 0.102s
  training loss:		0.294271
  validation loss:		0.404841
  validation accuracy:		88.80 %
Epoch 603 of 2000 took 0.101s
  training loss:		0.301781
  validation loss:		0.404801
  validation accuracy:		87.83 %
Epoch 604 of 2000 took 0.102s
  training loss:		0.296588
  validation loss:		0.411563
  validation accuracy:		88.04 %
Epoch 605 of 2000 took 0.102s
  training loss:		0.293771
  validation loss:		0.413081
  validation accuracy:		88.59 %
Epoch 606 of 2000 took 0.102s
  training loss:		0.296857
  validation loss:		0.441565
  validation accuracy:		87.28 %
Epoch 607 of 2000 took 0.102s
  training loss:		0.302084
  validation loss:		0.396195
  validation accuracy:		88.37 %
Epoch 608 of 2000 took 0.102s
  training loss:		0.294799
  validation loss:		0.417340
  validation accuracy:		88.59 %
Epoch 609 of 2000 took 0.102s
  training loss:		0.297002
  validation loss:		0.400485
  validation accuracy:		89.13 %
Epoch 610 of 2000 took 0.101s
  training loss:		0.296225
  validation loss:		0.405410
  validation accuracy:		88.37 %
Epoch 611 of 2000 took 0.104s
  training loss:		0.295851
  validation loss:		0.441349
  validation accuracy:		87.28 %
Epoch 612 of 2000 took 0.102s
  training loss:		0.299359
  validation loss:		0.395915
  validation accuracy:		88.26 %
Epoch 613 of 2000 took 0.102s
  training loss:		0.303978
  validation loss:		0.399034
  validation accuracy:		89.02 %
Epoch 614 of 2000 took 0.101s
  training loss:		0.299851
  validation loss:		0.409835
  validation accuracy:		88.26 %
Epoch 615 of 2000 took 0.103s
  training loss:		0.287160
  validation loss:		0.419095
  validation accuracy:		88.15 %
Epoch 616 of 2000 took 0.102s
  training loss:		0.301799
  validation loss:		0.397132
  validation accuracy:		88.70 %
Epoch 617 of 2000 took 0.102s
  training loss:		0.294261
  validation loss:		0.409503
  validation accuracy:		88.48 %
Epoch 618 of 2000 took 0.101s
  training loss:		0.290812
  validation loss:		0.417274
  validation accuracy:		88.37 %
Epoch 619 of 2000 took 0.103s
  training loss:		0.299892
  validation loss:		0.418841
  validation accuracy:		88.80 %
Epoch 620 of 2000 took 0.101s
  training loss:		0.293669
  validation loss:		0.443982
  validation accuracy:		87.07 %
Epoch 621 of 2000 took 0.101s
  training loss:		0.294326
  validation loss:		0.391399
  validation accuracy:		88.80 %
Epoch 622 of 2000 took 0.103s
  training loss:		0.306634
  validation loss:		0.409039
  validation accuracy:		89.02 %
Epoch 623 of 2000 took 0.101s
  training loss:		0.292645
  validation loss:		0.398972
  validation accuracy:		88.70 %
Epoch 624 of 2000 took 0.104s
  training loss:		0.298433
  validation loss:		0.402390
  validation accuracy:		88.91 %
Epoch 625 of 2000 took 0.104s
  training loss:		0.293678
  validation loss:		0.423748
  validation accuracy:		87.83 %
Epoch 626 of 2000 took 0.169s
  training loss:		0.295101
  validation loss:		0.434952
  validation accuracy:		87.72 %
Epoch 627 of 2000 took 0.166s
  training loss:		0.285993
  validation loss:		0.403258
  validation accuracy:		89.02 %
Epoch 628 of 2000 took 0.166s
  training loss:		0.298566
  validation loss:		0.412329
  validation accuracy:		88.59 %
Epoch 629 of 2000 took 0.166s
  training loss:		0.297574
  validation loss:		0.410684
  validation accuracy:		88.26 %
Epoch 630 of 2000 took 0.166s
  training loss:		0.296630
  validation loss:		0.410783
  validation accuracy:		88.70 %
Epoch 631 of 2000 took 0.166s
  training loss:		0.287295
  validation loss:		0.418391
  validation accuracy:		88.04 %
Epoch 632 of 2000 took 0.166s
  training loss:		0.292959
  validation loss:		0.414343
  validation accuracy:		88.37 %
Epoch 633 of 2000 took 0.166s
  training loss:		0.292774
  validation loss:		0.439003
  validation accuracy:		86.96 %
Epoch 634 of 2000 took 0.166s
  training loss:		0.290940
  validation loss:		0.409783
  validation accuracy:		88.04 %
Epoch 635 of 2000 took 0.166s
  training loss:		0.289473
  validation loss:		0.401254
  validation accuracy:		88.91 %
Epoch 636 of 2000 took 0.166s
  training loss:		0.283124
  validation loss:		0.415503
  validation accuracy:		88.37 %
Epoch 637 of 2000 took 0.122s
  training loss:		0.297475
  validation loss:		0.407237
  validation accuracy:		88.48 %
Epoch 638 of 2000 took 0.102s
  training loss:		0.290754
  validation loss:		0.395651
  validation accuracy:		88.91 %
Epoch 639 of 2000 took 0.103s
  training loss:		0.288602
  validation loss:		0.440634
  validation accuracy:		88.15 %
Epoch 640 of 2000 took 0.105s
  training loss:		0.290170
  validation loss:		0.405246
  validation accuracy:		88.37 %
Epoch 641 of 2000 took 0.113s
  training loss:		0.291383
  validation loss:		0.410349
  validation accuracy:		88.80 %
Epoch 642 of 2000 took 0.104s
  training loss:		0.282258
  validation loss:		0.417008
  validation accuracy:		88.37 %
Epoch 643 of 2000 took 0.101s
  training loss:		0.285969
  validation loss:		0.405315
  validation accuracy:		88.15 %
Epoch 644 of 2000 took 0.102s
  training loss:		0.285251
  validation loss:		0.445100
  validation accuracy:		87.93 %
Epoch 645 of 2000 took 0.102s
  training loss:		0.293295
  validation loss:		0.422614
  validation accuracy:		87.72 %
Epoch 646 of 2000 took 0.101s
  training loss:		0.301026
  validation loss:		0.420205
  validation accuracy:		88.59 %
Epoch 647 of 2000 took 0.102s
  training loss:		0.281513
  validation loss:		0.408503
  validation accuracy:		88.59 %
Epoch 648 of 2000 took 0.102s
  training loss:		0.294518
  validation loss:		0.443804
  validation accuracy:		88.04 %
Epoch 649 of 2000 took 0.101s
  training loss:		0.297904
  validation loss:		0.388435
  validation accuracy:		88.80 %
Epoch 650 of 2000 took 0.102s
  training loss:		0.294522
  validation loss:		0.398459
  validation accuracy:		88.37 %
Epoch 651 of 2000 took 0.101s
  training loss:		0.286816
  validation loss:		0.419372
  validation accuracy:		87.93 %
Epoch 652 of 2000 took 0.104s
  training loss:		0.293125
  validation loss:		0.396254
  validation accuracy:		89.13 %
Epoch 653 of 2000 took 0.106s
  training loss:		0.284798
  validation loss:		0.435407
  validation accuracy:		88.26 %
Epoch 654 of 2000 took 0.101s
  training loss:		0.294403
  validation loss:		0.402194
  validation accuracy:		89.46 %
Epoch 655 of 2000 took 0.103s
  training loss:		0.282612
  validation loss:		0.424926
  validation accuracy:		87.50 %
Epoch 656 of 2000 took 0.106s
  training loss:		0.283129
  validation loss:		0.407424
  validation accuracy:		88.70 %
Epoch 657 of 2000 took 0.106s
  training loss:		0.287052
  validation loss:		0.406942
  validation accuracy:		88.80 %
Epoch 658 of 2000 took 0.106s
  training loss:		0.270649
  validation loss:		0.426503
  validation accuracy:		88.26 %
Epoch 659 of 2000 took 0.106s
  training loss:		0.283680
  validation loss:		0.437435
  validation accuracy:		87.83 %
Epoch 660 of 2000 took 0.106s
  training loss:		0.293776
  validation loss:		0.395305
  validation accuracy:		88.59 %
Epoch 661 of 2000 took 0.106s
  training loss:		0.289490
  validation loss:		0.425758
  validation accuracy:		88.48 %
Epoch 662 of 2000 took 0.106s
  training loss:		0.292758
  validation loss:		0.397844
  validation accuracy:		88.26 %
Epoch 663 of 2000 took 0.107s
  training loss:		0.287589
  validation loss:		0.406765
  validation accuracy:		87.72 %
Epoch 664 of 2000 took 0.107s
  training loss:		0.283127
  validation loss:		0.403564
  validation accuracy:		89.13 %
Epoch 665 of 2000 took 0.107s
  training loss:		0.286954
  validation loss:		0.415336
  validation accuracy:		87.28 %
Epoch 666 of 2000 took 0.107s
  training loss:		0.280616
  validation loss:		0.398340
  validation accuracy:		88.80 %
Epoch 667 of 2000 took 0.106s
  training loss:		0.287111
  validation loss:		0.415846
  validation accuracy:		88.59 %
Epoch 668 of 2000 took 0.106s
  training loss:		0.284075
  validation loss:		0.427515
  validation accuracy:		88.26 %
Epoch 669 of 2000 took 0.106s
  training loss:		0.288571
  validation loss:		0.428901
  validation accuracy:		88.48 %
Epoch 670 of 2000 took 0.106s
  training loss:		0.271949
  validation loss:		0.411137
  validation accuracy:		88.59 %
Epoch 671 of 2000 took 0.106s
  training loss:		0.282257
  validation loss:		0.414228
  validation accuracy:		88.37 %
Epoch 672 of 2000 took 0.106s
  training loss:		0.282855
  validation loss:		0.405818
  validation accuracy:		88.04 %
Epoch 673 of 2000 took 0.106s
  training loss:		0.281791
  validation loss:		0.419726
  validation accuracy:		88.91 %
Epoch 674 of 2000 took 0.106s
  training loss:		0.291933
  validation loss:		0.393106
  validation accuracy:		89.13 %
Epoch 675 of 2000 took 0.107s
  training loss:		0.279861
  validation loss:		0.403886
  validation accuracy:		89.24 %
Epoch 676 of 2000 took 0.107s
  training loss:		0.283018
  validation loss:		0.421107
  validation accuracy:		87.83 %
Epoch 677 of 2000 took 0.106s
  training loss:		0.283619
  validation loss:		0.417012
  validation accuracy:		88.91 %
Epoch 678 of 2000 took 0.106s
  training loss:		0.287249
  validation loss:		0.431191
  validation accuracy:		88.37 %
Epoch 679 of 2000 took 0.106s
  training loss:		0.285376
  validation loss:		0.441396
  validation accuracy:		88.26 %
Epoch 680 of 2000 took 0.106s
  training loss:		0.286871
  validation loss:		0.422051
  validation accuracy:		88.04 %
Epoch 681 of 2000 took 0.106s
  training loss:		0.279138
  validation loss:		0.433259
  validation accuracy:		88.26 %
Epoch 682 of 2000 took 0.106s
  training loss:		0.278598
  validation loss:		0.410873
  validation accuracy:		88.37 %
Epoch 683 of 2000 took 0.106s
  training loss:		0.282066
  validation loss:		0.439662
  validation accuracy:		88.26 %
Epoch 684 of 2000 took 0.106s
  training loss:		0.279613
  validation loss:		0.415536
  validation accuracy:		88.26 %
Epoch 685 of 2000 took 0.106s
  training loss:		0.281412
  validation loss:		0.417948
  validation accuracy:		89.02 %
Epoch 686 of 2000 took 0.106s
  training loss:		0.285730
  validation loss:		0.430245
  validation accuracy:		87.72 %
Epoch 687 of 2000 took 0.106s
  training loss:		0.283654
  validation loss:		0.409488
  validation accuracy:		89.02 %
Epoch 688 of 2000 took 0.106s
  training loss:		0.278487
  validation loss:		0.416027
  validation accuracy:		89.02 %
Epoch 689 of 2000 took 0.106s
  training loss:		0.278374
  validation loss:		0.421063
  validation accuracy:		89.13 %
Epoch 690 of 2000 took 0.111s
  training loss:		0.276078
  validation loss:		0.405161
  validation accuracy:		88.91 %
Epoch 691 of 2000 took 0.107s
  training loss:		0.287876
  validation loss:		0.420911
  validation accuracy:		88.59 %
Epoch 692 of 2000 took 0.107s
  training loss:		0.277764
  validation loss:		0.415242
  validation accuracy:		89.13 %
Epoch 693 of 2000 took 0.106s
  training loss:		0.269146
  validation loss:		0.410990
  validation accuracy:		88.37 %
Epoch 694 of 2000 took 0.106s
  training loss:		0.275953
  validation loss:		0.402643
  validation accuracy:		88.91 %
Epoch 695 of 2000 took 0.107s
  training loss:		0.279844
  validation loss:		0.408495
  validation accuracy:		88.04 %
Epoch 696 of 2000 took 0.106s
  training loss:		0.282550
  validation loss:		0.414295
  validation accuracy:		89.13 %
Epoch 697 of 2000 took 0.106s
  training loss:		0.280716
  validation loss:		0.412866
  validation accuracy:		88.70 %
Epoch 698 of 2000 took 0.106s
  training loss:		0.279946
  validation loss:		0.407387
  validation accuracy:		88.70 %
Epoch 699 of 2000 took 0.106s
  training loss:		0.278360
  validation loss:		0.391985
  validation accuracy:		89.35 %
Epoch 700 of 2000 took 0.107s
  training loss:		0.272770
  validation loss:		0.391373
  validation accuracy:		89.02 %
Epoch 701 of 2000 took 0.106s
  training loss:		0.283542
  validation loss:		0.419595
  validation accuracy:		87.72 %
Epoch 702 of 2000 took 0.106s
  training loss:		0.278471
  validation loss:		0.415734
  validation accuracy:		89.24 %
Epoch 703 of 2000 took 0.107s
  training loss:		0.277299
  validation loss:		0.402106
  validation accuracy:		88.48 %
Epoch 704 of 2000 took 0.106s
  training loss:		0.280243
  validation loss:		0.409592
  validation accuracy:		88.37 %
Epoch 705 of 2000 took 0.106s
  training loss:		0.273231
  validation loss:		0.396290
  validation accuracy:		89.02 %
Epoch 706 of 2000 took 0.106s
  training loss:		0.280794
  validation loss:		0.399437
  validation accuracy:		89.46 %
Epoch 707 of 2000 took 0.106s
  training loss:		0.274777
  validation loss:		0.448778
  validation accuracy:		87.93 %
Epoch 708 of 2000 took 0.106s
  training loss:		0.276624
  validation loss:		0.394660
  validation accuracy:		88.80 %
Epoch 709 of 2000 took 0.106s
  training loss:		0.276291
  validation loss:		0.453931
  validation accuracy:		88.15 %
Epoch 710 of 2000 took 0.106s
  training loss:		0.283183
  validation loss:		0.430907
  validation accuracy:		87.28 %
Epoch 711 of 2000 took 0.106s
  training loss:		0.281816
  validation loss:		0.411117
  validation accuracy:		89.02 %
Epoch 712 of 2000 took 0.106s
  training loss:		0.282228
  validation loss:		0.397554
  validation accuracy:		89.24 %
Epoch 713 of 2000 took 0.106s
  training loss:		0.273803
  validation loss:		0.432959
  validation accuracy:		87.28 %
Epoch 714 of 2000 took 0.106s
  training loss:		0.286435
  validation loss:		0.406803
  validation accuracy:		88.26 %
Epoch 715 of 2000 took 0.106s
  training loss:		0.276449
  validation loss:		0.397458
  validation accuracy:		89.02 %
Epoch 716 of 2000 took 0.106s
  training loss:		0.283097
  validation loss:		0.414184
  validation accuracy:		89.13 %
Epoch 717 of 2000 took 0.106s
  training loss:		0.264438
  validation loss:		0.406091
  validation accuracy:		88.26 %
Epoch 718 of 2000 took 0.106s
  training loss:		0.280421
  validation loss:		0.427814
  validation accuracy:		89.13 %
Epoch 719 of 2000 took 0.106s
  training loss:		0.275234
  validation loss:		0.414483
  validation accuracy:		88.26 %
Epoch 720 of 2000 took 0.106s
  training loss:		0.275643
  validation loss:		0.413241
  validation accuracy:		89.13 %
Epoch 721 of 2000 took 0.106s
  training loss:		0.287990
  validation loss:		0.402398
  validation accuracy:		88.37 %
Epoch 722 of 2000 took 0.106s
  training loss:		0.280502
  validation loss:		0.402258
  validation accuracy:		88.26 %
Epoch 723 of 2000 took 0.108s
  training loss:		0.276641
  validation loss:		0.388336
  validation accuracy:		89.35 %
Epoch 724 of 2000 took 0.109s
  training loss:		0.281252
  validation loss:		0.407116
  validation accuracy:		88.59 %
Epoch 725 of 2000 took 0.109s
  training loss:		0.282019
  validation loss:		0.430323
  validation accuracy:		88.48 %
Epoch 726 of 2000 took 0.109s
  training loss:		0.282243
  validation loss:		0.445566
  validation accuracy:		87.28 %
Epoch 727 of 2000 took 0.109s
  training loss:		0.284225
  validation loss:		0.423565
  validation accuracy:		88.70 %
Epoch 728 of 2000 took 0.109s
  training loss:		0.277705
  validation loss:		0.438131
  validation accuracy:		88.37 %
Epoch 729 of 2000 took 0.109s
  training loss:		0.274157
  validation loss:		0.405142
  validation accuracy:		88.26 %
Epoch 730 of 2000 took 0.109s
  training loss:		0.275958
  validation loss:		0.442617
  validation accuracy:		87.61 %
Epoch 731 of 2000 took 0.109s
  training loss:		0.278363
  validation loss:		0.409905
  validation accuracy:		88.80 %
Epoch 732 of 2000 took 0.109s
  training loss:		0.272399
  validation loss:		0.407967
  validation accuracy:		88.48 %
Epoch 733 of 2000 took 0.109s
  training loss:		0.271363
  validation loss:		0.436600
  validation accuracy:		88.59 %
Epoch 734 of 2000 took 0.109s
  training loss:		0.284116
  validation loss:		0.445549
  validation accuracy:		87.93 %
Epoch 735 of 2000 took 0.109s
  training loss:		0.281942
  validation loss:		0.405954
  validation accuracy:		88.91 %
Epoch 736 of 2000 took 0.109s
  training loss:		0.268776
  validation loss:		0.397716
  validation accuracy:		88.80 %
Epoch 737 of 2000 took 0.109s
  training loss:		0.277176
  validation loss:		0.400605
  validation accuracy:		88.59 %
Epoch 738 of 2000 took 0.109s
  training loss:		0.276497
  validation loss:		0.412867
  validation accuracy:		88.37 %
Epoch 739 of 2000 took 0.109s
  training loss:		0.280082
  validation loss:		0.422853
  validation accuracy:		88.80 %
Epoch 740 of 2000 took 0.109s
  training loss:		0.273488
  validation loss:		0.400894
  validation accuracy:		89.02 %
Epoch 741 of 2000 took 0.108s
  training loss:		0.278603
  validation loss:		0.419460
  validation accuracy:		87.72 %
Epoch 742 of 2000 took 0.106s
  training loss:		0.276073
  validation loss:		0.388327
  validation accuracy:		89.13 %
Epoch 743 of 2000 took 0.106s
  training loss:		0.273245
  validation loss:		0.414154
  validation accuracy:		88.91 %
Epoch 744 of 2000 took 0.106s
  training loss:		0.271162
  validation loss:		0.397820
  validation accuracy:		89.24 %
Epoch 745 of 2000 took 0.106s
  training loss:		0.267589
  validation loss:		0.405336
  validation accuracy:		89.13 %
Epoch 746 of 2000 took 0.106s
  training loss:		0.268526
  validation loss:		0.399261
  validation accuracy:		88.48 %
Epoch 747 of 2000 took 0.106s
  training loss:		0.270029
  validation loss:		0.413466
  validation accuracy:		88.37 %
Epoch 748 of 2000 took 0.106s
  training loss:		0.274920
  validation loss:		0.413785
  validation accuracy:		89.13 %
Epoch 749 of 2000 took 0.106s
  training loss:		0.274610
  validation loss:		0.402558
  validation accuracy:		89.24 %
Epoch 750 of 2000 took 0.106s
  training loss:		0.277862
  validation loss:		0.465063
  validation accuracy:		87.72 %
Epoch 751 of 2000 took 0.107s
  training loss:		0.269557
  validation loss:		0.415128
  validation accuracy:		88.59 %
Epoch 752 of 2000 took 0.106s
  training loss:		0.279077
  validation loss:		0.407344
  validation accuracy:		89.35 %
Epoch 753 of 2000 took 0.106s
  training loss:		0.277414
  validation loss:		0.429734
  validation accuracy:		88.80 %
Epoch 754 of 2000 took 0.106s
  training loss:		0.270786
  validation loss:		0.427253
  validation accuracy:		88.91 %
Epoch 755 of 2000 took 0.106s
  training loss:		0.276876
  validation loss:		0.418647
  validation accuracy:		88.91 %
Epoch 756 of 2000 took 0.106s
  training loss:		0.277996
  validation loss:		0.396788
  validation accuracy:		88.80 %
Epoch 757 of 2000 took 0.106s
  training loss:		0.272601
  validation loss:		0.441536
  validation accuracy:		88.26 %
Epoch 758 of 2000 took 0.106s
  training loss:		0.275054
  validation loss:		0.428986
  validation accuracy:		89.02 %
Epoch 759 of 2000 took 0.106s
  training loss:		0.271262
  validation loss:		0.402998
  validation accuracy:		88.37 %
Epoch 760 of 2000 took 0.106s
  training loss:		0.281945
  validation loss:		0.424094
  validation accuracy:		89.46 %
Epoch 761 of 2000 took 0.106s
  training loss:		0.270029
  validation loss:		0.422269
  validation accuracy:		88.70 %
Epoch 762 of 2000 took 0.106s
  training loss:		0.276255
  validation loss:		0.405426
  validation accuracy:		88.91 %
Epoch 763 of 2000 took 0.106s
  training loss:		0.273523
  validation loss:		0.409802
  validation accuracy:		88.80 %
Epoch 764 of 2000 took 0.106s
  training loss:		0.276633
  validation loss:		0.415239
  validation accuracy:		89.13 %
Epoch 765 of 2000 took 0.106s
  training loss:		0.273979
  validation loss:		0.412174
  validation accuracy:		89.35 %
Epoch 766 of 2000 took 0.106s
  training loss:		0.274052
  validation loss:		0.385029
  validation accuracy:		89.35 %
Epoch 767 of 2000 took 0.106s
  training loss:		0.270801
  validation loss:		0.406330
  validation accuracy:		88.26 %
Epoch 768 of 2000 took 0.106s
  training loss:		0.274457
  validation loss:		0.415751
  validation accuracy:		88.91 %
Epoch 769 of 2000 took 0.106s
  training loss:		0.275970
  validation loss:		0.424511
  validation accuracy:		88.70 %
Epoch 770 of 2000 took 0.106s
  training loss:		0.268069
  validation loss:		0.402892
  validation accuracy:		88.91 %
Epoch 771 of 2000 took 0.106s
  training loss:		0.271798
  validation loss:		0.392247
  validation accuracy:		89.02 %
Epoch 772 of 2000 took 0.106s
  training loss:		0.273253
  validation loss:		0.445598
  validation accuracy:		88.70 %
Epoch 773 of 2000 took 0.106s
  training loss:		0.273117
  validation loss:		0.403068
  validation accuracy:		88.59 %
Epoch 774 of 2000 took 0.106s
  training loss:		0.274727
  validation loss:		0.424705
  validation accuracy:		89.02 %
Epoch 775 of 2000 took 0.106s
  training loss:		0.269730
  validation loss:		0.379959
  validation accuracy:		89.57 %
Epoch 776 of 2000 took 0.106s
  training loss:		0.268652
  validation loss:		0.438565
  validation accuracy:		88.15 %
Epoch 777 of 2000 took 0.106s
  training loss:		0.275879
  validation loss:		0.384735
  validation accuracy:		88.80 %
Epoch 778 of 2000 took 0.106s
  training loss:		0.272146
  validation loss:		0.402681
  validation accuracy:		88.70 %
Epoch 779 of 2000 took 0.107s
  training loss:		0.276410
  validation loss:		0.387798
  validation accuracy:		88.91 %
Epoch 780 of 2000 took 0.106s
  training loss:		0.268809
  validation loss:		0.404497
  validation accuracy:		88.80 %
Epoch 781 of 2000 took 0.106s
  training loss:		0.272652
  validation loss:		0.423207
  validation accuracy:		88.70 %
Epoch 782 of 2000 took 0.106s
  training loss:		0.265616
  validation loss:		0.399292
  validation accuracy:		88.59 %
Epoch 783 of 2000 took 0.106s
  training loss:		0.265012
  validation loss:		0.414013
  validation accuracy:		89.02 %
Epoch 784 of 2000 took 0.106s
  training loss:		0.266914
  validation loss:		0.414882
  validation accuracy:		87.39 %
Epoch 785 of 2000 took 0.106s
  training loss:		0.271801
  validation loss:		0.398366
  validation accuracy:		88.70 %
Epoch 786 of 2000 took 0.106s
  training loss:		0.264115
  validation loss:		0.386450
  validation accuracy:		88.91 %
Epoch 787 of 2000 took 0.106s
  training loss:		0.273311
  validation loss:		0.397047
  validation accuracy:		88.59 %
Epoch 788 of 2000 took 0.106s
  training loss:		0.276104
  validation loss:		0.406728
  validation accuracy:		89.02 %
Epoch 789 of 2000 took 0.106s
  training loss:		0.267256
  validation loss:		0.409133
  validation accuracy:		89.46 %
Epoch 790 of 2000 took 0.106s
  training loss:		0.269690
  validation loss:		0.395269
  validation accuracy:		89.02 %
Epoch 791 of 2000 took 0.106s
  training loss:		0.264423
  validation loss:		0.412180
  validation accuracy:		88.37 %
Epoch 792 of 2000 took 0.106s
  training loss:		0.264818
  validation loss:		0.393781
  validation accuracy:		89.02 %
Epoch 793 of 2000 took 0.106s
  training loss:		0.260163
  validation loss:		0.409614
  validation accuracy:		88.80 %
Epoch 794 of 2000 took 0.106s
  training loss:		0.272110
  validation loss:		0.407809
  validation accuracy:		88.80 %
Epoch 795 of 2000 took 0.111s
  training loss:		0.264987
  validation loss:		0.421957
  validation accuracy:		88.80 %
Epoch 796 of 2000 took 0.106s
  training loss:		0.267252
  validation loss:		0.395940
  validation accuracy:		89.24 %
Epoch 797 of 2000 took 0.106s
  training loss:		0.263297
  validation loss:		0.389936
  validation accuracy:		89.13 %
Epoch 798 of 2000 took 0.106s
  training loss:		0.276602
  validation loss:		0.408128
  validation accuracy:		88.91 %
Epoch 799 of 2000 took 0.106s
  training loss:		0.270786
  validation loss:		0.402464
  validation accuracy:		88.37 %
Epoch 800 of 2000 took 0.106s
  training loss:		0.265559
  validation loss:		0.388870
  validation accuracy:		89.13 %
Epoch 801 of 2000 took 0.106s
  training loss:		0.264926
  validation loss:		0.396199
  validation accuracy:		88.80 %
Epoch 802 of 2000 took 0.106s
  training loss:		0.274937
  validation loss:		0.394412
  validation accuracy:		89.24 %
Epoch 803 of 2000 took 0.106s
  training loss:		0.264498
  validation loss:		0.401061
  validation accuracy:		88.59 %
Epoch 804 of 2000 took 0.106s
  training loss:		0.270070
  validation loss:		0.384134
  validation accuracy:		89.35 %
Epoch 805 of 2000 took 0.106s
  training loss:		0.270350
  validation loss:		0.403315
  validation accuracy:		89.24 %
Epoch 806 of 2000 took 0.106s
  training loss:		0.266807
  validation loss:		0.398190
  validation accuracy:		89.02 %
Epoch 807 of 2000 took 0.106s
  training loss:		0.267857
  validation loss:		0.408318
  validation accuracy:		89.13 %
Epoch 808 of 2000 took 0.107s
  training loss:		0.263080
  validation loss:		0.386877
  validation accuracy:		89.24 %
Epoch 809 of 2000 took 0.106s
  training loss:		0.262119
  validation loss:		0.393668
  validation accuracy:		89.24 %
Epoch 810 of 2000 took 0.106s
  training loss:		0.267359
  validation loss:		0.434660
  validation accuracy:		88.37 %
Epoch 811 of 2000 took 0.106s
  training loss:		0.260387
  validation loss:		0.396308
  validation accuracy:		88.91 %
Epoch 812 of 2000 took 0.106s
  training loss:		0.258063
  validation loss:		0.410989
  validation accuracy:		88.59 %
Epoch 813 of 2000 took 0.106s
  training loss:		0.271894
  validation loss:		0.385294
  validation accuracy:		89.35 %
Epoch 814 of 2000 took 0.106s
  training loss:		0.273916
  validation loss:		0.410705
  validation accuracy:		88.91 %
Epoch 815 of 2000 took 0.106s
  training loss:		0.272901
  validation loss:		0.411388
  validation accuracy:		89.02 %
Epoch 816 of 2000 took 0.106s
  training loss:		0.268718
  validation loss:		0.392827
  validation accuracy:		88.70 %
Epoch 817 of 2000 took 0.106s
  training loss:		0.267986
  validation loss:		0.395741
  validation accuracy:		88.70 %
Epoch 818 of 2000 took 0.106s
  training loss:		0.258632
  validation loss:		0.392099
  validation accuracy:		89.78 %
Epoch 819 of 2000 took 0.106s
  training loss:		0.260127
  validation loss:		0.389584
  validation accuracy:		89.46 %
Epoch 820 of 2000 took 0.106s
  training loss:		0.260500
  validation loss:		0.438173
  validation accuracy:		88.04 %
Epoch 821 of 2000 took 0.106s
  training loss:		0.265717
  validation loss:		0.401945
  validation accuracy:		89.13 %
Epoch 822 of 2000 took 0.106s
  training loss:		0.269144
  validation loss:		0.384234
  validation accuracy:		89.57 %
Epoch 823 of 2000 took 0.106s
  training loss:		0.266099
  validation loss:		0.397028
  validation accuracy:		88.91 %
Epoch 824 of 2000 took 0.106s
  training loss:		0.267560
  validation loss:		0.385721
  validation accuracy:		88.91 %
Epoch 825 of 2000 took 0.106s
  training loss:		0.261229
  validation loss:		0.419894
  validation accuracy:		88.26 %
Epoch 826 of 2000 took 0.106s
  training loss:		0.272496
  validation loss:		0.397034
  validation accuracy:		89.02 %
Epoch 827 of 2000 took 0.106s
  training loss:		0.262135
  validation loss:		0.384894
  validation accuracy:		89.13 %
Epoch 828 of 2000 took 0.106s
  training loss:		0.264900
  validation loss:		0.431282
  validation accuracy:		87.72 %
Epoch 829 of 2000 took 0.106s
  training loss:		0.262236
  validation loss:		0.386694
  validation accuracy:		89.35 %
Epoch 830 of 2000 took 0.106s
  training loss:		0.273630
  validation loss:		0.401367
  validation accuracy:		89.02 %
Epoch 831 of 2000 took 0.106s
  training loss:		0.262097
  validation loss:		0.403020
  validation accuracy:		89.24 %
Epoch 832 of 2000 took 0.106s
  training loss:		0.265888
  validation loss:		0.391826
  validation accuracy:		89.67 %
Epoch 833 of 2000 took 0.106s
  training loss:		0.261929
  validation loss:		0.399510
  validation accuracy:		89.02 %
Epoch 834 of 2000 took 0.106s
  training loss:		0.264272
  validation loss:		0.387100
  validation accuracy:		89.57 %
Epoch 835 of 2000 took 0.106s
  training loss:		0.257922
  validation loss:		0.399566
  validation accuracy:		89.24 %
Epoch 836 of 2000 took 0.107s
  training loss:		0.260045
  validation loss:		0.408691
  validation accuracy:		88.37 %
Epoch 837 of 2000 took 0.106s
  training loss:		0.260303
  validation loss:		0.420975
  validation accuracy:		88.26 %
Epoch 838 of 2000 took 0.106s
  training loss:		0.260286
  validation loss:		0.413246
  validation accuracy:		88.37 %
Epoch 839 of 2000 took 0.106s
  training loss:		0.263295
  validation loss:		0.446370
  validation accuracy:		88.26 %
Epoch 840 of 2000 took 0.106s
  training loss:		0.265017
  validation loss:		0.388775
  validation accuracy:		89.46 %
Epoch 841 of 2000 took 0.106s
  training loss:		0.264881
  validation loss:		0.392720
  validation accuracy:		88.91 %
Epoch 842 of 2000 took 0.106s
  training loss:		0.257837
  validation loss:		0.405391
  validation accuracy:		88.91 %
Epoch 843 of 2000 took 0.106s
  training loss:		0.262025
  validation loss:		0.410264
  validation accuracy:		88.15 %
Epoch 844 of 2000 took 0.106s
  training loss:		0.260299
  validation loss:		0.392802
  validation accuracy:		88.80 %
Epoch 845 of 2000 took 0.106s
  training loss:		0.259625
  validation loss:		0.400334
  validation accuracy:		88.70 %
Epoch 846 of 2000 took 0.106s
  training loss:		0.266491
  validation loss:		0.401530
  validation accuracy:		89.02 %
Epoch 847 of 2000 took 0.106s
  training loss:		0.260191
  validation loss:		0.415179
  validation accuracy:		88.48 %
Epoch 848 of 2000 took 0.107s
  training loss:		0.264685
  validation loss:		0.389819
  validation accuracy:		89.67 %
Epoch 849 of 2000 took 0.106s
  training loss:		0.264460
  validation loss:		0.391166
  validation accuracy:		89.35 %
Epoch 850 of 2000 took 0.106s
  training loss:		0.264302
  validation loss:		0.412022
  validation accuracy:		88.37 %
Epoch 851 of 2000 took 0.106s
  training loss:		0.273956
  validation loss:		0.406459
  validation accuracy:		88.91 %
Epoch 852 of 2000 took 0.106s
  training loss:		0.265803
  validation loss:		0.399603
  validation accuracy:		88.91 %
Epoch 853 of 2000 took 0.107s
  training loss:		0.266139
  validation loss:		0.386543
  validation accuracy:		88.59 %
Epoch 854 of 2000 took 0.106s
  training loss:		0.262130
  validation loss:		0.382826
  validation accuracy:		89.35 %
Epoch 855 of 2000 took 0.106s
  training loss:		0.265305
  validation loss:		0.404287
  validation accuracy:		89.24 %
Epoch 856 of 2000 took 0.106s
  training loss:		0.266207
  validation loss:		0.415030
  validation accuracy:		88.26 %
Epoch 857 of 2000 took 0.106s
  training loss:		0.262965
  validation loss:		0.394579
  validation accuracy:		88.80 %
Epoch 858 of 2000 took 0.106s
  training loss:		0.259646
  validation loss:		0.392752
  validation accuracy:		88.91 %
Epoch 859 of 2000 took 0.106s
  training loss:		0.250395
  validation loss:		0.396767
  validation accuracy:		88.70 %
Epoch 860 of 2000 took 0.106s
  training loss:		0.260218
  validation loss:		0.380813
  validation accuracy:		89.46 %
Epoch 861 of 2000 took 0.106s
  training loss:		0.273362
  validation loss:		0.401868
  validation accuracy:		89.02 %
Epoch 862 of 2000 took 0.106s
  training loss:		0.259248
  validation loss:		0.381016
  validation accuracy:		89.35 %
Epoch 863 of 2000 took 0.106s
  training loss:		0.258195
  validation loss:		0.408031
  validation accuracy:		88.91 %
Epoch 864 of 2000 took 0.106s
  training loss:		0.262078
  validation loss:		0.388391
  validation accuracy:		88.80 %
Epoch 865 of 2000 took 0.107s
  training loss:		0.260851
  validation loss:		0.402839
  validation accuracy:		88.91 %
Epoch 866 of 2000 took 0.106s
  training loss:		0.261133
  validation loss:		0.448235
  validation accuracy:		87.93 %
Epoch 867 of 2000 took 0.106s
  training loss:		0.262914
  validation loss:		0.407643
  validation accuracy:		89.35 %
Epoch 868 of 2000 took 0.106s
  training loss:		0.258312
  validation loss:		0.390229
  validation accuracy:		89.13 %
Epoch 869 of 2000 took 0.111s
  training loss:		0.257864
  validation loss:		0.388658
  validation accuracy:		89.02 %
Epoch 870 of 2000 took 0.106s
  training loss:		0.267752
  validation loss:		0.379541
  validation accuracy:		89.02 %
Epoch 871 of 2000 took 0.106s
  training loss:		0.260826
  validation loss:		0.411637
  validation accuracy:		88.59 %
Epoch 872 of 2000 took 0.106s
  training loss:		0.264993
  validation loss:		0.402474
  validation accuracy:		88.91 %
Epoch 873 of 2000 took 0.106s
  training loss:		0.255959
  validation loss:		0.398638
  validation accuracy:		89.24 %
Epoch 874 of 2000 took 0.106s
  training loss:		0.264716
  validation loss:		0.381044
  validation accuracy:		88.80 %
Epoch 875 of 2000 took 0.106s
  training loss:		0.266182
  validation loss:		0.417528
  validation accuracy:		89.46 %
Epoch 876 of 2000 took 0.106s
  training loss:		0.267748
  validation loss:		0.408117
  validation accuracy:		89.02 %
Epoch 877 of 2000 took 0.106s
  training loss:		0.261254
  validation loss:		0.382766
  validation accuracy:		89.02 %
Epoch 878 of 2000 took 0.106s
  training loss:		0.258005
  validation loss:		0.379608
  validation accuracy:		89.24 %
Epoch 879 of 2000 took 0.106s
  training loss:		0.264668
  validation loss:		0.379640
  validation accuracy:		88.26 %
Epoch 880 of 2000 took 0.106s
  training loss:		0.263344
  validation loss:		0.443239
  validation accuracy:		88.26 %
Epoch 881 of 2000 took 0.106s
  training loss:		0.266217
  validation loss:		0.411728
  validation accuracy:		88.80 %
Epoch 882 of 2000 took 0.106s
  training loss:		0.264076
  validation loss:		0.381075
  validation accuracy:		88.80 %
Epoch 883 of 2000 took 0.106s
  training loss:		0.256169
  validation loss:		0.380609
  validation accuracy:		89.13 %
Epoch 884 of 2000 took 0.106s
  training loss:		0.268451
  validation loss:		0.406444
  validation accuracy:		88.91 %
Epoch 885 of 2000 took 0.106s
  training loss:		0.262901
  validation loss:		0.384853
  validation accuracy:		89.24 %
Epoch 886 of 2000 took 0.106s
  training loss:		0.263284
  validation loss:		0.413482
  validation accuracy:		89.02 %
Epoch 887 of 2000 took 0.106s
  training loss:		0.256201
  validation loss:		0.409117
  validation accuracy:		88.37 %
Epoch 888 of 2000 took 0.107s
  training loss:		0.261796
  validation loss:		0.400318
  validation accuracy:		89.13 %
Epoch 889 of 2000 took 0.106s
  training loss:		0.264007
  validation loss:		0.397878
  validation accuracy:		89.35 %
Epoch 890 of 2000 took 0.106s
  training loss:		0.261290
  validation loss:		0.404555
  validation accuracy:		89.57 %
Epoch 891 of 2000 took 0.106s
  training loss:		0.265765
  validation loss:		0.404314
  validation accuracy:		89.24 %
Epoch 892 of 2000 took 0.106s
  training loss:		0.261004
  validation loss:		0.414690
  validation accuracy:		89.35 %
Epoch 893 of 2000 took 0.107s
  training loss:		0.266958
  validation loss:		0.407870
  validation accuracy:		88.59 %
Epoch 894 of 2000 took 0.106s
  training loss:		0.261653
  validation loss:		0.400247
  validation accuracy:		88.59 %
Epoch 895 of 2000 took 0.106s
  training loss:		0.259165
  validation loss:		0.391668
  validation accuracy:		89.46 %
Epoch 896 of 2000 took 0.106s
  training loss:		0.255916
  validation loss:		0.394742
  validation accuracy:		88.91 %
Epoch 897 of 2000 took 0.106s
  training loss:		0.266480
  validation loss:		0.397408
  validation accuracy:		88.80 %
Epoch 898 of 2000 took 0.106s
  training loss:		0.263703
  validation loss:		0.389606
  validation accuracy:		88.91 %
Epoch 899 of 2000 took 0.106s
  training loss:		0.261151
  validation loss:		0.450827
  validation accuracy:		88.37 %
Epoch 900 of 2000 took 0.106s
  training loss:		0.261424
  validation loss:		0.403323
  validation accuracy:		88.91 %
Epoch 901 of 2000 took 0.106s
  training loss:		0.257340
  validation loss:		0.396893
  validation accuracy:		88.80 %
Epoch 902 of 2000 took 0.106s
  training loss:		0.259282
  validation loss:		0.399990
  validation accuracy:		89.35 %
Epoch 903 of 2000 took 0.107s
  training loss:		0.267387
  validation loss:		0.382441
  validation accuracy:		89.24 %
Epoch 904 of 2000 took 0.105s
  training loss:		0.258492
  validation loss:		0.387042
  validation accuracy:		89.35 %
Epoch 905 of 2000 took 0.104s
  training loss:		0.249929
  validation loss:		0.385102
  validation accuracy:		89.13 %
Epoch 906 of 2000 took 0.104s
  training loss:		0.257365
  validation loss:		0.381283
  validation accuracy:		89.35 %
Epoch 907 of 2000 took 0.104s
  training loss:		0.260581
  validation loss:		0.416337
  validation accuracy:		89.02 %
Epoch 908 of 2000 took 0.104s
  training loss:		0.258058
  validation loss:		0.383616
  validation accuracy:		89.13 %
Epoch 909 of 2000 took 0.104s
  training loss:		0.259844
  validation loss:		0.389182
  validation accuracy:		89.13 %
Epoch 910 of 2000 took 0.104s
  training loss:		0.263300
  validation loss:		0.387256
  validation accuracy:		89.24 %
Epoch 911 of 2000 took 0.106s
  training loss:		0.255952
  validation loss:		0.391736
  validation accuracy:		89.13 %
Epoch 912 of 2000 took 0.105s
  training loss:		0.259290
  validation loss:		0.395761
  validation accuracy:		88.91 %
Epoch 913 of 2000 took 0.105s
  training loss:		0.255730
  validation loss:		0.406460
  validation accuracy:		88.48 %
Epoch 914 of 2000 took 0.105s
  training loss:		0.256800
  validation loss:		0.412058
  validation accuracy:		88.59 %
Epoch 915 of 2000 took 0.106s
  training loss:		0.261562
  validation loss:		0.399647
  validation accuracy:		89.13 %
Epoch 916 of 2000 took 0.105s
  training loss:		0.260862
  validation loss:		0.379922
  validation accuracy:		89.57 %
Epoch 917 of 2000 took 0.105s
  training loss:		0.265327
  validation loss:		0.394506
  validation accuracy:		89.13 %
Epoch 918 of 2000 took 0.105s
  training loss:		0.253044
  validation loss:		0.416036
  validation accuracy:		88.91 %
Epoch 919 of 2000 took 0.105s
  training loss:		0.262541
  validation loss:		0.408029
  validation accuracy:		89.35 %
Epoch 920 of 2000 took 0.109s
  training loss:		0.260523
  validation loss:		0.398843
  validation accuracy:		89.35 %
Epoch 921 of 2000 took 0.106s
  training loss:		0.256682
  validation loss:		0.402114
  validation accuracy:		89.24 %
Epoch 922 of 2000 took 0.106s
  training loss:		0.260918
  validation loss:		0.409252
  validation accuracy:		88.37 %
Epoch 923 of 2000 took 0.105s
  training loss:		0.255014
  validation loss:		0.381356
  validation accuracy:		89.24 %
Epoch 924 of 2000 took 0.105s
  training loss:		0.256404
  validation loss:		0.382617
  validation accuracy:		89.24 %
Epoch 925 of 2000 took 0.105s
  training loss:		0.265598
  validation loss:		0.395347
  validation accuracy:		89.46 %
Epoch 926 of 2000 took 0.105s
  training loss:		0.256066
  validation loss:		0.385737
  validation accuracy:		89.02 %
Epoch 927 of 2000 took 0.105s
  training loss:		0.261483
  validation loss:		0.381760
  validation accuracy:		89.35 %
Epoch 928 of 2000 took 0.105s
  training loss:		0.258907
  validation loss:		0.388077
  validation accuracy:		89.24 %
Epoch 929 of 2000 took 0.106s
  training loss:		0.256964
  validation loss:		0.376385
  validation accuracy:		89.02 %
Epoch 930 of 2000 took 0.105s
  training loss:		0.258649
  validation loss:		0.382521
  validation accuracy:		88.91 %
Epoch 931 of 2000 took 0.105s
  training loss:		0.257950
  validation loss:		0.380741
  validation accuracy:		89.24 %
Epoch 932 of 2000 took 0.105s
  training loss:		0.258747
  validation loss:		0.382926
  validation accuracy:		89.13 %
Epoch 933 of 2000 took 0.105s
  training loss:		0.254313
  validation loss:		0.392834
  validation accuracy:		89.13 %
Epoch 934 of 2000 took 0.105s
  training loss:		0.261118
  validation loss:		0.396846
  validation accuracy:		88.80 %
Epoch 935 of 2000 took 0.105s
  training loss:		0.256748
  validation loss:		0.378149
  validation accuracy:		89.24 %
Epoch 936 of 2000 took 0.105s
  training loss:		0.255664
  validation loss:		0.396609
  validation accuracy:		89.13 %
Epoch 937 of 2000 took 0.105s
  training loss:		0.254903
  validation loss:		0.383547
  validation accuracy:		89.02 %
Epoch 938 of 2000 took 0.105s
  training loss:		0.256876
  validation loss:		0.395423
  validation accuracy:		88.91 %
Epoch 939 of 2000 took 0.105s
  training loss:		0.256539
  validation loss:		0.378999
  validation accuracy:		89.02 %
Epoch 940 of 2000 took 0.105s
  training loss:		0.257856
  validation loss:		0.402625
  validation accuracy:		89.35 %
Epoch 941 of 2000 took 0.105s
  training loss:		0.261436
  validation loss:		0.392351
  validation accuracy:		89.13 %
Epoch 942 of 2000 took 0.105s
  training loss:		0.254628
  validation loss:		0.400729
  validation accuracy:		89.35 %
Epoch 943 of 2000 took 0.106s
  training loss:		0.254883
  validation loss:		0.381340
  validation accuracy:		89.46 %
Epoch 944 of 2000 took 0.105s
  training loss:		0.261219
  validation loss:		0.387972
  validation accuracy:		89.24 %
Epoch 945 of 2000 took 0.105s
  training loss:		0.255673
  validation loss:		0.405081
  validation accuracy:		89.57 %
Epoch 946 of 2000 took 0.105s
  training loss:		0.263269
  validation loss:		0.386743
  validation accuracy:		89.35 %
Epoch 947 of 2000 took 0.105s
  training loss:		0.260095
  validation loss:		0.383286
  validation accuracy:		89.46 %
Epoch 948 of 2000 took 0.105s
  training loss:		0.260223
  validation loss:		0.386983
  validation accuracy:		88.91 %
Epoch 949 of 2000 took 0.105s
  training loss:		0.253405
  validation loss:		0.375707
  validation accuracy:		88.80 %
Epoch 950 of 2000 took 0.106s
  training loss:		0.255311
  validation loss:		0.392847
  validation accuracy:		88.70 %
Epoch 951 of 2000 took 0.105s
  training loss:		0.248836
  validation loss:		0.376041
  validation accuracy:		89.24 %
Epoch 952 of 2000 took 0.105s
  training loss:		0.254556
  validation loss:		0.382572
  validation accuracy:		89.35 %
Epoch 953 of 2000 took 0.105s
  training loss:		0.255824
  validation loss:		0.373962
  validation accuracy:		89.57 %
Epoch 954 of 2000 took 0.105s
  training loss:		0.257600
  validation loss:		0.407647
  validation accuracy:		89.24 %
Epoch 955 of 2000 took 0.105s
  training loss:		0.253118
  validation loss:		0.382531
  validation accuracy:		89.35 %
Epoch 956 of 2000 took 0.105s
  training loss:		0.256014
  validation loss:		0.388149
  validation accuracy:		89.46 %
Epoch 957 of 2000 took 0.109s
  training loss:		0.255041
  validation loss:		0.384025
  validation accuracy:		88.59 %
Epoch 958 of 2000 took 0.105s
  training loss:		0.265742
  validation loss:		0.383506
  validation accuracy:		89.02 %
Epoch 959 of 2000 took 0.105s
  training loss:		0.259625
  validation loss:		0.399214
  validation accuracy:		88.91 %
Epoch 960 of 2000 took 0.105s
  training loss:		0.257225
  validation loss:		0.382294
  validation accuracy:		89.67 %
Epoch 961 of 2000 took 0.105s
  training loss:		0.254663
  validation loss:		0.384992
  validation accuracy:		89.13 %
Epoch 962 of 2000 took 0.105s
  training loss:		0.249408
  validation loss:		0.372606
  validation accuracy:		89.57 %
Epoch 963 of 2000 took 0.105s
  training loss:		0.258082
  validation loss:		0.369681
  validation accuracy:		89.78 %
Epoch 964 of 2000 took 0.105s
  training loss:		0.255023
  validation loss:		0.396200
  validation accuracy:		89.57 %
Epoch 965 of 2000 took 0.105s
  training loss:		0.255138
  validation loss:		0.397373
  validation accuracy:		89.57 %
Epoch 966 of 2000 took 0.106s
  training loss:		0.252744
  validation loss:		0.404558
  validation accuracy:		89.35 %
Epoch 967 of 2000 took 0.105s
  training loss:		0.258930
  validation loss:		0.396580
  validation accuracy:		89.35 %
Epoch 968 of 2000 took 0.106s
  training loss:		0.256403
  validation loss:		0.391604
  validation accuracy:		89.46 %
Epoch 969 of 2000 took 0.105s
  training loss:		0.258157
  validation loss:		0.396732
  validation accuracy:		88.91 %
Epoch 970 of 2000 took 0.105s
  training loss:		0.256784
  validation loss:		0.389564
  validation accuracy:		89.35 %
Epoch 971 of 2000 took 0.105s
  training loss:		0.252995
  validation loss:		0.380138
  validation accuracy:		89.67 %
Epoch 972 of 2000 took 0.105s
  training loss:		0.256946
  validation loss:		0.407424
  validation accuracy:		88.37 %
Epoch 973 of 2000 took 0.105s
  training loss:		0.262496
  validation loss:		0.391662
  validation accuracy:		89.13 %
Epoch 974 of 2000 took 0.105s
  training loss:		0.255265
  validation loss:		0.398542
  validation accuracy:		88.80 %
Epoch 975 of 2000 took 0.105s
  training loss:		0.255709
  validation loss:		0.390595
  validation accuracy:		89.35 %
Epoch 976 of 2000 took 0.105s
  training loss:		0.254885
  validation loss:		0.383156
  validation accuracy:		89.78 %
Epoch 977 of 2000 took 0.106s
  training loss:		0.259460
  validation loss:		0.371277
  validation accuracy:		88.59 %
Epoch 978 of 2000 took 0.105s
  training loss:		0.260943
  validation loss:		0.387959
  validation accuracy:		89.35 %
Epoch 979 of 2000 took 0.106s
  training loss:		0.259639
  validation loss:		0.384235
  validation accuracy:		89.57 %
Epoch 980 of 2000 took 0.105s
  training loss:		0.250661
  validation loss:		0.390166
  validation accuracy:		89.35 %
Epoch 981 of 2000 took 0.105s
  training loss:		0.250623
  validation loss:		0.385230
  validation accuracy:		89.57 %
Epoch 982 of 2000 took 0.109s
  training loss:		0.255900
  validation loss:		0.396002
  validation accuracy:		89.67 %
Epoch 983 of 2000 took 0.106s
  training loss:		0.255432
  validation loss:		0.384593
  validation accuracy:		89.46 %
Epoch 984 of 2000 took 0.106s
  training loss:		0.256383
  validation loss:		0.383663
  validation accuracy:		89.35 %
Epoch 985 of 2000 took 0.105s
  training loss:		0.250401
  validation loss:		0.410561
  validation accuracy:		89.24 %
Epoch 986 of 2000 took 0.105s
  training loss:		0.256862
  validation loss:		0.389069
  validation accuracy:		89.24 %
Epoch 987 of 2000 took 0.105s
  training loss:		0.254763
  validation loss:		0.387082
  validation accuracy:		89.13 %
Epoch 988 of 2000 took 0.105s
  training loss:		0.253081
  validation loss:		0.378950
  validation accuracy:		89.46 %
Epoch 989 of 2000 took 0.105s
  training loss:		0.253035
  validation loss:		0.368859
  validation accuracy:		89.57 %
Epoch 990 of 2000 took 0.105s
  training loss:		0.252607
  validation loss:		0.367910
  validation accuracy:		89.67 %
Epoch 991 of 2000 took 0.106s
  training loss:		0.251849
  validation loss:		0.384251
  validation accuracy:		89.24 %
Epoch 992 of 2000 took 0.105s
  training loss:		0.253418
  validation loss:		0.383372
  validation accuracy:		88.91 %
Epoch 993 of 2000 took 0.105s
  training loss:		0.258209
  validation loss:		0.387667
  validation accuracy:		89.24 %
Epoch 994 of 2000 took 0.105s
  training loss:		0.254971
  validation loss:		0.408114
  validation accuracy:		89.78 %
Epoch 995 of 2000 took 0.105s
  training loss:		0.257900
  validation loss:		0.397960
  validation accuracy:		89.24 %
Epoch 996 of 2000 took 0.105s
  training loss:		0.260578
  validation loss:		0.389837
  validation accuracy:		89.46 %
Epoch 997 of 2000 took 0.105s
  training loss:		0.253379
  validation loss:		0.384230
  validation accuracy:		89.57 %
Epoch 998 of 2000 took 0.105s
  training loss:		0.252497
  validation loss:		0.392004
  validation accuracy:		89.24 %
Epoch 999 of 2000 took 0.104s
  training loss:		0.252544
  validation loss:		0.392101
  validation accuracy:		89.35 %
Epoch 1000 of 2000 took 0.104s
  training loss:		0.248817
  validation loss:		0.405011
  validation accuracy:		89.02 %
Epoch 1001 of 2000 took 0.099s
  training loss:		0.256101
  validation loss:		0.390454
  validation accuracy:		89.57 %
Epoch 1002 of 2000 took 0.098s
  training loss:		0.256628
  validation loss:		0.380105
  validation accuracy:		89.78 %
Epoch 1003 of 2000 took 0.097s
  training loss:		0.261002
  validation loss:		0.410439
  validation accuracy:		88.91 %
Epoch 1004 of 2000 took 0.096s
  training loss:		0.253218
  validation loss:		0.393270
  validation accuracy:		89.13 %
Epoch 1005 of 2000 took 0.095s
  training loss:		0.258830
  validation loss:		0.380505
  validation accuracy:		89.02 %
Epoch 1006 of 2000 took 0.101s
  training loss:		0.251497
  validation loss:		0.383693
  validation accuracy:		89.57 %
Epoch 1007 of 2000 took 0.107s
  training loss:		0.260854
  validation loss:		0.387589
  validation accuracy:		89.78 %
Epoch 1008 of 2000 took 0.109s
  training loss:		0.254530
  validation loss:		0.371262
  validation accuracy:		89.67 %
Epoch 1009 of 2000 took 0.108s
  training loss:		0.256516
  validation loss:		0.368247
  validation accuracy:		89.67 %
Epoch 1010 of 2000 took 0.108s
  training loss:		0.251580
  validation loss:		0.388990
  validation accuracy:		89.46 %
Epoch 1011 of 2000 took 0.109s
  training loss:		0.245392
  validation loss:		0.391610
  validation accuracy:		89.24 %
Epoch 1012 of 2000 took 0.108s
  training loss:		0.260787
  validation loss:		0.388268
  validation accuracy:		89.24 %
Epoch 1013 of 2000 took 0.097s
  training loss:		0.249994
  validation loss:		0.386001
  validation accuracy:		89.24 %
Epoch 1014 of 2000 took 0.093s
  training loss:		0.252964
  validation loss:		0.375611
  validation accuracy:		89.89 %
Epoch 1015 of 2000 took 0.092s
  training loss:		0.249641
  validation loss:		0.374964
  validation accuracy:		89.67 %
Epoch 1016 of 2000 took 0.093s
  training loss:		0.259014
  validation loss:		0.380839
  validation accuracy:		89.13 %
Epoch 1017 of 2000 took 0.092s
  training loss:		0.260748
  validation loss:		0.387081
  validation accuracy:		89.46 %
Epoch 1018 of 2000 took 0.092s
  training loss:		0.255999
  validation loss:		0.377601
  validation accuracy:		89.46 %
Epoch 1019 of 2000 took 0.092s
  training loss:		0.258319
  validation loss:		0.371491
  validation accuracy:		89.13 %
Epoch 1020 of 2000 took 0.092s
  training loss:		0.248589
  validation loss:		0.393585
  validation accuracy:		89.13 %
Epoch 1021 of 2000 took 0.092s
  training loss:		0.260051
  validation loss:		0.375246
  validation accuracy:		89.24 %
Epoch 1022 of 2000 took 0.092s
  training loss:		0.251223
  validation loss:		0.384709
  validation accuracy:		89.35 %
Epoch 1023 of 2000 took 0.094s
  training loss:		0.265309
  validation loss:		0.393411
  validation accuracy:		89.57 %
Epoch 1024 of 2000 took 0.093s
  training loss:		0.253555
  validation loss:		0.426341
  validation accuracy:		88.48 %
Epoch 1025 of 2000 took 0.092s
  training loss:		0.258144
  validation loss:		0.374951
  validation accuracy:		89.35 %
Epoch 1026 of 2000 took 0.092s
  training loss:		0.253965
  validation loss:		0.367661
  validation accuracy:		89.35 %
Epoch 1027 of 2000 took 0.093s
  training loss:		0.252768
  validation loss:		0.378770
  validation accuracy:		89.67 %
Epoch 1028 of 2000 took 0.092s
  training loss:		0.256504
  validation loss:		0.370354
  validation accuracy:		89.46 %
Epoch 1029 of 2000 took 0.092s
  training loss:		0.251223
  validation loss:		0.393200
  validation accuracy:		88.48 %
Epoch 1030 of 2000 took 0.092s
  training loss:		0.257840
  validation loss:		0.369540
  validation accuracy:		89.67 %
Epoch 1031 of 2000 took 0.092s
  training loss:		0.253396
  validation loss:		0.382484
  validation accuracy:		89.13 %
Epoch 1032 of 2000 took 0.092s
  training loss:		0.256151
  validation loss:		0.366042
  validation accuracy:		90.00 %
Epoch 1033 of 2000 took 0.092s
  training loss:		0.257052
  validation loss:		0.373467
  validation accuracy:		89.78 %
Epoch 1034 of 2000 took 0.094s
  training loss:		0.256686
  validation loss:		0.368515
  validation accuracy:		89.67 %
Epoch 1035 of 2000 took 0.092s
  training loss:		0.258300
  validation loss:		0.367208
  validation accuracy:		89.78 %
Epoch 1036 of 2000 took 0.092s
  training loss:		0.251407
  validation loss:		0.397008
  validation accuracy:		89.24 %
Epoch 1037 of 2000 took 0.092s
  training loss:		0.245432
  validation loss:		0.365436
  validation accuracy:		89.46 %
Epoch 1038 of 2000 took 0.093s
  training loss:		0.251801
  validation loss:		0.371581
  validation accuracy:		89.46 %
Epoch 1039 of 2000 took 0.093s
  training loss:		0.259412
  validation loss:		0.384869
  validation accuracy:		89.24 %
Epoch 1040 of 2000 took 0.092s
  training loss:		0.249346
  validation loss:		0.394579
  validation accuracy:		89.13 %
Epoch 1041 of 2000 took 0.092s
  training loss:		0.251026
  validation loss:		0.384381
  validation accuracy:		89.46 %
Epoch 1042 of 2000 took 0.093s
  training loss:		0.257436
  validation loss:		0.368966
  validation accuracy:		89.67 %
Epoch 1043 of 2000 took 0.092s
  training loss:		0.253263
  validation loss:		0.388853
  validation accuracy:		89.35 %
Epoch 1044 of 2000 took 0.092s
  training loss:		0.249352
  validation loss:		0.391499
  validation accuracy:		88.80 %
Epoch 1045 of 2000 took 0.093s
  training loss:		0.256802
  validation loss:		0.376158
  validation accuracy:		89.67 %
Epoch 1046 of 2000 took 0.093s
  training loss:		0.256899
  validation loss:		0.392425
  validation accuracy:		89.57 %
Epoch 1047 of 2000 took 0.092s
  training loss:		0.254815
  validation loss:		0.378361
  validation accuracy:		89.67 %
Epoch 1048 of 2000 took 0.093s
  training loss:		0.256308
  validation loss:		0.370871
  validation accuracy:		90.11 %
Epoch 1049 of 2000 took 0.093s
  training loss:		0.248623
  validation loss:		0.388919
  validation accuracy:		89.35 %
Epoch 1050 of 2000 took 0.092s
  training loss:		0.252861
  validation loss:		0.388108
  validation accuracy:		89.78 %
Epoch 1051 of 2000 took 0.093s
  training loss:		0.252365
  validation loss:		0.380187
  validation accuracy:		89.78 %
Epoch 1052 of 2000 took 0.093s
  training loss:		0.260091
  validation loss:		0.369946
  validation accuracy:		89.13 %
Epoch 1053 of 2000 took 0.100s
  training loss:		0.258143
  validation loss:		0.392857
  validation accuracy:		89.46 %
Epoch 1054 of 2000 took 0.105s
  training loss:		0.250012
  validation loss:		0.373095
  validation accuracy:		89.78 %
Epoch 1055 of 2000 took 0.105s
  training loss:		0.253308
  validation loss:		0.372939
  validation accuracy:		89.57 %
Epoch 1056 of 2000 took 0.105s
  training loss:		0.253602
  validation loss:		0.378878
  validation accuracy:		89.78 %
Epoch 1057 of 2000 took 0.105s
  training loss:		0.250962
  validation loss:		0.364011
  validation accuracy:		89.35 %
Epoch 1058 of 2000 took 0.105s
  training loss:		0.256829
  validation loss:		0.399091
  validation accuracy:		88.91 %
Epoch 1059 of 2000 took 0.105s
  training loss:		0.253178
  validation loss:		0.375470
  validation accuracy:		89.57 %
Epoch 1060 of 2000 took 0.105s
  training loss:		0.247244
  validation loss:		0.377307
  validation accuracy:		89.46 %
Epoch 1061 of 2000 took 0.105s
  training loss:		0.252553
  validation loss:		0.386019
  validation accuracy:		89.46 %
Epoch 1062 of 2000 took 0.105s
  training loss:		0.257038
  validation loss:		0.382910
  validation accuracy:		89.78 %
Epoch 1063 of 2000 took 0.105s
  training loss:		0.253927
  validation loss:		0.366829
  validation accuracy:		89.57 %
Epoch 1064 of 2000 took 0.105s
  training loss:		0.251833
  validation loss:		0.386714
  validation accuracy:		89.35 %
Epoch 1065 of 2000 took 0.105s
  training loss:		0.255654
  validation loss:		0.379336
  validation accuracy:		89.67 %
Epoch 1066 of 2000 took 0.105s
  training loss:		0.250342
  validation loss:		0.380897
  validation accuracy:		89.89 %
Epoch 1067 of 2000 took 0.105s
  training loss:		0.251234
  validation loss:		0.374001
  validation accuracy:		89.57 %
Epoch 1068 of 2000 took 0.105s
  training loss:		0.249716
  validation loss:		0.381168
  validation accuracy:		89.67 %
Epoch 1069 of 2000 took 0.105s
  training loss:		0.256622
  validation loss:		0.380883
  validation accuracy:		89.57 %
Epoch 1070 of 2000 took 0.106s
  training loss:		0.245912
  validation loss:		0.382615
  validation accuracy:		89.46 %
Epoch 1071 of 2000 took 0.105s
  training loss:		0.256395
  validation loss:		0.373795
  validation accuracy:		89.89 %
Epoch 1072 of 2000 took 0.105s
  training loss:		0.250790
  validation loss:		0.370412
  validation accuracy:		89.89 %
Epoch 1073 of 2000 took 0.105s
  training loss:		0.250346
  validation loss:		0.375025
  validation accuracy:		89.78 %
Epoch 1074 of 2000 took 0.105s
  training loss:		0.247923
  validation loss:		0.388879
  validation accuracy:		89.57 %
Epoch 1075 of 2000 took 0.105s
  training loss:		0.253767
  validation loss:		0.369195
  validation accuracy:		89.46 %
Epoch 1076 of 2000 took 0.101s
  training loss:		0.248711
  validation loss:		0.381377
  validation accuracy:		89.57 %
Epoch 1077 of 2000 took 0.093s
  training loss:		0.255579
  validation loss:		0.384617
  validation accuracy:		89.46 %
Epoch 1078 of 2000 took 0.093s
  training loss:		0.251071
  validation loss:		0.371806
  validation accuracy:		89.46 %
Epoch 1079 of 2000 took 0.092s
  training loss:		0.256189
  validation loss:		0.379830
  validation accuracy:		89.67 %
Epoch 1080 of 2000 took 0.092s
  training loss:		0.252041
  validation loss:		0.378239
  validation accuracy:		89.78 %
Epoch 1081 of 2000 took 0.092s
  training loss:		0.252986
  validation loss:		0.386591
  validation accuracy:		89.35 %
Epoch 1082 of 2000 took 0.094s
  training loss:		0.253848
  validation loss:		0.372540
  validation accuracy:		89.46 %
Epoch 1083 of 2000 took 0.105s
  training loss:		0.255284
  validation loss:		0.386302
  validation accuracy:		89.89 %
Epoch 1084 of 2000 took 0.106s
  training loss:		0.253914
  validation loss:		0.400219
  validation accuracy:		88.59 %
Epoch 1085 of 2000 took 0.105s
  training loss:		0.247006
  validation loss:		0.375616
  validation accuracy:		90.11 %
Epoch 1086 of 2000 took 0.105s
  training loss:		0.247391
  validation loss:		0.408625
  validation accuracy:		88.48 %
Epoch 1087 of 2000 took 0.105s
  training loss:		0.254715
  validation loss:		0.366198
  validation accuracy:		89.67 %
Epoch 1088 of 2000 took 0.105s
  training loss:		0.251430
  validation loss:		0.368805
  validation accuracy:		90.33 %
Epoch 1089 of 2000 took 0.105s
  training loss:		0.254821
  validation loss:		0.377105
  validation accuracy:		89.35 %
Epoch 1090 of 2000 took 0.105s
  training loss:		0.248808
  validation loss:		0.376074
  validation accuracy:		89.89 %
Epoch 1091 of 2000 took 0.105s
  training loss:		0.246566
  validation loss:		0.365948
  validation accuracy:		89.67 %
Epoch 1092 of 2000 took 0.105s
  training loss:		0.248975
  validation loss:		0.364018
  validation accuracy:		89.46 %
Epoch 1093 of 2000 took 0.105s
  training loss:		0.252832
  validation loss:		0.383956
  validation accuracy:		89.57 %
Epoch 1094 of 2000 took 0.105s
  training loss:		0.251200
  validation loss:		0.361732
  validation accuracy:		89.89 %
Epoch 1095 of 2000 took 0.105s
  training loss:		0.251039
  validation loss:		0.373812
  validation accuracy:		89.89 %
Epoch 1096 of 2000 took 0.105s
  training loss:		0.258258
  validation loss:		0.398166
  validation accuracy:		89.13 %
Epoch 1097 of 2000 took 0.105s
  training loss:		0.248611
  validation loss:		0.404142
  validation accuracy:		89.13 %
Epoch 1098 of 2000 took 0.105s
  training loss:		0.246082
  validation loss:		0.370795
  validation accuracy:		90.22 %
Epoch 1099 of 2000 took 0.106s
  training loss:		0.258090
  validation loss:		0.383785
  validation accuracy:		89.78 %
Epoch 1100 of 2000 took 0.105s
  training loss:		0.249184
  validation loss:		0.413669
  validation accuracy:		88.59 %
Epoch 1101 of 2000 took 0.105s
  training loss:		0.252052
  validation loss:		0.396822
  validation accuracy:		89.24 %
Epoch 1102 of 2000 took 0.105s
  training loss:		0.246715
  validation loss:		0.369765
  validation accuracy:		89.78 %
Epoch 1103 of 2000 took 0.105s
  training loss:		0.253335
  validation loss:		0.392242
  validation accuracy:		89.89 %
Epoch 1104 of 2000 took 0.105s
  training loss:		0.249899
  validation loss:		0.404678
  validation accuracy:		88.91 %
Epoch 1105 of 2000 took 0.105s
  training loss:		0.256051
  validation loss:		0.365170
  validation accuracy:		89.35 %
Epoch 1106 of 2000 took 0.105s
  training loss:		0.252996
  validation loss:		0.376539
  validation accuracy:		89.89 %
Epoch 1107 of 2000 took 0.105s
  training loss:		0.252005
  validation loss:		0.384349
  validation accuracy:		89.67 %
Epoch 1108 of 2000 took 0.105s
  training loss:		0.258538
  validation loss:		0.377781
  validation accuracy:		90.00 %
Epoch 1109 of 2000 took 0.105s
  training loss:		0.251470
  validation loss:		0.380011
  validation accuracy:		89.78 %
Epoch 1110 of 2000 took 0.105s
  training loss:		0.262120
  validation loss:		0.390793
  validation accuracy:		89.89 %
Epoch 1111 of 2000 took 0.105s
  training loss:		0.252722
  validation loss:		0.400109
  validation accuracy:		89.24 %
Epoch 1112 of 2000 took 0.105s
  training loss:		0.257717
  validation loss:		0.374421
  validation accuracy:		90.33 %
Epoch 1113 of 2000 took 0.105s
  training loss:		0.248350
  validation loss:		0.400262
  validation accuracy:		89.46 %
Epoch 1114 of 2000 took 0.105s
  training loss:		0.252474
  validation loss:		0.376275
  validation accuracy:		89.89 %
Epoch 1115 of 2000 took 0.105s
  training loss:		0.250143
  validation loss:		0.371300
  validation accuracy:		90.22 %
Epoch 1116 of 2000 took 0.097s
  training loss:		0.249536
  validation loss:		0.367067
  validation accuracy:		89.89 %
Epoch 1117 of 2000 took 0.093s
  training loss:		0.251881
  validation loss:		0.372808
  validation accuracy:		90.33 %
Epoch 1118 of 2000 took 0.092s
  training loss:		0.253537
  validation loss:		0.397097
  validation accuracy:		89.35 %
Epoch 1119 of 2000 took 0.092s
  training loss:		0.253212
  validation loss:		0.402604
  validation accuracy:		89.78 %
Epoch 1120 of 2000 took 0.092s
  training loss:		0.254515
  validation loss:		0.376464
  validation accuracy:		89.46 %
Epoch 1121 of 2000 took 0.092s
  training loss:		0.252036
  validation loss:		0.365402
  validation accuracy:		89.67 %
Epoch 1122 of 2000 took 0.092s
  training loss:		0.248909
  validation loss:		0.393893
  validation accuracy:		89.57 %
Epoch 1123 of 2000 took 0.097s
  training loss:		0.249676
  validation loss:		0.381145
  validation accuracy:		89.46 %
Epoch 1124 of 2000 took 0.105s
  training loss:		0.246154
  validation loss:		0.371244
  validation accuracy:		89.78 %
Epoch 1125 of 2000 took 0.105s
  training loss:		0.252477
  validation loss:		0.376424
  validation accuracy:		89.67 %
Epoch 1126 of 2000 took 0.102s
  training loss:		0.248915
  validation loss:		0.385614
  validation accuracy:		89.67 %
Epoch 1127 of 2000 took 0.093s
  training loss:		0.246136
  validation loss:		0.399576
  validation accuracy:		88.91 %
Epoch 1128 of 2000 took 0.092s
  training loss:		0.254204
  validation loss:		0.393235
  validation accuracy:		89.46 %
Epoch 1129 of 2000 took 0.093s
  training loss:		0.249541
  validation loss:		0.382163
  validation accuracy:		89.89 %
Epoch 1130 of 2000 took 0.092s
  training loss:		0.251142
  validation loss:		0.371782
  validation accuracy:		89.35 %
Epoch 1131 of 2000 took 0.092s
  training loss:		0.247349
  validation loss:		0.371361
  validation accuracy:		90.43 %
Epoch 1132 of 2000 took 0.092s
  training loss:		0.251881
  validation loss:		0.378758
  validation accuracy:		89.78 %
Epoch 1133 of 2000 took 0.092s
  training loss:		0.245980
  validation loss:		0.381708
  validation accuracy:		89.89 %
Epoch 1134 of 2000 took 0.093s
  training loss:		0.254245
  validation loss:		0.379661
  validation accuracy:		89.89 %
Epoch 1135 of 2000 took 0.105s
  training loss:		0.247843
  validation loss:		0.378333
  validation accuracy:		90.00 %
Epoch 1136 of 2000 took 0.107s
  training loss:		0.246452
  validation loss:		0.371775
  validation accuracy:		90.11 %
Epoch 1137 of 2000 took 0.094s
  training loss:		0.251413
  validation loss:		0.374696
  validation accuracy:		89.57 %
Epoch 1138 of 2000 took 0.093s
  training loss:		0.251601
  validation loss:		0.381811
  validation accuracy:		89.78 %
Epoch 1139 of 2000 took 0.092s
  training loss:		0.250715
  validation loss:		0.376836
  validation accuracy:		90.00 %
Epoch 1140 of 2000 took 0.092s
  training loss:		0.247860
  validation loss:		0.398109
  validation accuracy:		89.67 %
Epoch 1141 of 2000 took 0.093s
  training loss:		0.249048
  validation loss:		0.383426
  validation accuracy:		89.78 %
Epoch 1142 of 2000 took 0.092s
  training loss:		0.255856
  validation loss:		0.375987
  validation accuracy:		90.33 %
Epoch 1143 of 2000 took 0.092s
  training loss:		0.245876
  validation loss:		0.373831
  validation accuracy:		89.89 %
Epoch 1144 of 2000 took 0.092s
  training loss:		0.253456
  validation loss:		0.382452
  validation accuracy:		89.78 %
Epoch 1145 of 2000 took 0.099s
  training loss:		0.246695
  validation loss:		0.381087
  validation accuracy:		90.11 %
Epoch 1146 of 2000 took 0.105s
  training loss:		0.249806
  validation loss:		0.363016
  validation accuracy:		89.67 %
Epoch 1147 of 2000 took 0.102s
  training loss:		0.244956
  validation loss:		0.393180
  validation accuracy:		89.24 %
Epoch 1148 of 2000 took 0.093s
  training loss:		0.246607
  validation loss:		0.388230
  validation accuracy:		90.00 %
Epoch 1149 of 2000 took 0.093s
  training loss:		0.255161
  validation loss:		0.387252
  validation accuracy:		89.67 %
Epoch 1150 of 2000 took 0.092s
  training loss:		0.251159
  validation loss:		0.377581
  validation accuracy:		89.35 %
Epoch 1151 of 2000 took 0.092s
  training loss:		0.250371
  validation loss:		0.382202
  validation accuracy:		89.57 %
Epoch 1152 of 2000 took 0.092s
  training loss:		0.252155
  validation loss:		0.369509
  validation accuracy:		89.67 %
Epoch 1153 of 2000 took 0.092s
  training loss:		0.244072
  validation loss:		0.368402
  validation accuracy:		89.57 %
Epoch 1154 of 2000 took 0.092s
  training loss:		0.246758
  validation loss:		0.364414
  validation accuracy:		89.67 %
Epoch 1155 of 2000 took 0.093s
  training loss:		0.252874
  validation loss:		0.391876
  validation accuracy:		89.89 %
Epoch 1156 of 2000 took 0.108s
  training loss:		0.258909
  validation loss:		0.368517
  validation accuracy:		90.11 %
Epoch 1157 of 2000 took 0.107s
  training loss:		0.250650
  validation loss:		0.367569
  validation accuracy:		89.78 %
Epoch 1158 of 2000 took 0.094s
  training loss:		0.242823
  validation loss:		0.371243
  validation accuracy:		89.67 %
Epoch 1159 of 2000 took 0.093s
  training loss:		0.251458
  validation loss:		0.378160
  validation accuracy:		89.67 %
Epoch 1160 of 2000 took 0.093s
  training loss:		0.252518
  validation loss:		0.385070
  validation accuracy:		89.67 %
Epoch 1161 of 2000 took 0.092s
  training loss:		0.246864
  validation loss:		0.401225
  validation accuracy:		89.02 %
Epoch 1162 of 2000 took 0.092s
  training loss:		0.242415
  validation loss:		0.365701
  validation accuracy:		89.67 %
Epoch 1163 of 2000 took 0.092s
  training loss:		0.250506
  validation loss:		0.388110
  validation accuracy:		89.02 %
Epoch 1164 of 2000 took 0.092s
  training loss:		0.246330
  validation loss:		0.376461
  validation accuracy:		89.67 %
Epoch 1165 of 2000 took 0.092s
  training loss:		0.253465
  validation loss:		0.382877
  validation accuracy:		89.67 %
Epoch 1166 of 2000 took 0.096s
  training loss:		0.249609
  validation loss:		0.399651
  validation accuracy:		89.24 %
Epoch 1167 of 2000 took 0.105s
  training loss:		0.246487
  validation loss:		0.382618
  validation accuracy:		89.02 %
Epoch 1168 of 2000 took 0.103s
  training loss:		0.248836
  validation loss:		0.370314
  validation accuracy:		89.67 %
Epoch 1169 of 2000 took 0.093s
  training loss:		0.250523
  validation loss:		0.381630
  validation accuracy:		90.00 %
Epoch 1170 of 2000 took 0.092s
  training loss:		0.253633
  validation loss:		0.392593
  validation accuracy:		89.57 %
Epoch 1171 of 2000 took 0.092s
  training loss:		0.245569
  validation loss:		0.389724
  validation accuracy:		89.67 %
Epoch 1172 of 2000 took 0.092s
  training loss:		0.254379
  validation loss:		0.371046
  validation accuracy:		89.67 %
Epoch 1173 of 2000 took 0.092s
  training loss:		0.245849
  validation loss:		0.369462
  validation accuracy:		90.22 %
Epoch 1174 of 2000 took 0.092s
  training loss:		0.252701
  validation loss:		0.401876
  validation accuracy:		89.24 %
Epoch 1175 of 2000 took 0.092s
  training loss:		0.246718
  validation loss:		0.371197
  validation accuracy:		89.67 %
Epoch 1176 of 2000 took 0.092s
  training loss:		0.255424
  validation loss:		0.375908
  validation accuracy:		89.67 %
Epoch 1177 of 2000 took 0.098s
  training loss:		0.246633
  validation loss:		0.369099
  validation accuracy:		89.78 %
Epoch 1178 of 2000 took 0.105s
  training loss:		0.251532
  validation loss:		0.373723
  validation accuracy:		90.11 %
Epoch 1179 of 2000 took 0.097s
  training loss:		0.244689
  validation loss:		0.373796
  validation accuracy:		90.33 %
Epoch 1180 of 2000 took 0.093s
  training loss:		0.247981
  validation loss:		0.361392
  validation accuracy:		89.57 %
Epoch 1181 of 2000 took 0.092s
  training loss:		0.250225
  validation loss:		0.385660
  validation accuracy:		89.46 %
Epoch 1182 of 2000 took 0.092s
  training loss:		0.254110
  validation loss:		0.380278
  validation accuracy:		89.35 %
Epoch 1183 of 2000 took 0.092s
  training loss:		0.260194
  validation loss:		0.361828
  validation accuracy:		90.22 %
Epoch 1184 of 2000 took 0.092s
  training loss:		0.247707
  validation loss:		0.382404
  validation accuracy:		89.78 %
Epoch 1185 of 2000 took 0.092s
  training loss:		0.244956
  validation loss:		0.373493
  validation accuracy:		90.00 %
Epoch 1186 of 2000 took 0.092s
  training loss:		0.248185
  validation loss:		0.400691
  validation accuracy:		89.35 %
Epoch 1187 of 2000 took 0.092s
  training loss:		0.249620
  validation loss:		0.386095
  validation accuracy:		90.00 %
Epoch 1188 of 2000 took 0.092s
  training loss:		0.251366
  validation loss:		0.407246
  validation accuracy:		88.91 %
Epoch 1189 of 2000 took 0.103s
  training loss:		0.248289
  validation loss:		0.368196
  validation accuracy:		89.89 %
Epoch 1190 of 2000 took 0.093s
  training loss:		0.252790
  validation loss:		0.379309
  validation accuracy:		89.57 %
Epoch 1191 of 2000 took 0.092s
  training loss:		0.248297
  validation loss:		0.372236
  validation accuracy:		90.33 %
Epoch 1192 of 2000 took 0.093s
  training loss:		0.249017
  validation loss:		0.385755
  validation accuracy:		89.78 %
Epoch 1193 of 2000 took 0.092s
  training loss:		0.243637
  validation loss:		0.385654
  validation accuracy:		89.13 %
Epoch 1194 of 2000 took 0.092s
  training loss:		0.249207
  validation loss:		0.387692
  validation accuracy:		89.67 %
Epoch 1195 of 2000 took 0.092s
  training loss:		0.256469
  validation loss:		0.371614
  validation accuracy:		89.89 %
Epoch 1196 of 2000 took 0.092s
  training loss:		0.245277
  validation loss:		0.374612
  validation accuracy:		89.57 %
Epoch 1197 of 2000 took 0.092s
  training loss:		0.246562
  validation loss:		0.404873
  validation accuracy:		89.78 %
Epoch 1198 of 2000 took 0.092s
  training loss:		0.258965
  validation loss:		0.398237
  validation accuracy:		89.57 %
Epoch 1199 of 2000 took 0.092s
  training loss:		0.250737
  validation loss:		0.394902
  validation accuracy:		89.02 %
Epoch 1200 of 2000 took 0.096s
  training loss:		0.247249
  validation loss:		0.387186
  validation accuracy:		88.91 %
Epoch 1201 of 2000 took 0.106s
  training loss:		0.243782
  validation loss:		0.379203
  validation accuracy:		90.00 %
Epoch 1202 of 2000 took 0.105s
  training loss:		0.248224
  validation loss:		0.377948
  validation accuracy:		89.89 %
Epoch 1203 of 2000 took 0.105s
  training loss:		0.250465
  validation loss:		0.415538
  validation accuracy:		89.24 %
Epoch 1204 of 2000 took 0.105s
  training loss:		0.250705
  validation loss:		0.376472
  validation accuracy:		90.43 %
Epoch 1205 of 2000 took 0.105s
  training loss:		0.246962
  validation loss:		0.375452
  validation accuracy:		89.89 %
Epoch 1206 of 2000 took 0.105s
  training loss:		0.242012
  validation loss:		0.368011
  validation accuracy:		90.43 %
Epoch 1207 of 2000 took 0.105s
  training loss:		0.246107
  validation loss:		0.378413
  validation accuracy:		89.89 %
Epoch 1208 of 2000 took 0.105s
  training loss:		0.252442
  validation loss:		0.375670
  validation accuracy:		89.57 %
Epoch 1209 of 2000 took 0.106s
  training loss:		0.252705
  validation loss:		0.376338
  validation accuracy:		90.00 %
Epoch 1210 of 2000 took 0.100s
  training loss:		0.249439
  validation loss:		0.396788
  validation accuracy:		89.13 %
Epoch 1211 of 2000 took 0.105s
  training loss:		0.249611
  validation loss:		0.373890
  validation accuracy:		90.22 %
Epoch 1212 of 2000 took 0.105s
  training loss:		0.253829
  validation loss:		0.378306
  validation accuracy:		90.11 %
Epoch 1213 of 2000 took 0.105s
  training loss:		0.247542
  validation loss:		0.366921
  validation accuracy:		89.78 %
Epoch 1214 of 2000 took 0.105s
  training loss:		0.249132
  validation loss:		0.366993
  validation accuracy:		89.78 %
Epoch 1215 of 2000 took 0.105s
  training loss:		0.239554
  validation loss:		0.370067
  validation accuracy:		90.43 %
Epoch 1216 of 2000 took 0.105s
  training loss:		0.248838
  validation loss:		0.389949
  validation accuracy:		89.57 %
Epoch 1217 of 2000 took 0.105s
  training loss:		0.249619
  validation loss:		0.383091
  validation accuracy:		90.00 %
Epoch 1218 of 2000 took 0.105s
  training loss:		0.242838
  validation loss:		0.378203
  validation accuracy:		89.24 %
Epoch 1219 of 2000 took 0.106s
  training loss:		0.246542
  validation loss:		0.390864
  validation accuracy:		89.67 %
Epoch 1220 of 2000 took 0.101s
  training loss:		0.251577
  validation loss:		0.366398
  validation accuracy:		89.89 %
Epoch 1221 of 2000 took 0.106s
  training loss:		0.247166
  validation loss:		0.405311
  validation accuracy:		89.57 %
Epoch 1222 of 2000 took 0.106s
  training loss:		0.257900
  validation loss:		0.409566
  validation accuracy:		89.35 %
Epoch 1223 of 2000 took 0.106s
  training loss:		0.252557
  validation loss:		0.374077
  validation accuracy:		89.67 %
Epoch 1224 of 2000 took 0.105s
  training loss:		0.243428
  validation loss:		0.373482
  validation accuracy:		90.54 %
Epoch 1225 of 2000 took 0.105s
  training loss:		0.246999
  validation loss:		0.374278
  validation accuracy:		90.22 %
Epoch 1226 of 2000 took 0.105s
  training loss:		0.250327
  validation loss:		0.368008
  validation accuracy:		90.33 %
Epoch 1227 of 2000 took 0.105s
  training loss:		0.247840
  validation loss:		0.363872
  validation accuracy:		89.78 %
Epoch 1228 of 2000 took 0.105s
  training loss:		0.250926
  validation loss:		0.363043
  validation accuracy:		89.89 %
Epoch 1229 of 2000 took 0.104s
  training loss:		0.245777
  validation loss:		0.374058
  validation accuracy:		90.22 %
Epoch 1230 of 2000 took 0.104s
  training loss:		0.246132
  validation loss:		0.362990
  validation accuracy:		89.89 %
Epoch 1231 of 2000 took 0.106s
  training loss:		0.244004
  validation loss:		0.364522
  validation accuracy:		90.00 %
Epoch 1232 of 2000 took 0.105s
  training loss:		0.250456
  validation loss:		0.374072
  validation accuracy:		90.22 %
Epoch 1233 of 2000 took 0.105s
  training loss:		0.252589
  validation loss:		0.363739
  validation accuracy:		90.00 %
Epoch 1234 of 2000 took 0.105s
  training loss:		0.249785
  validation loss:		0.362997
  validation accuracy:		89.89 %
Epoch 1235 of 2000 took 0.105s
  training loss:		0.249525
  validation loss:		0.387508
  validation accuracy:		89.35 %
Epoch 1236 of 2000 took 0.105s
  training loss:		0.248077
  validation loss:		0.387242
  validation accuracy:		89.57 %
Epoch 1237 of 2000 took 0.105s
  training loss:		0.251361
  validation loss:		0.401696
  validation accuracy:		89.02 %
Epoch 1238 of 2000 took 0.108s
  training loss:		0.246428
  validation loss:		0.391919
  validation accuracy:		89.57 %
Epoch 1239 of 2000 took 0.099s
  training loss:		0.246627
  validation loss:		0.384812
  validation accuracy:		89.13 %
Epoch 1240 of 2000 took 0.104s
  training loss:		0.254153
  validation loss:		0.369961
  validation accuracy:		90.33 %
Epoch 1241 of 2000 took 0.105s
  training loss:		0.245061
  validation loss:		0.365189
  validation accuracy:		90.11 %
Epoch 1242 of 2000 took 0.106s
  training loss:		0.252835
  validation loss:		0.382308
  validation accuracy:		89.67 %
Epoch 1243 of 2000 took 0.105s
  training loss:		0.248228
  validation loss:		0.365399
  validation accuracy:		89.57 %
Epoch 1244 of 2000 took 0.105s
  training loss:		0.252016
  validation loss:		0.372990
  validation accuracy:		89.89 %
Epoch 1245 of 2000 took 0.105s
  training loss:		0.248572
  validation loss:		0.372892
  validation accuracy:		90.22 %
Epoch 1246 of 2000 took 0.105s
  training loss:		0.242317
  validation loss:		0.379760
  validation accuracy:		89.57 %
Epoch 1247 of 2000 took 0.105s
  training loss:		0.244948
  validation loss:		0.365927
  validation accuracy:		90.00 %
Epoch 1248 of 2000 took 0.105s
  training loss:		0.249676
  validation loss:		0.371539
  validation accuracy:		90.22 %
Epoch 1249 of 2000 took 0.096s
  training loss:		0.252315
  validation loss:		0.360881
  validation accuracy:		89.46 %
Epoch 1250 of 2000 took 0.106s
  training loss:		0.244014
  validation loss:		0.383017
  validation accuracy:		89.57 %
Epoch 1251 of 2000 took 0.106s
  training loss:		0.245678
  validation loss:		0.365770
  validation accuracy:		90.65 %
Epoch 1252 of 2000 took 0.105s
  training loss:		0.243364
  validation loss:		0.389264
  validation accuracy:		89.67 %
Epoch 1253 of 2000 took 0.105s
  training loss:		0.251122
  validation loss:		0.378363
  validation accuracy:		89.89 %
Epoch 1254 of 2000 took 0.105s
  training loss:		0.250053
  validation loss:		0.376870
  validation accuracy:		90.00 %
Epoch 1255 of 2000 took 0.105s
  training loss:		0.248476
  validation loss:		0.383167
  validation accuracy:		89.13 %
Epoch 1256 of 2000 took 0.105s
  training loss:		0.247247
  validation loss:		0.371480
  validation accuracy:		90.54 %
Epoch 1257 of 2000 took 0.107s
  training loss:		0.250920
  validation loss:		0.387513
  validation accuracy:		89.35 %
Epoch 1258 of 2000 took 0.093s
  training loss:		0.248227
  validation loss:		0.372141
  validation accuracy:		90.43 %
Epoch 1259 of 2000 took 0.098s
  training loss:		0.247229
  validation loss:		0.383081
  validation accuracy:		89.89 %
Epoch 1260 of 2000 took 0.106s
  training loss:		0.252799
  validation loss:		0.377701
  validation accuracy:		90.11 %
Epoch 1261 of 2000 took 0.105s
  training loss:		0.257058
  validation loss:		0.417423
  validation accuracy:		88.70 %
Epoch 1262 of 2000 took 0.105s
  training loss:		0.240957
  validation loss:		0.357960
  validation accuracy:		90.00 %
Epoch 1263 of 2000 took 0.105s
  training loss:		0.246638
  validation loss:		0.362040
  validation accuracy:		90.11 %
Epoch 1264 of 2000 took 0.105s
  training loss:		0.245526
  validation loss:		0.384541
  validation accuracy:		89.89 %
Epoch 1265 of 2000 took 0.105s
  training loss:		0.242300
  validation loss:		0.374132
  validation accuracy:		90.11 %
Epoch 1266 of 2000 took 0.105s
  training loss:		0.250172
  validation loss:		0.371808
  validation accuracy:		90.00 %
Epoch 1267 of 2000 took 0.104s
  training loss:		0.248537
  validation loss:		0.378050
  validation accuracy:		90.00 %
Epoch 1268 of 2000 took 0.093s
  training loss:		0.248627
  validation loss:		0.373262
  validation accuracy:		89.78 %
Epoch 1269 of 2000 took 0.097s
  training loss:		0.250285
  validation loss:		0.365787
  validation accuracy:		89.67 %
Epoch 1270 of 2000 took 0.105s
  training loss:		0.244063
  validation loss:		0.369437
  validation accuracy:		89.67 %
Epoch 1271 of 2000 took 0.105s
  training loss:		0.251333
  validation loss:		0.381005
  validation accuracy:		89.89 %
Epoch 1272 of 2000 took 0.105s
  training loss:		0.249979
  validation loss:		0.395175
  validation accuracy:		89.78 %
Epoch 1273 of 2000 took 0.105s
  training loss:		0.246503
  validation loss:		0.396867
  validation accuracy:		89.35 %
Epoch 1274 of 2000 took 0.105s
  training loss:		0.250167
  validation loss:		0.380305
  validation accuracy:		90.00 %
Epoch 1275 of 2000 took 0.106s
  training loss:		0.252375
  validation loss:		0.381654
  validation accuracy:		89.57 %
Epoch 1276 of 2000 took 0.105s
  training loss:		0.249321
  validation loss:		0.363350
  validation accuracy:		90.00 %
Epoch 1277 of 2000 took 0.100s
  training loss:		0.247821
  validation loss:		0.377804
  validation accuracy:		90.22 %
Epoch 1278 of 2000 took 0.093s
  training loss:		0.251941
  validation loss:		0.370412
  validation accuracy:		90.11 %
Epoch 1279 of 2000 took 0.102s
  training loss:		0.241499
  validation loss:		0.361142
  validation accuracy:		90.33 %
Epoch 1280 of 2000 took 0.106s
  training loss:		0.247850
  validation loss:		0.379333
  validation accuracy:		89.78 %
Epoch 1281 of 2000 took 0.105s
  training loss:		0.249402
  validation loss:		0.393720
  validation accuracy:		89.78 %
Epoch 1282 of 2000 took 0.106s
  training loss:		0.247119
  validation loss:		0.391091
  validation accuracy:		89.24 %
Epoch 1283 of 2000 took 0.106s
  training loss:		0.251390
  validation loss:		0.385500
  validation accuracy:		90.43 %
Epoch 1284 of 2000 took 0.105s
  training loss:		0.249283
  validation loss:		0.371313
  validation accuracy:		90.33 %
Epoch 1285 of 2000 took 0.106s
  training loss:		0.245514
  validation loss:		0.378145
  validation accuracy:		90.00 %
Epoch 1286 of 2000 took 0.109s
  training loss:		0.250665
  validation loss:		0.387870
  validation accuracy:		89.67 %
Epoch 1287 of 2000 took 0.094s
  training loss:		0.240445
  validation loss:		0.396186
  validation accuracy:		90.00 %
Epoch 1288 of 2000 took 0.093s
  training loss:		0.252925
  validation loss:		0.371596
  validation accuracy:		90.43 %
Epoch 1289 of 2000 took 0.098s
  training loss:		0.242852
  validation loss:		0.364298
  validation accuracy:		90.00 %
Epoch 1290 of 2000 took 0.105s
  training loss:		0.247593
  validation loss:		0.386177
  validation accuracy:		89.35 %
Epoch 1291 of 2000 took 0.105s
  training loss:		0.250675
  validation loss:		0.403294
  validation accuracy:		89.46 %
Epoch 1292 of 2000 took 0.105s
  training loss:		0.244085
  validation loss:		0.385614
  validation accuracy:		89.78 %
Epoch 1293 of 2000 took 0.105s
  training loss:		0.249061
  validation loss:		0.368382
  validation accuracy:		90.00 %
Epoch 1294 of 2000 took 0.105s
  training loss:		0.254382
  validation loss:		0.374862
  validation accuracy:		90.33 %
Epoch 1295 of 2000 took 0.105s
  training loss:		0.242292
  validation loss:		0.373276
  validation accuracy:		90.22 %
Epoch 1296 of 2000 took 0.107s
  training loss:		0.242739
  validation loss:		0.378890
  validation accuracy:		89.67 %
Epoch 1297 of 2000 took 0.093s
  training loss:		0.249042
  validation loss:		0.393178
  validation accuracy:		89.02 %
Epoch 1298 of 2000 took 0.092s
  training loss:		0.247276
  validation loss:		0.392012
  validation accuracy:		89.78 %
Epoch 1299 of 2000 took 0.092s
  training loss:		0.248316
  validation loss:		0.376576
  validation accuracy:		90.11 %
Epoch 1300 of 2000 took 0.092s
  training loss:		0.245120
  validation loss:		0.373307
  validation accuracy:		89.78 %
Epoch 1301 of 2000 took 0.102s
  training loss:		0.252589
  validation loss:		0.369653
  validation accuracy:		90.54 %
Epoch 1302 of 2000 took 0.105s
  training loss:		0.246384
  validation loss:		0.381715
  validation accuracy:		89.57 %
Epoch 1303 of 2000 took 0.105s
  training loss:		0.246028
  validation loss:		0.398786
  validation accuracy:		89.78 %
Epoch 1304 of 2000 took 0.105s
  training loss:		0.249241
  validation loss:		0.382255
  validation accuracy:		90.33 %
Epoch 1305 of 2000 took 0.105s
  training loss:		0.246011
  validation loss:		0.399493
  validation accuracy:		89.78 %
Epoch 1306 of 2000 took 0.107s
  training loss:		0.243896
  validation loss:		0.380300
  validation accuracy:		90.00 %
Epoch 1307 of 2000 took 0.093s
  training loss:		0.251528
  validation loss:		0.391369
  validation accuracy:		90.11 %
Epoch 1308 of 2000 took 0.093s
  training loss:		0.251025
  validation loss:		0.368285
  validation accuracy:		90.65 %
Epoch 1309 of 2000 took 0.092s
  training loss:		0.250032
  validation loss:		0.377064
  validation accuracy:		90.00 %
Epoch 1310 of 2000 took 0.093s
  training loss:		0.248093
  validation loss:		0.361446
  validation accuracy:		90.33 %
Epoch 1311 of 2000 took 0.102s
  training loss:		0.246908
  validation loss:		0.378161
  validation accuracy:		90.22 %
Epoch 1312 of 2000 took 0.105s
  training loss:		0.251555
  validation loss:		0.372011
  validation accuracy:		89.78 %
Epoch 1313 of 2000 took 0.105s
  training loss:		0.248069
  validation loss:		0.375026
  validation accuracy:		89.67 %
Epoch 1314 of 2000 took 0.105s
  training loss:		0.251298
  validation loss:		0.388193
  validation accuracy:		89.89 %
Epoch 1315 of 2000 took 0.105s
  training loss:		0.247769
  validation loss:		0.379272
  validation accuracy:		89.89 %
Epoch 1316 of 2000 took 0.108s
  training loss:		0.250299
  validation loss:		0.398516
  validation accuracy:		89.46 %
Epoch 1317 of 2000 took 0.093s
  training loss:		0.242806
  validation loss:		0.390929
  validation accuracy:		89.89 %
Epoch 1318 of 2000 took 0.092s
  training loss:		0.248174
  validation loss:		0.386601
  validation accuracy:		89.24 %
Epoch 1319 of 2000 took 0.092s
  training loss:		0.242794
  validation loss:		0.394198
  validation accuracy:		89.67 %
Epoch 1320 of 2000 took 0.092s
  training loss:		0.247490
  validation loss:		0.370066
  validation accuracy:		90.11 %
Epoch 1321 of 2000 took 0.092s
  training loss:		0.251393
  validation loss:		0.382602
  validation accuracy:		90.00 %
Epoch 1322 of 2000 took 0.101s
  training loss:		0.248298
  validation loss:		0.378267
  validation accuracy:		90.43 %
Epoch 1323 of 2000 took 0.105s
  training loss:		0.240293
  validation loss:		0.422766
  validation accuracy:		88.80 %
Epoch 1324 of 2000 took 0.105s
  training loss:		0.252504
  validation loss:		0.385295
  validation accuracy:		90.11 %
Epoch 1325 of 2000 took 0.106s
  training loss:		0.248773
  validation loss:		0.369267
  validation accuracy:		90.11 %
Epoch 1326 of 2000 took 0.109s
  training loss:		0.247986
  validation loss:		0.387837
  validation accuracy:		90.00 %
Epoch 1327 of 2000 took 0.094s
  training loss:		0.250102
  validation loss:		0.373079
  validation accuracy:		89.57 %
Epoch 1328 of 2000 took 0.093s
  training loss:		0.253214
  validation loss:		0.373165
  validation accuracy:		90.00 %
Epoch 1329 of 2000 took 0.092s
  training loss:		0.245785
  validation loss:		0.389178
  validation accuracy:		89.46 %
Epoch 1330 of 2000 took 0.092s
  training loss:		0.242952
  validation loss:		0.387553
  validation accuracy:		89.13 %
Epoch 1331 of 2000 took 0.092s
  training loss:		0.245896
  validation loss:		0.384535
  validation accuracy:		89.89 %
Epoch 1332 of 2000 took 0.099s
  training loss:		0.247003
  validation loss:		0.392916
  validation accuracy:		88.80 %
Epoch 1333 of 2000 took 0.105s
  training loss:		0.248912
  validation loss:		0.390808
  validation accuracy:		89.67 %
Epoch 1334 of 2000 took 0.105s
  training loss:		0.245929
  validation loss:		0.391090
  validation accuracy:		89.89 %
Epoch 1335 of 2000 took 0.105s
  training loss:		0.256226
  validation loss:		0.408729
  validation accuracy:		89.57 %
Epoch 1336 of 2000 took 0.109s
  training loss:		0.246947
  validation loss:		0.388246
  validation accuracy:		90.00 %
Epoch 1337 of 2000 took 0.096s
  training loss:		0.254797
  validation loss:		0.373846
  validation accuracy:		90.11 %
Epoch 1338 of 2000 took 0.093s
  training loss:		0.247238
  validation loss:		0.394296
  validation accuracy:		89.57 %
Epoch 1339 of 2000 took 0.092s
  training loss:		0.253102
  validation loss:		0.372591
  validation accuracy:		90.11 %
Epoch 1340 of 2000 took 0.093s
  training loss:		0.245477
  validation loss:		0.379823
  validation accuracy:		89.67 %
Epoch 1341 of 2000 took 0.093s
  training loss:		0.253218
  validation loss:		0.362282
  validation accuracy:		90.00 %
Epoch 1342 of 2000 took 0.092s
  training loss:		0.249005
  validation loss:		0.371537
  validation accuracy:		89.57 %
Epoch 1343 of 2000 took 0.101s
  training loss:		0.248672
  validation loss:		0.370366
  validation accuracy:		89.89 %
Epoch 1344 of 2000 took 0.105s
  training loss:		0.247504
  validation loss:		0.407352
  validation accuracy:		89.35 %
Epoch 1345 of 2000 took 0.105s
  training loss:		0.247145
  validation loss:		0.382472
  validation accuracy:		90.00 %
Epoch 1346 of 2000 took 0.107s
  training loss:		0.243373
  validation loss:		0.391635
  validation accuracy:		89.24 %
Epoch 1347 of 2000 took 0.101s
  training loss:		0.245499
  validation loss:		0.371763
  validation accuracy:		90.22 %
Epoch 1348 of 2000 took 0.093s
  training loss:		0.239847
  validation loss:		0.363697
  validation accuracy:		89.78 %
Epoch 1349 of 2000 took 0.092s
  training loss:		0.251934
  validation loss:		0.374865
  validation accuracy:		89.89 %
Epoch 1350 of 2000 took 0.092s
  training loss:		0.240465
  validation loss:		0.380178
  validation accuracy:		89.89 %
Epoch 1351 of 2000 took 0.092s
  training loss:		0.248521
  validation loss:		0.380367
  validation accuracy:		89.13 %
Epoch 1352 of 2000 took 0.092s
  training loss:		0.252168
  validation loss:		0.383558
  validation accuracy:		89.24 %
Epoch 1353 of 2000 took 0.094s
  training loss:		0.241322
  validation loss:		0.364999
  validation accuracy:		89.67 %
Epoch 1354 of 2000 took 0.105s
  training loss:		0.248326
  validation loss:		0.383505
  validation accuracy:		89.57 %
Epoch 1355 of 2000 took 0.105s
  training loss:		0.240929
  validation loss:		0.365236
  validation accuracy:		90.22 %
Epoch 1356 of 2000 took 0.105s
  training loss:		0.244167
  validation loss:		0.391899
  validation accuracy:		89.24 %
Epoch 1357 of 2000 took 0.106s
  training loss:		0.247434
  validation loss:		0.414736
  validation accuracy:		89.46 %
Epoch 1358 of 2000 took 0.093s
  training loss:		0.250579
  validation loss:		0.379700
  validation accuracy:		90.43 %
Epoch 1359 of 2000 took 0.092s
  training loss:		0.245464
  validation loss:		0.365420
  validation accuracy:		90.00 %
Epoch 1360 of 2000 took 0.092s
  training loss:		0.249856
  validation loss:		0.373199
  validation accuracy:		90.22 %
Epoch 1361 of 2000 took 0.092s
  training loss:		0.249527
  validation loss:		0.368931
  validation accuracy:		90.33 %
Epoch 1362 of 2000 took 0.092s
  training loss:		0.246324
  validation loss:		0.392847
  validation accuracy:		89.57 %
Epoch 1363 of 2000 took 0.092s
  training loss:		0.252117
  validation loss:		0.384053
  validation accuracy:		88.48 %
Epoch 1364 of 2000 took 0.092s
  training loss:		0.245293
  validation loss:		0.369695
  validation accuracy:		89.89 %
Epoch 1365 of 2000 took 0.093s
  training loss:		0.251123
  validation loss:		0.375226
  validation accuracy:		90.11 %
Epoch 1366 of 2000 took 0.098s
  training loss:		0.236470
  validation loss:		0.360715
  validation accuracy:		89.78 %
Epoch 1367 of 2000 took 0.109s
  training loss:		0.246094
  validation loss:		0.382712
  validation accuracy:		89.67 %
Epoch 1368 of 2000 took 0.097s
  training loss:		0.246520
  validation loss:		0.380010
  validation accuracy:		90.33 %
Epoch 1369 of 2000 took 0.093s
  training loss:		0.249534
  validation loss:		0.386859
  validation accuracy:		89.78 %
Epoch 1370 of 2000 took 0.092s
  training loss:		0.251935
  validation loss:		0.386314
  validation accuracy:		89.24 %
Epoch 1371 of 2000 took 0.093s
  training loss:		0.249464
  validation loss:		0.379438
  validation accuracy:		89.24 %
Epoch 1372 of 2000 took 0.092s
  training loss:		0.253021
  validation loss:		0.365572
  validation accuracy:		90.33 %
Epoch 1373 of 2000 took 0.092s
  training loss:		0.246676
  validation loss:		0.363701
  validation accuracy:		90.43 %
Epoch 1374 of 2000 took 0.092s
  training loss:		0.246161
  validation loss:		0.369584
  validation accuracy:		90.00 %
Epoch 1375 of 2000 took 0.092s
  training loss:		0.240177
  validation loss:		0.364592
  validation accuracy:		90.22 %
Epoch 1376 of 2000 took 0.092s
  training loss:		0.243201
  validation loss:		0.403225
  validation accuracy:		90.00 %
Epoch 1377 of 2000 took 0.103s
  training loss:		0.249093
  validation loss:		0.379139
  validation accuracy:		89.67 %
Epoch 1378 of 2000 took 0.107s
  training loss:		0.237523
  validation loss:		0.374508
  validation accuracy:		90.33 %
Epoch 1379 of 2000 took 0.093s
  training loss:		0.246184
  validation loss:		0.378321
  validation accuracy:		90.54 %
Epoch 1380 of 2000 took 0.092s
  training loss:		0.244470
  validation loss:		0.371491
  validation accuracy:		90.33 %
Epoch 1381 of 2000 took 0.092s
  training loss:		0.246865
  validation loss:		0.380223
  validation accuracy:		89.78 %
Epoch 1382 of 2000 took 0.092s
  training loss:		0.242701
  validation loss:		0.400158
  validation accuracy:		89.46 %
Epoch 1383 of 2000 took 0.092s
  training loss:		0.240376
  validation loss:		0.370840
  validation accuracy:		90.43 %
Epoch 1384 of 2000 took 0.092s
  training loss:		0.253244
  validation loss:		0.366354
  validation accuracy:		90.33 %
Epoch 1385 of 2000 took 0.092s
  training loss:		0.247597
  validation loss:		0.387205
  validation accuracy:		90.11 %
Epoch 1386 of 2000 took 0.092s
  training loss:		0.251083
  validation loss:		0.376639
  validation accuracy:		89.67 %
Epoch 1387 of 2000 took 0.092s
  training loss:		0.247127
  validation loss:		0.371870
  validation accuracy:		90.43 %
Epoch 1388 of 2000 took 0.105s
  training loss:		0.236856
  validation loss:		0.368770
  validation accuracy:		90.00 %
Epoch 1389 of 2000 took 0.100s
  training loss:		0.249139
  validation loss:		0.371197
  validation accuracy:		90.43 %
Epoch 1390 of 2000 took 0.093s
  training loss:		0.243609
  validation loss:		0.370327
  validation accuracy:		90.22 %
Epoch 1391 of 2000 took 0.092s
  training loss:		0.247673
  validation loss:		0.394280
  validation accuracy:		89.57 %
Epoch 1392 of 2000 took 0.092s
  training loss:		0.251253
  validation loss:		0.380227
  validation accuracy:		89.67 %
Epoch 1393 of 2000 took 0.092s
  training loss:		0.245580
  validation loss:		0.380589
  validation accuracy:		89.24 %
Epoch 1394 of 2000 took 0.092s
  training loss:		0.247706
  validation loss:		0.380262
  validation accuracy:		89.46 %
Epoch 1395 of 2000 took 0.092s
  training loss:		0.234746
  validation loss:		0.377482
  validation accuracy:		90.22 %
Epoch 1396 of 2000 took 0.092s
  training loss:		0.245454
  validation loss:		0.361720
  validation accuracy:		90.22 %
Epoch 1397 of 2000 took 0.092s
  training loss:		0.250078
  validation loss:		0.390771
  validation accuracy:		89.57 %
Epoch 1398 of 2000 took 0.093s
  training loss:		0.248578
  validation loss:		0.361411
  validation accuracy:		89.67 %
Epoch 1399 of 2000 took 0.108s
  training loss:		0.251492
  validation loss:		0.409870
  validation accuracy:		89.02 %
Epoch 1400 of 2000 took 0.096s
  training loss:		0.250998
  validation loss:		0.372700
  validation accuracy:		90.00 %
Epoch 1401 of 2000 took 0.093s
  training loss:		0.252794
  validation loss:		0.375693
  validation accuracy:		89.78 %
Epoch 1402 of 2000 took 0.092s
  training loss:		0.248882
  validation loss:		0.393076
  validation accuracy:		89.89 %
Epoch 1403 of 2000 took 0.093s
  training loss:		0.258894
  validation loss:		0.391369
  validation accuracy:		89.35 %
Epoch 1404 of 2000 took 0.092s
  training loss:		0.244048
  validation loss:		0.384925
  validation accuracy:		89.35 %
Epoch 1405 of 2000 took 0.092s
  training loss:		0.250143
  validation loss:		0.371726
  validation accuracy:		90.22 %
Epoch 1406 of 2000 took 0.092s
  training loss:		0.241549
  validation loss:		0.373561
  validation accuracy:		90.11 %
Epoch 1407 of 2000 took 0.093s
  training loss:		0.242666
  validation loss:		0.376084
  validation accuracy:		89.89 %
Epoch 1408 of 2000 took 0.093s
  training loss:		0.242992
  validation loss:		0.364694
  validation accuracy:		89.89 %
Epoch 1409 of 2000 took 0.092s
  training loss:		0.245992
  validation loss:		0.387675
  validation accuracy:		89.02 %
Epoch 1410 of 2000 took 0.096s
  training loss:		0.247136
  validation loss:		0.382165
  validation accuracy:		89.67 %
Epoch 1411 of 2000 took 0.099s
  training loss:		0.246099
  validation loss:		0.389100
  validation accuracy:		89.78 %
Epoch 1412 of 2000 took 0.098s
  training loss:		0.245113
  validation loss:		0.366889
  validation accuracy:		90.11 %
Epoch 1413 of 2000 took 0.098s
  training loss:		0.242960
  validation loss:		0.384910
  validation accuracy:		90.00 %
Epoch 1414 of 2000 took 0.098s
  training loss:		0.249587
  validation loss:		0.365020
  validation accuracy:		89.89 %
Epoch 1415 of 2000 took 0.098s
  training loss:		0.250980
  validation loss:		0.376197
  validation accuracy:		89.35 %
Epoch 1416 of 2000 took 0.098s
  training loss:		0.248469
  validation loss:		0.398870
  validation accuracy:		89.24 %
Epoch 1417 of 2000 took 0.098s
  training loss:		0.245667
  validation loss:		0.367482
  validation accuracy:		90.33 %
Epoch 1418 of 2000 took 0.098s
  training loss:		0.248321
  validation loss:		0.379036
  validation accuracy:		90.33 %
Epoch 1419 of 2000 took 0.098s
  training loss:		0.245894
  validation loss:		0.378595
  validation accuracy:		89.78 %
Epoch 1420 of 2000 took 0.100s
  training loss:		0.246676
  validation loss:		0.362797
  validation accuracy:		89.89 %
Epoch 1421 of 2000 took 0.099s
  training loss:		0.248831
  validation loss:		0.365989
  validation accuracy:		90.54 %
Epoch 1422 of 2000 took 0.098s
  training loss:		0.248315
  validation loss:		0.367555
  validation accuracy:		89.89 %
Epoch 1423 of 2000 took 0.098s
  training loss:		0.246669
  validation loss:		0.380082
  validation accuracy:		89.78 %
Epoch 1424 of 2000 took 0.098s
  training loss:		0.243683
  validation loss:		0.380994
  validation accuracy:		90.00 %
Epoch 1425 of 2000 took 0.098s
  training loss:		0.251416
  validation loss:		0.388563
  validation accuracy:		89.67 %
Epoch 1426 of 2000 took 0.098s
  training loss:		0.245234
  validation loss:		0.390112
  validation accuracy:		89.67 %
Epoch 1427 of 2000 took 0.098s
  training loss:		0.244125
  validation loss:		0.374357
  validation accuracy:		90.22 %
Epoch 1428 of 2000 took 0.098s
  training loss:		0.247218
  validation loss:		0.369291
  validation accuracy:		90.11 %
Epoch 1429 of 2000 took 0.098s
  training loss:		0.247340
  validation loss:		0.368745
  validation accuracy:		90.43 %
Epoch 1430 of 2000 took 0.100s
  training loss:		0.244159
  validation loss:		0.369974
  validation accuracy:		89.67 %
Epoch 1431 of 2000 took 0.101s
  training loss:		0.244440
  validation loss:		0.367491
  validation accuracy:		90.43 %
Epoch 1432 of 2000 took 0.105s
  training loss:		0.245484
  validation loss:		0.367255
  validation accuracy:		90.11 %
Epoch 1433 of 2000 took 0.105s
  training loss:		0.241376
  validation loss:		0.372847
  validation accuracy:		89.89 %
Epoch 1434 of 2000 took 0.106s
  training loss:		0.245961
  validation loss:		0.382140
  validation accuracy:		89.46 %
Epoch 1435 of 2000 took 0.105s
  training loss:		0.245117
  validation loss:		0.378298
  validation accuracy:		90.33 %
Epoch 1436 of 2000 took 0.105s
  training loss:		0.244359
  validation loss:		0.368015
  validation accuracy:		90.00 %
Epoch 1437 of 2000 took 0.105s
  training loss:		0.246463
  validation loss:		0.364147
  validation accuracy:		89.89 %
Epoch 1438 of 2000 took 0.105s
  training loss:		0.249348
  validation loss:		0.371646
  validation accuracy:		90.33 %
Epoch 1439 of 2000 took 0.106s
  training loss:		0.244934
  validation loss:		0.360394
  validation accuracy:		90.33 %
Epoch 1440 of 2000 took 0.106s
  training loss:		0.245599
  validation loss:		0.368507
  validation accuracy:		90.00 %
Epoch 1441 of 2000 took 0.104s
  training loss:		0.243293
  validation loss:		0.389448
  validation accuracy:		89.57 %
Epoch 1442 of 2000 took 0.105s
  training loss:		0.249387
  validation loss:		0.372479
  validation accuracy:		90.33 %
Epoch 1443 of 2000 took 0.105s
  training loss:		0.246951
  validation loss:		0.363728
  validation accuracy:		89.67 %
Epoch 1444 of 2000 took 0.106s
  training loss:		0.247503
  validation loss:		0.372630
  validation accuracy:		90.33 %
Epoch 1445 of 2000 took 0.105s
  training loss:		0.251113
  validation loss:		0.364443
  validation accuracy:		90.00 %
Epoch 1446 of 2000 took 0.106s
  training loss:		0.243956
  validation loss:		0.375450
  validation accuracy:		89.67 %
Epoch 1447 of 2000 took 0.105s
  training loss:		0.245452
  validation loss:		0.369148
  validation accuracy:		89.35 %
Epoch 1448 of 2000 took 0.105s
  training loss:		0.251291
  validation loss:		0.388059
  validation accuracy:		89.78 %
Epoch 1449 of 2000 took 0.105s
  training loss:		0.251908
  validation loss:		0.367490
  validation accuracy:		90.43 %
Epoch 1450 of 2000 took 0.105s
  training loss:		0.242725
  validation loss:		0.367218
  validation accuracy:		89.78 %
Epoch 1451 of 2000 took 0.105s
  training loss:		0.247112
  validation loss:		0.371488
  validation accuracy:		89.78 %
Epoch 1452 of 2000 took 0.105s
  training loss:		0.248646
  validation loss:		0.374765
  validation accuracy:		90.11 %
Epoch 1453 of 2000 took 0.105s
  training loss:		0.242526
  validation loss:		0.370905
  validation accuracy:		90.11 %
Epoch 1454 of 2000 took 0.105s
  training loss:		0.246300
  validation loss:		0.379330
  validation accuracy:		89.89 %
Epoch 1455 of 2000 took 0.101s
  training loss:		0.243035
  validation loss:		0.382833
  validation accuracy:		89.35 %
Epoch 1456 of 2000 took 0.095s
  training loss:		0.247716
  validation loss:		0.373289
  validation accuracy:		90.43 %
Epoch 1457 of 2000 took 0.095s
  training loss:		0.244519
  validation loss:		0.373834
  validation accuracy:		90.11 %
Epoch 1458 of 2000 took 0.095s
  training loss:		0.245630
  validation loss:		0.375326
  validation accuracy:		90.22 %
Epoch 1459 of 2000 took 0.095s
  training loss:		0.249580
  validation loss:		0.388638
  validation accuracy:		88.80 %
Epoch 1460 of 2000 took 0.095s
  training loss:		0.247147
  validation loss:		0.399915
  validation accuracy:		89.67 %
Epoch 1461 of 2000 took 0.102s
  training loss:		0.247900
  validation loss:		0.391366
  validation accuracy:		89.78 %
Epoch 1462 of 2000 took 0.105s
  training loss:		0.241038
  validation loss:		0.363788
  validation accuracy:		90.22 %
Epoch 1463 of 2000 took 0.106s
  training loss:		0.246102
  validation loss:		0.384860
  validation accuracy:		89.67 %
Epoch 1464 of 2000 took 0.105s
  training loss:		0.247572
  validation loss:		0.384702
  validation accuracy:		89.67 %
Epoch 1465 of 2000 took 0.101s
  training loss:		0.247709
  validation loss:		0.391095
  validation accuracy:		89.46 %
Epoch 1466 of 2000 took 0.096s
  training loss:		0.239252
  validation loss:		0.373178
  validation accuracy:		89.78 %
Epoch 1467 of 2000 took 0.095s
  training loss:		0.242375
  validation loss:		0.373904
  validation accuracy:		90.11 %
Epoch 1468 of 2000 took 0.095s
  training loss:		0.248549
  validation loss:		0.381618
  validation accuracy:		89.57 %
Epoch 1469 of 2000 took 0.095s
  training loss:		0.244245
  validation loss:		0.367262
  validation accuracy:		90.33 %
Epoch 1470 of 2000 took 0.095s
  training loss:		0.246724
  validation loss:		0.386206
  validation accuracy:		89.46 %
Epoch 1471 of 2000 took 0.096s
  training loss:		0.248367
  validation loss:		0.384894
  validation accuracy:		89.67 %
Epoch 1472 of 2000 took 0.105s
  training loss:		0.248465
  validation loss:		0.371937
  validation accuracy:		89.46 %
Epoch 1473 of 2000 took 0.105s
  training loss:		0.247613
  validation loss:		0.368894
  validation accuracy:		89.78 %
Epoch 1474 of 2000 took 0.105s
  training loss:		0.242209
  validation loss:		0.379832
  validation accuracy:		89.78 %
Epoch 1475 of 2000 took 0.102s
  training loss:		0.239784
  validation loss:		0.364162
  validation accuracy:		89.89 %
Epoch 1476 of 2000 took 0.096s
  training loss:		0.246850
  validation loss:		0.376192
  validation accuracy:		90.00 %
Epoch 1477 of 2000 took 0.095s
  training loss:		0.250378
  validation loss:		0.373343
  validation accuracy:		90.11 %
Epoch 1478 of 2000 took 0.095s
  training loss:		0.244756
  validation loss:		0.362827
  validation accuracy:		89.78 %
Epoch 1479 of 2000 took 0.095s
  training loss:		0.244381
  validation loss:		0.377416
  validation accuracy:		90.11 %
Epoch 1480 of 2000 took 0.095s
  training loss:		0.246689
  validation loss:		0.392678
  validation accuracy:		89.24 %
Epoch 1481 of 2000 took 0.095s
  training loss:		0.244845
  validation loss:		0.364368
  validation accuracy:		90.54 %
Epoch 1482 of 2000 took 0.095s
  training loss:		0.247904
  validation loss:		0.381615
  validation accuracy:		90.00 %
Epoch 1483 of 2000 took 0.095s
  training loss:		0.244752
  validation loss:		0.382613
  validation accuracy:		89.67 %
Epoch 1484 of 2000 took 0.104s
  training loss:		0.250230
  validation loss:		0.376686
  validation accuracy:		89.89 %
Epoch 1485 of 2000 took 0.105s
  training loss:		0.238462
  validation loss:		0.370112
  validation accuracy:		90.11 %
Epoch 1486 of 2000 took 0.096s
  training loss:		0.243887
  validation loss:		0.363106
  validation accuracy:		90.00 %
Epoch 1487 of 2000 took 0.096s
  training loss:		0.247661
  validation loss:		0.387866
  validation accuracy:		89.46 %
Epoch 1488 of 2000 took 0.095s
  training loss:		0.250088
  validation loss:		0.371275
  validation accuracy:		89.57 %
Epoch 1489 of 2000 took 0.095s
  training loss:		0.243941
  validation loss:		0.378581
  validation accuracy:		90.00 %
Epoch 1490 of 2000 took 0.096s
  training loss:		0.241965
  validation loss:		0.378117
  validation accuracy:		89.57 %
Epoch 1491 of 2000 took 0.093s
  training loss:		0.247156
  validation loss:		0.374943
  validation accuracy:		89.57 %
Epoch 1492 of 2000 took 0.092s
  training loss:		0.245404
  validation loss:		0.382497
  validation accuracy:		90.11 %
Epoch 1493 of 2000 took 0.092s
  training loss:		0.245184
  validation loss:		0.375897
  validation accuracy:		90.11 %
Epoch 1494 of 2000 took 0.093s
  training loss:		0.242834
  validation loss:		0.366618
  validation accuracy:		90.00 %
Epoch 1495 of 2000 took 0.104s
  training loss:		0.248100
  validation loss:		0.360345
  validation accuracy:		90.00 %
Epoch 1496 of 2000 took 0.097s
  training loss:		0.241168
  validation loss:		0.374040
  validation accuracy:		89.78 %
Epoch 1497 of 2000 took 0.093s
  training loss:		0.241378
  validation loss:		0.362283
  validation accuracy:		90.43 %
Epoch 1498 of 2000 took 0.092s
  training loss:		0.249110
  validation loss:		0.375894
  validation accuracy:		89.89 %
Epoch 1499 of 2000 took 0.092s
  training loss:		0.244419
  validation loss:		0.366486
  validation accuracy:		90.00 %
Epoch 1500 of 2000 took 0.092s
  training loss:		0.248040
  validation loss:		0.370804
  validation accuracy:		89.46 %
Epoch 1501 of 2000 took 0.092s
  training loss:		0.247866
  validation loss:		0.368969
  validation accuracy:		90.22 %
Epoch 1502 of 2000 took 0.092s
  training loss:		0.244661
  validation loss:		0.360842
  validation accuracy:		89.89 %
Epoch 1503 of 2000 took 0.092s
  training loss:		0.242749
  validation loss:		0.373606
  validation accuracy:		90.11 %
Epoch 1504 of 2000 took 0.092s
  training loss:		0.243078
  validation loss:		0.362097
  validation accuracy:		90.00 %
Epoch 1505 of 2000 took 0.092s
  training loss:		0.244923
  validation loss:		0.381737
  validation accuracy:		89.78 %
Epoch 1506 of 2000 took 0.105s
  training loss:		0.241753
  validation loss:		0.373841
  validation accuracy:		90.11 %
Epoch 1507 of 2000 took 0.093s
  training loss:		0.250653
  validation loss:		0.362173
  validation accuracy:		90.22 %
Epoch 1508 of 2000 took 0.092s
  training loss:		0.243202
  validation loss:		0.373549
  validation accuracy:		89.78 %
Epoch 1509 of 2000 took 0.092s
  training loss:		0.242308
  validation loss:		0.381149
  validation accuracy:		89.89 %
Epoch 1510 of 2000 took 0.092s
  training loss:		0.241068
  validation loss:		0.382680
  validation accuracy:		89.89 %
Epoch 1511 of 2000 took 0.092s
  training loss:		0.244521
  validation loss:		0.382679
  validation accuracy:		89.78 %
Epoch 1512 of 2000 took 0.092s
  training loss:		0.248172
  validation loss:		0.398921
  validation accuracy:		89.35 %
Epoch 1513 of 2000 took 0.092s
  training loss:		0.247927
  validation loss:		0.361287
  validation accuracy:		90.22 %
Epoch 1514 of 2000 took 0.092s
  training loss:		0.245728
  validation loss:		0.394261
  validation accuracy:		89.57 %
Epoch 1515 of 2000 took 0.092s
  training loss:		0.245234
  validation loss:		0.376100
  validation accuracy:		89.78 %
Epoch 1516 of 2000 took 0.092s
  training loss:		0.247970
  validation loss:		0.376696
  validation accuracy:		89.35 %
Epoch 1517 of 2000 took 0.101s
  training loss:		0.243392
  validation loss:		0.377497
  validation accuracy:		89.57 %
Epoch 1518 of 2000 took 0.093s
  training loss:		0.241819
  validation loss:		0.390107
  validation accuracy:		89.57 %
Epoch 1519 of 2000 took 0.092s
  training loss:		0.250841
  validation loss:		0.368636
  validation accuracy:		89.89 %
Epoch 1520 of 2000 took 0.092s
  training loss:		0.243969
  validation loss:		0.378014
  validation accuracy:		89.89 %
Epoch 1521 of 2000 took 0.092s
  training loss:		0.246254
  validation loss:		0.373383
  validation accuracy:		90.43 %
Epoch 1522 of 2000 took 0.092s
  training loss:		0.246708
  validation loss:		0.398857
  validation accuracy:		89.35 %
Epoch 1523 of 2000 took 0.092s
  training loss:		0.244785
  validation loss:		0.379965
  validation accuracy:		89.46 %
Epoch 1524 of 2000 took 0.092s
  training loss:		0.248111
  validation loss:		0.388488
  validation accuracy:		89.78 %
Epoch 1525 of 2000 took 0.092s
  training loss:		0.244381
  validation loss:		0.365129
  validation accuracy:		90.11 %
Epoch 1526 of 2000 took 0.093s
  training loss:		0.248727
  validation loss:		0.373841
  validation accuracy:		89.24 %
Epoch 1527 of 2000 took 0.094s
  training loss:		0.246898
  validation loss:		0.380598
  validation accuracy:		89.35 %
Epoch 1528 of 2000 took 0.098s
  training loss:		0.251776
  validation loss:		0.401121
  validation accuracy:		89.46 %
Epoch 1529 of 2000 took 0.093s
  training loss:		0.248909
  validation loss:		0.375774
  validation accuracy:		89.57 %
Epoch 1530 of 2000 took 0.093s
  training loss:		0.244104
  validation loss:		0.378471
  validation accuracy:		89.89 %
Epoch 1531 of 2000 took 0.092s
  training loss:		0.245092
  validation loss:		0.370702
  validation accuracy:		90.00 %
Epoch 1532 of 2000 took 0.093s
  training loss:		0.239010
  validation loss:		0.371929
  validation accuracy:		90.00 %
Epoch 1533 of 2000 took 0.098s
  training loss:		0.248557
  validation loss:		0.363920
  validation accuracy:		90.11 %
Epoch 1534 of 2000 took 0.098s
  training loss:		0.242437
  validation loss:		0.377769
  validation accuracy:		89.89 %
Epoch 1535 of 2000 took 0.098s
  training loss:		0.239718
  validation loss:		0.376870
  validation accuracy:		90.00 %
Epoch 1536 of 2000 took 0.098s
  training loss:		0.245940
  validation loss:		0.373312
  validation accuracy:		89.46 %
Epoch 1537 of 2000 took 0.098s
  training loss:		0.245886
  validation loss:		0.373740
  validation accuracy:		89.67 %
Epoch 1538 of 2000 took 0.102s
  training loss:		0.243131
  validation loss:		0.380308
  validation accuracy:		89.57 %
Epoch 1539 of 2000 took 0.106s
  training loss:		0.246086
  validation loss:		0.371347
  validation accuracy:		89.46 %
Epoch 1540 of 2000 took 0.105s
  training loss:		0.243330
  validation loss:		0.390744
  validation accuracy:		89.02 %
Epoch 1541 of 2000 took 0.105s
  training loss:		0.252249
  validation loss:		0.364673
  validation accuracy:		89.78 %
Epoch 1542 of 2000 took 0.105s
  training loss:		0.245202
  validation loss:		0.369554
  validation accuracy:		89.89 %
Epoch 1543 of 2000 took 0.106s
  training loss:		0.245967
  validation loss:		0.375664
  validation accuracy:		88.91 %
Epoch 1544 of 2000 took 0.105s
  training loss:		0.247314
  validation loss:		0.375381
  validation accuracy:		90.00 %
Epoch 1545 of 2000 took 0.105s
  training loss:		0.248481
  validation loss:		0.390887
  validation accuracy:		89.35 %
Epoch 1546 of 2000 took 0.106s
  training loss:		0.244550
  validation loss:		0.375065
  validation accuracy:		89.57 %
Epoch 1547 of 2000 took 0.105s
  training loss:		0.242602
  validation loss:		0.398886
  validation accuracy:		89.24 %
Epoch 1548 of 2000 took 0.109s
  training loss:		0.247295
  validation loss:		0.373667
  validation accuracy:		90.11 %
Epoch 1549 of 2000 took 0.105s
  training loss:		0.241427
  validation loss:		0.371396
  validation accuracy:		89.89 %
Epoch 1550 of 2000 took 0.105s
  training loss:		0.240877
  validation loss:		0.377296
  validation accuracy:		89.78 %
Epoch 1551 of 2000 took 0.105s
  training loss:		0.246322
  validation loss:		0.373764
  validation accuracy:		90.43 %
Epoch 1552 of 2000 took 0.105s
  training loss:		0.245307
  validation loss:		0.362036
  validation accuracy:		90.00 %
Epoch 1553 of 2000 took 0.105s
  training loss:		0.248314
  validation loss:		0.377082
  validation accuracy:		89.46 %
Epoch 1554 of 2000 took 0.105s
  training loss:		0.248523
  validation loss:		0.392379
  validation accuracy:		88.91 %
Epoch 1555 of 2000 took 0.105s
  training loss:		0.246226
  validation loss:		0.370539
  validation accuracy:		89.89 %
Epoch 1556 of 2000 took 0.106s
  training loss:		0.246240
  validation loss:		0.372922
  validation accuracy:		89.89 %
Epoch 1557 of 2000 took 0.107s
  training loss:		0.254940
  validation loss:		0.372345
  validation accuracy:		89.78 %
Epoch 1558 of 2000 took 0.106s
  training loss:		0.254623
  validation loss:		0.364232
  validation accuracy:		89.78 %
Epoch 1559 of 2000 took 0.105s
  training loss:		0.249108
  validation loss:		0.373182
  validation accuracy:		90.43 %
Epoch 1560 of 2000 took 0.105s
  training loss:		0.249635
  validation loss:		0.373196
  validation accuracy:		90.00 %
Epoch 1561 of 2000 took 0.106s
  training loss:		0.255556
  validation loss:		0.375655
  validation accuracy:		89.57 %
Epoch 1562 of 2000 took 0.105s
  training loss:		0.245243
  validation loss:		0.385754
  validation accuracy:		89.78 %
Epoch 1563 of 2000 took 0.105s
  training loss:		0.243329
  validation loss:		0.379649
  validation accuracy:		90.11 %
Epoch 1564 of 2000 took 0.105s
  training loss:		0.251433
  validation loss:		0.359029
  validation accuracy:		90.11 %
Epoch 1565 of 2000 took 0.105s
  training loss:		0.242130
  validation loss:		0.397217
  validation accuracy:		89.24 %
Epoch 1566 of 2000 took 0.105s
  training loss:		0.249821
  validation loss:		0.369570
  validation accuracy:		90.11 %
Epoch 1567 of 2000 took 0.105s
  training loss:		0.243613
  validation loss:		0.377664
  validation accuracy:		90.22 %
Epoch 1568 of 2000 took 0.106s
  training loss:		0.242054
  validation loss:		0.364166
  validation accuracy:		90.22 %
Epoch 1569 of 2000 took 0.105s
  training loss:		0.243361
  validation loss:		0.366986
  validation accuracy:		90.43 %
Epoch 1570 of 2000 took 0.105s
  training loss:		0.243449
  validation loss:		0.377223
  validation accuracy:		89.78 %
Epoch 1571 of 2000 took 0.105s
  training loss:		0.245610
  validation loss:		0.383354
  validation accuracy:		89.57 %
Epoch 1572 of 2000 took 0.105s
  training loss:		0.241882
  validation loss:		0.379967
  validation accuracy:		89.35 %
Epoch 1573 of 2000 took 0.105s
  training loss:		0.246127
  validation loss:		0.371062
  validation accuracy:		90.11 %
Epoch 1574 of 2000 took 0.105s
  training loss:		0.245087
  validation loss:		0.381629
  validation accuracy:		89.89 %
Epoch 1575 of 2000 took 0.105s
  training loss:		0.247573
  validation loss:		0.376300
  validation accuracy:		89.35 %
Epoch 1576 of 2000 took 0.107s
  training loss:		0.241514
  validation loss:		0.362639
  validation accuracy:		90.11 %
Epoch 1577 of 2000 took 0.100s
  training loss:		0.248234
  validation loss:		0.370657
  validation accuracy:		89.57 %
Epoch 1578 of 2000 took 0.105s
  training loss:		0.247547
  validation loss:		0.385427
  validation accuracy:		89.24 %
Epoch 1579 of 2000 took 0.105s
  training loss:		0.244767
  validation loss:		0.365369
  validation accuracy:		89.67 %
Epoch 1580 of 2000 took 0.105s
  training loss:		0.248441
  validation loss:		0.383008
  validation accuracy:		89.78 %
Epoch 1581 of 2000 took 0.105s
  training loss:		0.242242
  validation loss:		0.371721
  validation accuracy:		90.33 %
Epoch 1582 of 2000 took 0.105s
  training loss:		0.245285
  validation loss:		0.371048
  validation accuracy:		90.54 %
Epoch 1583 of 2000 took 0.105s
  training loss:		0.240697
  validation loss:		0.371906
  validation accuracy:		89.46 %
Epoch 1584 of 2000 took 0.106s
  training loss:		0.238170
  validation loss:		0.370808
  validation accuracy:		90.65 %
Epoch 1585 of 2000 took 0.105s
  training loss:		0.245737
  validation loss:		0.392158
  validation accuracy:		89.67 %
Epoch 1586 of 2000 took 0.104s
  training loss:		0.248475
  validation loss:		0.372006
  validation accuracy:		90.65 %
Epoch 1587 of 2000 took 0.103s
  training loss:		0.248530
  validation loss:		0.370011
  validation accuracy:		90.54 %
Epoch 1588 of 2000 took 0.105s
  training loss:		0.243871
  validation loss:		0.376852
  validation accuracy:		90.11 %
Epoch 1589 of 2000 took 0.105s
  training loss:		0.244945
  validation loss:		0.371792
  validation accuracy:		89.46 %
Epoch 1590 of 2000 took 0.105s
  training loss:		0.247040
  validation loss:		0.381973
  validation accuracy:		90.11 %
Epoch 1591 of 2000 took 0.105s
  training loss:		0.245471
  validation loss:		0.380193
  validation accuracy:		89.89 %
Epoch 1592 of 2000 took 0.105s
  training loss:		0.242239
  validation loss:		0.371351
  validation accuracy:		90.22 %
Epoch 1593 of 2000 took 0.105s
  training loss:		0.245134
  validation loss:		0.376339
  validation accuracy:		89.24 %
Epoch 1594 of 2000 took 0.105s
  training loss:		0.246071
  validation loss:		0.385908
  validation accuracy:		89.46 %
Epoch 1595 of 2000 took 0.108s
  training loss:		0.249199
  validation loss:		0.367226
  validation accuracy:		89.67 %
Epoch 1596 of 2000 took 0.099s
  training loss:		0.250423
  validation loss:		0.386953
  validation accuracy:		89.78 %
Epoch 1597 of 2000 took 0.101s
  training loss:		0.246178
  validation loss:		0.392816
  validation accuracy:		89.46 %
Epoch 1598 of 2000 took 0.106s
  training loss:		0.238623
  validation loss:		0.376156
  validation accuracy:		89.24 %
Epoch 1599 of 2000 took 0.105s
  training loss:		0.247877
  validation loss:		0.373944
  validation accuracy:		89.78 %
Epoch 1600 of 2000 took 0.105s
  training loss:		0.254179
  validation loss:		0.383405
  validation accuracy:		89.35 %
Epoch 1601 of 2000 took 0.105s
  training loss:		0.249906
  validation loss:		0.389669
  validation accuracy:		88.91 %
Epoch 1602 of 2000 took 0.105s
  training loss:		0.241388
  validation loss:		0.379594
  validation accuracy:		90.00 %
Epoch 1603 of 2000 took 0.105s
  training loss:		0.249604
  validation loss:		0.372006
  validation accuracy:		90.00 %
Epoch 1604 of 2000 took 0.105s
  training loss:		0.248335
  validation loss:		0.373259
  validation accuracy:		89.67 %
Epoch 1605 of 2000 took 0.105s
  training loss:		0.242541
  validation loss:		0.369846
  validation accuracy:		89.89 %
Epoch 1606 of 2000 took 0.099s
  training loss:		0.248431
  validation loss:		0.390263
  validation accuracy:		89.35 %
Epoch 1607 of 2000 took 0.100s
  training loss:		0.241230
  validation loss:		0.375202
  validation accuracy:		89.67 %
Epoch 1608 of 2000 took 0.108s
  training loss:		0.241641
  validation loss:		0.370101
  validation accuracy:		89.89 %
Epoch 1609 of 2000 took 0.108s
  training loss:		0.244730
  validation loss:		0.409923
  validation accuracy:		88.80 %
Epoch 1610 of 2000 took 0.106s
  training loss:		0.238009
  validation loss:		0.378653
  validation accuracy:		89.24 %
Epoch 1611 of 2000 took 0.106s
  training loss:		0.240088
  validation loss:		0.392050
  validation accuracy:		88.91 %
Epoch 1612 of 2000 took 0.106s
  training loss:		0.245344
  validation loss:		0.393468
  validation accuracy:		89.89 %
Epoch 1613 of 2000 took 0.107s
  training loss:		0.245793
  validation loss:		0.385924
  validation accuracy:		89.89 %
Epoch 1614 of 2000 took 0.114s
  training loss:		0.248307
  validation loss:		0.394782
  validation accuracy:		89.46 %
Epoch 1615 of 2000 took 0.123s
  training loss:		0.249701
  validation loss:		0.383675
  validation accuracy:		89.89 %
Epoch 1616 of 2000 took 0.122s
  training loss:		0.245123
  validation loss:		0.359397
  validation accuracy:		90.43 %
Epoch 1617 of 2000 took 0.114s
  training loss:		0.245036
  validation loss:		0.383531
  validation accuracy:		89.46 %
Epoch 1618 of 2000 took 0.106s
  training loss:		0.246178
  validation loss:		0.375561
  validation accuracy:		89.89 %
Epoch 1619 of 2000 took 0.106s
  training loss:		0.242872
  validation loss:		0.372144
  validation accuracy:		89.89 %
Epoch 1620 of 2000 took 0.106s
  training loss:		0.245372
  validation loss:		0.395178
  validation accuracy:		89.13 %
Epoch 1621 of 2000 took 0.106s
  training loss:		0.237583
  validation loss:		0.377700
  validation accuracy:		89.67 %
Epoch 1622 of 2000 took 0.106s
  training loss:		0.248050
  validation loss:		0.372346
  validation accuracy:		90.11 %
Epoch 1623 of 2000 took 0.114s
  training loss:		0.245552
  validation loss:		0.386218
  validation accuracy:		89.78 %
Epoch 1624 of 2000 took 0.123s
  training loss:		0.245081
  validation loss:		0.392533
  validation accuracy:		89.02 %
Epoch 1625 of 2000 took 0.122s
  training loss:		0.248184
  validation loss:		0.384550
  validation accuracy:		89.78 %
Epoch 1626 of 2000 took 0.114s
  training loss:		0.249145
  validation loss:		0.381937
  validation accuracy:		89.24 %
Epoch 1627 of 2000 took 0.106s
  training loss:		0.249080
  validation loss:		0.364840
  validation accuracy:		89.57 %
Epoch 1628 of 2000 took 0.106s
  training loss:		0.245779
  validation loss:		0.396542
  validation accuracy:		89.35 %
Epoch 1629 of 2000 took 0.106s
  training loss:		0.240454
  validation loss:		0.388249
  validation accuracy:		89.89 %
Epoch 1630 of 2000 took 0.106s
  training loss:		0.249142
  validation loss:		0.401378
  validation accuracy:		88.48 %
Epoch 1631 of 2000 took 0.106s
  training loss:		0.245029
  validation loss:		0.364800
  validation accuracy:		90.43 %
Epoch 1632 of 2000 took 0.114s
  training loss:		0.251918
  validation loss:		0.365616
  validation accuracy:		90.00 %
Epoch 1633 of 2000 took 0.123s
  training loss:		0.251352
  validation loss:		0.391334
  validation accuracy:		89.57 %
Epoch 1634 of 2000 took 0.122s
  training loss:		0.246678
  validation loss:		0.365535
  validation accuracy:		89.67 %
Epoch 1635 of 2000 took 0.114s
  training loss:		0.248162
  validation loss:		0.372683
  validation accuracy:		89.78 %
Epoch 1636 of 2000 took 0.105s
  training loss:		0.244612
  validation loss:		0.382456
  validation accuracy:		88.91 %
Epoch 1637 of 2000 took 0.106s
  training loss:		0.242699
  validation loss:		0.393746
  validation accuracy:		88.91 %
Epoch 1638 of 2000 took 0.106s
  training loss:		0.247112
  validation loss:		0.366804
  validation accuracy:		90.22 %
Epoch 1639 of 2000 took 0.106s
  training loss:		0.241184
  validation loss:		0.387131
  validation accuracy:		89.67 %
Epoch 1640 of 2000 took 0.107s
  training loss:		0.245459
  validation loss:		0.379627
  validation accuracy:		90.00 %
Epoch 1641 of 2000 took 0.115s
  training loss:		0.241653
  validation loss:		0.363114
  validation accuracy:		90.00 %
Epoch 1642 of 2000 took 0.123s
  training loss:		0.247207
  validation loss:		0.371051
  validation accuracy:		90.00 %
Epoch 1643 of 2000 took 0.122s
  training loss:		0.244929
  validation loss:		0.383128
  validation accuracy:		89.57 %
Epoch 1644 of 2000 took 0.122s
  training loss:		0.237568
  validation loss:		0.378594
  validation accuracy:		90.22 %
Epoch 1645 of 2000 took 0.108s
  training loss:		0.247038
  validation loss:		0.380357
  validation accuracy:		89.35 %
Epoch 1646 of 2000 took 0.106s
  training loss:		0.242384
  validation loss:		0.378201
  validation accuracy:		90.00 %
Epoch 1647 of 2000 took 0.106s
  training loss:		0.244220
  validation loss:		0.382483
  validation accuracy:		89.67 %
Epoch 1648 of 2000 took 0.106s
  training loss:		0.242657
  validation loss:		0.372436
  validation accuracy:		89.46 %
Epoch 1649 of 2000 took 0.106s
  training loss:		0.243308
  validation loss:		0.381991
  validation accuracy:		89.89 %
Epoch 1650 of 2000 took 0.116s
  training loss:		0.246723
  validation loss:		0.377907
  validation accuracy:		90.00 %
Epoch 1651 of 2000 took 0.123s
  training loss:		0.244072
  validation loss:		0.387576
  validation accuracy:		89.67 %
Epoch 1652 of 2000 took 0.122s
  training loss:		0.248644
  validation loss:		0.373310
  validation accuracy:		89.57 %
Epoch 1653 of 2000 took 0.122s
  training loss:		0.247230
  validation loss:		0.370007
  validation accuracy:		89.89 %
Epoch 1654 of 2000 took 0.110s
  training loss:		0.244741
  validation loss:		0.390263
  validation accuracy:		89.02 %
Epoch 1655 of 2000 took 0.106s
  training loss:		0.245028
  validation loss:		0.371581
  validation accuracy:		90.22 %
Epoch 1656 of 2000 took 0.106s
  training loss:		0.248415
  validation loss:		0.386860
  validation accuracy:		89.57 %
Epoch 1657 of 2000 took 0.105s
  training loss:		0.247088
  validation loss:		0.370586
  validation accuracy:		90.00 %
Epoch 1658 of 2000 took 0.105s
  training loss:		0.246715
  validation loss:		0.394016
  validation accuracy:		88.48 %
Epoch 1659 of 2000 took 0.105s
  training loss:		0.244283
  validation loss:		0.374781
  validation accuracy:		89.46 %
Epoch 1660 of 2000 took 0.093s
  training loss:		0.244304
  validation loss:		0.372265
  validation accuracy:		89.78 %
Epoch 1661 of 2000 took 0.093s
  training loss:		0.245308
  validation loss:		0.367251
  validation accuracy:		89.57 %
Epoch 1662 of 2000 took 0.092s
  training loss:		0.243792
  validation loss:		0.369183
  validation accuracy:		90.11 %
Epoch 1663 of 2000 took 0.092s
  training loss:		0.241999
  validation loss:		0.382257
  validation accuracy:		88.80 %
Epoch 1664 of 2000 took 0.099s
  training loss:		0.242278
  validation loss:		0.378640
  validation accuracy:		89.78 %
Epoch 1665 of 2000 took 0.105s
  training loss:		0.248651
  validation loss:		0.389134
  validation accuracy:		89.57 %
Epoch 1666 of 2000 took 0.105s
  training loss:		0.250158
  validation loss:		0.373228
  validation accuracy:		89.89 %
Epoch 1667 of 2000 took 0.105s
  training loss:		0.243400
  validation loss:		0.369046
  validation accuracy:		89.57 %
Epoch 1668 of 2000 took 0.106s
  training loss:		0.254334
  validation loss:		0.384782
  validation accuracy:		89.78 %
Epoch 1669 of 2000 took 0.106s
  training loss:		0.248774
  validation loss:		0.390806
  validation accuracy:		89.78 %
Epoch 1670 of 2000 took 0.093s
  training loss:		0.249523
  validation loss:		0.363239
  validation accuracy:		90.22 %
Epoch 1671 of 2000 took 0.092s
  training loss:		0.251341
  validation loss:		0.386661
  validation accuracy:		88.80 %
Epoch 1672 of 2000 took 0.092s
  training loss:		0.248544
  validation loss:		0.384845
  validation accuracy:		89.24 %
Epoch 1673 of 2000 took 0.092s
  training loss:		0.243906
  validation loss:		0.385191
  validation accuracy:		89.57 %
Epoch 1674 of 2000 took 0.099s
  training loss:		0.247569
  validation loss:		0.392907
  validation accuracy:		89.46 %
Epoch 1675 of 2000 took 0.105s
  training loss:		0.244863
  validation loss:		0.369238
  validation accuracy:		90.54 %
Epoch 1676 of 2000 took 0.105s
  training loss:		0.246405
  validation loss:		0.376393
  validation accuracy:		89.57 %
Epoch 1677 of 2000 took 0.105s
  training loss:		0.245691
  validation loss:		0.389805
  validation accuracy:		89.57 %
Epoch 1678 of 2000 took 0.105s
  training loss:		0.243403
  validation loss:		0.371683
  validation accuracy:		89.67 %
Epoch 1679 of 2000 took 0.106s
  training loss:		0.249437
  validation loss:		0.364467
  validation accuracy:		90.43 %
Epoch 1680 of 2000 took 0.093s
  training loss:		0.241341
  validation loss:		0.368993
  validation accuracy:		90.22 %
Epoch 1681 of 2000 took 0.093s
  training loss:		0.240083
  validation loss:		0.394423
  validation accuracy:		89.57 %
Epoch 1682 of 2000 took 0.092s
  training loss:		0.247612
  validation loss:		0.365264
  validation accuracy:		90.00 %
Epoch 1683 of 2000 took 0.093s
  training loss:		0.247355
  validation loss:		0.374459
  validation accuracy:		89.57 %
Epoch 1684 of 2000 took 0.093s
  training loss:		0.240818
  validation loss:		0.365432
  validation accuracy:		90.22 %
Epoch 1685 of 2000 took 0.104s
  training loss:		0.236681
  validation loss:		0.368604
  validation accuracy:		89.78 %
Epoch 1686 of 2000 took 0.105s
  training loss:		0.247535
  validation loss:		0.368707
  validation accuracy:		90.00 %
Epoch 1687 of 2000 took 0.105s
  training loss:		0.249253
  validation loss:		0.376723
  validation accuracy:		89.89 %
Epoch 1688 of 2000 took 0.105s
  training loss:		0.240979
  validation loss:		0.379138
  validation accuracy:		89.67 %
Epoch 1689 of 2000 took 0.108s
  training loss:		0.248808
  validation loss:		0.384846
  validation accuracy:		89.57 %
Epoch 1690 of 2000 took 0.094s
  training loss:		0.238542
  validation loss:		0.382675
  validation accuracy:		89.35 %
Epoch 1691 of 2000 took 0.092s
  training loss:		0.241412
  validation loss:		0.367921
  validation accuracy:		89.67 %
Epoch 1692 of 2000 took 0.092s
  training loss:		0.247297
  validation loss:		0.393733
  validation accuracy:		89.24 %
Epoch 1693 of 2000 took 0.092s
  training loss:		0.245676
  validation loss:		0.380441
  validation accuracy:		89.67 %
Epoch 1694 of 2000 took 0.092s
  training loss:		0.241760
  validation loss:		0.376698
  validation accuracy:		89.46 %
Epoch 1695 of 2000 took 0.094s
  training loss:		0.249700
  validation loss:		0.375838
  validation accuracy:		89.02 %
Epoch 1696 of 2000 took 0.105s
  training loss:		0.244893
  validation loss:		0.379781
  validation accuracy:		89.24 %
Epoch 1697 of 2000 took 0.105s
  training loss:		0.241646
  validation loss:		0.378176
  validation accuracy:		90.00 %
Epoch 1698 of 2000 took 0.106s
  training loss:		0.238273
  validation loss:		0.381542
  validation accuracy:		89.67 %
Epoch 1699 of 2000 took 0.108s
  training loss:		0.243102
  validation loss:		0.382791
  validation accuracy:		89.78 %
Epoch 1700 of 2000 took 0.095s
  training loss:		0.248950
  validation loss:		0.377530
  validation accuracy:		89.78 %
Epoch 1701 of 2000 took 0.093s
  training loss:		0.239037
  validation loss:		0.365745
  validation accuracy:		89.89 %
Epoch 1702 of 2000 took 0.092s
  training loss:		0.242535
  validation loss:		0.381948
  validation accuracy:		89.67 %
Epoch 1703 of 2000 took 0.092s
  training loss:		0.246028
  validation loss:		0.378219
  validation accuracy:		89.57 %
Epoch 1704 of 2000 took 0.092s
  training loss:		0.236408
  validation loss:		0.371832
  validation accuracy:		89.89 %
Epoch 1705 of 2000 took 0.092s
  training loss:		0.246260
  validation loss:		0.397636
  validation accuracy:		89.89 %
Epoch 1706 of 2000 took 0.105s
  training loss:		0.243490
  validation loss:		0.363307
  validation accuracy:		90.22 %
Epoch 1707 of 2000 took 0.105s
  training loss:		0.241782
  validation loss:		0.375212
  validation accuracy:		90.11 %
Epoch 1708 of 2000 took 0.105s
  training loss:		0.241735
  validation loss:		0.373411
  validation accuracy:		89.89 %
Epoch 1709 of 2000 took 0.109s
  training loss:		0.243468
  validation loss:		0.372684
  validation accuracy:		89.57 %
Epoch 1710 of 2000 took 0.098s
  training loss:		0.248491
  validation loss:		0.392079
  validation accuracy:		89.24 %
Epoch 1711 of 2000 took 0.093s
  training loss:		0.240518
  validation loss:		0.385270
  validation accuracy:		89.67 %
Epoch 1712 of 2000 took 0.093s
  training loss:		0.240296
  validation loss:		0.380819
  validation accuracy:		89.78 %
Epoch 1713 of 2000 took 0.092s
  training loss:		0.239151
  validation loss:		0.363926
  validation accuracy:		90.22 %
Epoch 1714 of 2000 took 0.092s
  training loss:		0.240345
  validation loss:		0.363072
  validation accuracy:		90.11 %
Epoch 1715 of 2000 took 0.092s
  training loss:		0.243085
  validation loss:		0.382308
  validation accuracy:		89.57 %
Epoch 1716 of 2000 took 0.102s
  training loss:		0.242826
  validation loss:		0.378363
  validation accuracy:		89.02 %
Epoch 1717 of 2000 took 0.105s
  training loss:		0.247084
  validation loss:		0.382955
  validation accuracy:		89.57 %
Epoch 1718 of 2000 took 0.105s
  training loss:		0.236579
  validation loss:		0.370859
  validation accuracy:		89.67 %
Epoch 1719 of 2000 took 0.107s
  training loss:		0.247128
  validation loss:		0.376870
  validation accuracy:		89.67 %
Epoch 1720 of 2000 took 0.100s
  training loss:		0.250713
  validation loss:		0.371495
  validation accuracy:		90.33 %
Epoch 1721 of 2000 took 0.098s
  training loss:		0.237493
  validation loss:		0.373450
  validation accuracy:		89.89 %
Epoch 1722 of 2000 took 0.099s
  training loss:		0.244738
  validation loss:		0.373048
  validation accuracy:		89.67 %
Epoch 1723 of 2000 took 0.096s
  training loss:		0.250319
  validation loss:		0.394531
  validation accuracy:		89.46 %
Epoch 1724 of 2000 took 0.098s
  training loss:		0.241508
  validation loss:		0.388108
  validation accuracy:		89.35 %
Epoch 1725 of 2000 took 0.101s
  training loss:		0.236891
  validation loss:		0.364848
  validation accuracy:		90.00 %
Epoch 1726 of 2000 took 0.096s
  training loss:		0.247165
  validation loss:		0.368044
  validation accuracy:		89.57 %
Epoch 1727 of 2000 took 0.097s
  training loss:		0.245972
  validation loss:		0.385825
  validation accuracy:		89.67 %
Epoch 1728 of 2000 took 0.096s
  training loss:		0.240274
  validation loss:		0.364498
  validation accuracy:		90.33 %
Epoch 1729 of 2000 took 0.100s
  training loss:		0.238274
  validation loss:		0.383687
  validation accuracy:		89.46 %
Epoch 1730 of 2000 took 0.100s
  training loss:		0.244045
  validation loss:		0.370964
  validation accuracy:		89.89 %
Epoch 1731 of 2000 took 0.096s
  training loss:		0.245882
  validation loss:		0.381377
  validation accuracy:		89.67 %
Epoch 1732 of 2000 took 0.101s
  training loss:		0.244216
  validation loss:		0.368186
  validation accuracy:		89.67 %
Epoch 1733 of 2000 took 0.098s
  training loss:		0.244802
  validation loss:		0.370206
  validation accuracy:		89.89 %
Epoch 1734 of 2000 took 0.096s
  training loss:		0.242010
  validation loss:		0.369377
  validation accuracy:		90.00 %
Epoch 1735 of 2000 took 0.096s
  training loss:		0.247894
  validation loss:		0.381488
  validation accuracy:		89.35 %
Epoch 1736 of 2000 took 0.096s
  training loss:		0.245159
  validation loss:		0.388289
  validation accuracy:		89.46 %
Epoch 1737 of 2000 took 0.100s
  training loss:		0.245788
  validation loss:		0.384838
  validation accuracy:		89.57 %
Epoch 1738 of 2000 took 0.097s
  training loss:		0.247491
  validation loss:		0.372659
  validation accuracy:		89.78 %
Epoch 1739 of 2000 took 0.096s
  training loss:		0.238809
  validation loss:		0.385069
  validation accuracy:		89.57 %
Epoch 1740 of 2000 took 0.105s
  training loss:		0.245543
  validation loss:		0.389184
  validation accuracy:		90.00 %
Epoch 1741 of 2000 took 0.096s
  training loss:		0.250803
  validation loss:		0.364197
  validation accuracy:		90.11 %
Epoch 1742 of 2000 took 0.097s
  training loss:		0.247842
  validation loss:		0.384171
  validation accuracy:		89.67 %
Epoch 1743 of 2000 took 0.096s
  training loss:		0.240430
  validation loss:		0.377120
  validation accuracy:		89.24 %
Epoch 1744 of 2000 took 0.097s
  training loss:		0.247719
  validation loss:		0.381692
  validation accuracy:		89.67 %
Epoch 1745 of 2000 took 0.100s
  training loss:		0.244094
  validation loss:		0.363163
  validation accuracy:		90.11 %
Epoch 1746 of 2000 took 0.096s
  training loss:		0.243224
  validation loss:		0.395805
  validation accuracy:		89.67 %
Epoch 1747 of 2000 took 0.097s
  training loss:		0.241678
  validation loss:		0.368186
  validation accuracy:		89.89 %
Epoch 1748 of 2000 took 0.102s
  training loss:		0.243256
  validation loss:		0.374551
  validation accuracy:		89.57 %
Epoch 1749 of 2000 took 0.096s
  training loss:		0.243099
  validation loss:		0.373155
  validation accuracy:		89.78 %
Epoch 1750 of 2000 took 0.099s
  training loss:		0.245267
  validation loss:		0.394576
  validation accuracy:		89.67 %
Epoch 1751 of 2000 took 0.096s
  training loss:		0.246069
  validation loss:		0.397835
  validation accuracy:		89.24 %
Epoch 1752 of 2000 took 0.099s
  training loss:		0.250784
  validation loss:		0.374495
  validation accuracy:		89.89 %
Epoch 1753 of 2000 took 0.099s
  training loss:		0.243884
  validation loss:		0.396693
  validation accuracy:		89.35 %
Epoch 1754 of 2000 took 0.096s
  training loss:		0.246004
  validation loss:		0.369840
  validation accuracy:		89.78 %
Epoch 1755 of 2000 took 0.098s
  training loss:		0.240314
  validation loss:		0.382998
  validation accuracy:		89.46 %
Epoch 1756 of 2000 took 0.097s
  training loss:		0.238829
  validation loss:		0.382628
  validation accuracy:		88.91 %
Epoch 1757 of 2000 took 0.096s
  training loss:		0.241278
  validation loss:		0.369381
  validation accuracy:		89.89 %
Epoch 1758 of 2000 took 0.099s
  training loss:		0.246890
  validation loss:		0.381009
  validation accuracy:		89.89 %
Epoch 1759 of 2000 took 0.096s
  training loss:		0.242888
  validation loss:		0.378028
  validation accuracy:		89.57 %
Epoch 1760 of 2000 took 0.099s
  training loss:		0.245425
  validation loss:		0.375933
  validation accuracy:		89.89 %
Epoch 1761 of 2000 took 0.096s
  training loss:		0.242526
  validation loss:		0.368413
  validation accuracy:		90.11 %
Epoch 1762 of 2000 took 0.097s
  training loss:		0.243590
  validation loss:		0.365688
  validation accuracy:		90.11 %
Epoch 1763 of 2000 took 0.099s
  training loss:		0.245243
  validation loss:		0.388336
  validation accuracy:		89.57 %
Epoch 1764 of 2000 took 0.096s
  training loss:		0.242855
  validation loss:		0.376835
  validation accuracy:		89.35 %
Epoch 1765 of 2000 took 0.100s
  training loss:		0.249199
  validation loss:		0.409473
  validation accuracy:		89.02 %
Epoch 1766 of 2000 took 0.097s
  training loss:		0.247240
  validation loss:		0.371621
  validation accuracy:		89.67 %
Epoch 1767 of 2000 took 0.099s
  training loss:		0.248169
  validation loss:		0.395058
  validation accuracy:		89.46 %
Epoch 1768 of 2000 took 0.097s
  training loss:		0.241587
  validation loss:		0.374387
  validation accuracy:		90.00 %
Epoch 1769 of 2000 took 0.096s
  training loss:		0.246032
  validation loss:		0.385692
  validation accuracy:		89.67 %
Epoch 1770 of 2000 took 0.099s
  training loss:		0.243437
  validation loss:		0.382755
  validation accuracy:		89.67 %
Epoch 1771 of 2000 took 0.096s
  training loss:		0.244579
  validation loss:		0.370986
  validation accuracy:		89.57 %
Epoch 1772 of 2000 took 0.100s
  training loss:		0.242945
  validation loss:		0.382586
  validation accuracy:		89.46 %
Epoch 1773 of 2000 took 0.096s
  training loss:		0.239550
  validation loss:		0.383010
  validation accuracy:		89.35 %
Epoch 1774 of 2000 took 0.097s
  training loss:		0.244914
  validation loss:		0.372794
  validation accuracy:		90.22 %
Epoch 1775 of 2000 took 0.099s
  training loss:		0.248114
  validation loss:		0.375204
  validation accuracy:		89.67 %
Epoch 1776 of 2000 took 0.096s
  training loss:		0.244157
  validation loss:		0.383860
  validation accuracy:		89.57 %
Epoch 1777 of 2000 took 0.099s
  training loss:		0.238403
  validation loss:		0.371431
  validation accuracy:		89.13 %
Epoch 1778 of 2000 took 0.096s
  training loss:		0.245436
  validation loss:		0.366744
  validation accuracy:		89.89 %
Epoch 1779 of 2000 took 0.098s
  training loss:		0.244408
  validation loss:		0.376982
  validation accuracy:		89.46 %
Epoch 1780 of 2000 took 0.097s
  training loss:		0.239028
  validation loss:		0.379582
  validation accuracy:		89.67 %
Epoch 1781 of 2000 took 0.096s
  training loss:		0.238190
  validation loss:		0.371056
  validation accuracy:		90.33 %
Epoch 1782 of 2000 took 0.102s
  training loss:		0.248568
  validation loss:		0.364693
  validation accuracy:		90.11 %
Epoch 1783 of 2000 took 0.096s
  training loss:		0.243066
  validation loss:		0.384001
  validation accuracy:		89.46 %
Epoch 1784 of 2000 took 0.097s
  training loss:		0.249420
  validation loss:		0.380979
  validation accuracy:		90.00 %
Epoch 1785 of 2000 took 0.096s
  training loss:		0.241344
  validation loss:		0.396416
  validation accuracy:		89.67 %
Epoch 1786 of 2000 took 0.098s
  training loss:		0.239117
  validation loss:		0.385830
  validation accuracy:		89.24 %
Epoch 1787 of 2000 took 0.099s
  training loss:		0.244591
  validation loss:		0.392610
  validation accuracy:		89.24 %
Epoch 1788 of 2000 took 0.096s
  training loss:		0.239491
  validation loss:		0.369455
  validation accuracy:		89.57 %
Epoch 1789 of 2000 took 0.097s
  training loss:		0.249901
  validation loss:		0.392292
  validation accuracy:		89.89 %
Epoch 1790 of 2000 took 0.101s
  training loss:		0.242083
  validation loss:		0.397343
  validation accuracy:		88.91 %
Epoch 1791 of 2000 took 0.096s
  training loss:		0.248538
  validation loss:		0.389512
  validation accuracy:		88.80 %
Epoch 1792 of 2000 took 0.097s
  training loss:		0.245457
  validation loss:		0.381393
  validation accuracy:		89.24 %
Epoch 1793 of 2000 took 0.098s
  training loss:		0.235910
  validation loss:		0.413900
  validation accuracy:		88.91 %
Epoch 1794 of 2000 took 0.100s
  training loss:		0.248923
  validation loss:		0.388424
  validation accuracy:		89.67 %
Epoch 1795 of 2000 took 0.098s
  training loss:		0.237944
  validation loss:		0.385312
  validation accuracy:		89.78 %
Epoch 1796 of 2000 took 0.096s
  training loss:		0.239089
  validation loss:		0.369491
  validation accuracy:		89.46 %
Epoch 1797 of 2000 took 0.100s
  training loss:		0.241740
  validation loss:		0.414802
  validation accuracy:		89.35 %
Epoch 1798 of 2000 took 0.098s
  training loss:		0.244477
  validation loss:		0.382808
  validation accuracy:		89.67 %
Epoch 1799 of 2000 took 0.096s
  training loss:		0.249077
  validation loss:		0.375017
  validation accuracy:		90.00 %
Epoch 1800 of 2000 took 0.096s
  training loss:		0.237791
  validation loss:		0.385456
  validation accuracy:		89.13 %
Epoch 1801 of 2000 took 0.096s
  training loss:		0.244336
  validation loss:		0.380788
  validation accuracy:		89.78 %
Epoch 1802 of 2000 took 0.100s
  training loss:		0.246698
  validation loss:		0.376102
  validation accuracy:		90.00 %
Epoch 1803 of 2000 took 0.097s
  training loss:		0.242316
  validation loss:		0.368673
  validation accuracy:		89.57 %
Epoch 1804 of 2000 took 0.096s
  training loss:		0.244745
  validation loss:		0.390213
  validation accuracy:		89.46 %
Epoch 1805 of 2000 took 0.102s
  training loss:		0.245047
  validation loss:		0.365148
  validation accuracy:		90.22 %
Epoch 1806 of 2000 took 0.096s
  training loss:		0.246057
  validation loss:		0.381513
  validation accuracy:		89.78 %
Epoch 1807 of 2000 took 0.097s
  training loss:		0.251617
  validation loss:		0.382380
  validation accuracy:		89.35 %
Epoch 1808 of 2000 took 0.096s
  training loss:		0.240050
  validation loss:		0.397997
  validation accuracy:		89.35 %
Epoch 1809 of 2000 took 0.097s
  training loss:		0.238919
  validation loss:		0.401875
  validation accuracy:		89.78 %
Epoch 1810 of 2000 took 0.100s
  training loss:		0.244104
  validation loss:		0.388014
  validation accuracy:		89.57 %
Epoch 1811 of 2000 took 0.096s
  training loss:		0.247938
  validation loss:		0.382455
  validation accuracy:		89.13 %
Epoch 1812 of 2000 took 0.097s
  training loss:		0.246263
  validation loss:		0.380624
  validation accuracy:		90.00 %
Epoch 1813 of 2000 took 0.102s
  training loss:		0.246203
  validation loss:		0.383241
  validation accuracy:		89.46 %
Epoch 1814 of 2000 took 0.096s
  training loss:		0.244082
  validation loss:		0.364957
  validation accuracy:		90.11 %
Epoch 1815 of 2000 took 0.097s
  training loss:		0.244150
  validation loss:		0.376720
  validation accuracy:		89.02 %
Epoch 1816 of 2000 took 0.096s
  training loss:		0.243492
  validation loss:		0.390677
  validation accuracy:		89.46 %
Epoch 1817 of 2000 took 0.098s
  training loss:		0.246436
  validation loss:		0.370628
  validation accuracy:		89.46 %
Epoch 1818 of 2000 took 0.099s
  training loss:		0.248535
  validation loss:		0.387648
  validation accuracy:		89.67 %
Epoch 1819 of 2000 took 0.096s
  training loss:		0.240495
  validation loss:		0.382467
  validation accuracy:		89.67 %
Epoch 1820 of 2000 took 0.098s
  training loss:		0.244159
  validation loss:		0.379162
  validation accuracy:		89.78 %
Epoch 1821 of 2000 took 0.100s
  training loss:		0.243183
  validation loss:		0.371122
  validation accuracy:		89.35 %
Epoch 1822 of 2000 took 0.096s
  training loss:		0.238341
  validation loss:		0.375540
  validation accuracy:		89.35 %
Epoch 1823 of 2000 took 0.097s
  training loss:		0.239349
  validation loss:		0.373338
  validation accuracy:		89.89 %
Epoch 1824 of 2000 took 0.096s
  training loss:		0.250175
  validation loss:		0.384086
  validation accuracy:		89.89 %
Epoch 1825 of 2000 took 0.100s
  training loss:		0.242651
  validation loss:		0.371372
  validation accuracy:		89.78 %
Epoch 1826 of 2000 took 0.097s
  training loss:		0.244501
  validation loss:		0.373036
  validation accuracy:		90.00 %
Epoch 1827 of 2000 took 0.096s
  training loss:		0.243557
  validation loss:		0.372326
  validation accuracy:		90.00 %
Epoch 1828 of 2000 took 0.101s
  training loss:		0.243046
  validation loss:		0.368214
  validation accuracy:		89.67 %
Epoch 1829 of 2000 took 0.097s
  training loss:		0.242656
  validation loss:		0.384695
  validation accuracy:		89.35 %
Epoch 1830 of 2000 took 0.096s
  training loss:		0.243630
  validation loss:		0.382876
  validation accuracy:		89.13 %
Epoch 1831 of 2000 took 0.096s
  training loss:		0.246261
  validation loss:		0.390481
  validation accuracy:		88.70 %
Epoch 1832 of 2000 took 0.097s
  training loss:		0.236375
  validation loss:		0.376880
  validation accuracy:		89.35 %
Epoch 1833 of 2000 took 0.100s
  training loss:		0.244463
  validation loss:		0.379128
  validation accuracy:		89.24 %
Epoch 1834 of 2000 took 0.097s
  training loss:		0.247045
  validation loss:		0.382346
  validation accuracy:		89.46 %
Epoch 1835 of 2000 took 0.098s
  training loss:		0.242807
  validation loss:		0.380980
  validation accuracy:		89.78 %
Epoch 1836 of 2000 took 0.102s
  training loss:		0.250422
  validation loss:		0.366891
  validation accuracy:		90.43 %
Epoch 1837 of 2000 took 0.096s
  training loss:		0.247390
  validation loss:		0.370949
  validation accuracy:		89.35 %
Epoch 1838 of 2000 took 0.097s
  training loss:		0.243911
  validation loss:		0.396500
  validation accuracy:		89.24 %
Epoch 1839 of 2000 took 0.096s
  training loss:		0.241074
  validation loss:		0.378347
  validation accuracy:		89.46 %
Epoch 1840 of 2000 took 0.098s
  training loss:		0.243969
  validation loss:		0.381469
  validation accuracy:		89.02 %
Epoch 1841 of 2000 took 0.100s
  training loss:		0.245225
  validation loss:		0.382441
  validation accuracy:		89.35 %
Epoch 1842 of 2000 took 0.096s
  training loss:		0.249517
  validation loss:		0.368776
  validation accuracy:		89.57 %
Epoch 1843 of 2000 took 0.097s
  training loss:		0.243059
  validation loss:		0.368533
  validation accuracy:		89.89 %
Epoch 1844 of 2000 took 0.102s
  training loss:		0.243404
  validation loss:		0.369978
  validation accuracy:		90.11 %
Epoch 1845 of 2000 took 0.096s
  training loss:		0.241627
  validation loss:		0.398332
  validation accuracy:		88.70 %
Epoch 1846 of 2000 took 0.097s
  training loss:		0.246249
  validation loss:		0.380828
  validation accuracy:		89.35 %
Epoch 1847 of 2000 took 0.096s
  training loss:		0.251706
  validation loss:		0.369143
  validation accuracy:		89.35 %
Epoch 1848 of 2000 took 0.099s
  training loss:		0.244476
  validation loss:		0.371632
  validation accuracy:		90.33 %
Epoch 1849 of 2000 took 0.098s
  training loss:		0.243071
  validation loss:		0.368678
  validation accuracy:		89.46 %
Epoch 1850 of 2000 took 0.096s
  training loss:		0.245918
  validation loss:		0.378654
  validation accuracy:		89.57 %
Epoch 1851 of 2000 took 0.099s
  training loss:		0.243034
  validation loss:		0.378615
  validation accuracy:		89.24 %
Epoch 1852 of 2000 took 0.099s
  training loss:		0.240174
  validation loss:		0.382005
  validation accuracy:		89.78 %
Epoch 1853 of 2000 took 0.096s
  training loss:		0.243881
  validation loss:		0.382282
  validation accuracy:		89.02 %
Epoch 1854 of 2000 took 0.097s
  training loss:		0.244959
  validation loss:		0.384529
  validation accuracy:		88.80 %
Epoch 1855 of 2000 took 0.096s
  training loss:		0.242309
  validation loss:		0.392116
  validation accuracy:		89.46 %
Epoch 1856 of 2000 took 0.100s
  training loss:		0.247363
  validation loss:		0.375775
  validation accuracy:		90.22 %
Epoch 1857 of 2000 took 0.097s
  training loss:		0.247948
  validation loss:		0.371470
  validation accuracy:		89.57 %
Epoch 1858 of 2000 took 0.096s
  training loss:		0.236507
  validation loss:		0.388768
  validation accuracy:		89.57 %
Epoch 1859 of 2000 took 0.102s
  training loss:		0.239587
  validation loss:		0.395713
  validation accuracy:		89.24 %
Epoch 1860 of 2000 took 0.096s
  training loss:		0.243236
  validation loss:		0.384661
  validation accuracy:		89.35 %
Epoch 1861 of 2000 took 0.096s
  training loss:		0.243575
  validation loss:		0.372871
  validation accuracy:		89.89 %
Epoch 1862 of 2000 took 0.096s
  training loss:		0.241719
  validation loss:		0.366315
  validation accuracy:		90.11 %
Epoch 1863 of 2000 took 0.097s
  training loss:		0.240450
  validation loss:		0.375227
  validation accuracy:		88.80 %
Epoch 1864 of 2000 took 0.100s
  training loss:		0.246082
  validation loss:		0.377478
  validation accuracy:		90.00 %
Epoch 1865 of 2000 took 0.097s
  training loss:		0.246431
  validation loss:		0.367459
  validation accuracy:		89.89 %
Epoch 1866 of 2000 took 0.097s
  training loss:		0.243898
  validation loss:		0.393904
  validation accuracy:		89.78 %
Epoch 1867 of 2000 took 0.102s
  training loss:		0.245463
  validation loss:		0.393346
  validation accuracy:		89.67 %
Epoch 1868 of 2000 took 0.096s
  training loss:		0.238094
  validation loss:		0.372117
  validation accuracy:		89.67 %
Epoch 1869 of 2000 took 0.097s
  training loss:		0.244275
  validation loss:		0.380248
  validation accuracy:		89.13 %
Epoch 1870 of 2000 took 0.096s
  training loss:		0.238806
  validation loss:		0.383945
  validation accuracy:		89.24 %
Epoch 1871 of 2000 took 0.098s
  training loss:		0.244398
  validation loss:		0.372402
  validation accuracy:		90.65 %
Epoch 1872 of 2000 took 0.100s
  training loss:		0.247949
  validation loss:		0.387210
  validation accuracy:		89.78 %
Epoch 1873 of 2000 took 0.096s
  training loss:		0.242824
  validation loss:		0.442685
  validation accuracy:		88.70 %
Epoch 1874 of 2000 took 0.097s
  training loss:		0.250086
  validation loss:		0.375408
  validation accuracy:		89.35 %
Epoch 1875 of 2000 took 0.101s
  training loss:		0.243160
  validation loss:		0.368466
  validation accuracy:		89.57 %
Epoch 1876 of 2000 took 0.096s
  training loss:		0.246810
  validation loss:		0.383183
  validation accuracy:		89.46 %
Epoch 1877 of 2000 took 0.097s
  training loss:		0.250076
  validation loss:		0.380344
  validation accuracy:		89.46 %
Epoch 1878 of 2000 took 0.096s
  training loss:		0.244361
  validation loss:		0.393271
  validation accuracy:		89.24 %
Epoch 1879 of 2000 took 0.100s
  training loss:		0.251726
  validation loss:		0.371577
  validation accuracy:		89.57 %
Epoch 1880 of 2000 took 0.098s
  training loss:		0.245934
  validation loss:		0.372144
  validation accuracy:		89.89 %
Epoch 1881 of 2000 took 0.096s
  training loss:		0.245262
  validation loss:		0.372985
  validation accuracy:		89.46 %
Epoch 1882 of 2000 took 0.100s
  training loss:		0.242089
  validation loss:		0.377498
  validation accuracy:		89.89 %
Epoch 1883 of 2000 took 0.098s
  training loss:		0.247440
  validation loss:		0.367624
  validation accuracy:		89.67 %
Epoch 1884 of 2000 took 0.096s
  training loss:		0.239380
  validation loss:		0.391113
  validation accuracy:		89.78 %
Epoch 1885 of 2000 took 0.097s
  training loss:		0.248619
  validation loss:		0.377317
  validation accuracy:		88.91 %
Epoch 1886 of 2000 took 0.096s
  training loss:		0.247269
  validation loss:		0.377155
  validation accuracy:		89.35 %
Epoch 1887 of 2000 took 0.100s
  training loss:		0.241205
  validation loss:		0.365117
  validation accuracy:		90.11 %
Epoch 1888 of 2000 took 0.097s
  training loss:		0.233441
  validation loss:		0.381179
  validation accuracy:		89.78 %
Epoch 1889 of 2000 took 0.096s
  training loss:		0.243440
  validation loss:		0.373961
  validation accuracy:		89.78 %
Epoch 1890 of 2000 took 0.102s
  training loss:		0.242372
  validation loss:		0.386395
  validation accuracy:		89.89 %
Epoch 1891 of 2000 took 0.096s
  training loss:		0.243620
  validation loss:		0.376345
  validation accuracy:		89.24 %
Epoch 1892 of 2000 took 0.097s
  training loss:		0.240107
  validation loss:		0.369256
  validation accuracy:		89.67 %
Epoch 1893 of 2000 took 0.096s
  training loss:		0.248740
  validation loss:		0.384881
  validation accuracy:		89.46 %
Epoch 1894 of 2000 took 0.097s
  training loss:		0.244851
  validation loss:		0.368049
  validation accuracy:		90.11 %
Epoch 1895 of 2000 took 0.100s
  training loss:		0.240839
  validation loss:		0.376128
  validation accuracy:		90.33 %
Epoch 1896 of 2000 took 0.096s
  training loss:		0.254423
  validation loss:		0.369443
  validation accuracy:		89.89 %
Epoch 1897 of 2000 took 0.097s
  training loss:		0.241878
  validation loss:		0.369027
  validation accuracy:		90.54 %
Epoch 1898 of 2000 took 0.102s
  training loss:		0.247756
  validation loss:		0.411699
  validation accuracy:		89.02 %
Epoch 1899 of 2000 took 0.096s
  training loss:		0.239306
  validation loss:		0.385031
  validation accuracy:		89.46 %
Epoch 1900 of 2000 took 0.097s
  training loss:		0.241568
  validation loss:		0.373607
  validation accuracy:		89.46 %
Epoch 1901 of 2000 took 0.096s
  training loss:		0.242861
  validation loss:		0.385705
  validation accuracy:		89.67 %
Epoch 1902 of 2000 took 0.100s
  training loss:		0.243955
  validation loss:		0.373985
  validation accuracy:		89.67 %
Epoch 1903 of 2000 took 0.099s
  training loss:		0.245661
  validation loss:		0.371636
  validation accuracy:		90.11 %
Epoch 1904 of 2000 took 0.096s
  training loss:		0.240922
  validation loss:		0.363932
  validation accuracy:		89.89 %
Epoch 1905 of 2000 took 0.098s
  training loss:		0.246505
  validation loss:		0.378161
  validation accuracy:		89.13 %
Epoch 1906 of 2000 took 0.100s
  training loss:		0.242348
  validation loss:		0.384459
  validation accuracy:		89.67 %
Epoch 1907 of 2000 took 0.096s
  training loss:		0.246590
  validation loss:		0.378036
  validation accuracy:		89.02 %
Epoch 1908 of 2000 took 0.097s
  training loss:		0.244813
  validation loss:		0.370140
  validation accuracy:		90.11 %
Epoch 1909 of 2000 took 0.096s
  training loss:		0.241684
  validation loss:		0.381635
  validation accuracy:		89.67 %
Epoch 1910 of 2000 took 0.101s
  training loss:		0.246255
  validation loss:		0.383338
  validation accuracy:		89.35 %
Epoch 1911 of 2000 took 0.097s
  training loss:		0.245001
  validation loss:		0.380520
  validation accuracy:		89.46 %
Epoch 1912 of 2000 took 0.096s
  training loss:		0.246559
  validation loss:		0.378548
  validation accuracy:		88.91 %
Epoch 1913 of 2000 took 0.101s
  training loss:		0.246631
  validation loss:		0.377941
  validation accuracy:		89.89 %
Epoch 1914 of 2000 took 0.097s
  training loss:		0.244420
  validation loss:		0.371777
  validation accuracy:		90.33 %
Epoch 1915 of 2000 took 0.096s
  training loss:		0.241366
  validation loss:		0.380013
  validation accuracy:		89.46 %
Epoch 1916 of 2000 took 0.096s
  training loss:		0.243126
  validation loss:		0.384134
  validation accuracy:		89.24 %
Epoch 1917 of 2000 took 0.097s
  training loss:		0.241110
  validation loss:		0.364649
  validation accuracy:		90.33 %
Epoch 1918 of 2000 took 0.100s
  training loss:		0.251046
  validation loss:		0.374423
  validation accuracy:		89.46 %
Epoch 1919 of 2000 took 0.097s
  training loss:		0.245557
  validation loss:		0.387382
  validation accuracy:		89.02 %
Epoch 1920 of 2000 took 0.096s
  training loss:		0.241604
  validation loss:		0.366580
  validation accuracy:		90.00 %
Epoch 1921 of 2000 took 0.102s
  training loss:		0.244835
  validation loss:		0.370024
  validation accuracy:		90.00 %
Epoch 1922 of 2000 took 0.096s
  training loss:		0.246129
  validation loss:		0.381870
  validation accuracy:		89.78 %
Epoch 1923 of 2000 took 0.097s
  training loss:		0.243769
  validation loss:		0.372425
  validation accuracy:		89.57 %
Epoch 1924 of 2000 took 0.096s
  training loss:		0.242713
  validation loss:		0.372431
  validation accuracy:		89.89 %
Epoch 1925 of 2000 took 0.098s
  training loss:		0.241970
  validation loss:		0.378543
  validation accuracy:		89.78 %
Epoch 1926 of 2000 took 0.100s
  training loss:		0.243007
  validation loss:		0.374833
  validation accuracy:		89.67 %
Epoch 1927 of 2000 took 0.096s
  training loss:		0.247253
  validation loss:		0.370768
  validation accuracy:		90.00 %
Epoch 1928 of 2000 took 0.097s
  training loss:		0.243630
  validation loss:		0.380781
  validation accuracy:		89.57 %
Epoch 1929 of 2000 took 0.102s
  training loss:		0.251783
  validation loss:		0.374225
  validation accuracy:		89.46 %
Epoch 1930 of 2000 took 0.096s
  training loss:		0.242184
  validation loss:		0.366021
  validation accuracy:		90.00 %
Epoch 1931 of 2000 took 0.097s
  training loss:		0.242764
  validation loss:		0.374734
  validation accuracy:		89.67 %
Epoch 1932 of 2000 took 0.096s
  training loss:		0.239037
  validation loss:		0.377635
  validation accuracy:		89.67 %
Epoch 1933 of 2000 took 0.100s
  training loss:		0.244732
  validation loss:		0.377512
  validation accuracy:		89.67 %
Epoch 1934 of 2000 took 0.098s
  training loss:		0.240838
  validation loss:		0.388782
  validation accuracy:		89.57 %
Epoch 1935 of 2000 took 0.096s
  training loss:		0.239060
  validation loss:		0.375480
  validation accuracy:		89.67 %
Epoch 1936 of 2000 took 0.100s
  training loss:		0.239942
  validation loss:		0.391921
  validation accuracy:		89.13 %
Epoch 1937 of 2000 took 0.099s
  training loss:		0.240411
  validation loss:		0.373746
  validation accuracy:		89.67 %
Epoch 1938 of 2000 took 0.096s
  training loss:		0.241753
  validation loss:		0.377782
  validation accuracy:		90.00 %
Epoch 1939 of 2000 took 0.096s
  training loss:		0.241116
  validation loss:		0.379067
  validation accuracy:		89.57 %
Epoch 1940 of 2000 took 0.096s
  training loss:		0.247629
  validation loss:		0.385923
  validation accuracy:		89.78 %
Epoch 1941 of 2000 took 0.100s
  training loss:		0.242188
  validation loss:		0.372831
  validation accuracy:		89.89 %
Epoch 1942 of 2000 took 0.097s
  training loss:		0.244132
  validation loss:		0.371978
  validation accuracy:		89.57 %
Epoch 1943 of 2000 took 0.096s
  training loss:		0.242931
  validation loss:		0.379007
  validation accuracy:		89.57 %
Epoch 1944 of 2000 took 0.102s
  training loss:		0.240297
  validation loss:		0.372380
  validation accuracy:		90.00 %
Epoch 1945 of 2000 took 0.096s
  training loss:		0.239424
  validation loss:		0.380951
  validation accuracy:		89.89 %
Epoch 1946 of 2000 took 0.097s
  training loss:		0.247064
  validation loss:		0.383982
  validation accuracy:		89.57 %
Epoch 1947 of 2000 took 0.096s
  training loss:		0.242102
  validation loss:		0.368439
  validation accuracy:		89.89 %
Epoch 1948 of 2000 took 0.097s
  training loss:		0.241720
  validation loss:		0.393671
  validation accuracy:		89.13 %
Epoch 1949 of 2000 took 0.100s
  training loss:		0.248507
  validation loss:		0.375947
  validation accuracy:		89.57 %
Epoch 1950 of 2000 took 0.096s
  training loss:		0.241604
  validation loss:		0.374268
  validation accuracy:		89.46 %
Epoch 1951 of 2000 took 0.097s
  training loss:		0.245153
  validation loss:		0.394127
  validation accuracy:		88.80 %
Epoch 1952 of 2000 took 0.102s
  training loss:		0.244435
  validation loss:		0.394962
  validation accuracy:		89.46 %
Epoch 1953 of 2000 took 0.096s
  training loss:		0.244843
  validation loss:		0.373229
  validation accuracy:		89.57 %
Epoch 1954 of 2000 took 0.097s
  training loss:		0.236456
  validation loss:		0.394598
  validation accuracy:		89.24 %
Epoch 1955 of 2000 took 0.096s
  training loss:		0.245212
  validation loss:		0.386187
  validation accuracy:		89.35 %
Epoch 1956 of 2000 took 0.098s
  training loss:		0.244664
  validation loss:		0.383645
  validation accuracy:		89.24 %
Epoch 1957 of 2000 took 0.099s
  training loss:		0.243974
  validation loss:		0.375094
  validation accuracy:		89.67 %
Epoch 1958 of 2000 took 0.096s
  training loss:		0.245857
  validation loss:		0.377921
  validation accuracy:		89.78 %
Epoch 1959 of 2000 took 0.098s
  training loss:		0.246786
  validation loss:		0.374445
  validation accuracy:		90.00 %
Epoch 1960 of 2000 took 0.100s
  training loss:		0.239009
  validation loss:		0.374197
  validation accuracy:		89.13 %
Epoch 1961 of 2000 took 0.096s
  training loss:		0.233987
  validation loss:		0.392921
  validation accuracy:		89.46 %
Epoch 1962 of 2000 took 0.097s
  training loss:		0.247460
  validation loss:		0.374455
  validation accuracy:		89.67 %
Epoch 1963 of 2000 took 0.096s
  training loss:		0.240586
  validation loss:		0.381788
  validation accuracy:		89.24 %
Epoch 1964 of 2000 took 0.100s
  training loss:		0.245818
  validation loss:		0.376595
  validation accuracy:		89.24 %
Epoch 1965 of 2000 took 0.097s
  training loss:		0.246415
  validation loss:		0.370837
  validation accuracy:		90.00 %
Epoch 1966 of 2000 took 0.096s
  training loss:		0.240550
  validation loss:		0.385296
  validation accuracy:		89.57 %
Epoch 1967 of 2000 took 0.101s
  training loss:		0.243092
  validation loss:		0.378646
  validation accuracy:		89.78 %
Epoch 1968 of 2000 took 0.098s
  training loss:		0.233179
  validation loss:		0.382502
  validation accuracy:		89.02 %
Epoch 1969 of 2000 took 0.096s
  training loss:		0.240099
  validation loss:		0.390031
  validation accuracy:		89.46 %
Epoch 1970 of 2000 took 0.097s
  training loss:		0.239449
  validation loss:		0.410812
  validation accuracy:		88.80 %
Epoch 1971 of 2000 took 0.097s
  training loss:		0.239837
  validation loss:		0.391585
  validation accuracy:		89.35 %
Epoch 1972 of 2000 took 0.100s
  training loss:		0.235924
  validation loss:		0.379864
  validation accuracy:		89.46 %
Epoch 1973 of 2000 took 0.097s
  training loss:		0.247268
  validation loss:		0.395762
  validation accuracy:		89.67 %
Epoch 1974 of 2000 took 0.096s
  training loss:		0.240547
  validation loss:		0.373850
  validation accuracy:		89.57 %
Epoch 1975 of 2000 took 0.102s
  training loss:		0.247116
  validation loss:		0.368132
  validation accuracy:		89.89 %
Epoch 1976 of 2000 took 0.096s
  training loss:		0.244518
  validation loss:		0.371555
  validation accuracy:		89.89 %
Epoch 1977 of 2000 took 0.097s
  training loss:		0.244321
  validation loss:		0.378956
  validation accuracy:		89.24 %
Epoch 1978 of 2000 took 0.096s
  training loss:		0.244831
  validation loss:		0.386619
  validation accuracy:		89.57 %
Epoch 1979 of 2000 took 0.097s
  training loss:		0.252302
  validation loss:		0.382105
  validation accuracy:		89.24 %
Epoch 1980 of 2000 took 0.100s
  training loss:		0.242924
  validation loss:		0.371289
  validation accuracy:		90.00 %
Epoch 1981 of 2000 took 0.096s
  training loss:		0.242024
  validation loss:		0.371254
  validation accuracy:		89.35 %
Epoch 1982 of 2000 took 0.097s
  training loss:		0.247411
  validation loss:		0.389267
  validation accuracy:		89.13 %
Epoch 1983 of 2000 took 0.102s
  training loss:		0.245188
  validation loss:		0.371480
  validation accuracy:		89.35 %
Epoch 1984 of 2000 took 0.096s
  training loss:		0.242462
  validation loss:		0.373232
  validation accuracy:		88.91 %
Epoch 1985 of 2000 took 0.097s
  training loss:		0.250565
  validation loss:		0.365743
  validation accuracy:		90.22 %
Epoch 1986 of 2000 took 0.096s
  training loss:		0.243670
  validation loss:		0.403449
  validation accuracy:		89.46 %
Epoch 1987 of 2000 took 0.099s
  training loss:		0.245201
  validation loss:		0.373840
  validation accuracy:		89.46 %
Epoch 1988 of 2000 took 0.098s
  training loss:		0.234916
  validation loss:		0.389281
  validation accuracy:		89.46 %
Epoch 1989 of 2000 took 0.096s
  training loss:		0.245480
  validation loss:		0.395421
  validation accuracy:		89.57 %
Epoch 1990 of 2000 took 0.099s
  training loss:		0.239057
  validation loss:		0.376782
  validation accuracy:		89.57 %
Epoch 1991 of 2000 took 0.099s
  training loss:		0.246898
  validation loss:		0.368278
  validation accuracy:		90.00 %
Epoch 1992 of 2000 took 0.096s
  training loss:		0.245100
  validation loss:		0.373467
  validation accuracy:		89.67 %
Epoch 1993 of 2000 took 0.097s
  training loss:		0.240251
  validation loss:		0.366537
  validation accuracy:		89.89 %
Epoch 1994 of 2000 took 0.096s
  training loss:		0.240409
  validation loss:		0.378411
  validation accuracy:		89.89 %
Epoch 1995 of 2000 took 0.100s
  training loss:		0.239519
  validation loss:		0.373755
  validation accuracy:		90.22 %
Epoch 1996 of 2000 took 0.097s
  training loss:		0.240348
  validation loss:		0.367727
  validation accuracy:		90.11 %
Epoch 1997 of 2000 took 0.096s
  training loss:		0.237794
  validation loss:		0.379385
  validation accuracy:		89.78 %
Epoch 1998 of 2000 took 0.102s
  training loss:		0.242692
  validation loss:		0.376955
  validation accuracy:		90.00 %
Epoch 1999 of 2000 took 0.096s
  training loss:		0.236516
  validation loss:		0.380375
  validation accuracy:		89.78 %
Epoch 2000 of 2000 took 0.097s
  training loss:		0.243862
  validation loss:		0.377334
  validation accuracy:		89.13 %
Final results:
  test loss:			0.733262
  test accuracy:		80.43 %
