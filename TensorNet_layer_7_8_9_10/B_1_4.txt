Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (4, 50, 50)...
decomposing tensor B of shape (4, 50)...
Starting training...
Epoch 1 of 2000 took 0.109s
  training loss:		2.998608
  validation loss:		2.999325
  validation accuracy:		12.83 %
Epoch 2 of 2000 took 0.104s
  training loss:		2.991778
  validation loss:		2.989446
  validation accuracy:		12.83 %
Epoch 3 of 2000 took 0.104s
  training loss:		2.981548
  validation loss:		2.978184
  validation accuracy:		12.83 %
Epoch 4 of 2000 took 0.104s
  training loss:		2.971039
  validation loss:		2.966459
  validation accuracy:		12.83 %
Epoch 5 of 2000 took 0.104s
  training loss:		2.960013
  validation loss:		2.954807
  validation accuracy:		12.83 %
Epoch 6 of 2000 took 0.104s
  training loss:		2.949375
  validation loss:		2.943341
  validation accuracy:		12.83 %
Epoch 7 of 2000 took 0.104s
  training loss:		2.939071
  validation loss:		2.931993
  validation accuracy:		12.83 %
Epoch 8 of 2000 took 0.104s
  training loss:		2.928564
  validation loss:		2.920915
  validation accuracy:		12.83 %
Epoch 9 of 2000 took 0.104s
  training loss:		2.918241
  validation loss:		2.910052
  validation accuracy:		12.83 %
Epoch 10 of 2000 took 0.104s
  training loss:		2.908359
  validation loss:		2.899433
  validation accuracy:		12.83 %
Epoch 11 of 2000 took 0.104s
  training loss:		2.898846
  validation loss:		2.888904
  validation accuracy:		12.83 %
Epoch 12 of 2000 took 0.104s
  training loss:		2.889138
  validation loss:		2.878623
  validation accuracy:		12.83 %
Epoch 13 of 2000 took 0.104s
  training loss:		2.879829
  validation loss:		2.868470
  validation accuracy:		12.83 %
Epoch 14 of 2000 took 0.104s
  training loss:		2.870523
  validation loss:		2.858435
  validation accuracy:		12.83 %
Epoch 15 of 2000 took 0.104s
  training loss:		2.861610
  validation loss:		2.848618
  validation accuracy:		12.83 %
Epoch 16 of 2000 took 0.104s
  training loss:		2.852388
  validation loss:		2.838934
  validation accuracy:		12.83 %
Epoch 17 of 2000 took 0.104s
  training loss:		2.843793
  validation loss:		2.829396
  validation accuracy:		12.83 %
Epoch 18 of 2000 took 0.104s
  training loss:		2.835237
  validation loss:		2.820001
  validation accuracy:		12.83 %
Epoch 19 of 2000 took 0.104s
  training loss:		2.826549
  validation loss:		2.810725
  validation accuracy:		12.83 %
Epoch 20 of 2000 took 0.108s
  training loss:		2.817872
  validation loss:		2.801505
  validation accuracy:		12.83 %
Epoch 21 of 2000 took 0.105s
  training loss:		2.809779
  validation loss:		2.792395
  validation accuracy:		12.83 %
Epoch 22 of 2000 took 0.104s
  training loss:		2.801687
  validation loss:		2.783446
  validation accuracy:		12.83 %
Epoch 23 of 2000 took 0.104s
  training loss:		2.793104
  validation loss:		2.774574
  validation accuracy:		12.83 %
Epoch 24 of 2000 took 0.104s
  training loss:		2.785328
  validation loss:		2.765893
  validation accuracy:		12.83 %
Epoch 25 of 2000 took 0.104s
  training loss:		2.777676
  validation loss:		2.757295
  validation accuracy:		12.83 %
Epoch 26 of 2000 took 0.104s
  training loss:		2.769545
  validation loss:		2.748763
  validation accuracy:		12.83 %
Epoch 27 of 2000 took 0.104s
  training loss:		2.761759
  validation loss:		2.740366
  validation accuracy:		12.83 %
Epoch 28 of 2000 took 0.104s
  training loss:		2.754403
  validation loss:		2.731979
  validation accuracy:		12.83 %
Epoch 29 of 2000 took 0.104s
  training loss:		2.747397
  validation loss:		2.723763
  validation accuracy:		12.83 %
Epoch 30 of 2000 took 0.105s
  training loss:		2.740067
  validation loss:		2.715657
  validation accuracy:		12.83 %
Epoch 31 of 2000 took 0.104s
  training loss:		2.732567
  validation loss:		2.707615
  validation accuracy:		12.83 %
Epoch 32 of 2000 took 0.104s
  training loss:		2.724611
  validation loss:		2.699700
  validation accuracy:		12.83 %
Epoch 33 of 2000 took 0.104s
  training loss:		2.717675
  validation loss:		2.691813
  validation accuracy:		12.83 %
Epoch 34 of 2000 took 0.104s
  training loss:		2.709986
  validation loss:		2.684026
  validation accuracy:		12.83 %
Epoch 35 of 2000 took 0.104s
  training loss:		2.704250
  validation loss:		2.676344
  validation accuracy:		12.83 %
Epoch 36 of 2000 took 0.104s
  training loss:		2.697201
  validation loss:		2.668791
  validation accuracy:		12.83 %
Epoch 37 of 2000 took 0.104s
  training loss:		2.689615
  validation loss:		2.661254
  validation accuracy:		12.83 %
Epoch 38 of 2000 took 0.104s
  training loss:		2.683604
  validation loss:		2.653838
  validation accuracy:		12.83 %
Epoch 39 of 2000 took 0.104s
  training loss:		2.676925
  validation loss:		2.646445
  validation accuracy:		12.83 %
Epoch 40 of 2000 took 0.104s
  training loss:		2.670510
  validation loss:		2.639222
  validation accuracy:		12.83 %
Epoch 41 of 2000 took 0.104s
  training loss:		2.663615
  validation loss:		2.632026
  validation accuracy:		12.83 %
Epoch 42 of 2000 took 0.104s
  training loss:		2.658542
  validation loss:		2.625022
  validation accuracy:		12.83 %
Epoch 43 of 2000 took 0.104s
  training loss:		2.650728
  validation loss:		2.618083
  validation accuracy:		12.83 %
Epoch 44 of 2000 took 0.104s
  training loss:		2.645410
  validation loss:		2.611200
  validation accuracy:		12.93 %
Epoch 45 of 2000 took 0.104s
  training loss:		2.639547
  validation loss:		2.604484
  validation accuracy:		12.93 %
Epoch 46 of 2000 took 0.104s
  training loss:		2.632626
  validation loss:		2.597837
  validation accuracy:		12.93 %
Epoch 47 of 2000 took 0.104s
  training loss:		2.626966
  validation loss:		2.591235
  validation accuracy:		12.93 %
Epoch 48 of 2000 took 0.104s
  training loss:		2.621271
  validation loss:		2.584775
  validation accuracy:		12.93 %
Epoch 49 of 2000 took 0.104s
  training loss:		2.615692
  validation loss:		2.578328
  validation accuracy:		12.93 %
Epoch 50 of 2000 took 0.104s
  training loss:		2.609223
  validation loss:		2.571936
  validation accuracy:		12.93 %
Epoch 51 of 2000 took 0.108s
  training loss:		2.604653
  validation loss:		2.565713
  validation accuracy:		12.93 %
Epoch 52 of 2000 took 0.104s
  training loss:		2.598266
  validation loss:		2.559545
  validation accuracy:		12.93 %
Epoch 53 of 2000 took 0.104s
  training loss:		2.592907
  validation loss:		2.553525
  validation accuracy:		12.93 %
Epoch 54 of 2000 took 0.104s
  training loss:		2.588440
  validation loss:		2.547590
  validation accuracy:		12.93 %
Epoch 55 of 2000 took 0.104s
  training loss:		2.582206
  validation loss:		2.541704
  validation accuracy:		12.93 %
Epoch 56 of 2000 took 0.104s
  training loss:		2.577533
  validation loss:		2.535959
  validation accuracy:		12.93 %
Epoch 57 of 2000 took 0.104s
  training loss:		2.572653
  validation loss:		2.530314
  validation accuracy:		12.93 %
Epoch 58 of 2000 took 0.104s
  training loss:		2.568327
  validation loss:		2.524819
  validation accuracy:		12.93 %
Epoch 59 of 2000 took 0.105s
  training loss:		2.562385
  validation loss:		2.519342
  validation accuracy:		12.93 %
Epoch 60 of 2000 took 0.104s
  training loss:		2.556365
  validation loss:		2.513916
  validation accuracy:		12.93 %
Epoch 61 of 2000 took 0.104s
  training loss:		2.553573
  validation loss:		2.508615
  validation accuracy:		12.93 %
Epoch 62 of 2000 took 0.104s
  training loss:		2.548165
  validation loss:		2.503475
  validation accuracy:		12.93 %
Epoch 63 of 2000 took 0.104s
  training loss:		2.544242
  validation loss:		2.498360
  validation accuracy:		12.93 %
Epoch 64 of 2000 took 0.104s
  training loss:		2.539538
  validation loss:		2.493371
  validation accuracy:		12.93 %
Epoch 65 of 2000 took 0.104s
  training loss:		2.533991
  validation loss:		2.488395
  validation accuracy:		12.93 %
Epoch 66 of 2000 took 0.104s
  training loss:		2.530502
  validation loss:		2.483639
  validation accuracy:		12.93 %
Epoch 67 of 2000 took 0.104s
  training loss:		2.526040
  validation loss:		2.478882
  validation accuracy:		12.93 %
Epoch 68 of 2000 took 0.105s
  training loss:		2.522139
  validation loss:		2.474233
  validation accuracy:		12.93 %
Epoch 69 of 2000 took 0.104s
  training loss:		2.518847
  validation loss:		2.469646
  validation accuracy:		12.93 %
Epoch 70 of 2000 took 0.104s
  training loss:		2.513613
  validation loss:		2.465129
  validation accuracy:		12.93 %
Epoch 71 of 2000 took 0.104s
  training loss:		2.509558
  validation loss:		2.460759
  validation accuracy:		12.93 %
Epoch 72 of 2000 took 0.104s
  training loss:		2.505336
  validation loss:		2.456401
  validation accuracy:		12.93 %
Epoch 73 of 2000 took 0.104s
  training loss:		2.501472
  validation loss:		2.452134
  validation accuracy:		12.93 %
Epoch 74 of 2000 took 0.104s
  training loss:		2.497958
  validation loss:		2.447838
  validation accuracy:		12.93 %
Epoch 75 of 2000 took 0.104s
  training loss:		2.494936
  validation loss:		2.443745
  validation accuracy:		12.93 %
Epoch 76 of 2000 took 0.104s
  training loss:		2.491298
  validation loss:		2.439740
  validation accuracy:		12.93 %
Epoch 77 of 2000 took 0.104s
  training loss:		2.487839
  validation loss:		2.435868
  validation accuracy:		12.93 %
Epoch 78 of 2000 took 0.104s
  training loss:		2.483299
  validation loss:		2.432055
  validation accuracy:		12.93 %
Epoch 79 of 2000 took 0.104s
  training loss:		2.480333
  validation loss:		2.428322
  validation accuracy:		12.93 %
Epoch 80 of 2000 took 0.104s
  training loss:		2.477449
  validation loss:		2.424697
  validation accuracy:		12.93 %
Epoch 81 of 2000 took 0.104s
  training loss:		2.473750
  validation loss:		2.421066
  validation accuracy:		12.93 %
Epoch 82 of 2000 took 0.104s
  training loss:		2.471664
  validation loss:		2.417515
  validation accuracy:		12.93 %
Epoch 83 of 2000 took 0.104s
  training loss:		2.467393
  validation loss:		2.414086
  validation accuracy:		12.93 %
Epoch 84 of 2000 took 0.112s
  training loss:		2.464474
  validation loss:		2.410733
  validation accuracy:		12.93 %
Epoch 85 of 2000 took 0.104s
  training loss:		2.460352
  validation loss:		2.407317
  validation accuracy:		12.93 %
Epoch 86 of 2000 took 0.103s
  training loss:		2.458632
  validation loss:		2.404029
  validation accuracy:		12.93 %
Epoch 87 of 2000 took 0.103s
  training loss:		2.454698
  validation loss:		2.400751
  validation accuracy:		12.93 %
Epoch 88 of 2000 took 0.104s
  training loss:		2.453228
  validation loss:		2.397634
  validation accuracy:		12.93 %
Epoch 89 of 2000 took 0.103s
  training loss:		2.450780
  validation loss:		2.394705
  validation accuracy:		12.93 %
Epoch 90 of 2000 took 0.103s
  training loss:		2.447305
  validation loss:		2.391795
  validation accuracy:		12.93 %
Epoch 91 of 2000 took 0.103s
  training loss:		2.444149
  validation loss:		2.388898
  validation accuracy:		12.93 %
Epoch 92 of 2000 took 0.103s
  training loss:		2.441931
  validation loss:		2.386099
  validation accuracy:		12.93 %
Epoch 93 of 2000 took 0.103s
  training loss:		2.439377
  validation loss:		2.383301
  validation accuracy:		12.93 %
Epoch 94 of 2000 took 0.103s
  training loss:		2.436936
  validation loss:		2.380625
  validation accuracy:		12.93 %
Epoch 95 of 2000 took 0.103s
  training loss:		2.434737
  validation loss:		2.377968
  validation accuracy:		12.93 %
Epoch 96 of 2000 took 0.103s
  training loss:		2.432328
  validation loss:		2.375391
  validation accuracy:		12.93 %
Epoch 97 of 2000 took 0.103s
  training loss:		2.428722
  validation loss:		2.372842
  validation accuracy:		12.93 %
Epoch 98 of 2000 took 0.103s
  training loss:		2.426543
  validation loss:		2.370335
  validation accuracy:		12.93 %
Epoch 99 of 2000 took 0.103s
  training loss:		2.425196
  validation loss:		2.367860
  validation accuracy:		12.93 %
Epoch 100 of 2000 took 0.103s
  training loss:		2.423233
  validation loss:		2.365569
  validation accuracy:		12.93 %
Epoch 101 of 2000 took 0.103s
  training loss:		2.420240
  validation loss:		2.363209
  validation accuracy:		12.93 %
Epoch 102 of 2000 took 0.103s
  training loss:		2.417917
  validation loss:		2.360853
  validation accuracy:		12.93 %
Epoch 103 of 2000 took 0.103s
  training loss:		2.416089
  validation loss:		2.358638
  validation accuracy:		12.93 %
Epoch 104 of 2000 took 0.103s
  training loss:		2.414407
  validation loss:		2.356522
  validation accuracy:		12.93 %
Epoch 105 of 2000 took 0.103s
  training loss:		2.412633
  validation loss:		2.354502
  validation accuracy:		12.93 %
Epoch 106 of 2000 took 0.103s
  training loss:		2.410305
  validation loss:		2.352442
  validation accuracy:		12.93 %
Epoch 107 of 2000 took 0.103s
  training loss:		2.407941
  validation loss:		2.350420
  validation accuracy:		12.93 %
Epoch 108 of 2000 took 0.105s
  training loss:		2.406823
  validation loss:		2.348445
  validation accuracy:		12.93 %
Epoch 109 of 2000 took 0.103s
  training loss:		2.404962
  validation loss:		2.346569
  validation accuracy:		12.93 %
Epoch 110 of 2000 took 0.103s
  training loss:		2.402413
  validation loss:		2.344657
  validation accuracy:		12.93 %
Epoch 111 of 2000 took 0.103s
  training loss:		2.401614
  validation loss:		2.342816
  validation accuracy:		12.93 %
Epoch 112 of 2000 took 0.103s
  training loss:		2.399588
  validation loss:		2.341174
  validation accuracy:		12.93 %
Epoch 113 of 2000 took 0.103s
  training loss:		2.397945
  validation loss:		2.339504
  validation accuracy:		12.93 %
Epoch 114 of 2000 took 0.100s
  training loss:		2.396408
  validation loss:		2.337725
  validation accuracy:		12.93 %
Epoch 115 of 2000 took 0.100s
  training loss:		2.394719
  validation loss:		2.336052
  validation accuracy:		12.93 %
Epoch 116 of 2000 took 0.096s
  training loss:		2.392761
  validation loss:		2.334523
  validation accuracy:		12.93 %
Epoch 117 of 2000 took 0.097s
  training loss:		2.390020
  validation loss:		2.332943
  validation accuracy:		12.93 %
Epoch 118 of 2000 took 0.103s
  training loss:		2.388509
  validation loss:		2.331214
  validation accuracy:		12.93 %
Epoch 119 of 2000 took 0.096s
  training loss:		2.388312
  validation loss:		2.329639
  validation accuracy:		12.93 %
Epoch 120 of 2000 took 0.097s
  training loss:		2.386190
  validation loss:		2.328121
  validation accuracy:		12.93 %
Epoch 121 of 2000 took 0.096s
  training loss:		2.385582
  validation loss:		2.326799
  validation accuracy:		12.93 %
Epoch 122 of 2000 took 0.099s
  training loss:		2.383852
  validation loss:		2.325356
  validation accuracy:		12.93 %
Epoch 123 of 2000 took 0.100s
  training loss:		2.382447
  validation loss:		2.323983
  validation accuracy:		12.93 %
Epoch 124 of 2000 took 0.096s
  training loss:		2.381063
  validation loss:		2.322690
  validation accuracy:		12.93 %
Epoch 125 of 2000 took 0.101s
  training loss:		2.378900
  validation loss:		2.321291
  validation accuracy:		12.93 %
Epoch 126 of 2000 took 0.101s
  training loss:		2.378658
  validation loss:		2.320084
  validation accuracy:		12.93 %
Epoch 127 of 2000 took 0.096s
  training loss:		2.377126
  validation loss:		2.318782
  validation accuracy:		12.93 %
Epoch 128 of 2000 took 0.097s
  training loss:		2.375728
  validation loss:		2.317645
  validation accuracy:		12.93 %
Epoch 129 of 2000 took 0.096s
  training loss:		2.374578
  validation loss:		2.316373
  validation accuracy:		12.93 %
Epoch 130 of 2000 took 0.102s
  training loss:		2.374031
  validation loss:		2.315266
  validation accuracy:		12.93 %
Epoch 131 of 2000 took 0.098s
  training loss:		2.372260
  validation loss:		2.314142
  validation accuracy:		12.93 %
Epoch 132 of 2000 took 0.096s
  training loss:		2.370698
  validation loss:		2.313060
  validation accuracy:		12.93 %
Epoch 133 of 2000 took 0.103s
  training loss:		2.369599
  validation loss:		2.311880
  validation accuracy:		12.93 %
Epoch 134 of 2000 took 0.097s
  training loss:		2.369554
  validation loss:		2.310969
  validation accuracy:		12.93 %
Epoch 135 of 2000 took 0.096s
  training loss:		2.367624
  validation loss:		2.309882
  validation accuracy:		12.93 %
Epoch 136 of 2000 took 0.096s
  training loss:		2.366737
  validation loss:		2.308956
  validation accuracy:		12.93 %
Epoch 137 of 2000 took 0.106s
  training loss:		2.365899
  validation loss:		2.307930
  validation accuracy:		12.93 %
Epoch 138 of 2000 took 0.100s
  training loss:		2.363454
  validation loss:		2.306756
  validation accuracy:		12.93 %
Epoch 139 of 2000 took 0.096s
  training loss:		2.362836
  validation loss:		2.305735
  validation accuracy:		12.93 %
Epoch 140 of 2000 took 0.096s
  training loss:		2.363436
  validation loss:		2.304795
  validation accuracy:		12.93 %
Epoch 141 of 2000 took 0.102s
  training loss:		2.360056
  validation loss:		2.303829
  validation accuracy:		12.93 %
Epoch 142 of 2000 took 0.095s
  training loss:		2.360174
  validation loss:		2.302840
  validation accuracy:		12.93 %
Epoch 143 of 2000 took 0.097s
  training loss:		2.359979
  validation loss:		2.301794
  validation accuracy:		12.93 %
Epoch 144 of 2000 took 0.096s
  training loss:		2.358241
  validation loss:		2.300965
  validation accuracy:		12.93 %
Epoch 145 of 2000 took 0.098s
  training loss:		2.357080
  validation loss:		2.300092
  validation accuracy:		12.93 %
Epoch 146 of 2000 took 0.098s
  training loss:		2.356662
  validation loss:		2.299308
  validation accuracy:		12.93 %
Epoch 147 of 2000 took 0.095s
  training loss:		2.355687
  validation loss:		2.298495
  validation accuracy:		12.93 %
Epoch 148 of 2000 took 0.098s
  training loss:		2.355681
  validation loss:		2.297826
  validation accuracy:		12.93 %
Epoch 149 of 2000 took 0.096s
  training loss:		2.353701
  validation loss:		2.297011
  validation accuracy:		12.93 %
Epoch 150 of 2000 took 0.098s
  training loss:		2.353280
  validation loss:		2.296352
  validation accuracy:		12.93 %
Epoch 151 of 2000 took 0.099s
  training loss:		2.353190
  validation loss:		2.295683
  validation accuracy:		12.93 %
Epoch 152 of 2000 took 0.096s
  training loss:		2.351489
  validation loss:		2.294919
  validation accuracy:		12.93 %
Epoch 153 of 2000 took 0.099s
  training loss:		2.350947
  validation loss:		2.294201
  validation accuracy:		12.93 %
Epoch 154 of 2000 took 0.096s
  training loss:		2.349389
  validation loss:		2.293400
  validation accuracy:		12.93 %
Epoch 155 of 2000 took 0.096s
  training loss:		2.349365
  validation loss:		2.292654
  validation accuracy:		12.93 %
Epoch 156 of 2000 took 0.098s
  training loss:		2.348275
  validation loss:		2.291977
  validation accuracy:		12.93 %
Epoch 157 of 2000 took 0.097s
  training loss:		2.347820
  validation loss:		2.291385
  validation accuracy:		12.93 %
Epoch 158 of 2000 took 0.096s
  training loss:		2.347120
  validation loss:		2.290746
  validation accuracy:		12.93 %
Epoch 159 of 2000 took 0.096s
  training loss:		2.345919
  validation loss:		2.290082
  validation accuracy:		12.93 %
Epoch 160 of 2000 took 0.096s
  training loss:		2.344001
  validation loss:		2.289298
  validation accuracy:		12.93 %
Epoch 161 of 2000 took 0.096s
  training loss:		2.344584
  validation loss:		2.288622
  validation accuracy:		12.93 %
Epoch 162 of 2000 took 0.096s
  training loss:		2.344823
  validation loss:		2.288118
  validation accuracy:		12.93 %
Epoch 163 of 2000 took 0.096s
  training loss:		2.344940
  validation loss:		2.287561
  validation accuracy:		12.93 %
Epoch 164 of 2000 took 0.096s
  training loss:		2.342656
  validation loss:		2.287135
  validation accuracy:		12.93 %
Epoch 165 of 2000 took 0.096s
  training loss:		2.342023
  validation loss:		2.286464
  validation accuracy:		12.93 %
Epoch 166 of 2000 took 0.096s
  training loss:		2.341474
  validation loss:		2.285807
  validation accuracy:		12.93 %
Epoch 167 of 2000 took 0.096s
  training loss:		2.340958
  validation loss:		2.285322
  validation accuracy:		12.93 %
Epoch 168 of 2000 took 0.099s
  training loss:		2.340551
  validation loss:		2.284756
  validation accuracy:		12.93 %
Epoch 169 of 2000 took 0.096s
  training loss:		2.339823
  validation loss:		2.284198
  validation accuracy:		12.93 %
Epoch 170 of 2000 took 0.096s
  training loss:		2.339164
  validation loss:		2.283570
  validation accuracy:		12.93 %
Epoch 171 of 2000 took 0.096s
  training loss:		2.338920
  validation loss:		2.283153
  validation accuracy:		12.93 %
Epoch 172 of 2000 took 0.096s
  training loss:		2.337147
  validation loss:		2.282439
  validation accuracy:		12.93 %
Epoch 173 of 2000 took 0.096s
  training loss:		2.337885
  validation loss:		2.282037
  validation accuracy:		12.93 %
Epoch 174 of 2000 took 0.096s
  training loss:		2.337184
  validation loss:		2.281652
  validation accuracy:		12.93 %
Epoch 175 of 2000 took 0.096s
  training loss:		2.336838
  validation loss:		2.281098
  validation accuracy:		12.93 %
Epoch 176 of 2000 took 0.096s
  training loss:		2.336193
  validation loss:		2.280744
  validation accuracy:		12.93 %
Epoch 177 of 2000 took 0.096s
  training loss:		2.336090
  validation loss:		2.280473
  validation accuracy:		12.93 %
Epoch 178 of 2000 took 0.096s
  training loss:		2.334689
  validation loss:		2.280061
  validation accuracy:		12.93 %
Epoch 179 of 2000 took 0.097s
  training loss:		2.334360
  validation loss:		2.279582
  validation accuracy:		12.93 %
Epoch 180 of 2000 took 0.096s
  training loss:		2.332635
  validation loss:		2.279005
  validation accuracy:		12.93 %
Epoch 181 of 2000 took 0.096s
  training loss:		2.333769
  validation loss:		2.278436
  validation accuracy:		12.93 %
Epoch 182 of 2000 took 0.096s
  training loss:		2.333027
  validation loss:		2.278212
  validation accuracy:		12.93 %
Epoch 183 of 2000 took 0.096s
  training loss:		2.332264
  validation loss:		2.277716
  validation accuracy:		12.93 %
Epoch 184 of 2000 took 0.096s
  training loss:		2.331378
  validation loss:		2.277339
  validation accuracy:		12.93 %
Epoch 185 of 2000 took 0.096s
  training loss:		2.330945
  validation loss:		2.276831
  validation accuracy:		12.93 %
Epoch 186 of 2000 took 0.096s
  training loss:		2.331294
  validation loss:		2.276513
  validation accuracy:		12.93 %
Epoch 187 of 2000 took 0.096s
  training loss:		2.331938
  validation loss:		2.276210
  validation accuracy:		12.93 %
Epoch 188 of 2000 took 0.096s
  training loss:		2.330763
  validation loss:		2.275967
  validation accuracy:		12.93 %
Epoch 189 of 2000 took 0.096s
  training loss:		2.329052
  validation loss:		2.275590
  validation accuracy:		12.93 %
Epoch 190 of 2000 took 0.096s
  training loss:		2.329965
  validation loss:		2.275372
  validation accuracy:		12.93 %
Epoch 191 of 2000 took 0.106s
  training loss:		2.329357
  validation loss:		2.275213
  validation accuracy:		12.93 %
Epoch 192 of 2000 took 0.097s
  training loss:		2.328525
  validation loss:		2.274859
  validation accuracy:		12.93 %
Epoch 193 of 2000 took 0.096s
  training loss:		2.328237
  validation loss:		2.274560
  validation accuracy:		12.93 %
Epoch 194 of 2000 took 0.096s
  training loss:		2.327507
  validation loss:		2.274314
  validation accuracy:		12.93 %
Epoch 195 of 2000 took 0.096s
  training loss:		2.327546
  validation loss:		2.273906
  validation accuracy:		12.93 %
Epoch 196 of 2000 took 0.096s
  training loss:		2.327057
  validation loss:		2.273497
  validation accuracy:		12.93 %
Epoch 197 of 2000 took 0.098s
  training loss:		2.326696
  validation loss:		2.273053
  validation accuracy:		12.93 %
Epoch 198 of 2000 took 0.099s
  training loss:		2.325093
  validation loss:		2.272637
  validation accuracy:		12.93 %
Epoch 199 of 2000 took 0.099s
  training loss:		2.325253
  validation loss:		2.272235
  validation accuracy:		12.93 %
Epoch 200 of 2000 took 0.099s
  training loss:		2.324726
  validation loss:		2.271860
  validation accuracy:		12.93 %
Epoch 201 of 2000 took 0.099s
  training loss:		2.325798
  validation loss:		2.271448
  validation accuracy:		12.93 %
Epoch 202 of 2000 took 0.099s
  training loss:		2.325155
  validation loss:		2.271193
  validation accuracy:		12.93 %
Epoch 203 of 2000 took 0.099s
  training loss:		2.324471
  validation loss:		2.271223
  validation accuracy:		12.93 %
Epoch 204 of 2000 took 0.099s
  training loss:		2.323244
  validation loss:		2.270882
  validation accuracy:		12.93 %
Epoch 205 of 2000 took 0.099s
  training loss:		2.324058
  validation loss:		2.270484
  validation accuracy:		12.93 %
Epoch 206 of 2000 took 0.099s
  training loss:		2.323039
  validation loss:		2.270103
  validation accuracy:		12.93 %
Epoch 207 of 2000 took 0.099s
  training loss:		2.322702
  validation loss:		2.269810
  validation accuracy:		12.93 %
Epoch 208 of 2000 took 0.099s
  training loss:		2.322239
  validation loss:		2.269449
  validation accuracy:		12.93 %
Epoch 209 of 2000 took 0.099s
  training loss:		2.321836
  validation loss:		2.269200
  validation accuracy:		12.93 %
Epoch 210 of 2000 took 0.100s
  training loss:		2.322180
  validation loss:		2.268707
  validation accuracy:		12.93 %
Epoch 211 of 2000 took 0.099s
  training loss:		2.322491
  validation loss:		2.268433
  validation accuracy:		12.93 %
Epoch 212 of 2000 took 0.099s
  training loss:		2.321352
  validation loss:		2.268254
  validation accuracy:		12.93 %
Epoch 213 of 2000 took 0.099s
  training loss:		2.321141
  validation loss:		2.268069
  validation accuracy:		12.93 %
Epoch 214 of 2000 took 0.099s
  training loss:		2.320281
  validation loss:		2.267766
  validation accuracy:		12.93 %
Epoch 215 of 2000 took 0.099s
  training loss:		2.320487
  validation loss:		2.267632
  validation accuracy:		12.93 %
Epoch 216 of 2000 took 0.099s
  training loss:		2.320020
  validation loss:		2.267330
  validation accuracy:		12.93 %
Epoch 217 of 2000 took 0.099s
  training loss:		2.319238
  validation loss:		2.267057
  validation accuracy:		12.93 %
Epoch 218 of 2000 took 0.099s
  training loss:		2.319231
  validation loss:		2.266703
  validation accuracy:		12.93 %
Epoch 219 of 2000 took 0.099s
  training loss:		2.319465
  validation loss:		2.266610
  validation accuracy:		12.93 %
Epoch 220 of 2000 took 0.099s
  training loss:		2.318207
  validation loss:		2.266287
  validation accuracy:		12.93 %
Epoch 221 of 2000 took 0.099s
  training loss:		2.318972
  validation loss:		2.265890
  validation accuracy:		12.93 %
Epoch 222 of 2000 took 0.099s
  training loss:		2.319001
  validation loss:		2.265675
  validation accuracy:		12.93 %
Epoch 223 of 2000 took 0.099s
  training loss:		2.317869
  validation loss:		2.265401
  validation accuracy:		12.93 %
Epoch 224 of 2000 took 0.099s
  training loss:		2.318132
  validation loss:		2.265181
  validation accuracy:		12.93 %
Epoch 225 of 2000 took 0.099s
  training loss:		2.317098
  validation loss:		2.265059
  validation accuracy:		12.93 %
Epoch 226 of 2000 took 0.099s
  training loss:		2.317795
  validation loss:		2.264790
  validation accuracy:		12.93 %
Epoch 227 of 2000 took 0.101s
  training loss:		2.317826
  validation loss:		2.264627
  validation accuracy:		12.93 %
Epoch 228 of 2000 took 0.099s
  training loss:		2.317367
  validation loss:		2.264385
  validation accuracy:		12.93 %
Epoch 229 of 2000 took 0.099s
  training loss:		2.316796
  validation loss:		2.264233
  validation accuracy:		12.93 %
Epoch 230 of 2000 took 0.099s
  training loss:		2.317347
  validation loss:		2.264038
  validation accuracy:		12.93 %
Epoch 231 of 2000 took 0.099s
  training loss:		2.316348
  validation loss:		2.263749
  validation accuracy:		12.93 %
Epoch 232 of 2000 took 0.099s
  training loss:		2.316574
  validation loss:		2.263610
  validation accuracy:		12.93 %
Epoch 233 of 2000 took 0.099s
  training loss:		2.316187
  validation loss:		2.263465
  validation accuracy:		12.93 %
Epoch 234 of 2000 took 0.099s
  training loss:		2.315288
  validation loss:		2.263198
  validation accuracy:		12.93 %
Epoch 235 of 2000 took 0.098s
  training loss:		2.315821
  validation loss:		2.263140
  validation accuracy:		12.93 %
Epoch 236 of 2000 took 0.096s
  training loss:		2.315399
  validation loss:		2.262955
  validation accuracy:		12.93 %
Epoch 237 of 2000 took 0.096s
  training loss:		2.314877
  validation loss:		2.262883
  validation accuracy:		12.93 %
Epoch 238 of 2000 took 0.096s
  training loss:		2.315174
  validation loss:		2.262735
  validation accuracy:		12.93 %
Epoch 239 of 2000 took 0.096s
  training loss:		2.314155
  validation loss:		2.262580
  validation accuracy:		12.93 %
Epoch 240 of 2000 took 0.097s
  training loss:		2.313885
  validation loss:		2.262364
  validation accuracy:		12.93 %
Epoch 241 of 2000 took 0.096s
  training loss:		2.313402
  validation loss:		2.262071
  validation accuracy:		12.93 %
Epoch 242 of 2000 took 0.096s
  training loss:		2.313736
  validation loss:		2.261901
  validation accuracy:		12.93 %
Epoch 243 of 2000 took 0.096s
  training loss:		2.314264
  validation loss:		2.261808
  validation accuracy:		12.93 %
Epoch 244 of 2000 took 0.096s
  training loss:		2.313470
  validation loss:		2.261738
  validation accuracy:		12.93 %
Epoch 245 of 2000 took 0.096s
  training loss:		2.314449
  validation loss:		2.261415
  validation accuracy:		12.93 %
Epoch 246 of 2000 took 0.096s
  training loss:		2.313050
  validation loss:		2.261305
  validation accuracy:		12.93 %
Epoch 247 of 2000 took 0.096s
  training loss:		2.313176
  validation loss:		2.261359
  validation accuracy:		12.93 %
Epoch 248 of 2000 took 0.096s
  training loss:		2.313892
  validation loss:		2.261163
  validation accuracy:		12.93 %
Epoch 249 of 2000 took 0.096s
  training loss:		2.312467
  validation loss:		2.261083
  validation accuracy:		12.93 %
Epoch 250 of 2000 took 0.096s
  training loss:		2.312824
  validation loss:		2.261051
  validation accuracy:		12.93 %
Epoch 251 of 2000 took 0.096s
  training loss:		2.312007
  validation loss:		2.260764
  validation accuracy:		12.93 %
Epoch 252 of 2000 took 0.096s
  training loss:		2.311868
  validation loss:		2.260648
  validation accuracy:		12.93 %
Epoch 253 of 2000 took 0.098s
  training loss:		2.312059
  validation loss:		2.260255
  validation accuracy:		13.04 %
Epoch 254 of 2000 took 0.096s
  training loss:		2.312375
  validation loss:		2.260155
  validation accuracy:		13.04 %
Epoch 255 of 2000 took 0.096s
  training loss:		2.311710
  validation loss:		2.260279
  validation accuracy:		13.04 %
Epoch 256 of 2000 took 0.096s
  training loss:		2.311465
  validation loss:		2.260097
  validation accuracy:		12.93 %
Epoch 257 of 2000 took 0.096s
  training loss:		2.310556
  validation loss:		2.260074
  validation accuracy:		12.93 %
Epoch 258 of 2000 took 0.096s
  training loss:		2.311242
  validation loss:		2.259802
  validation accuracy:		13.04 %
Epoch 259 of 2000 took 0.096s
  training loss:		2.310217
  validation loss:		2.259371
  validation accuracy:		13.04 %
Epoch 260 of 2000 took 0.096s
  training loss:		2.311540
  validation loss:		2.259440
  validation accuracy:		12.93 %
Epoch 261 of 2000 took 0.096s
  training loss:		2.310073
  validation loss:		2.259123
  validation accuracy:		12.93 %
Epoch 262 of 2000 took 0.096s
  training loss:		2.310268
  validation loss:		2.258961
  validation accuracy:		12.93 %
Epoch 263 of 2000 took 0.096s
  training loss:		2.310712
  validation loss:		2.258851
  validation accuracy:		12.93 %
Epoch 264 of 2000 took 0.096s
  training loss:		2.309544
  validation loss:		2.258653
  validation accuracy:		12.93 %
Epoch 265 of 2000 took 0.096s
  training loss:		2.310017
  validation loss:		2.258593
  validation accuracy:		12.93 %
Epoch 266 of 2000 took 0.096s
  training loss:		2.309781
  validation loss:		2.258507
  validation accuracy:		12.93 %
Epoch 267 of 2000 took 0.096s
  training loss:		2.309089
  validation loss:		2.258037
  validation accuracy:		12.93 %
Epoch 268 of 2000 took 0.096s
  training loss:		2.309078
  validation loss:		2.257819
  validation accuracy:		12.93 %
Epoch 269 of 2000 took 0.096s
  training loss:		2.310380
  validation loss:		2.257663
  validation accuracy:		12.93 %
Epoch 270 of 2000 took 0.097s
  training loss:		2.309360
  validation loss:		2.257783
  validation accuracy:		12.93 %
Epoch 271 of 2000 took 0.098s
  training loss:		2.308967
  validation loss:		2.257701
  validation accuracy:		12.93 %
Epoch 272 of 2000 took 0.097s
  training loss:		2.309123
  validation loss:		2.257616
  validation accuracy:		12.93 %
Epoch 273 of 2000 took 0.096s
  training loss:		2.308784
  validation loss:		2.257368
  validation accuracy:		12.93 %
Epoch 274 of 2000 took 0.096s
  training loss:		2.307826
  validation loss:		2.257194
  validation accuracy:		12.93 %
Epoch 275 of 2000 took 0.096s
  training loss:		2.307365
  validation loss:		2.256709
  validation accuracy:		12.93 %
Epoch 276 of 2000 took 0.097s
  training loss:		2.308749
  validation loss:		2.256478
  validation accuracy:		12.93 %
Epoch 277 of 2000 took 0.096s
  training loss:		2.307916
  validation loss:		2.256259
  validation accuracy:		12.93 %
Epoch 278 of 2000 took 0.096s
  training loss:		2.308136
  validation loss:		2.256201
  validation accuracy:		12.93 %
Epoch 279 of 2000 took 0.096s
  training loss:		2.307450
  validation loss:		2.255797
  validation accuracy:		12.93 %
Epoch 280 of 2000 took 0.096s
  training loss:		2.308341
  validation loss:		2.255870
  validation accuracy:		12.93 %
Epoch 281 of 2000 took 0.096s
  training loss:		2.307691
  validation loss:		2.255695
  validation accuracy:		12.93 %
Epoch 282 of 2000 took 0.096s
  training loss:		2.307835
  validation loss:		2.255740
  validation accuracy:		12.93 %
Epoch 283 of 2000 took 0.096s
  training loss:		2.307050
  validation loss:		2.255688
  validation accuracy:		12.93 %
Epoch 284 of 2000 took 0.096s
  training loss:		2.306787
  validation loss:		2.255535
  validation accuracy:		12.93 %
Epoch 285 of 2000 took 0.096s
  training loss:		2.306737
  validation loss:		2.255146
  validation accuracy:		12.93 %
Epoch 286 of 2000 took 0.096s
  training loss:		2.307166
  validation loss:		2.254935
  validation accuracy:		12.93 %
Epoch 287 of 2000 took 0.096s
  training loss:		2.306401
  validation loss:		2.255026
  validation accuracy:		12.93 %
Epoch 288 of 2000 took 0.096s
  training loss:		2.305939
  validation loss:		2.254813
  validation accuracy:		13.04 %
Epoch 289 of 2000 took 0.096s
  training loss:		2.307635
  validation loss:		2.254626
  validation accuracy:		12.93 %
Epoch 290 of 2000 took 0.096s
  training loss:		2.306123
  validation loss:		2.254367
  validation accuracy:		12.93 %
Epoch 291 of 2000 took 0.099s
  training loss:		2.306521
  validation loss:		2.254179
  validation accuracy:		12.93 %
Epoch 292 of 2000 took 0.096s
  training loss:		2.306053
  validation loss:		2.254157
  validation accuracy:		12.93 %
Epoch 293 of 2000 took 0.096s
  training loss:		2.306346
  validation loss:		2.254142
  validation accuracy:		13.04 %
Epoch 294 of 2000 took 0.096s
  training loss:		2.306211
  validation loss:		2.253997
  validation accuracy:		13.04 %
Epoch 295 of 2000 took 0.096s
  training loss:		2.305692
  validation loss:		2.253929
  validation accuracy:		12.93 %
Epoch 296 of 2000 took 0.096s
  training loss:		2.306637
  validation loss:		2.253945
  validation accuracy:		12.93 %
Epoch 297 of 2000 took 0.096s
  training loss:		2.306498
  validation loss:		2.254325
  validation accuracy:		12.93 %
Epoch 298 of 2000 took 0.096s
  training loss:		2.305659
  validation loss:		2.254333
  validation accuracy:		12.93 %
Epoch 299 of 2000 took 0.096s
  training loss:		2.306498
  validation loss:		2.254323
  validation accuracy:		12.93 %
Epoch 300 of 2000 took 0.096s
  training loss:		2.305865
  validation loss:		2.254472
  validation accuracy:		12.93 %
Epoch 301 of 2000 took 0.097s
  training loss:		2.305168
  validation loss:		2.254307
  validation accuracy:		12.93 %
Epoch 302 of 2000 took 0.096s
  training loss:		2.304829
  validation loss:		2.254029
  validation accuracy:		13.04 %
Epoch 303 of 2000 took 0.097s
  training loss:		2.305757
  validation loss:		2.253988
  validation accuracy:		13.04 %
Epoch 304 of 2000 took 0.096s
  training loss:		2.305165
  validation loss:		2.253651
  validation accuracy:		13.04 %
Epoch 305 of 2000 took 0.096s
  training loss:		2.304600
  validation loss:		2.253531
  validation accuracy:		13.04 %
Epoch 306 of 2000 took 0.096s
  training loss:		2.304509
  validation loss:		2.253210
  validation accuracy:		13.04 %
Epoch 307 of 2000 took 0.096s
  training loss:		2.304906
  validation loss:		2.252929
  validation accuracy:		13.04 %
Epoch 308 of 2000 took 0.096s
  training loss:		2.304714
  validation loss:		2.252987
  validation accuracy:		13.04 %
Epoch 309 of 2000 took 0.096s
  training loss:		2.305345
  validation loss:		2.253118
  validation accuracy:		12.93 %
Epoch 310 of 2000 took 0.096s
  training loss:		2.303977
  validation loss:		2.252920
  validation accuracy:		12.93 %
Epoch 311 of 2000 took 0.096s
  training loss:		2.304298
  validation loss:		2.252982
  validation accuracy:		12.93 %
Epoch 312 of 2000 took 0.096s
  training loss:		2.305152
  validation loss:		2.252813
  validation accuracy:		12.93 %
Epoch 313 of 2000 took 0.098s
  training loss:		2.304379
  validation loss:		2.252887
  validation accuracy:		12.93 %
Epoch 314 of 2000 took 0.096s
  training loss:		2.303985
  validation loss:		2.252743
  validation accuracy:		13.04 %
Epoch 315 of 2000 took 0.096s
  training loss:		2.304133
  validation loss:		2.252515
  validation accuracy:		13.04 %
Epoch 316 of 2000 took 0.096s
  training loss:		2.304443
  validation loss:		2.252687
  validation accuracy:		12.93 %
Epoch 317 of 2000 took 0.096s
  training loss:		2.303992
  validation loss:		2.252680
  validation accuracy:		13.04 %
Epoch 318 of 2000 took 0.096s
  training loss:		2.303665
  validation loss:		2.252552
  validation accuracy:		12.93 %
Epoch 319 of 2000 took 0.096s
  training loss:		2.303840
  validation loss:		2.252461
  validation accuracy:		13.04 %
Epoch 320 of 2000 took 0.098s
  training loss:		2.304277
  validation loss:		2.252293
  validation accuracy:		12.93 %
Epoch 321 of 2000 took 0.099s
  training loss:		2.304345
  validation loss:		2.252174
  validation accuracy:		12.93 %
Epoch 322 of 2000 took 0.099s
  training loss:		2.303582
  validation loss:		2.252051
  validation accuracy:		12.93 %
Epoch 323 of 2000 took 0.099s
  training loss:		2.303914
  validation loss:		2.252265
  validation accuracy:		12.93 %
Epoch 324 of 2000 took 0.099s
  training loss:		2.303720
  validation loss:		2.252396
  validation accuracy:		13.04 %
Epoch 325 of 2000 took 0.099s
  training loss:		2.303470
  validation loss:		2.252493
  validation accuracy:		12.93 %
Epoch 326 of 2000 took 0.099s
  training loss:		2.303470
  validation loss:		2.252464
  validation accuracy:		13.04 %
Epoch 327 of 2000 took 0.099s
  training loss:		2.303288
  validation loss:		2.252439
  validation accuracy:		12.93 %
Epoch 328 of 2000 took 0.099s
  training loss:		2.303363
  validation loss:		2.252424
  validation accuracy:		12.93 %
Epoch 329 of 2000 took 0.099s
  training loss:		2.302984
  validation loss:		2.252249
  validation accuracy:		12.93 %
Epoch 330 of 2000 took 0.099s
  training loss:		2.303249
  validation loss:		2.252173
  validation accuracy:		12.93 %
Epoch 331 of 2000 took 0.099s
  training loss:		2.302897
  validation loss:		2.252032
  validation accuracy:		12.93 %
Epoch 332 of 2000 took 0.099s
  training loss:		2.302827
  validation loss:		2.251867
  validation accuracy:		12.93 %
Epoch 333 of 2000 took 0.099s
  training loss:		2.302804
  validation loss:		2.251764
  validation accuracy:		12.93 %
Epoch 334 of 2000 took 0.100s
  training loss:		2.303943
  validation loss:		2.251679
  validation accuracy:		12.93 %
Epoch 335 of 2000 took 0.099s
  training loss:		2.303927
  validation loss:		2.251956
  validation accuracy:		12.93 %
Epoch 336 of 2000 took 0.101s
  training loss:		2.301923
  validation loss:		2.251979
  validation accuracy:		12.93 %
Epoch 337 of 2000 took 0.099s
  training loss:		2.302813
  validation loss:		2.251928
  validation accuracy:		13.04 %
Epoch 338 of 2000 took 0.099s
  training loss:		2.302153
  validation loss:		2.251814
  validation accuracy:		13.04 %
Epoch 339 of 2000 took 0.099s
  training loss:		2.301979
  validation loss:		2.251578
  validation accuracy:		13.04 %
Epoch 340 of 2000 took 0.099s
  training loss:		2.302601
  validation loss:		2.251621
  validation accuracy:		13.04 %
Epoch 341 of 2000 took 0.099s
  training loss:		2.303411
  validation loss:		2.251971
  validation accuracy:		13.04 %
Epoch 342 of 2000 took 0.099s
  training loss:		2.302859
  validation loss:		2.252129
  validation accuracy:		12.93 %
Epoch 343 of 2000 took 0.099s
  training loss:		2.302992
  validation loss:		2.251848
  validation accuracy:		12.93 %
Epoch 344 of 2000 took 0.099s
  training loss:		2.302665
  validation loss:		2.251725
  validation accuracy:		12.93 %
Epoch 345 of 2000 took 0.099s
  training loss:		2.303064
  validation loss:		2.251969
  validation accuracy:		12.93 %
Epoch 346 of 2000 took 0.099s
  training loss:		2.302197
  validation loss:		2.251759
  validation accuracy:		12.93 %
Epoch 347 of 2000 took 0.099s
  training loss:		2.301618
  validation loss:		2.251741
  validation accuracy:		12.93 %
Epoch 348 of 2000 took 0.099s
  training loss:		2.301472
  validation loss:		2.251758
  validation accuracy:		12.93 %
Epoch 349 of 2000 took 0.099s
  training loss:		2.302326
  validation loss:		2.251444
  validation accuracy:		12.93 %
Epoch 350 of 2000 took 0.099s
  training loss:		2.301315
  validation loss:		2.251363
  validation accuracy:		12.93 %
Epoch 351 of 2000 took 0.099s
  training loss:		2.301752
  validation loss:		2.251057
  validation accuracy:		12.93 %
Epoch 352 of 2000 took 0.099s
  training loss:		2.301570
  validation loss:		2.250833
  validation accuracy:		12.93 %
Epoch 353 of 2000 took 0.099s
  training loss:		2.301751
  validation loss:		2.250958
  validation accuracy:		12.93 %
Epoch 354 of 2000 took 0.099s
  training loss:		2.302665
  validation loss:		2.250844
  validation accuracy:		13.04 %
Epoch 355 of 2000 took 0.099s
  training loss:		2.301857
  validation loss:		2.250945
  validation accuracy:		13.04 %
Epoch 356 of 2000 took 0.099s
  training loss:		2.301463
  validation loss:		2.250890
  validation accuracy:		13.04 %
Epoch 357 of 2000 took 0.099s
  training loss:		2.302054
  validation loss:		2.250945
  validation accuracy:		13.04 %
Epoch 358 of 2000 took 0.099s
  training loss:		2.301826
  validation loss:		2.250920
  validation accuracy:		13.04 %
Epoch 359 of 2000 took 0.099s
  training loss:		2.301133
  validation loss:		2.250710
  validation accuracy:		13.04 %
Epoch 360 of 2000 took 0.099s
  training loss:		2.301261
  validation loss:		2.250615
  validation accuracy:		13.04 %
Epoch 361 of 2000 took 0.099s
  training loss:		2.301649
  validation loss:		2.250913
  validation accuracy:		13.04 %
Epoch 362 of 2000 took 0.101s
  training loss:		2.301432
  validation loss:		2.250988
  validation accuracy:		13.04 %
Epoch 363 of 2000 took 0.099s
  training loss:		2.301544
  validation loss:		2.250813
  validation accuracy:		13.04 %
Epoch 364 of 2000 took 0.100s
  training loss:		2.300875
  validation loss:		2.250581
  validation accuracy:		12.93 %
Epoch 365 of 2000 took 0.099s
  training loss:		2.300674
  validation loss:		2.250327
  validation accuracy:		12.93 %
Epoch 366 of 2000 took 0.099s
  training loss:		2.301244
  validation loss:		2.250187
  validation accuracy:		12.93 %
Epoch 367 of 2000 took 0.099s
  training loss:		2.300866
  validation loss:		2.250322
  validation accuracy:		12.93 %
Epoch 368 of 2000 took 0.099s
  training loss:		2.301076
  validation loss:		2.250018
  validation accuracy:		12.93 %
Epoch 369 of 2000 took 0.099s
  training loss:		2.301257
  validation loss:		2.250182
  validation accuracy:		12.93 %
Epoch 370 of 2000 took 0.099s
  training loss:		2.301259
  validation loss:		2.250260
  validation accuracy:		12.93 %
Epoch 371 of 2000 took 0.099s
  training loss:		2.300023
  validation loss:		2.249980
  validation accuracy:		12.93 %
Epoch 372 of 2000 took 0.099s
  training loss:		2.300661
  validation loss:		2.249777
  validation accuracy:		12.93 %
Epoch 373 of 2000 took 0.099s
  training loss:		2.301439
  validation loss:		2.249579
  validation accuracy:		13.04 %
Epoch 374 of 2000 took 0.099s
  training loss:		2.300683
  validation loss:		2.249549
  validation accuracy:		13.04 %
Epoch 375 of 2000 took 0.099s
  training loss:		2.300337
  validation loss:		2.249462
  validation accuracy:		13.04 %
Epoch 376 of 2000 took 0.099s
  training loss:		2.301159
  validation loss:		2.249438
  validation accuracy:		13.04 %
Epoch 377 of 2000 took 0.099s
  training loss:		2.300467
  validation loss:		2.249498
  validation accuracy:		12.93 %
Epoch 378 of 2000 took 0.099s
  training loss:		2.300166
  validation loss:		2.249661
  validation accuracy:		12.93 %
Epoch 379 of 2000 took 0.099s
  training loss:		2.300931
  validation loss:		2.249602
  validation accuracy:		12.93 %
Epoch 380 of 2000 took 0.099s
  training loss:		2.300109
  validation loss:		2.249578
  validation accuracy:		12.93 %
Epoch 381 of 2000 took 0.099s
  training loss:		2.301011
  validation loss:		2.249496
  validation accuracy:		13.04 %
Epoch 382 of 2000 took 0.099s
  training loss:		2.300353
  validation loss:		2.249574
  validation accuracy:		13.04 %
Epoch 383 of 2000 took 0.099s
  training loss:		2.300940
  validation loss:		2.249367
  validation accuracy:		13.04 %
Epoch 384 of 2000 took 0.099s
  training loss:		2.300823
  validation loss:		2.249588
  validation accuracy:		13.04 %
Epoch 385 of 2000 took 0.099s
  training loss:		2.301271
  validation loss:		2.249823
  validation accuracy:		12.93 %
Epoch 386 of 2000 took 0.099s
  training loss:		2.300130
  validation loss:		2.249875
  validation accuracy:		12.83 %
Epoch 387 of 2000 took 0.099s
  training loss:		2.300243
  validation loss:		2.250055
  validation accuracy:		12.93 %
Epoch 388 of 2000 took 0.099s
  training loss:		2.300258
  validation loss:		2.249748
  validation accuracy:		12.93 %
Epoch 389 of 2000 took 0.099s
  training loss:		2.300519
  validation loss:		2.249880
  validation accuracy:		12.93 %
Epoch 390 of 2000 took 0.101s
  training loss:		2.299874
  validation loss:		2.249806
  validation accuracy:		12.93 %
Epoch 391 of 2000 took 0.099s
  training loss:		2.300315
  validation loss:		2.249722
  validation accuracy:		12.93 %
Epoch 392 of 2000 took 0.099s
  training loss:		2.300280
  validation loss:		2.249611
  validation accuracy:		12.93 %
Epoch 393 of 2000 took 0.099s
  training loss:		2.299026
  validation loss:		2.249346
  validation accuracy:		12.93 %
Epoch 394 of 2000 took 0.099s
  training loss:		2.299081
  validation loss:		2.248894
  validation accuracy:		12.93 %
Epoch 395 of 2000 took 0.100s
  training loss:		2.300551
  validation loss:		2.248909
  validation accuracy:		12.93 %
Epoch 396 of 2000 took 0.099s
  training loss:		2.299680
  validation loss:		2.248908
  validation accuracy:		12.93 %
Epoch 397 of 2000 took 0.099s
  training loss:		2.299865
  validation loss:		2.248734
  validation accuracy:		12.93 %
Epoch 398 of 2000 took 0.099s
  training loss:		2.299621
  validation loss:		2.248431
  validation accuracy:		12.93 %
Epoch 399 of 2000 took 0.099s
  training loss:		2.300362
  validation loss:		2.248726
  validation accuracy:		12.93 %
Epoch 400 of 2000 took 0.099s
  training loss:		2.299550
  validation loss:		2.248835
  validation accuracy:		12.93 %
Epoch 401 of 2000 took 0.099s
  training loss:		2.299465
  validation loss:		2.248923
  validation accuracy:		12.93 %
Epoch 402 of 2000 took 0.099s
  training loss:		2.299386
  validation loss:		2.248839
  validation accuracy:		12.93 %
Epoch 403 of 2000 took 0.099s
  training loss:		2.299655
  validation loss:		2.248759
  validation accuracy:		13.04 %
Epoch 404 of 2000 took 0.099s
  training loss:		2.298891
  validation loss:		2.248501
  validation accuracy:		12.93 %
Epoch 405 of 2000 took 0.099s
  training loss:		2.299183
  validation loss:		2.248457
  validation accuracy:		12.93 %
Epoch 406 of 2000 took 0.099s
  training loss:		2.299755
  validation loss:		2.248476
  validation accuracy:		12.93 %
Epoch 407 of 2000 took 0.099s
  training loss:		2.299976
  validation loss:		2.248683
  validation accuracy:		12.93 %
Epoch 408 of 2000 took 0.099s
  training loss:		2.299913
  validation loss:		2.248787
  validation accuracy:		13.04 %
Epoch 409 of 2000 took 0.099s
  training loss:		2.299143
  validation loss:		2.248488
  validation accuracy:		13.04 %
Epoch 410 of 2000 took 0.099s
  training loss:		2.299879
  validation loss:		2.248399
  validation accuracy:		13.04 %
Epoch 411 of 2000 took 0.099s
  training loss:		2.299772
  validation loss:		2.248366
  validation accuracy:		13.04 %
Epoch 412 of 2000 took 0.099s
  training loss:		2.299212
  validation loss:		2.248420
  validation accuracy:		13.04 %
Epoch 413 of 2000 took 0.099s
  training loss:		2.300108
  validation loss:		2.248613
  validation accuracy:		13.04 %
Epoch 414 of 2000 took 0.099s
  training loss:		2.298989
  validation loss:		2.248765
  validation accuracy:		13.04 %
Epoch 415 of 2000 took 0.099s
  training loss:		2.299240
  validation loss:		2.248528
  validation accuracy:		13.04 %
Epoch 416 of 2000 took 0.099s
  training loss:		2.298676
  validation loss:		2.248214
  validation accuracy:		13.04 %
Epoch 417 of 2000 took 0.099s
  training loss:		2.299057
  validation loss:		2.248485
  validation accuracy:		12.93 %
Epoch 418 of 2000 took 0.099s
  training loss:		2.299426
  validation loss:		2.248477
  validation accuracy:		12.93 %
Epoch 419 of 2000 took 0.099s
  training loss:		2.299225
  validation loss:		2.248645
  validation accuracy:		12.93 %
Epoch 420 of 2000 took 0.099s
  training loss:		2.298741
  validation loss:		2.248456
  validation accuracy:		13.04 %
Epoch 421 of 2000 took 0.101s
  training loss:		2.299085
  validation loss:		2.248097
  validation accuracy:		12.93 %
Epoch 422 of 2000 took 0.099s
  training loss:		2.299208
  validation loss:		2.248138
  validation accuracy:		12.93 %
Epoch 423 of 2000 took 0.099s
  training loss:		2.299220
  validation loss:		2.247819
  validation accuracy:		12.93 %
Epoch 424 of 2000 took 0.099s
  training loss:		2.298606
  validation loss:		2.247822
  validation accuracy:		12.93 %
Epoch 425 of 2000 took 0.100s
  training loss:		2.298453
  validation loss:		2.247676
  validation accuracy:		12.93 %
Epoch 426 of 2000 took 0.099s
  training loss:		2.299276
  validation loss:		2.247853
  validation accuracy:		12.93 %
Epoch 427 of 2000 took 0.099s
  training loss:		2.299804
  validation loss:		2.248224
  validation accuracy:		12.93 %
Epoch 428 of 2000 took 0.099s
  training loss:		2.298170
  validation loss:		2.248239
  validation accuracy:		12.93 %
Epoch 429 of 2000 took 0.099s
  training loss:		2.299371
  validation loss:		2.248214
  validation accuracy:		13.04 %
Epoch 430 of 2000 took 0.099s
  training loss:		2.299389
  validation loss:		2.248255
  validation accuracy:		13.04 %
Epoch 431 of 2000 took 0.099s
  training loss:		2.298864
  validation loss:		2.248300
  validation accuracy:		13.04 %
Epoch 432 of 2000 took 0.099s
  training loss:		2.299409
  validation loss:		2.248243
  validation accuracy:		13.04 %
Epoch 433 of 2000 took 0.099s
  training loss:		2.298702
  validation loss:		2.248344
  validation accuracy:		13.04 %
Epoch 434 of 2000 took 0.099s
  training loss:		2.298447
  validation loss:		2.248542
  validation accuracy:		12.93 %
Epoch 435 of 2000 took 0.099s
  training loss:		2.298927
  validation loss:		2.248361
  validation accuracy:		12.93 %
Epoch 436 of 2000 took 0.099s
  training loss:		2.298193
  validation loss:		2.248175
  validation accuracy:		12.93 %
Epoch 437 of 2000 took 0.099s
  training loss:		2.298311
  validation loss:		2.248004
  validation accuracy:		12.93 %
Epoch 438 of 2000 took 0.099s
  training loss:		2.298971
  validation loss:		2.247936
  validation accuracy:		12.93 %
Epoch 439 of 2000 took 0.099s
  training loss:		2.297864
  validation loss:		2.247797
  validation accuracy:		12.93 %
Epoch 440 of 2000 took 0.100s
  training loss:		2.299416
  validation loss:		2.247708
  validation accuracy:		12.93 %
Epoch 441 of 2000 took 0.100s
  training loss:		2.299028
  validation loss:		2.247638
  validation accuracy:		12.93 %
Epoch 442 of 2000 took 0.096s
  training loss:		2.298220
  validation loss:		2.247899
  validation accuracy:		12.93 %
Epoch 443 of 2000 took 0.096s
  training loss:		2.298786
  validation loss:		2.248055
  validation accuracy:		12.93 %
Epoch 444 of 2000 took 0.096s
  training loss:		2.298654
  validation loss:		2.247875
  validation accuracy:		12.93 %
Epoch 445 of 2000 took 0.096s
  training loss:		2.297771
  validation loss:		2.247803
  validation accuracy:		13.04 %
Epoch 446 of 2000 took 0.096s
  training loss:		2.298982
  validation loss:		2.248003
  validation accuracy:		13.04 %
Epoch 447 of 2000 took 0.096s
  training loss:		2.298526
  validation loss:		2.247640
  validation accuracy:		12.93 %
Epoch 448 of 2000 took 0.096s
  training loss:		2.298746
  validation loss:		2.247861
  validation accuracy:		13.04 %
Epoch 449 of 2000 took 0.096s
  training loss:		2.297517
  validation loss:		2.247964
  validation accuracy:		13.04 %
Epoch 450 of 2000 took 0.096s
  training loss:		2.298995
  validation loss:		2.247961
  validation accuracy:		13.04 %
Epoch 451 of 2000 took 0.096s
  training loss:		2.297599
  validation loss:		2.247609
  validation accuracy:		13.04 %
Epoch 452 of 2000 took 0.096s
  training loss:		2.299124
  validation loss:		2.247721
  validation accuracy:		13.04 %
Epoch 453 of 2000 took 0.096s
  training loss:		2.298189
  validation loss:		2.247892
  validation accuracy:		12.93 %
Epoch 454 of 2000 took 0.096s
  training loss:		2.298844
  validation loss:		2.248153
  validation accuracy:		12.93 %
Epoch 455 of 2000 took 0.096s
  training loss:		2.298044
  validation loss:		2.248031
  validation accuracy:		12.93 %
Epoch 456 of 2000 took 0.099s
  training loss:		2.298216
  validation loss:		2.248117
  validation accuracy:		12.93 %
Epoch 457 of 2000 took 0.096s
  training loss:		2.297716
  validation loss:		2.247889
  validation accuracy:		13.04 %
Epoch 458 of 2000 took 0.096s
  training loss:		2.297626
  validation loss:		2.248042
  validation accuracy:		13.04 %
Epoch 459 of 2000 took 0.096s
  training loss:		2.297906
  validation loss:		2.247479
  validation accuracy:		12.93 %
Epoch 460 of 2000 took 0.096s
  training loss:		2.297499
  validation loss:		2.247372
  validation accuracy:		12.93 %
Epoch 461 of 2000 took 0.096s
  training loss:		2.298210
  validation loss:		2.247367
  validation accuracy:		12.93 %
Epoch 462 of 2000 took 0.096s
  training loss:		2.298448
  validation loss:		2.247183
  validation accuracy:		12.93 %
Epoch 463 of 2000 took 0.096s
  training loss:		2.298272
  validation loss:		2.247305
  validation accuracy:		12.93 %
Epoch 464 of 2000 took 0.096s
  training loss:		2.298986
  validation loss:		2.247295
  validation accuracy:		12.93 %
Epoch 465 of 2000 took 0.096s
  training loss:		2.297843
  validation loss:		2.247366
  validation accuracy:		12.93 %
Epoch 466 of 2000 took 0.096s
  training loss:		2.298393
  validation loss:		2.247511
  validation accuracy:		13.04 %
Epoch 467 of 2000 took 0.096s
  training loss:		2.297719
  validation loss:		2.247688
  validation accuracy:		12.93 %
Epoch 468 of 2000 took 0.096s
  training loss:		2.298232
  validation loss:		2.247790
  validation accuracy:		12.93 %
Epoch 469 of 2000 took 0.096s
  training loss:		2.298360
  validation loss:		2.247800
  validation accuracy:		12.93 %
Epoch 470 of 2000 took 0.096s
  training loss:		2.297742
  validation loss:		2.247505
  validation accuracy:		12.93 %
Epoch 471 of 2000 took 0.096s
  training loss:		2.297775
  validation loss:		2.247349
  validation accuracy:		12.93 %
Epoch 472 of 2000 took 0.096s
  training loss:		2.297338
  validation loss:		2.247074
  validation accuracy:		13.04 %
Epoch 473 of 2000 took 0.096s
  training loss:		2.296746
  validation loss:		2.246666
  validation accuracy:		13.04 %
Epoch 474 of 2000 took 0.096s
  training loss:		2.298164
  validation loss:		2.246714
  validation accuracy:		12.93 %
Epoch 475 of 2000 took 0.096s
  training loss:		2.297009
  validation loss:		2.246213
  validation accuracy:		13.04 %
Epoch 476 of 2000 took 0.096s
  training loss:		2.298298
  validation loss:		2.246027
  validation accuracy:		13.04 %
Epoch 477 of 2000 took 0.096s
  training loss:		2.297563
  validation loss:		2.246463
  validation accuracy:		13.04 %
Epoch 478 of 2000 took 0.096s
  training loss:		2.296943
  validation loss:		2.246363
  validation accuracy:		12.93 %
Epoch 479 of 2000 took 0.096s
  training loss:		2.297306
  validation loss:		2.246313
  validation accuracy:		12.93 %
Epoch 480 of 2000 took 0.096s
  training loss:		2.297271
  validation loss:		2.246276
  validation accuracy:		12.93 %
Epoch 481 of 2000 took 0.096s
  training loss:		2.297786
  validation loss:		2.246368
  validation accuracy:		12.93 %
Epoch 482 of 2000 took 0.096s
  training loss:		2.297481
  validation loss:		2.246454
  validation accuracy:		12.93 %
Epoch 483 of 2000 took 0.096s
  training loss:		2.298569
  validation loss:		2.246546
  validation accuracy:		12.93 %
Epoch 484 of 2000 took 0.096s
  training loss:		2.297927
  validation loss:		2.246979
  validation accuracy:		12.93 %
Epoch 485 of 2000 took 0.096s
  training loss:		2.297790
  validation loss:		2.247215
  validation accuracy:		12.93 %
Epoch 486 of 2000 took 0.096s
  training loss:		2.298193
  validation loss:		2.247064
  validation accuracy:		12.93 %
Epoch 487 of 2000 took 0.097s
  training loss:		2.297719
  validation loss:		2.247277
  validation accuracy:		12.93 %
Epoch 488 of 2000 took 0.096s
  training loss:		2.297375
  validation loss:		2.247229
  validation accuracy:		12.93 %
Epoch 489 of 2000 took 0.096s
  training loss:		2.297685
  validation loss:		2.247291
  validation accuracy:		12.93 %
Epoch 490 of 2000 took 0.096s
  training loss:		2.298372
  validation loss:		2.247321
  validation accuracy:		12.93 %
Epoch 491 of 2000 took 0.096s
  training loss:		2.298452
  validation loss:		2.247443
  validation accuracy:		12.93 %
Epoch 492 of 2000 took 0.096s
  training loss:		2.297763
  validation loss:		2.247424
  validation accuracy:		13.04 %
Epoch 493 of 2000 took 0.096s
  training loss:		2.297434
  validation loss:		2.247318
  validation accuracy:		13.04 %
Epoch 494 of 2000 took 0.096s
  training loss:		2.298017
  validation loss:		2.247209
  validation accuracy:		13.04 %
Epoch 495 of 2000 took 0.098s
  training loss:		2.297719
  validation loss:		2.247574
  validation accuracy:		13.04 %
Epoch 496 of 2000 took 0.096s
  training loss:		2.297475
  validation loss:		2.247526
  validation accuracy:		12.93 %
Epoch 497 of 2000 took 0.096s
  training loss:		2.297284
  validation loss:		2.247636
  validation accuracy:		12.93 %
Epoch 498 of 2000 took 0.096s
  training loss:		2.297762
  validation loss:		2.247534
  validation accuracy:		12.93 %
Epoch 499 of 2000 took 0.096s
  training loss:		2.297057
  validation loss:		2.247504
  validation accuracy:		13.04 %
Epoch 500 of 2000 took 0.096s
  training loss:		2.297177
  validation loss:		2.247019
  validation accuracy:		13.04 %
Epoch 501 of 2000 took 0.096s
  training loss:		2.296918
  validation loss:		2.246776
  validation accuracy:		13.04 %
Epoch 502 of 2000 took 0.096s
  training loss:		2.297213
  validation loss:		2.246667
  validation accuracy:		13.04 %
Epoch 503 of 2000 took 0.096s
  training loss:		2.296993
  validation loss:		2.246632
  validation accuracy:		13.04 %
Epoch 504 of 2000 took 0.096s
  training loss:		2.297295
  validation loss:		2.246641
  validation accuracy:		13.04 %
Epoch 505 of 2000 took 0.096s
  training loss:		2.297831
  validation loss:		2.246664
  validation accuracy:		12.93 %
Epoch 506 of 2000 took 0.096s
  training loss:		2.297621
  validation loss:		2.246757
  validation accuracy:		12.93 %
Epoch 507 of 2000 took 0.096s
  training loss:		2.296436
  validation loss:		2.246763
  validation accuracy:		12.93 %
Epoch 508 of 2000 took 0.096s
  training loss:		2.297890
  validation loss:		2.246791
  validation accuracy:		12.93 %
Epoch 509 of 2000 took 0.096s
  training loss:		2.297200
  validation loss:		2.246877
  validation accuracy:		13.04 %
Epoch 510 of 2000 took 0.096s
  training loss:		2.297540
  validation loss:		2.246904
  validation accuracy:		12.93 %
Epoch 511 of 2000 took 0.096s
  training loss:		2.296655
  validation loss:		2.246842
  validation accuracy:		12.93 %
Epoch 512 of 2000 took 0.096s
  training loss:		2.297954
  validation loss:		2.246781
  validation accuracy:		12.93 %
Epoch 513 of 2000 took 0.096s
  training loss:		2.297195
  validation loss:		2.246820
  validation accuracy:		12.93 %
Epoch 514 of 2000 took 0.096s
  training loss:		2.297735
  validation loss:		2.246552
  validation accuracy:		12.93 %
Epoch 515 of 2000 took 0.096s
  training loss:		2.297577
  validation loss:		2.246852
  validation accuracy:		13.04 %
Epoch 516 of 2000 took 0.096s
  training loss:		2.296805
  validation loss:		2.246970
  validation accuracy:		13.04 %
Epoch 517 of 2000 took 0.096s
  training loss:		2.297185
  validation loss:		2.246751
  validation accuracy:		12.93 %
Epoch 518 of 2000 took 0.097s
  training loss:		2.297221
  validation loss:		2.246599
  validation accuracy:		13.04 %
Epoch 519 of 2000 took 0.096s
  training loss:		2.296791
  validation loss:		2.246583
  validation accuracy:		13.04 %
Epoch 520 of 2000 took 0.096s
  training loss:		2.297267
  validation loss:		2.246466
  validation accuracy:		12.93 %
Epoch 521 of 2000 took 0.096s
  training loss:		2.297976
  validation loss:		2.246483
  validation accuracy:		12.93 %
Epoch 522 of 2000 took 0.096s
  training loss:		2.296620
  validation loss:		2.246512
  validation accuracy:		12.93 %
Epoch 523 of 2000 took 0.096s
  training loss:		2.297174
  validation loss:		2.246082
  validation accuracy:		12.93 %
Epoch 524 of 2000 took 0.096s
  training loss:		2.296518
  validation loss:		2.246158
  validation accuracy:		12.93 %
Epoch 525 of 2000 took 0.096s
  training loss:		2.296378
  validation loss:		2.246220
  validation accuracy:		12.93 %
Epoch 526 of 2000 took 0.096s
  training loss:		2.296422
  validation loss:		2.246228
  validation accuracy:		12.93 %
Epoch 527 of 2000 took 0.096s
  training loss:		2.298191
  validation loss:		2.246557
  validation accuracy:		12.93 %
Epoch 528 of 2000 took 0.096s
  training loss:		2.297224
  validation loss:		2.246523
  validation accuracy:		12.93 %
Epoch 529 of 2000 took 0.096s
  training loss:		2.297260
  validation loss:		2.246632
  validation accuracy:		12.93 %
Epoch 530 of 2000 took 0.096s
  training loss:		2.296267
  validation loss:		2.246691
  validation accuracy:		12.93 %
Epoch 531 of 2000 took 0.096s
  training loss:		2.297267
  validation loss:		2.246373
  validation accuracy:		12.93 %
Epoch 532 of 2000 took 0.096s
  training loss:		2.296908
  validation loss:		2.246436
  validation accuracy:		12.93 %
Epoch 533 of 2000 took 0.096s
  training loss:		2.297480
  validation loss:		2.246793
  validation accuracy:		12.93 %
Epoch 534 of 2000 took 0.096s
  training loss:		2.297818
  validation loss:		2.247225
  validation accuracy:		13.04 %
Epoch 535 of 2000 took 0.096s
  training loss:		2.296811
  validation loss:		2.247284
  validation accuracy:		13.04 %
Epoch 536 of 2000 took 0.096s
  training loss:		2.295958
  validation loss:		2.247063
  validation accuracy:		13.04 %
Epoch 537 of 2000 took 0.096s
  training loss:		2.297128
  validation loss:		2.246815
  validation accuracy:		13.04 %
Epoch 538 of 2000 took 0.098s
  training loss:		2.296822
  validation loss:		2.246837
  validation accuracy:		13.04 %
Epoch 539 of 2000 took 0.096s
  training loss:		2.296471
  validation loss:		2.246540
  validation accuracy:		13.04 %
Epoch 540 of 2000 took 0.096s
  training loss:		2.297036
  validation loss:		2.246367
  validation accuracy:		12.93 %
Epoch 541 of 2000 took 0.096s
  training loss:		2.297689
  validation loss:		2.246184
  validation accuracy:		12.93 %
Epoch 542 of 2000 took 0.096s
  training loss:		2.296215
  validation loss:		2.245802
  validation accuracy:		13.04 %
Epoch 543 of 2000 took 0.096s
  training loss:		2.296954
  validation loss:		2.245813
  validation accuracy:		13.04 %
Epoch 544 of 2000 took 0.096s
  training loss:		2.297457
  validation loss:		2.246003
  validation accuracy:		12.93 %
Epoch 545 of 2000 took 0.096s
  training loss:		2.297011
  validation loss:		2.246539
  validation accuracy:		12.93 %
Epoch 546 of 2000 took 0.096s
  training loss:		2.297396
  validation loss:		2.246468
  validation accuracy:		12.93 %
Epoch 547 of 2000 took 0.096s
  training loss:		2.296740
  validation loss:		2.246657
  validation accuracy:		12.93 %
Epoch 548 of 2000 took 0.096s
  training loss:		2.296432
  validation loss:		2.246219
  validation accuracy:		12.93 %
Epoch 549 of 2000 took 0.096s
  training loss:		2.296988
  validation loss:		2.246356
  validation accuracy:		12.93 %
Epoch 550 of 2000 took 0.097s
  training loss:		2.296187
  validation loss:		2.246437
  validation accuracy:		12.83 %
Epoch 551 of 2000 took 0.096s
  training loss:		2.296820
  validation loss:		2.246803
  validation accuracy:		13.04 %
Epoch 552 of 2000 took 0.096s
  training loss:		2.297124
  validation loss:		2.246586
  validation accuracy:		13.04 %
Epoch 553 of 2000 took 0.096s
  training loss:		2.296940
  validation loss:		2.246299
  validation accuracy:		13.04 %
Epoch 554 of 2000 took 0.096s
  training loss:		2.297613
  validation loss:		2.246218
  validation accuracy:		13.04 %
Epoch 555 of 2000 took 0.096s
  training loss:		2.296485
  validation loss:		2.246396
  validation accuracy:		13.04 %
Epoch 556 of 2000 took 0.096s
  training loss:		2.296108
  validation loss:		2.246460
  validation accuracy:		13.04 %
Epoch 557 of 2000 took 0.096s
  training loss:		2.296620
  validation loss:		2.246104
  validation accuracy:		12.93 %
Epoch 558 of 2000 took 0.096s
  training loss:		2.296791
  validation loss:		2.246172
  validation accuracy:		13.04 %
Epoch 559 of 2000 took 0.096s
  training loss:		2.295951
  validation loss:		2.246332
  validation accuracy:		13.04 %
Epoch 560 of 2000 took 0.096s
  training loss:		2.296730
  validation loss:		2.246135
  validation accuracy:		13.04 %
Epoch 561 of 2000 took 0.096s
  training loss:		2.297096
  validation loss:		2.246048
  validation accuracy:		13.04 %
Epoch 562 of 2000 took 0.096s
  training loss:		2.296258
  validation loss:		2.246060
  validation accuracy:		12.93 %
Epoch 563 of 2000 took 0.096s
  training loss:		2.297210
  validation loss:		2.245923
  validation accuracy:		12.93 %
Epoch 564 of 2000 took 0.096s
  training loss:		2.295979
  validation loss:		2.246160
  validation accuracy:		12.93 %
Epoch 565 of 2000 took 0.096s
  training loss:		2.296959
  validation loss:		2.246124
  validation accuracy:		12.93 %
Epoch 566 of 2000 took 0.096s
  training loss:		2.296819
  validation loss:		2.246199
  validation accuracy:		12.83 %
Epoch 567 of 2000 took 0.096s
  training loss:		2.297019
  validation loss:		2.246125
  validation accuracy:		12.93 %
Epoch 568 of 2000 took 0.096s
  training loss:		2.297409
  validation loss:		2.246720
  validation accuracy:		12.93 %
Epoch 569 of 2000 took 0.096s
  training loss:		2.295025
  validation loss:		2.246744
  validation accuracy:		13.04 %
Epoch 570 of 2000 took 0.096s
  training loss:		2.296736
  validation loss:		2.246462
  validation accuracy:		13.04 %
Epoch 571 of 2000 took 0.096s
  training loss:		2.295734
  validation loss:		2.246630
  validation accuracy:		13.04 %
Epoch 572 of 2000 took 0.096s
  training loss:		2.296435
  validation loss:		2.246353
  validation accuracy:		13.04 %
Epoch 573 of 2000 took 0.096s
  training loss:		2.297080
  validation loss:		2.246241
  validation accuracy:		12.93 %
Epoch 574 of 2000 took 0.096s
  training loss:		2.295519
  validation loss:		2.246316
  validation accuracy:		12.93 %
Epoch 575 of 2000 took 0.096s
  training loss:		2.296939
  validation loss:		2.246319
  validation accuracy:		13.04 %
Epoch 576 of 2000 took 0.096s
  training loss:		2.296769
  validation loss:		2.246431
  validation accuracy:		12.93 %
Epoch 577 of 2000 took 0.096s
  training loss:		2.296595
  validation loss:		2.246716
  validation accuracy:		12.93 %
Epoch 578 of 2000 took 0.096s
  training loss:		2.295420
  validation loss:		2.246573
  validation accuracy:		12.93 %
Epoch 579 of 2000 took 0.097s
  training loss:		2.296299
  validation loss:		2.245923
  validation accuracy:		12.93 %
Epoch 580 of 2000 took 0.096s
  training loss:		2.296919
  validation loss:		2.246198
  validation accuracy:		12.93 %
Epoch 581 of 2000 took 0.097s
  training loss:		2.296240
  validation loss:		2.246074
  validation accuracy:		12.93 %
Epoch 582 of 2000 took 0.096s
  training loss:		2.297291
  validation loss:		2.246570
  validation accuracy:		12.93 %
Epoch 583 of 2000 took 0.096s
  training loss:		2.296038
  validation loss:		2.246471
  validation accuracy:		12.93 %
Epoch 584 of 2000 took 0.096s
  training loss:		2.296681
  validation loss:		2.246009
  validation accuracy:		12.93 %
Epoch 585 of 2000 took 0.098s
  training loss:		2.296362
  validation loss:		2.245902
  validation accuracy:		12.93 %
Epoch 586 of 2000 took 0.096s
  training loss:		2.296263
  validation loss:		2.245992
  validation accuracy:		12.93 %
Epoch 587 of 2000 took 0.096s
  training loss:		2.295087
  validation loss:		2.245361
  validation accuracy:		12.93 %
Epoch 588 of 2000 took 0.096s
  training loss:		2.295907
  validation loss:		2.245293
  validation accuracy:		12.93 %
Epoch 589 of 2000 took 0.096s
  training loss:		2.295711
  validation loss:		2.245349
  validation accuracy:		12.93 %
Epoch 590 of 2000 took 0.096s
  training loss:		2.296131
  validation loss:		2.245303
  validation accuracy:		12.93 %
Epoch 591 of 2000 took 0.096s
  training loss:		2.296535
  validation loss:		2.245115
  validation accuracy:		12.93 %
Epoch 592 of 2000 took 0.096s
  training loss:		2.295306
  validation loss:		2.245052
  validation accuracy:		12.93 %
Epoch 593 of 2000 took 0.096s
  training loss:		2.296483
  validation loss:		2.245305
  validation accuracy:		12.93 %
Epoch 594 of 2000 took 0.096s
  training loss:		2.296712
  validation loss:		2.245448
  validation accuracy:		12.93 %
Epoch 595 of 2000 took 0.096s
  training loss:		2.296962
  validation loss:		2.246173
  validation accuracy:		13.04 %
Epoch 596 of 2000 took 0.096s
  training loss:		2.296776
  validation loss:		2.246511
  validation accuracy:		12.83 %
Epoch 597 of 2000 took 0.096s
  training loss:		2.296920
  validation loss:		2.246700
  validation accuracy:		13.04 %
Epoch 598 of 2000 took 0.096s
  training loss:		2.296364
  validation loss:		2.246854
  validation accuracy:		12.93 %
Epoch 599 of 2000 took 0.096s
  training loss:		2.296394
  validation loss:		2.247065
  validation accuracy:		13.04 %
Epoch 600 of 2000 took 0.096s
  training loss:		2.297467
  validation loss:		2.247004
  validation accuracy:		13.04 %
Epoch 601 of 2000 took 0.097s
  training loss:		2.297261
  validation loss:		2.246954
  validation accuracy:		13.04 %
Epoch 602 of 2000 took 0.102s
  training loss:		2.295079
  validation loss:		2.247068
  validation accuracy:		13.04 %
Epoch 603 of 2000 took 0.118s
  training loss:		2.296154
  validation loss:		2.246776
  validation accuracy:		13.04 %
Epoch 604 of 2000 took 0.126s
  training loss:		2.296202
  validation loss:		2.246430
  validation accuracy:		13.04 %
Epoch 605 of 2000 took 0.097s
  training loss:		2.296712
  validation loss:		2.246422
  validation accuracy:		13.04 %
Epoch 606 of 2000 took 0.098s
  training loss:		2.296408
  validation loss:		2.246521
  validation accuracy:		13.04 %
Epoch 607 of 2000 took 0.100s
  training loss:		2.296460
  validation loss:		2.246345
  validation accuracy:		13.04 %
Epoch 608 of 2000 took 0.123s
  training loss:		2.296968
  validation loss:		2.246519
  validation accuracy:		13.04 %
Epoch 609 of 2000 took 0.099s
  training loss:		2.295112
  validation loss:		2.246562
  validation accuracy:		13.04 %
Epoch 610 of 2000 took 0.099s
  training loss:		2.296395
  validation loss:		2.245925
  validation accuracy:		13.04 %
Epoch 611 of 2000 took 0.098s
  training loss:		2.295700
  validation loss:		2.245882
  validation accuracy:		13.04 %
Epoch 612 of 2000 took 0.096s
  training loss:		2.296510
  validation loss:		2.246028
  validation accuracy:		13.04 %
Epoch 613 of 2000 took 0.096s
  training loss:		2.296079
  validation loss:		2.246150
  validation accuracy:		13.04 %
Epoch 614 of 2000 took 0.096s
  training loss:		2.295580
  validation loss:		2.245933
  validation accuracy:		12.93 %
Epoch 615 of 2000 took 0.096s
  training loss:		2.296103
  validation loss:		2.245548
  validation accuracy:		13.04 %
Epoch 616 of 2000 took 0.096s
  training loss:		2.296476
  validation loss:		2.245707
  validation accuracy:		13.04 %
Epoch 617 of 2000 took 0.096s
  training loss:		2.296224
  validation loss:		2.245956
  validation accuracy:		13.04 %
Epoch 618 of 2000 took 0.096s
  training loss:		2.295657
  validation loss:		2.245947
  validation accuracy:		13.04 %
Epoch 619 of 2000 took 0.096s
  training loss:		2.296584
  validation loss:		2.245803
  validation accuracy:		13.04 %
Epoch 620 of 2000 took 0.096s
  training loss:		2.295356
  validation loss:		2.245833
  validation accuracy:		13.04 %
Epoch 621 of 2000 took 0.096s
  training loss:		2.296356
  validation loss:		2.245920
  validation accuracy:		12.93 %
Epoch 622 of 2000 took 0.096s
  training loss:		2.296512
  validation loss:		2.245965
  validation accuracy:		12.93 %
Epoch 623 of 2000 took 0.096s
  training loss:		2.296157
  validation loss:		2.246092
  validation accuracy:		12.93 %
Epoch 624 of 2000 took 0.096s
  training loss:		2.297066
  validation loss:		2.246469
  validation accuracy:		12.93 %
Epoch 625 of 2000 took 0.096s
  training loss:		2.296466
  validation loss:		2.246623
  validation accuracy:		12.93 %
Epoch 626 of 2000 took 0.096s
  training loss:		2.296114
  validation loss:		2.246527
  validation accuracy:		13.04 %
Epoch 627 of 2000 took 0.096s
  training loss:		2.295955
  validation loss:		2.246185
  validation accuracy:		13.04 %
Epoch 628 of 2000 took 0.096s
  training loss:		2.296069
  validation loss:		2.246464
  validation accuracy:		13.04 %
Epoch 629 of 2000 took 0.096s
  training loss:		2.296219
  validation loss:		2.246301
  validation accuracy:		13.04 %
Epoch 630 of 2000 took 0.096s
  training loss:		2.295638
  validation loss:		2.246329
  validation accuracy:		12.93 %
Epoch 631 of 2000 took 0.096s
  training loss:		2.296630
  validation loss:		2.246413
  validation accuracy:		12.93 %
Epoch 632 of 2000 took 0.096s
  training loss:		2.296217
  validation loss:		2.246337
  validation accuracy:		12.93 %
Epoch 633 of 2000 took 0.096s
  training loss:		2.296152
  validation loss:		2.246197
  validation accuracy:		13.04 %
Epoch 634 of 2000 took 0.096s
  training loss:		2.296588
  validation loss:		2.246038
  validation accuracy:		13.04 %
Epoch 635 of 2000 took 0.096s
  training loss:		2.296890
  validation loss:		2.246375
  validation accuracy:		13.04 %
Epoch 636 of 2000 took 0.098s
  training loss:		2.294882
  validation loss:		2.246136
  validation accuracy:		13.04 %
Epoch 637 of 2000 took 0.096s
  training loss:		2.296446
  validation loss:		2.246222
  validation accuracy:		12.93 %
Epoch 638 of 2000 took 0.096s
  training loss:		2.295348
  validation loss:		2.246059
  validation accuracy:		13.04 %
Epoch 639 of 2000 took 0.096s
  training loss:		2.295454
  validation loss:		2.245731
  validation accuracy:		13.04 %
Epoch 640 of 2000 took 0.096s
  training loss:		2.296174
  validation loss:		2.245968
  validation accuracy:		13.04 %
Epoch 641 of 2000 took 0.096s
  training loss:		2.295503
  validation loss:		2.245810
  validation accuracy:		13.04 %
Epoch 642 of 2000 took 0.098s
  training loss:		2.296162
  validation loss:		2.245707
  validation accuracy:		13.04 %
Epoch 643 of 2000 took 0.098s
  training loss:		2.296415
  validation loss:		2.246188
  validation accuracy:		13.04 %
Epoch 644 of 2000 took 0.117s
  training loss:		2.296254
  validation loss:		2.245902
  validation accuracy:		13.04 %
Epoch 645 of 2000 took 0.156s
  training loss:		2.296092
  validation loss:		2.246065
  validation accuracy:		12.93 %
Epoch 646 of 2000 took 0.158s
  training loss:		2.295249
  validation loss:		2.245651
  validation accuracy:		12.93 %
Epoch 647 of 2000 took 0.140s
  training loss:		2.295124
  validation loss:		2.245182
  validation accuracy:		12.93 %
Epoch 648 of 2000 took 0.156s
  training loss:		2.296078
  validation loss:		2.245016
  validation accuracy:		12.93 %
Epoch 649 of 2000 took 0.156s
  training loss:		2.296181
  validation loss:		2.245168
  validation accuracy:		12.93 %
Epoch 650 of 2000 took 0.156s
  training loss:		2.295006
  validation loss:		2.245388
  validation accuracy:		12.93 %
Epoch 651 of 2000 took 0.154s
  training loss:		2.295041
  validation loss:		2.245344
  validation accuracy:		12.93 %
Epoch 652 of 2000 took 0.156s
  training loss:		2.296488
  validation loss:		2.245239
  validation accuracy:		12.93 %
Epoch 653 of 2000 took 0.156s
  training loss:		2.296452
  validation loss:		2.245510
  validation accuracy:		12.93 %
Epoch 654 of 2000 took 0.156s
  training loss:		2.295327
  validation loss:		2.245581
  validation accuracy:		12.93 %
Epoch 655 of 2000 took 0.161s
  training loss:		2.295817
  validation loss:		2.245225
  validation accuracy:		12.83 %
Epoch 656 of 2000 took 0.159s
  training loss:		2.295327
  validation loss:		2.244989
  validation accuracy:		12.93 %
Epoch 657 of 2000 took 0.156s
  training loss:		2.296357
  validation loss:		2.245336
  validation accuracy:		12.83 %
Epoch 658 of 2000 took 0.156s
  training loss:		2.297046
  validation loss:		2.245777
  validation accuracy:		13.04 %
Epoch 659 of 2000 took 0.156s
  training loss:		2.295664
  validation loss:		2.245736
  validation accuracy:		12.83 %
Epoch 660 of 2000 took 0.156s
  training loss:		2.296740
  validation loss:		2.246064
  validation accuracy:		12.93 %
Epoch 661 of 2000 took 0.156s
  training loss:		2.296554
  validation loss:		2.246131
  validation accuracy:		12.93 %
Epoch 662 of 2000 took 0.156s
  training loss:		2.295474
  validation loss:		2.246138
  validation accuracy:		12.93 %
Epoch 663 of 2000 took 0.165s
  training loss:		2.295463
  validation loss:		2.245988
  validation accuracy:		12.93 %
Epoch 664 of 2000 took 0.156s
  training loss:		2.295623
  validation loss:		2.246033
  validation accuracy:		12.93 %
Epoch 665 of 2000 took 0.156s
  training loss:		2.296278
  validation loss:		2.246219
  validation accuracy:		12.93 %
Epoch 666 of 2000 took 0.156s
  training loss:		2.296267
  validation loss:		2.246251
  validation accuracy:		12.93 %
Epoch 667 of 2000 took 0.156s
  training loss:		2.295561
  validation loss:		2.246138
  validation accuracy:		12.93 %
Epoch 668 of 2000 took 0.161s
  training loss:		2.295428
  validation loss:		2.246422
  validation accuracy:		12.93 %
Epoch 669 of 2000 took 0.160s
  training loss:		2.295575
  validation loss:		2.245892
  validation accuracy:		12.93 %
Epoch 670 of 2000 took 0.156s
  training loss:		2.295553
  validation loss:		2.245628
  validation accuracy:		12.83 %
Epoch 671 of 2000 took 0.156s
  training loss:		2.295332
  validation loss:		2.245341
  validation accuracy:		12.83 %
Epoch 672 of 2000 took 0.155s
  training loss:		2.296206
  validation loss:		2.245004
  validation accuracy:		12.93 %
Epoch 673 of 2000 took 0.156s
  training loss:		2.295606
  validation loss:		2.244830
  validation accuracy:		13.04 %
Epoch 674 of 2000 took 0.160s
  training loss:		2.295629
  validation loss:		2.244863
  validation accuracy:		12.93 %
Epoch 675 of 2000 took 0.156s
  training loss:		2.295705
  validation loss:		2.245078
  validation accuracy:		13.04 %
Epoch 676 of 2000 took 0.157s
  training loss:		2.295778
  validation loss:		2.245634
  validation accuracy:		12.93 %
Epoch 677 of 2000 took 0.156s
  training loss:		2.296248
  validation loss:		2.245364
  validation accuracy:		13.04 %
Epoch 678 of 2000 took 0.156s
  training loss:		2.295728
  validation loss:		2.245329
  validation accuracy:		12.93 %
Epoch 679 of 2000 took 0.161s
  training loss:		2.295484
  validation loss:		2.245583
  validation accuracy:		12.93 %
Epoch 680 of 2000 took 0.157s
  training loss:		2.296141
  validation loss:		2.245604
  validation accuracy:		12.93 %
Epoch 681 of 2000 took 0.161s
  training loss:		2.295599
  validation loss:		2.245193
  validation accuracy:		12.93 %
Epoch 682 of 2000 took 0.158s
  training loss:		2.294895
  validation loss:		2.244998
  validation accuracy:		12.93 %
Epoch 683 of 2000 took 0.156s
  training loss:		2.296265
  validation loss:		2.244655
  validation accuracy:		12.93 %
Epoch 684 of 2000 took 0.156s
  training loss:		2.295234
  validation loss:		2.244633
  validation accuracy:		12.93 %
Epoch 685 of 2000 took 0.153s
  training loss:		2.294723
  validation loss:		2.244325
  validation accuracy:		12.93 %
Epoch 686 of 2000 took 0.156s
  training loss:		2.294868
  validation loss:		2.244199
  validation accuracy:		12.93 %
Epoch 687 of 2000 took 0.156s
  training loss:		2.295902
  validation loss:		2.244351
  validation accuracy:		13.04 %
Epoch 688 of 2000 took 0.156s
  training loss:		2.296025
  validation loss:		2.244863
  validation accuracy:		13.04 %
Epoch 689 of 2000 took 0.158s
  training loss:		2.294954
  validation loss:		2.244624
  validation accuracy:		13.04 %
Epoch 690 of 2000 took 0.141s
  training loss:		2.295896
  validation loss:		2.245163
  validation accuracy:		13.04 %
Epoch 691 of 2000 took 0.156s
  training loss:		2.294846
  validation loss:		2.245180
  validation accuracy:		13.04 %
Epoch 692 of 2000 took 0.156s
  training loss:		2.295564
  validation loss:		2.244793
  validation accuracy:		13.04 %
Epoch 693 of 2000 took 0.156s
  training loss:		2.296104
  validation loss:		2.244777
  validation accuracy:		13.04 %
Epoch 694 of 2000 took 0.156s
  training loss:		2.295327
  validation loss:		2.244875
  validation accuracy:		13.04 %
Epoch 695 of 2000 took 0.156s
  training loss:		2.295591
  validation loss:		2.244749
  validation accuracy:		13.04 %
Epoch 696 of 2000 took 0.156s
  training loss:		2.295936
  validation loss:		2.245208
  validation accuracy:		12.93 %
Epoch 697 of 2000 took 0.156s
  training loss:		2.295289
  validation loss:		2.245301
  validation accuracy:		12.93 %
Epoch 698 of 2000 took 0.156s
  training loss:		2.294483
  validation loss:		2.245341
  validation accuracy:		12.93 %
Epoch 699 of 2000 took 0.161s
  training loss:		2.295664
  validation loss:		2.245082
  validation accuracy:		12.93 %
Epoch 700 of 2000 took 0.159s
  training loss:		2.295356
  validation loss:		2.244959
  validation accuracy:		12.93 %
Epoch 701 of 2000 took 0.157s
  training loss:		2.296070
  validation loss:		2.244706
  validation accuracy:		12.93 %
Epoch 702 of 2000 took 0.156s
  training loss:		2.295241
  validation loss:		2.244837
  validation accuracy:		12.93 %
Epoch 703 of 2000 took 0.156s
  training loss:		2.295670
  validation loss:		2.244562
  validation accuracy:		13.04 %
Epoch 704 of 2000 took 0.156s
  training loss:		2.295607
  validation loss:		2.244514
  validation accuracy:		13.04 %
Epoch 705 of 2000 took 0.143s
  training loss:		2.295886
  validation loss:		2.244811
  validation accuracy:		13.04 %
Epoch 706 of 2000 took 0.147s
  training loss:		2.296271
  validation loss:		2.244560
  validation accuracy:		13.04 %
Epoch 707 of 2000 took 0.157s
  training loss:		2.296060
  validation loss:		2.245468
  validation accuracy:		12.83 %
Epoch 708 of 2000 took 0.161s
  training loss:		2.296228
  validation loss:		2.245920
  validation accuracy:		12.83 %
Epoch 709 of 2000 took 0.158s
  training loss:		2.296212
  validation loss:		2.246123
  validation accuracy:		13.04 %
Epoch 710 of 2000 took 0.156s
  training loss:		2.295800
  validation loss:		2.246047
  validation accuracy:		13.04 %
Epoch 711 of 2000 took 0.156s
  training loss:		2.296100
  validation loss:		2.246434
  validation accuracy:		13.04 %
Epoch 712 of 2000 took 0.158s
  training loss:		2.295862
  validation loss:		2.246431
  validation accuracy:		13.04 %
Epoch 713 of 2000 took 0.156s
  training loss:		2.295267
  validation loss:		2.246287
  validation accuracy:		13.04 %
Epoch 714 of 2000 took 0.156s
  training loss:		2.295813
  validation loss:		2.246410
  validation accuracy:		13.04 %
Epoch 715 of 2000 took 0.156s
  training loss:		2.295245
  validation loss:		2.246714
  validation accuracy:		12.93 %
Epoch 716 of 2000 took 0.156s
  training loss:		2.295915
  validation loss:		2.246389
  validation accuracy:		12.93 %
Epoch 717 of 2000 took 0.156s
  training loss:		2.294904
  validation loss:		2.246142
  validation accuracy:		12.93 %
Epoch 718 of 2000 took 0.156s
  training loss:		2.296342
  validation loss:		2.246129
  validation accuracy:		12.83 %
Epoch 719 of 2000 took 0.156s
  training loss:		2.295678
  validation loss:		2.245867
  validation accuracy:		12.83 %
Epoch 720 of 2000 took 0.157s
  training loss:		2.295868
  validation loss:		2.245745
  validation accuracy:		13.04 %
Epoch 721 of 2000 took 0.162s
  training loss:		2.294674
  validation loss:		2.245444
  validation accuracy:		13.04 %
Epoch 722 of 2000 took 0.158s
  training loss:		2.295817
  validation loss:		2.245630
  validation accuracy:		12.93 %
Epoch 723 of 2000 took 0.156s
  training loss:		2.294834
  validation loss:		2.245642
  validation accuracy:		12.93 %
Epoch 724 of 2000 took 0.156s
  training loss:		2.295275
  validation loss:		2.245261
  validation accuracy:		12.93 %
Epoch 725 of 2000 took 0.156s
  training loss:		2.294923
  validation loss:		2.245020
  validation accuracy:		12.93 %
Epoch 726 of 2000 took 0.156s
  training loss:		2.295426
  validation loss:		2.244798
  validation accuracy:		12.93 %
Epoch 727 of 2000 took 0.156s
  training loss:		2.295078
  validation loss:		2.244526
  validation accuracy:		13.04 %
Epoch 728 of 2000 took 0.156s
  training loss:		2.295261
  validation loss:		2.244824
  validation accuracy:		13.04 %
Epoch 729 of 2000 took 0.156s
  training loss:		2.295524
  validation loss:		2.244812
  validation accuracy:		13.04 %
Epoch 730 of 2000 took 0.156s
  training loss:		2.294929
  validation loss:		2.244993
  validation accuracy:		13.04 %
Epoch 731 of 2000 took 0.150s
  training loss:		2.295151
  validation loss:		2.245140
  validation accuracy:		13.04 %
Epoch 732 of 2000 took 0.149s
  training loss:		2.295333
  validation loss:		2.244619
  validation accuracy:		13.04 %
Epoch 733 of 2000 took 0.156s
  training loss:		2.295184
  validation loss:		2.244176
  validation accuracy:		13.04 %
Epoch 734 of 2000 took 0.156s
  training loss:		2.294725
  validation loss:		2.243864
  validation accuracy:		13.04 %
Epoch 735 of 2000 took 0.156s
  training loss:		2.294384
  validation loss:		2.243801
  validation accuracy:		12.93 %
Epoch 736 of 2000 took 0.156s
  training loss:		2.295531
  validation loss:		2.243905
  validation accuracy:		13.04 %
Epoch 737 of 2000 took 0.156s
  training loss:		2.294934
  validation loss:		2.243879
  validation accuracy:		12.93 %
Epoch 738 of 2000 took 0.156s
  training loss:		2.295659
  validation loss:		2.243941
  validation accuracy:		12.93 %
Epoch 739 of 2000 took 0.156s
  training loss:		2.296115
  validation loss:		2.244115
  validation accuracy:		12.93 %
Epoch 740 of 2000 took 0.156s
  training loss:		2.294678
  validation loss:		2.244000
  validation accuracy:		12.93 %
Epoch 741 of 2000 took 0.156s
  training loss:		2.295595
  validation loss:		2.244378
  validation accuracy:		12.93 %
Epoch 742 of 2000 took 0.159s
  training loss:		2.295940
  validation loss:		2.244467
  validation accuracy:		12.93 %
Epoch 743 of 2000 took 0.161s
  training loss:		2.294885
  validation loss:		2.244431
  validation accuracy:		13.04 %
Epoch 744 of 2000 took 0.156s
  training loss:		2.294923
  validation loss:		2.244463
  validation accuracy:		13.04 %
Epoch 745 of 2000 took 0.156s
  training loss:		2.295388
  validation loss:		2.244252
  validation accuracy:		13.04 %
Epoch 746 of 2000 took 0.156s
  training loss:		2.295720
  validation loss:		2.244409
  validation accuracy:		13.04 %
Epoch 747 of 2000 took 0.156s
  training loss:		2.295432
  validation loss:		2.244641
  validation accuracy:		13.04 %
Epoch 748 of 2000 took 0.136s
  training loss:		2.296157
  validation loss:		2.244857
  validation accuracy:		13.04 %
Epoch 749 of 2000 took 0.161s
  training loss:		2.294989
  validation loss:		2.244701
  validation accuracy:		13.04 %
Epoch 750 of 2000 took 0.159s
  training loss:		2.294927
  validation loss:		2.244309
  validation accuracy:		13.04 %
Epoch 751 of 2000 took 0.156s
  training loss:		2.295853
  validation loss:		2.244539
  validation accuracy:		13.04 %
Epoch 752 of 2000 took 0.191s
  training loss:		2.296829
  validation loss:		2.244807
  validation accuracy:		13.04 %
Epoch 753 of 2000 took 0.199s
  training loss:		2.295102
  validation loss:		2.245211
  validation accuracy:		13.04 %
Epoch 754 of 2000 took 0.207s
  training loss:		2.295164
  validation loss:		2.245202
  validation accuracy:		13.04 %
Epoch 755 of 2000 took 0.192s
  training loss:		2.294731
  validation loss:		2.245063
  validation accuracy:		13.04 %
Epoch 756 of 2000 took 0.140s
  training loss:		2.295445
  validation loss:		2.245290
  validation accuracy:		13.04 %
Epoch 757 of 2000 took 0.164s
  training loss:		2.295453
  validation loss:		2.244856
  validation accuracy:		12.93 %
Epoch 758 of 2000 took 0.161s
  training loss:		2.295581
  validation loss:		2.244657
  validation accuracy:		12.93 %
Epoch 759 of 2000 took 0.161s
  training loss:		2.295161
  validation loss:		2.244926
  validation accuracy:		12.93 %
Epoch 760 of 2000 took 0.167s
  training loss:		2.295616
  validation loss:		2.244684
  validation accuracy:		13.04 %
Epoch 761 of 2000 took 0.159s
  training loss:		2.295568
  validation loss:		2.245114
  validation accuracy:		13.04 %
Epoch 762 of 2000 took 0.162s
  training loss:		2.295941
  validation loss:		2.245622
  validation accuracy:		13.04 %
Epoch 763 of 2000 took 0.160s
  training loss:		2.295241
  validation loss:		2.245641
  validation accuracy:		13.04 %
Epoch 764 of 2000 took 0.166s
  training loss:		2.294433
  validation loss:		2.245715
  validation accuracy:		13.04 %
Epoch 765 of 2000 took 0.169s
  training loss:		2.295708
  validation loss:		2.245142
  validation accuracy:		13.04 %
Epoch 766 of 2000 took 0.159s
  training loss:		2.295274
  validation loss:		2.244752
  validation accuracy:		13.04 %
Epoch 767 of 2000 took 0.162s
  training loss:		2.295865
  validation loss:		2.245061
  validation accuracy:		13.04 %
Epoch 768 of 2000 took 0.166s
  training loss:		2.294974
  validation loss:		2.245226
  validation accuracy:		13.04 %
Epoch 769 of 2000 took 0.158s
  training loss:		2.295584
  validation loss:		2.245160
  validation accuracy:		12.93 %
Epoch 770 of 2000 took 0.162s
  training loss:		2.295452
  validation loss:		2.245250
  validation accuracy:		12.93 %
Epoch 771 of 2000 took 0.159s
  training loss:		2.295936
  validation loss:		2.245314
  validation accuracy:		12.93 %
Epoch 772 of 2000 took 0.163s
  training loss:		2.295390
  validation loss:		2.245211
  validation accuracy:		12.93 %
Epoch 773 of 2000 took 0.165s
  training loss:		2.295514
  validation loss:		2.245167
  validation accuracy:		12.93 %
Epoch 774 of 2000 took 0.159s
  training loss:		2.295307
  validation loss:		2.244925
  validation accuracy:		12.93 %
Epoch 775 of 2000 took 0.162s
  training loss:		2.294534
  validation loss:		2.245191
  validation accuracy:		13.04 %
Epoch 776 of 2000 took 0.166s
  training loss:		2.295391
  validation loss:		2.244972
  validation accuracy:		13.04 %
Epoch 777 of 2000 took 0.166s
  training loss:		2.295280
  validation loss:		2.245060
  validation accuracy:		13.04 %
Epoch 778 of 2000 took 0.162s
  training loss:		2.296210
  validation loss:		2.245377
  validation accuracy:		13.04 %
Epoch 779 of 2000 took 0.158s
  training loss:		2.295274
  validation loss:		2.245497
  validation accuracy:		12.93 %
Epoch 780 of 2000 took 0.166s
  training loss:		2.295021
  validation loss:		2.245059
  validation accuracy:		13.04 %
Epoch 781 of 2000 took 0.161s
  training loss:		2.296258
  validation loss:		2.245480
  validation accuracy:		12.93 %
Epoch 782 of 2000 took 0.158s
  training loss:		2.295446
  validation loss:		2.245584
  validation accuracy:		12.93 %
Epoch 783 of 2000 took 0.166s
  training loss:		2.295472
  validation loss:		2.245705
  validation accuracy:		12.93 %
Epoch 784 of 2000 took 0.159s
  training loss:		2.294781
  validation loss:		2.245554
  validation accuracy:		13.04 %
Epoch 785 of 2000 took 0.160s
  training loss:		2.294743
  validation loss:		2.245115
  validation accuracy:		13.04 %
Epoch 786 of 2000 took 0.160s
  training loss:		2.294761
  validation loss:		2.244637
  validation accuracy:		13.04 %
Epoch 787 of 2000 took 0.162s
  training loss:		2.295504
  validation loss:		2.244802
  validation accuracy:		13.04 %
Epoch 788 of 2000 took 0.164s
  training loss:		2.295224
  validation loss:		2.244873
  validation accuracy:		13.04 %
Epoch 789 of 2000 took 0.159s
  training loss:		2.295669
  validation loss:		2.245249
  validation accuracy:		13.04 %
Epoch 790 of 2000 took 0.163s
  training loss:		2.294954
  validation loss:		2.245376
  validation accuracy:		12.93 %
Epoch 791 of 2000 took 0.168s
  training loss:		2.295476
  validation loss:		2.245190
  validation accuracy:		12.93 %
Epoch 792 of 2000 took 0.163s
  training loss:		2.295559
  validation loss:		2.245075
  validation accuracy:		13.04 %
Epoch 793 of 2000 took 0.162s
  training loss:		2.295243
  validation loss:		2.244894
  validation accuracy:		13.04 %
Epoch 794 of 2000 took 0.159s
  training loss:		2.295479
  validation loss:		2.244894
  validation accuracy:		13.04 %
Epoch 795 of 2000 took 0.165s
  training loss:		2.294752
  validation loss:		2.244982
  validation accuracy:		13.04 %
Epoch 796 of 2000 took 0.162s
  training loss:		2.293952
  validation loss:		2.244982
  validation accuracy:		13.04 %
Epoch 797 of 2000 took 0.158s
  training loss:		2.295475
  validation loss:		2.245133
  validation accuracy:		13.04 %
Epoch 798 of 2000 took 0.165s
  training loss:		2.295132
  validation loss:		2.244891
  validation accuracy:		13.04 %
Epoch 799 of 2000 took 0.165s
  training loss:		2.294698
  validation loss:		2.244793
  validation accuracy:		13.04 %
Epoch 800 of 2000 took 0.160s
  training loss:		2.294752
  validation loss:		2.244523
  validation accuracy:		13.04 %
Epoch 801 of 2000 took 0.160s
  training loss:		2.295312
  validation loss:		2.244457
  validation accuracy:		12.93 %
Epoch 802 of 2000 took 0.160s
  training loss:		2.295848
  validation loss:		2.244628
  validation accuracy:		12.93 %
Epoch 803 of 2000 took 0.165s
  training loss:		2.295340
  validation loss:		2.244893
  validation accuracy:		12.93 %
Epoch 804 of 2000 took 0.160s
  training loss:		2.295017
  validation loss:		2.244998
  validation accuracy:		12.93 %
Epoch 805 of 2000 took 0.161s
  training loss:		2.295516
  validation loss:		2.245153
  validation accuracy:		12.93 %
Epoch 806 of 2000 took 0.166s
  training loss:		2.293679
  validation loss:		2.244755
  validation accuracy:		12.93 %
Epoch 807 of 2000 took 0.164s
  training loss:		2.295337
  validation loss:		2.244279
  validation accuracy:		12.93 %
Epoch 808 of 2000 took 0.165s
  training loss:		2.295163
  validation loss:		2.244062
  validation accuracy:		12.93 %
Epoch 809 of 2000 took 0.159s
  training loss:		2.295745
  validation loss:		2.244221
  validation accuracy:		13.04 %
Epoch 810 of 2000 took 0.164s
  training loss:		2.294846
  validation loss:		2.244287
  validation accuracy:		13.04 %
Epoch 811 of 2000 took 0.162s
  training loss:		2.294787
  validation loss:		2.244351
  validation accuracy:		13.04 %
Epoch 812 of 2000 took 0.158s
  training loss:		2.295635
  validation loss:		2.245052
  validation accuracy:		13.04 %
Epoch 813 of 2000 took 0.163s
  training loss:		2.294809
  validation loss:		2.244502
  validation accuracy:		12.93 %
Epoch 814 of 2000 took 0.163s
  training loss:		2.295956
  validation loss:		2.244723
  validation accuracy:		13.04 %
Epoch 815 of 2000 took 0.159s
  training loss:		2.295669
  validation loss:		2.245105
  validation accuracy:		13.04 %
Epoch 816 of 2000 took 0.160s
  training loss:		2.295165
  validation loss:		2.245331
  validation accuracy:		13.04 %
Epoch 817 of 2000 took 0.159s
  training loss:		2.295160
  validation loss:		2.245158
  validation accuracy:		13.04 %
Epoch 818 of 2000 took 0.166s
  training loss:		2.294850
  validation loss:		2.244735
  validation accuracy:		13.04 %
Epoch 819 of 2000 took 0.160s
  training loss:		2.294029
  validation loss:		2.244554
  validation accuracy:		13.04 %
Epoch 820 of 2000 took 0.160s
  training loss:		2.294907
  validation loss:		2.244197
  validation accuracy:		13.04 %
Epoch 821 of 2000 took 0.166s
  training loss:		2.294701
  validation loss:		2.244209
  validation accuracy:		13.04 %
Epoch 822 of 2000 took 0.164s
  training loss:		2.295856
  validation loss:		2.244197
  validation accuracy:		13.04 %
Epoch 823 of 2000 took 0.165s
  training loss:		2.295011
  validation loss:		2.244198
  validation accuracy:		13.04 %
Epoch 824 of 2000 took 0.159s
  training loss:		2.294813
  validation loss:		2.244566
  validation accuracy:		13.04 %
Epoch 825 of 2000 took 0.162s
  training loss:		2.295189
  validation loss:		2.244470
  validation accuracy:		13.04 %
Epoch 826 of 2000 took 0.165s
  training loss:		2.294459
  validation loss:		2.244358
  validation accuracy:		13.04 %
Epoch 827 of 2000 took 0.158s
  training loss:		2.295178
  validation loss:		2.244296
  validation accuracy:		13.04 %
Epoch 828 of 2000 took 0.162s
  training loss:		2.296101
  validation loss:		2.244735
  validation accuracy:		13.04 %
Epoch 829 of 2000 took 0.165s
  training loss:		2.295310
  validation loss:		2.245436
  validation accuracy:		13.04 %
Epoch 830 of 2000 took 0.158s
  training loss:		2.295621
  validation loss:		2.245647
  validation accuracy:		13.04 %
Epoch 831 of 2000 took 0.161s
  training loss:		2.295341
  validation loss:		2.245788
  validation accuracy:		13.04 %
Epoch 832 of 2000 took 0.159s
  training loss:		2.295296
  validation loss:		2.245624
  validation accuracy:		13.04 %
Epoch 833 of 2000 took 0.166s
  training loss:		2.294161
  validation loss:		2.245127
  validation accuracy:		12.93 %
Epoch 834 of 2000 took 0.161s
  training loss:		2.294757
  validation loss:		2.244569
  validation accuracy:		12.93 %
Epoch 835 of 2000 took 0.159s
  training loss:		2.295279
  validation loss:		2.244651
  validation accuracy:		12.93 %
Epoch 836 of 2000 took 0.167s
  training loss:		2.295260
  validation loss:		2.244628
  validation accuracy:		13.04 %
Epoch 837 of 2000 took 0.161s
  training loss:		2.295649
  validation loss:		2.244947
  validation accuracy:		13.04 %
Epoch 838 of 2000 took 0.165s
  training loss:		2.294008
  validation loss:		2.244738
  validation accuracy:		13.04 %
Epoch 839 of 2000 took 0.164s
  training loss:		2.294172
  validation loss:		2.243972
  validation accuracy:		13.04 %
Epoch 840 of 2000 took 0.161s
  training loss:		2.294132
  validation loss:		2.243890
  validation accuracy:		13.04 %
Epoch 841 of 2000 took 0.166s
  training loss:		2.295161
  validation loss:		2.243893
  validation accuracy:		13.04 %
Epoch 842 of 2000 took 0.159s
  training loss:		2.295077
  validation loss:		2.244180
  validation accuracy:		13.04 %
Epoch 843 of 2000 took 0.161s
  training loss:		2.295302
  validation loss:		2.244813
  validation accuracy:		13.04 %
Epoch 844 of 2000 took 0.167s
  training loss:		2.295345
  validation loss:		2.245228
  validation accuracy:		13.04 %
Epoch 845 of 2000 took 0.158s
  training loss:		2.294595
  validation loss:		2.245045
  validation accuracy:		13.04 %
Epoch 846 of 2000 took 0.162s
  training loss:		2.295580
  validation loss:		2.244352
  validation accuracy:		13.04 %
Epoch 847 of 2000 took 0.159s
  training loss:		2.294415
  validation loss:		2.243822
  validation accuracy:		13.04 %
Epoch 848 of 2000 took 0.168s
  training loss:		2.295342
  validation loss:		2.244051
  validation accuracy:		13.04 %
Epoch 849 of 2000 took 0.164s
  training loss:		2.295896
  validation loss:		2.244353
  validation accuracy:		13.04 %
Epoch 850 of 2000 took 0.159s
  training loss:		2.295942
  validation loss:		2.244871
  validation accuracy:		12.93 %
Epoch 851 of 2000 took 0.165s
  training loss:		2.295101
  validation loss:		2.244822
  validation accuracy:		13.04 %
Epoch 852 of 2000 took 0.161s
  training loss:		2.295895
  validation loss:		2.245323
  validation accuracy:		13.04 %
Epoch 853 of 2000 took 0.160s
  training loss:		2.294482
  validation loss:		2.245296
  validation accuracy:		13.04 %
Epoch 854 of 2000 took 0.165s
  training loss:		2.294523
  validation loss:		2.244952
  validation accuracy:		13.04 %
Epoch 855 of 2000 took 0.164s
  training loss:		2.294481
  validation loss:		2.244922
  validation accuracy:		12.93 %
Epoch 856 of 2000 took 0.162s
  training loss:		2.295007
  validation loss:		2.244570
  validation accuracy:		12.93 %
Epoch 857 of 2000 took 0.163s
  training loss:		2.294989
  validation loss:		2.245109
  validation accuracy:		12.93 %
Epoch 858 of 2000 took 0.159s
  training loss:		2.295603
  validation loss:		2.244905
  validation accuracy:		12.93 %
Epoch 859 of 2000 took 0.163s
  training loss:		2.294363
  validation loss:		2.244805
  validation accuracy:		12.93 %
Epoch 860 of 2000 took 0.163s
  training loss:		2.294276
  validation loss:		2.244355
  validation accuracy:		12.93 %
Epoch 861 of 2000 took 0.159s
  training loss:		2.294716
  validation loss:		2.244175
  validation accuracy:		12.93 %
Epoch 862 of 2000 took 0.161s
  training loss:		2.294674
  validation loss:		2.244358
  validation accuracy:		12.93 %
Epoch 863 of 2000 took 0.159s
  training loss:		2.295103
  validation loss:		2.244469
  validation accuracy:		12.93 %
Epoch 864 of 2000 took 0.166s
  training loss:		2.295353
  validation loss:		2.244210
  validation accuracy:		12.93 %
Epoch 865 of 2000 took 0.165s
  training loss:		2.295003
  validation loss:		2.244437
  validation accuracy:		12.93 %
Epoch 866 of 2000 took 0.164s
  training loss:		2.295268
  validation loss:		2.244428
  validation accuracy:		12.93 %
Epoch 867 of 2000 took 0.166s
  training loss:		2.295229
  validation loss:		2.244884
  validation accuracy:		12.93 %
Epoch 868 of 2000 took 0.160s
  training loss:		2.295126
  validation loss:		2.244902
  validation accuracy:		12.93 %
Epoch 869 of 2000 took 0.161s
  training loss:		2.295955
  validation loss:		2.245411
  validation accuracy:		12.93 %
Epoch 870 of 2000 took 0.159s
  training loss:		2.294871
  validation loss:		2.244797
  validation accuracy:		12.93 %
Epoch 871 of 2000 took 0.162s
  training loss:		2.294792
  validation loss:		2.244907
  validation accuracy:		13.04 %
Epoch 872 of 2000 took 0.165s
  training loss:		2.295576
  validation loss:		2.245121
  validation accuracy:		12.93 %
Epoch 873 of 2000 took 0.158s
  training loss:		2.294650
  validation loss:		2.245053
  validation accuracy:		12.83 %
Epoch 874 of 2000 took 0.163s
  training loss:		2.294731
  validation loss:		2.245275
  validation accuracy:		12.83 %
Epoch 875 of 2000 took 0.165s
  training loss:		2.295063
  validation loss:		2.245118
  validation accuracy:		12.83 %
Epoch 876 of 2000 took 0.159s
  training loss:		2.295051
  validation loss:		2.244732
  validation accuracy:		12.93 %
Epoch 877 of 2000 took 0.161s
  training loss:		2.295313
  validation loss:		2.244719
  validation accuracy:		12.93 %
Epoch 878 of 2000 took 0.159s
  training loss:		2.295170
  validation loss:		2.245026
  validation accuracy:		12.93 %
Epoch 879 of 2000 took 0.167s
  training loss:		2.295420
  validation loss:		2.245219
  validation accuracy:		12.93 %
Epoch 880 of 2000 took 0.161s
  training loss:		2.294487
  validation loss:		2.245442
  validation accuracy:		12.93 %
Epoch 881 of 2000 took 0.163s
  training loss:		2.295374
  validation loss:		2.245282
  validation accuracy:		12.93 %
Epoch 882 of 2000 took 0.169s
  training loss:		2.294822
  validation loss:		2.245269
  validation accuracy:		12.93 %
Epoch 883 of 2000 took 0.164s
  training loss:		2.294761
  validation loss:		2.244997
  validation accuracy:		12.93 %
Epoch 884 of 2000 took 0.159s
  training loss:		2.294351
  validation loss:		2.244538
  validation accuracy:		12.93 %
Epoch 885 of 2000 took 0.159s
  training loss:		2.294525
  validation loss:		2.244404
  validation accuracy:		12.93 %
Epoch 886 of 2000 took 0.160s
  training loss:		2.294990
  validation loss:		2.244089
  validation accuracy:		13.04 %
Epoch 887 of 2000 took 0.166s
  training loss:		2.295040
  validation loss:		2.243967
  validation accuracy:		13.04 %
Epoch 888 of 2000 took 0.158s
  training loss:		2.295367
  validation loss:		2.244520
  validation accuracy:		13.04 %
Epoch 889 of 2000 took 0.162s
  training loss:		2.294530
  validation loss:		2.244378
  validation accuracy:		13.04 %
Epoch 890 of 2000 took 0.166s
  training loss:		2.294668
  validation loss:		2.244251
  validation accuracy:		13.04 %
Epoch 891 of 2000 took 0.159s
  training loss:		2.294542
  validation loss:		2.244503
  validation accuracy:		13.04 %
Epoch 892 of 2000 took 0.161s
  training loss:		2.296191
  validation loss:		2.244709
  validation accuracy:		13.04 %
Epoch 893 of 2000 took 0.159s
  training loss:		2.295591
  validation loss:		2.244731
  validation accuracy:		12.93 %
Epoch 894 of 2000 took 0.164s
  training loss:		2.295381
  validation loss:		2.244978
  validation accuracy:		12.83 %
Epoch 895 of 2000 took 0.164s
  training loss:		2.296011
  validation loss:		2.245258
  validation accuracy:		12.93 %
Epoch 896 of 2000 took 0.164s
  training loss:		2.294951
  validation loss:		2.245473
  validation accuracy:		12.93 %
Epoch 897 of 2000 took 0.168s
  training loss:		2.294898
  validation loss:		2.245887
  validation accuracy:		12.93 %
Epoch 898 of 2000 took 0.163s
  training loss:		2.295079
  validation loss:		2.245773
  validation accuracy:		12.93 %
Epoch 899 of 2000 took 0.160s
  training loss:		2.294966
  validation loss:		2.245356
  validation accuracy:		12.93 %
Epoch 900 of 2000 took 0.161s
  training loss:		2.295189
  validation loss:		2.245301
  validation accuracy:		12.93 %
Epoch 901 of 2000 took 0.159s
  training loss:		2.294485
  validation loss:		2.245500
  validation accuracy:		12.93 %
Epoch 902 of 2000 took 0.167s
  training loss:		2.294556
  validation loss:		2.245304
  validation accuracy:		12.93 %
Epoch 903 of 2000 took 0.163s
  training loss:		2.294409
  validation loss:		2.244799
  validation accuracy:		12.93 %
Epoch 904 of 2000 took 0.161s
  training loss:		2.295371
  validation loss:		2.244789
  validation accuracy:		12.93 %
Epoch 905 of 2000 took 0.168s
  training loss:		2.295106
  validation loss:		2.244762
  validation accuracy:		12.93 %
Epoch 906 of 2000 took 0.159s
  training loss:		2.294683
  validation loss:		2.244726
  validation accuracy:		12.93 %
Epoch 907 of 2000 took 0.162s
  training loss:		2.295037
  validation loss:		2.244648
  validation accuracy:		12.93 %
Epoch 908 of 2000 took 0.159s
  training loss:		2.295790
  validation loss:		2.244797
  validation accuracy:		12.93 %
Epoch 909 of 2000 took 0.164s
  training loss:		2.295353
  validation loss:		2.245318
  validation accuracy:		12.93 %
Epoch 910 of 2000 took 0.164s
  training loss:		2.294568
  validation loss:		2.245270
  validation accuracy:		13.04 %
Epoch 911 of 2000 took 0.159s
  training loss:		2.294633
  validation loss:		2.245022
  validation accuracy:		12.93 %
Epoch 912 of 2000 took 0.166s
  training loss:		2.294972
  validation loss:		2.244235
  validation accuracy:		13.04 %
Epoch 913 of 2000 took 0.168s
  training loss:		2.295047
  validation loss:		2.244280
  validation accuracy:		12.93 %
Epoch 914 of 2000 took 0.162s
  training loss:		2.294713
  validation loss:		2.244219
  validation accuracy:		12.93 %
Epoch 915 of 2000 took 0.162s
  training loss:		2.294739
  validation loss:		2.244592
  validation accuracy:		12.93 %
Epoch 916 of 2000 took 0.159s
  training loss:		2.294500
  validation loss:		2.244855
  validation accuracy:		12.93 %
Epoch 917 of 2000 took 0.166s
  training loss:		2.295121
  validation loss:		2.244938
  validation accuracy:		13.04 %
Epoch 918 of 2000 took 0.161s
  training loss:		2.294903
  validation loss:		2.245157
  validation accuracy:		12.93 %
Epoch 919 of 2000 took 0.159s
  training loss:		2.295319
  validation loss:		2.245288
  validation accuracy:		12.93 %
Epoch 920 of 2000 took 0.166s
  training loss:		2.295769
  validation loss:		2.245447
  validation accuracy:		12.93 %
Epoch 921 of 2000 took 0.159s
  training loss:		2.295438
  validation loss:		2.245736
  validation accuracy:		12.93 %
Epoch 922 of 2000 took 0.162s
  training loss:		2.294923
  validation loss:		2.245762
  validation accuracy:		12.93 %
Epoch 923 of 2000 took 0.159s
  training loss:		2.295242
  validation loss:		2.245526
  validation accuracy:		12.93 %
Epoch 924 of 2000 took 0.163s
  training loss:		2.295028
  validation loss:		2.245728
  validation accuracy:		12.93 %
Epoch 925 of 2000 took 0.165s
  training loss:		2.295971
  validation loss:		2.246034
  validation accuracy:		12.93 %
Epoch 926 of 2000 took 0.159s
  training loss:		2.295021
  validation loss:		2.246321
  validation accuracy:		12.93 %
Epoch 927 of 2000 took 0.164s
  training loss:		2.294939
  validation loss:		2.246145
  validation accuracy:		12.93 %
Epoch 928 of 2000 took 0.167s
  training loss:		2.294458
  validation loss:		2.245693
  validation accuracy:		12.93 %
Epoch 929 of 2000 took 0.162s
  training loss:		2.294967
  validation loss:		2.245404
  validation accuracy:		12.93 %
Epoch 930 of 2000 took 0.165s
  training loss:		2.295096
  validation loss:		2.245265
  validation accuracy:		12.93 %
Epoch 931 of 2000 took 0.159s
  training loss:		2.294105
  validation loss:		2.244338
  validation accuracy:		12.93 %
Epoch 932 of 2000 took 0.163s
  training loss:		2.295028
  validation loss:		2.244659
  validation accuracy:		12.93 %
Epoch 933 of 2000 took 0.160s
  training loss:		2.294882
  validation loss:		2.244361
  validation accuracy:		12.93 %
Epoch 934 of 2000 took 0.160s
  training loss:		2.294700
  validation loss:		2.244382
  validation accuracy:		13.04 %
Epoch 935 of 2000 took 0.165s
  training loss:		2.294658
  validation loss:		2.244767
  validation accuracy:		13.04 %
Epoch 936 of 2000 took 0.158s
  training loss:		2.294937
  validation loss:		2.244794
  validation accuracy:		13.04 %
Epoch 937 of 2000 took 0.164s
  training loss:		2.295169
  validation loss:		2.245101
  validation accuracy:		12.93 %
Epoch 938 of 2000 took 0.160s
  training loss:		2.294914
  validation loss:		2.244868
  validation accuracy:		12.93 %
Epoch 939 of 2000 took 0.161s
  training loss:		2.294223
  validation loss:		2.244721
  validation accuracy:		12.93 %
Epoch 940 of 2000 took 0.162s
  training loss:		2.295021
  validation loss:		2.245029
  validation accuracy:		12.93 %
Epoch 941 of 2000 took 0.159s
  training loss:		2.294907
  validation loss:		2.245144
  validation accuracy:		12.93 %
Epoch 942 of 2000 took 0.166s
  training loss:		2.294655
  validation loss:		2.245003
  validation accuracy:		12.93 %
Epoch 943 of 2000 took 0.165s
  training loss:		2.294264
  validation loss:		2.244550
  validation accuracy:		12.93 %
Epoch 944 of 2000 took 0.164s
  training loss:		2.294808
  validation loss:		2.244125
  validation accuracy:		12.93 %
Epoch 945 of 2000 took 0.164s
  training loss:		2.294378
  validation loss:		2.243685
  validation accuracy:		12.93 %
Epoch 946 of 2000 took 0.159s
  training loss:		2.294394
  validation loss:		2.243942
  validation accuracy:		13.04 %
Epoch 947 of 2000 took 0.165s
  training loss:		2.294519
  validation loss:		2.243973
  validation accuracy:		13.04 %
Epoch 948 of 2000 took 0.158s
  training loss:		2.295220
  validation loss:		2.243932
  validation accuracy:		13.04 %
Epoch 949 of 2000 took 0.163s
  training loss:		2.295431
  validation loss:		2.244101
  validation accuracy:		13.04 %
Epoch 950 of 2000 took 0.160s
  training loss:		2.294683
  validation loss:		2.244392
  validation accuracy:		12.93 %
Epoch 951 of 2000 took 0.160s
  training loss:		2.294893
  validation loss:		2.244886
  validation accuracy:		12.93 %
Epoch 952 of 2000 took 0.165s
  training loss:		2.294807
  validation loss:		2.244990
  validation accuracy:		12.93 %
Epoch 953 of 2000 took 0.159s
  training loss:		2.295016
  validation loss:		2.244549
  validation accuracy:		12.93 %
Epoch 954 of 2000 took 0.164s
  training loss:		2.295252
  validation loss:		2.244410
  validation accuracy:		12.93 %
Epoch 955 of 2000 took 0.163s
  training loss:		2.294845
  validation loss:		2.244979
  validation accuracy:		12.93 %
Epoch 956 of 2000 took 0.160s
  training loss:		2.295481
  validation loss:		2.245126
  validation accuracy:		12.93 %
Epoch 957 of 2000 took 0.160s
  training loss:		2.294366
  validation loss:		2.245052
  validation accuracy:		12.93 %
Epoch 958 of 2000 took 0.163s
  training loss:		2.294978
  validation loss:		2.245335
  validation accuracy:		12.93 %
Epoch 959 of 2000 took 0.168s
  training loss:		2.295077
  validation loss:		2.245393
  validation accuracy:		12.83 %
Epoch 960 of 2000 took 0.164s
  training loss:		2.294721
  validation loss:		2.245299
  validation accuracy:		12.83 %
Epoch 961 of 2000 took 0.160s
  training loss:		2.295678
  validation loss:		2.244967
  validation accuracy:		12.93 %
Epoch 962 of 2000 took 0.169s
  training loss:		2.294573
  validation loss:		2.245150
  validation accuracy:		13.04 %
Epoch 963 of 2000 took 0.160s
  training loss:		2.294799
  validation loss:		2.244999
  validation accuracy:		12.93 %
Epoch 964 of 2000 took 0.161s
  training loss:		2.295153
  validation loss:		2.245255
  validation accuracy:		12.93 %
Epoch 965 of 2000 took 0.158s
  training loss:		2.294565
  validation loss:		2.245523
  validation accuracy:		13.04 %
Epoch 966 of 2000 took 0.163s
  training loss:		2.295893
  validation loss:		2.245495
  validation accuracy:		12.93 %
Epoch 967 of 2000 took 0.164s
  training loss:		2.295213
  validation loss:		2.245443
  validation accuracy:		12.93 %
Epoch 968 of 2000 took 0.159s
  training loss:		2.294519
  validation loss:		2.245798
  validation accuracy:		12.93 %
Epoch 969 of 2000 took 0.163s
  training loss:		2.294733
  validation loss:		2.245443
  validation accuracy:		13.04 %
Epoch 970 of 2000 took 0.163s
  training loss:		2.294719
  validation loss:		2.245200
  validation accuracy:		13.04 %
Epoch 971 of 2000 took 0.159s
  training loss:		2.294794
  validation loss:		2.245046
  validation accuracy:		13.04 %
Epoch 972 of 2000 took 0.161s
  training loss:		2.294562
  validation loss:		2.244763
  validation accuracy:		13.04 %
Epoch 973 of 2000 took 0.159s
  training loss:		2.294709
  validation loss:		2.244972
  validation accuracy:		13.04 %
Epoch 974 of 2000 took 0.166s
  training loss:		2.295170
  validation loss:		2.245029
  validation accuracy:		13.04 %
Epoch 975 of 2000 took 0.165s
  training loss:		2.294867
  validation loss:		2.245155
  validation accuracy:		13.04 %
Epoch 976 of 2000 took 0.164s
  training loss:		2.295219
  validation loss:		2.245289
  validation accuracy:		13.04 %
Epoch 977 of 2000 took 0.166s
  training loss:		2.294895
  validation loss:		2.245423
  validation accuracy:		13.04 %
Epoch 978 of 2000 took 0.159s
  training loss:		2.294838
  validation loss:		2.245132
  validation accuracy:		13.04 %
Epoch 979 of 2000 took 0.161s
  training loss:		2.293805
  validation loss:		2.244876
  validation accuracy:		13.04 %
Epoch 980 of 2000 took 0.158s
  training loss:		2.294790
  validation loss:		2.244702
  validation accuracy:		12.93 %
Epoch 981 of 2000 took 0.162s
  training loss:		2.295157
  validation loss:		2.245139
  validation accuracy:		13.04 %
Epoch 982 of 2000 took 0.165s
  training loss:		2.294763
  validation loss:		2.244734
  validation accuracy:		13.04 %
Epoch 983 of 2000 took 0.159s
  training loss:		2.295567
  validation loss:		2.245085
  validation accuracy:		13.04 %
Epoch 984 of 2000 took 0.163s
  training loss:		2.294930
  validation loss:		2.245243
  validation accuracy:		13.04 %
Epoch 985 of 2000 took 0.164s
  training loss:		2.295429
  validation loss:		2.245308
  validation accuracy:		13.04 %
Epoch 986 of 2000 took 0.159s
  training loss:		2.294082
  validation loss:		2.245000
  validation accuracy:		13.04 %
Epoch 987 of 2000 took 0.161s
  training loss:		2.294168
  validation loss:		2.244795
  validation accuracy:		13.04 %
Epoch 988 of 2000 took 0.159s
  training loss:		2.295550
  validation loss:		2.244634
  validation accuracy:		13.04 %
Epoch 989 of 2000 took 0.167s
  training loss:		2.295204
  validation loss:		2.244629
  validation accuracy:		12.93 %
Epoch 990 of 2000 took 0.165s
  training loss:		2.294711
  validation loss:		2.244291
  validation accuracy:		12.93 %
Epoch 991 of 2000 took 0.163s
  training loss:		2.294900
  validation loss:		2.244331
  validation accuracy:		12.93 %
Epoch 992 of 2000 took 0.166s
  training loss:		2.294732
  validation loss:		2.244719
  validation accuracy:		12.93 %
Epoch 993 of 2000 took 0.161s
  training loss:		2.294007
  validation loss:		2.244215
  validation accuracy:		12.93 %
Epoch 994 of 2000 took 0.160s
  training loss:		2.295628
  validation loss:		2.243975
  validation accuracy:		12.93 %
Epoch 995 of 2000 took 0.158s
  training loss:		2.294292
  validation loss:		2.244072
  validation accuracy:		12.93 %
Epoch 996 of 2000 took 0.161s
  training loss:		2.294755
  validation loss:		2.243947
  validation accuracy:		12.93 %
Epoch 997 of 2000 took 0.166s
  training loss:		2.294764
  validation loss:		2.244380
  validation accuracy:		12.93 %
Epoch 998 of 2000 took 0.158s
  training loss:		2.294894
  validation loss:		2.244493
  validation accuracy:		12.93 %
Epoch 999 of 2000 took 0.163s
  training loss:		2.294497
  validation loss:		2.244336
  validation accuracy:		12.93 %
Epoch 1000 of 2000 took 0.166s
  training loss:		2.294514
  validation loss:		2.244160
  validation accuracy:		12.93 %
Epoch 1001 of 2000 took 0.159s
  training loss:		2.294943
  validation loss:		2.243936
  validation accuracy:		13.04 %
Epoch 1002 of 2000 took 0.161s
  training loss:		2.294506
  validation loss:		2.244185
  validation accuracy:		13.04 %
Epoch 1003 of 2000 took 0.159s
  training loss:		2.295646
  validation loss:		2.244757
  validation accuracy:		13.04 %
Epoch 1004 of 2000 took 0.166s
  training loss:		2.294543
  validation loss:		2.245263
  validation accuracy:		12.93 %
Epoch 1005 of 2000 took 0.161s
  training loss:		2.294762
  validation loss:		2.244890
  validation accuracy:		12.93 %
Epoch 1006 of 2000 took 0.163s
  training loss:		2.294270
  validation loss:		2.244638
  validation accuracy:		12.93 %
Epoch 1007 of 2000 took 0.169s
  training loss:		2.295233
  validation loss:		2.244398
  validation accuracy:		12.93 %
Epoch 1008 of 2000 took 0.163s
  training loss:		2.295060
  validation loss:		2.244534
  validation accuracy:		12.93 %
Epoch 1009 of 2000 took 0.160s
  training loss:		2.294433
  validation loss:		2.245031
  validation accuracy:		12.93 %
Epoch 1010 of 2000 took 0.160s
  training loss:		2.294475
  validation loss:		2.244498
  validation accuracy:		13.04 %
Epoch 1011 of 2000 took 0.160s
  training loss:		2.294659
  validation loss:		2.244919
  validation accuracy:		13.04 %
Epoch 1012 of 2000 took 0.165s
  training loss:		2.294763
  validation loss:		2.244706
  validation accuracy:		12.93 %
Epoch 1013 of 2000 took 0.159s
  training loss:		2.294679
  validation loss:		2.244732
  validation accuracy:		13.04 %
Epoch 1014 of 2000 took 0.161s
  training loss:		2.294791
  validation loss:		2.244689
  validation accuracy:		13.04 %
Epoch 1015 of 2000 took 0.167s
  training loss:		2.294314
  validation loss:		2.244555
  validation accuracy:		13.04 %
Epoch 1016 of 2000 took 0.159s
  training loss:		2.294549
  validation loss:		2.244389
  validation accuracy:		13.04 %
Epoch 1017 of 2000 took 0.162s
  training loss:		2.295059
  validation loss:		2.243884
  validation accuracy:		13.04 %
Epoch 1018 of 2000 took 0.159s
  training loss:		2.295708
  validation loss:		2.245042
  validation accuracy:		13.04 %
Epoch 1019 of 2000 took 0.164s
  training loss:		2.294969
  validation loss:		2.245210
  validation accuracy:		13.04 %
Epoch 1020 of 2000 took 0.163s
  training loss:		2.295357
  validation loss:		2.245908
  validation accuracy:		12.93 %
Epoch 1021 of 2000 took 0.164s
  training loss:		2.294539
  validation loss:		2.245771
  validation accuracy:		13.04 %
Epoch 1022 of 2000 took 0.167s
  training loss:		2.295278
  validation loss:		2.245469
  validation accuracy:		13.04 %
Epoch 1023 of 2000 took 0.165s
  training loss:		2.295424
  validation loss:		2.245032
  validation accuracy:		13.04 %
Epoch 1024 of 2000 took 0.160s
  training loss:		2.293993
  validation loss:		2.244929
  validation accuracy:		13.04 %
Epoch 1025 of 2000 took 0.161s
  training loss:		2.294416
  validation loss:		2.244955
  validation accuracy:		12.93 %
Epoch 1026 of 2000 took 0.159s
  training loss:		2.295384
  validation loss:		2.245227
  validation accuracy:		13.04 %
Epoch 1027 of 2000 took 0.166s
  training loss:		2.295558
  validation loss:		2.245225
  validation accuracy:		13.04 %
Epoch 1028 of 2000 took 0.163s
  training loss:		2.294781
  validation loss:		2.245488
  validation accuracy:		13.04 %
Epoch 1029 of 2000 took 0.161s
  training loss:		2.295014
  validation loss:		2.245543
  validation accuracy:		13.04 %
Epoch 1030 of 2000 took 0.168s
  training loss:		2.296067
  validation loss:		2.245579
  validation accuracy:		12.93 %
Epoch 1031 of 2000 took 0.159s
  training loss:		2.295164
  validation loss:		2.245841
  validation accuracy:		13.04 %
Epoch 1032 of 2000 took 0.162s
  training loss:		2.295312
  validation loss:		2.245771
  validation accuracy:		13.04 %
Epoch 1033 of 2000 took 0.159s
  training loss:		2.295430
  validation loss:		2.245844
  validation accuracy:		13.04 %
Epoch 1034 of 2000 took 0.163s
  training loss:		2.294767
  validation loss:		2.245742
  validation accuracy:		13.04 %
Epoch 1035 of 2000 took 0.166s
  training loss:		2.295247
  validation loss:		2.246078
  validation accuracy:		12.93 %
Epoch 1036 of 2000 took 0.155s
  training loss:		2.294204
  validation loss:		2.245897
  validation accuracy:		12.93 %
Epoch 1037 of 2000 took 0.110s
  training loss:		2.294485
  validation loss:		2.245164
  validation accuracy:		12.93 %
Epoch 1038 of 2000 took 0.116s
  training loss:		2.294170
  validation loss:		2.244796
  validation accuracy:		12.93 %
Epoch 1039 of 2000 took 0.106s
  training loss:		2.294072
  validation loss:		2.244431
  validation accuracy:		12.93 %
Epoch 1040 of 2000 took 0.106s
  training loss:		2.295495
  validation loss:		2.244823
  validation accuracy:		13.04 %
Epoch 1041 of 2000 took 0.106s
  training loss:		2.294651
  validation loss:		2.244540
  validation accuracy:		12.93 %
Epoch 1042 of 2000 took 0.116s
  training loss:		2.294042
  validation loss:		2.244849
  validation accuracy:		12.93 %
Epoch 1043 of 2000 took 0.115s
  training loss:		2.295009
  validation loss:		2.244223
  validation accuracy:		12.93 %
Epoch 1044 of 2000 took 0.104s
  training loss:		2.294968
  validation loss:		2.244073
  validation accuracy:		13.04 %
Epoch 1045 of 2000 took 0.108s
  training loss:		2.294832
  validation loss:		2.244307
  validation accuracy:		13.04 %
Epoch 1046 of 2000 took 0.116s
  training loss:		2.294302
  validation loss:		2.244171
  validation accuracy:		13.04 %
Epoch 1047 of 2000 took 0.105s
  training loss:		2.295207
  validation loss:		2.244615
  validation accuracy:		13.04 %
Epoch 1048 of 2000 took 0.110s
  training loss:		2.294697
  validation loss:		2.244447
  validation accuracy:		13.04 %
Epoch 1049 of 2000 took 0.100s
  training loss:		2.293988
  validation loss:		2.244374
  validation accuracy:		13.04 %
Epoch 1050 of 2000 took 0.102s
  training loss:		2.294544
  validation loss:		2.244031
  validation accuracy:		12.83 %
Epoch 1051 of 2000 took 0.104s
  training loss:		2.295066
  validation loss:		2.244244
  validation accuracy:		12.83 %
Epoch 1052 of 2000 took 0.100s
  training loss:		2.295547
  validation loss:		2.244893
  validation accuracy:		12.83 %
Epoch 1053 of 2000 took 0.100s
  training loss:		2.294085
  validation loss:		2.244450
  validation accuracy:		12.83 %
Epoch 1054 of 2000 took 0.106s
  training loss:		2.295581
  validation loss:		2.244954
  validation accuracy:		12.93 %
Epoch 1055 of 2000 took 0.101s
  training loss:		2.294468
  validation loss:		2.244735
  validation accuracy:		12.93 %
Epoch 1056 of 2000 took 0.100s
  training loss:		2.294391
  validation loss:		2.245146
  validation accuracy:		12.93 %
Epoch 1057 of 2000 took 0.104s
  training loss:		2.293976
  validation loss:		2.244648
  validation accuracy:		12.93 %
Epoch 1058 of 2000 took 0.103s
  training loss:		2.294674
  validation loss:		2.243879
  validation accuracy:		12.93 %
Epoch 1059 of 2000 took 0.103s
  training loss:		2.294706
  validation loss:		2.244239
  validation accuracy:		12.93 %
Epoch 1060 of 2000 took 0.103s
  training loss:		2.294655
  validation loss:		2.244232
  validation accuracy:		13.04 %
Epoch 1061 of 2000 took 0.103s
  training loss:		2.294616
  validation loss:		2.244691
  validation accuracy:		13.04 %
Epoch 1062 of 2000 took 0.103s
  training loss:		2.293932
  validation loss:		2.244733
  validation accuracy:		12.83 %
Epoch 1063 of 2000 took 0.103s
  training loss:		2.295586
  validation loss:		2.244494
  validation accuracy:		13.04 %
Epoch 1064 of 2000 took 0.103s
  training loss:		2.294473
  validation loss:		2.244266
  validation accuracy:		13.04 %
Epoch 1065 of 2000 took 0.103s
  training loss:		2.294302
  validation loss:		2.244404
  validation accuracy:		13.04 %
Epoch 1066 of 2000 took 0.104s
  training loss:		2.294833
  validation loss:		2.244005
  validation accuracy:		12.93 %
Epoch 1067 of 2000 took 0.103s
  training loss:		2.295313
  validation loss:		2.244584
  validation accuracy:		13.04 %
Epoch 1068 of 2000 took 0.103s
  training loss:		2.294483
  validation loss:		2.244567
  validation accuracy:		13.04 %
Epoch 1069 of 2000 took 0.103s
  training loss:		2.294675
  validation loss:		2.244494
  validation accuracy:		12.93 %
Epoch 1070 of 2000 took 0.103s
  training loss:		2.295597
  validation loss:		2.244859
  validation accuracy:		12.93 %
Epoch 1071 of 2000 took 0.103s
  training loss:		2.294894
  validation loss:		2.244918
  validation accuracy:		12.93 %
Epoch 1072 of 2000 took 0.103s
  training loss:		2.294073
  validation loss:		2.245049
  validation accuracy:		12.93 %
Epoch 1073 of 2000 took 0.103s
  training loss:		2.294875
  validation loss:		2.245342
  validation accuracy:		12.93 %
Epoch 1074 of 2000 took 0.103s
  training loss:		2.294711
  validation loss:		2.245039
  validation accuracy:		12.93 %
Epoch 1075 of 2000 took 0.103s
  training loss:		2.294096
  validation loss:		2.245085
  validation accuracy:		12.93 %
Epoch 1076 of 2000 took 0.103s
  training loss:		2.294469
  validation loss:		2.244291
  validation accuracy:		12.93 %
Epoch 1077 of 2000 took 0.103s
  training loss:		2.294735
  validation loss:		2.244115
  validation accuracy:		12.93 %
Epoch 1078 of 2000 took 0.103s
  training loss:		2.294744
  validation loss:		2.244000
  validation accuracy:		12.93 %
Epoch 1079 of 2000 took 0.103s
  training loss:		2.294669
  validation loss:		2.243861
  validation accuracy:		13.04 %
Epoch 1080 of 2000 took 0.103s
  training loss:		2.295080
  validation loss:		2.243973
  validation accuracy:		13.04 %
Epoch 1081 of 2000 took 0.103s
  training loss:		2.295355
  validation loss:		2.244567
  validation accuracy:		13.04 %
Epoch 1082 of 2000 took 0.103s
  training loss:		2.294977
  validation loss:		2.245050
  validation accuracy:		12.93 %
Epoch 1083 of 2000 took 0.103s
  training loss:		2.295268
  validation loss:		2.244989
  validation accuracy:		12.93 %
Epoch 1084 of 2000 took 0.103s
  training loss:		2.293743
  validation loss:		2.244928
  validation accuracy:		12.93 %
Epoch 1085 of 2000 took 0.103s
  training loss:		2.293800
  validation loss:		2.244426
  validation accuracy:		12.93 %
Epoch 1086 of 2000 took 0.103s
  training loss:		2.294207
  validation loss:		2.244251
  validation accuracy:		13.04 %
Epoch 1087 of 2000 took 0.103s
  training loss:		2.295291
  validation loss:		2.244302
  validation accuracy:		13.04 %
Epoch 1088 of 2000 took 0.103s
  training loss:		2.294839
  validation loss:		2.244221
  validation accuracy:		13.04 %
Epoch 1089 of 2000 took 0.103s
  training loss:		2.294477
  validation loss:		2.244801
  validation accuracy:		12.93 %
Epoch 1090 of 2000 took 0.103s
  training loss:		2.294830
  validation loss:		2.244784
  validation accuracy:		12.93 %
Epoch 1091 of 2000 took 0.103s
  training loss:		2.294791
  validation loss:		2.244879
  validation accuracy:		12.93 %
Epoch 1092 of 2000 took 0.103s
  training loss:		2.294438
  validation loss:		2.244901
  validation accuracy:		12.93 %
Epoch 1093 of 2000 took 0.103s
  training loss:		2.294478
  validation loss:		2.244276
  validation accuracy:		12.83 %
Epoch 1094 of 2000 took 0.103s
  training loss:		2.294665
  validation loss:		2.243751
  validation accuracy:		13.04 %
Epoch 1095 of 2000 took 0.104s
  training loss:		2.294854
  validation loss:		2.244051
  validation accuracy:		12.83 %
Epoch 1096 of 2000 took 0.103s
  training loss:		2.293950
  validation loss:		2.244388
  validation accuracy:		12.83 %
Epoch 1097 of 2000 took 0.103s
  training loss:		2.294205
  validation loss:		2.243968
  validation accuracy:		13.04 %
Epoch 1098 of 2000 took 0.103s
  training loss:		2.294492
  validation loss:		2.244014
  validation accuracy:		13.04 %
Epoch 1099 of 2000 took 0.103s
  training loss:		2.294770
  validation loss:		2.244065
  validation accuracy:		13.04 %
Epoch 1100 of 2000 took 0.103s
  training loss:		2.295100
  validation loss:		2.244481
  validation accuracy:		13.04 %
Epoch 1101 of 2000 took 0.103s
  training loss:		2.294145
  validation loss:		2.244500
  validation accuracy:		12.93 %
Epoch 1102 of 2000 took 0.102s
  training loss:		2.294862
  validation loss:		2.244251
  validation accuracy:		12.93 %
Epoch 1103 of 2000 took 0.106s
  training loss:		2.295888
  validation loss:		2.244682
  validation accuracy:		12.93 %
Epoch 1104 of 2000 took 0.106s
  training loss:		2.295190
  validation loss:		2.244915
  validation accuracy:		12.93 %
Epoch 1105 of 2000 took 0.100s
  training loss:		2.295033
  validation loss:		2.245581
  validation accuracy:		12.93 %
Epoch 1106 of 2000 took 0.102s
  training loss:		2.295054
  validation loss:		2.245652
  validation accuracy:		12.93 %
Epoch 1107 of 2000 took 0.100s
  training loss:		2.294759
  validation loss:		2.245876
  validation accuracy:		12.93 %
Epoch 1108 of 2000 took 0.108s
  training loss:		2.294885
  validation loss:		2.245604
  validation accuracy:		12.93 %
Epoch 1109 of 2000 took 0.103s
  training loss:		2.295172
  validation loss:		2.245959
  validation accuracy:		12.93 %
Epoch 1110 of 2000 took 0.100s
  training loss:		2.294473
  validation loss:		2.245639
  validation accuracy:		12.93 %
Epoch 1111 of 2000 took 0.104s
  training loss:		2.294311
  validation loss:		2.245362
  validation accuracy:		12.93 %
Epoch 1112 of 2000 took 0.103s
  training loss:		2.294496
  validation loss:		2.244756
  validation accuracy:		12.93 %
Epoch 1113 of 2000 took 0.103s
  training loss:		2.294417
  validation loss:		2.244372
  validation accuracy:		13.04 %
Epoch 1114 of 2000 took 0.102s
  training loss:		2.295098
  validation loss:		2.244388
  validation accuracy:		12.93 %
Epoch 1115 of 2000 took 0.100s
  training loss:		2.294751
  validation loss:		2.244708
  validation accuracy:		13.04 %
Epoch 1116 of 2000 took 0.108s
  training loss:		2.294128
  validation loss:		2.244381
  validation accuracy:		13.04 %
Epoch 1117 of 2000 took 0.102s
  training loss:		2.294954
  validation loss:		2.244373
  validation accuracy:		13.04 %
Epoch 1118 of 2000 took 0.100s
  training loss:		2.295097
  validation loss:		2.244884
  validation accuracy:		12.93 %
Epoch 1119 of 2000 took 0.107s
  training loss:		2.294416
  validation loss:		2.244857
  validation accuracy:		12.93 %
Epoch 1120 of 2000 took 0.104s
  training loss:		2.294056
  validation loss:		2.244621
  validation accuracy:		13.04 %
Epoch 1121 of 2000 took 0.101s
  training loss:		2.295160
  validation loss:		2.244706
  validation accuracy:		12.93 %
Epoch 1122 of 2000 took 0.103s
  training loss:		2.294909
  validation loss:		2.244247
  validation accuracy:		12.93 %
Epoch 1123 of 2000 took 0.103s
  training loss:		2.294775
  validation loss:		2.244361
  validation accuracy:		12.93 %
Epoch 1124 of 2000 took 0.104s
  training loss:		2.295118
  validation loss:		2.244337
  validation accuracy:		13.04 %
Epoch 1125 of 2000 took 0.103s
  training loss:		2.294601
  validation loss:		2.244812
  validation accuracy:		13.04 %
Epoch 1126 of 2000 took 0.103s
  training loss:		2.294398
  validation loss:		2.245153
  validation accuracy:		13.04 %
Epoch 1127 of 2000 took 0.103s
  training loss:		2.294115
  validation loss:		2.244429
  validation accuracy:		13.04 %
Epoch 1128 of 2000 took 0.103s
  training loss:		2.294141
  validation loss:		2.244233
  validation accuracy:		13.04 %
Epoch 1129 of 2000 took 0.103s
  training loss:		2.294314
  validation loss:		2.244153
  validation accuracy:		13.04 %
Epoch 1130 of 2000 took 0.103s
  training loss:		2.294318
  validation loss:		2.244193
  validation accuracy:		13.04 %
Epoch 1131 of 2000 took 0.103s
  training loss:		2.294545
  validation loss:		2.244031
  validation accuracy:		12.93 %
Epoch 1132 of 2000 took 0.103s
  training loss:		2.294166
  validation loss:		2.244612
  validation accuracy:		13.04 %
Epoch 1133 of 2000 took 0.103s
  training loss:		2.294887
  validation loss:		2.244660
  validation accuracy:		12.93 %
Epoch 1134 of 2000 took 0.103s
  training loss:		2.294638
  validation loss:		2.244300
  validation accuracy:		12.93 %
Epoch 1135 of 2000 took 0.103s
  training loss:		2.294842
  validation loss:		2.244539
  validation accuracy:		13.04 %
Epoch 1136 of 2000 took 0.103s
  training loss:		2.294676
  validation loss:		2.244957
  validation accuracy:		13.04 %
Epoch 1137 of 2000 took 0.106s
  training loss:		2.295307
  validation loss:		2.244924
  validation accuracy:		13.04 %
Epoch 1138 of 2000 took 0.103s
  training loss:		2.294960
  validation loss:		2.245507
  validation accuracy:		13.04 %
Epoch 1139 of 2000 took 0.103s
  training loss:		2.293871
  validation loss:		2.244933
  validation accuracy:		13.04 %
Epoch 1140 of 2000 took 0.103s
  training loss:		2.294819
  validation loss:		2.244396
  validation accuracy:		13.04 %
Epoch 1141 of 2000 took 0.103s
  training loss:		2.294775
  validation loss:		2.244517
  validation accuracy:		13.04 %
Epoch 1142 of 2000 took 0.103s
  training loss:		2.294394
  validation loss:		2.244455
  validation accuracy:		13.04 %
Epoch 1143 of 2000 took 0.103s
  training loss:		2.295168
  validation loss:		2.244797
  validation accuracy:		13.04 %
Epoch 1144 of 2000 took 0.103s
  training loss:		2.294354
  validation loss:		2.245013
  validation accuracy:		12.93 %
Epoch 1145 of 2000 took 0.103s
  training loss:		2.294914
  validation loss:		2.244854
  validation accuracy:		12.93 %
Epoch 1146 of 2000 took 0.103s
  training loss:		2.294688
  validation loss:		2.244991
  validation accuracy:		12.93 %
Epoch 1147 of 2000 took 0.103s
  training loss:		2.295844
  validation loss:		2.245562
  validation accuracy:		12.93 %
Epoch 1148 of 2000 took 0.103s
  training loss:		2.294472
  validation loss:		2.245462
  validation accuracy:		12.83 %
Epoch 1149 of 2000 took 0.103s
  training loss:		2.293865
  validation loss:		2.244691
  validation accuracy:		12.83 %
Epoch 1150 of 2000 took 0.103s
  training loss:		2.293757
  validation loss:		2.244647
  validation accuracy:		12.93 %
Epoch 1151 of 2000 took 0.103s
  training loss:		2.294847
  validation loss:		2.244725
  validation accuracy:		12.93 %
Epoch 1152 of 2000 took 0.103s
  training loss:		2.294762
  validation loss:		2.244665
  validation accuracy:		12.93 %
Epoch 1153 of 2000 took 0.104s
  training loss:		2.294429
  validation loss:		2.244551
  validation accuracy:		12.93 %
Epoch 1154 of 2000 took 0.103s
  training loss:		2.295044
  validation loss:		2.244484
  validation accuracy:		12.83 %
Epoch 1155 of 2000 took 0.103s
  training loss:		2.294869
  validation loss:		2.244753
  validation accuracy:		12.83 %
Epoch 1156 of 2000 took 0.103s
  training loss:		2.294539
  validation loss:		2.245124
  validation accuracy:		12.83 %
Epoch 1157 of 2000 took 0.103s
  training loss:		2.294591
  validation loss:		2.244587
  validation accuracy:		12.83 %
Epoch 1158 of 2000 took 0.103s
  training loss:		2.294742
  validation loss:		2.244736
  validation accuracy:		12.93 %
Epoch 1159 of 2000 took 0.103s
  training loss:		2.294984
  validation loss:		2.244664
  validation accuracy:		12.93 %
Epoch 1160 of 2000 took 0.103s
  training loss:		2.295195
  validation loss:		2.244683
  validation accuracy:		12.93 %
Epoch 1161 of 2000 took 0.103s
  training loss:		2.294286
  validation loss:		2.244943
  validation accuracy:		12.93 %
Epoch 1162 of 2000 took 0.104s
  training loss:		2.294728
  validation loss:		2.245691
  validation accuracy:		12.93 %
Epoch 1163 of 2000 took 0.103s
  training loss:		2.293726
  validation loss:		2.245272
  validation accuracy:		12.93 %
Epoch 1164 of 2000 took 0.103s
  training loss:		2.293282
  validation loss:		2.244693
  validation accuracy:		13.04 %
Epoch 1165 of 2000 took 0.103s
  training loss:		2.295801
  validation loss:		2.244623
  validation accuracy:		12.93 %
Epoch 1166 of 2000 took 0.103s
  training loss:		2.294342
  validation loss:		2.245299
  validation accuracy:		13.04 %
Epoch 1167 of 2000 took 0.103s
  training loss:		2.294953
  validation loss:		2.245317
  validation accuracy:		13.04 %
Epoch 1168 of 2000 took 0.103s
  training loss:		2.293780
  validation loss:		2.244511
  validation accuracy:		13.04 %
Epoch 1169 of 2000 took 0.103s
  training loss:		2.294711
  validation loss:		2.244098
  validation accuracy:		12.93 %
Epoch 1170 of 2000 took 0.103s
  training loss:		2.294045
  validation loss:		2.243655
  validation accuracy:		12.93 %
Epoch 1171 of 2000 took 0.103s
  training loss:		2.294049
  validation loss:		2.243614
  validation accuracy:		12.93 %
Epoch 1172 of 2000 took 0.103s
  training loss:		2.295797
  validation loss:		2.244264
  validation accuracy:		12.93 %
Epoch 1173 of 2000 took 0.103s
  training loss:		2.294622
  validation loss:		2.244349
  validation accuracy:		12.93 %
Epoch 1174 of 2000 took 0.103s
  training loss:		2.295273
  validation loss:		2.244412
  validation accuracy:		12.93 %
Epoch 1175 of 2000 took 0.103s
  training loss:		2.294252
  validation loss:		2.244227
  validation accuracy:		12.93 %
Epoch 1176 of 2000 took 0.103s
  training loss:		2.294130
  validation loss:		2.244033
  validation accuracy:		12.93 %
Epoch 1177 of 2000 took 0.103s
  training loss:		2.293867
  validation loss:		2.244617
  validation accuracy:		13.04 %
Epoch 1178 of 2000 took 0.103s
  training loss:		2.295095
  validation loss:		2.244259
  validation accuracy:		13.04 %
Epoch 1179 of 2000 took 0.103s
  training loss:		2.294500
  validation loss:		2.244250
  validation accuracy:		13.04 %
Epoch 1180 of 2000 took 0.103s
  training loss:		2.294236
  validation loss:		2.244026
  validation accuracy:		13.04 %
Epoch 1181 of 2000 took 0.103s
  training loss:		2.295357
  validation loss:		2.244396
  validation accuracy:		13.04 %
Epoch 1182 of 2000 took 0.103s
  training loss:		2.294331
  validation loss:		2.244824
  validation accuracy:		13.04 %
Epoch 1183 of 2000 took 0.104s
  training loss:		2.294233
  validation loss:		2.244790
  validation accuracy:		13.04 %
Epoch 1184 of 2000 took 0.103s
  training loss:		2.293799
  validation loss:		2.244073
  validation accuracy:		13.04 %
Epoch 1185 of 2000 took 0.103s
  training loss:		2.295016
  validation loss:		2.243618
  validation accuracy:		12.83 %
Epoch 1186 of 2000 took 0.103s
  training loss:		2.295437
  validation loss:		2.244123
  validation accuracy:		12.83 %
Epoch 1187 of 2000 took 0.103s
  training loss:		2.294157
  validation loss:		2.244397
  validation accuracy:		12.83 %
Epoch 1188 of 2000 took 0.103s
  training loss:		2.294788
  validation loss:		2.244502
  validation accuracy:		12.83 %
Epoch 1189 of 2000 took 0.103s
  training loss:		2.295038
  validation loss:		2.244667
  validation accuracy:		12.83 %
Epoch 1190 of 2000 took 0.103s
  training loss:		2.294869
  validation loss:		2.245343
  validation accuracy:		13.04 %
Epoch 1191 of 2000 took 0.103s
  training loss:		2.294273
  validation loss:		2.245379
  validation accuracy:		13.04 %
Epoch 1192 of 2000 took 0.103s
  training loss:		2.294139
  validation loss:		2.245042
  validation accuracy:		13.04 %
Epoch 1193 of 2000 took 0.103s
  training loss:		2.293510
  validation loss:		2.244896
  validation accuracy:		13.04 %
Epoch 1194 of 2000 took 0.103s
  training loss:		2.294634
  validation loss:		2.244376
  validation accuracy:		12.93 %
Epoch 1195 of 2000 took 0.103s
  training loss:		2.294627
  validation loss:		2.243796
  validation accuracy:		12.93 %
Epoch 1196 of 2000 took 0.103s
  training loss:		2.294542
  validation loss:		2.243476
  validation accuracy:		13.04 %
Epoch 1197 of 2000 took 0.103s
  training loss:		2.294941
  validation loss:		2.243951
  validation accuracy:		12.83 %
Epoch 1198 of 2000 took 0.103s
  training loss:		2.295050
  validation loss:		2.244012
  validation accuracy:		12.93 %
Epoch 1199 of 2000 took 0.103s
  training loss:		2.294685
  validation loss:		2.244775
  validation accuracy:		13.04 %
Epoch 1200 of 2000 took 0.103s
  training loss:		2.294832
  validation loss:		2.244338
  validation accuracy:		13.04 %
Epoch 1201 of 2000 took 0.103s
  training loss:		2.294192
  validation loss:		2.244577
  validation accuracy:		12.93 %
Epoch 1202 of 2000 took 0.103s
  training loss:		2.295253
  validation loss:		2.245097
  validation accuracy:		13.04 %
Epoch 1203 of 2000 took 0.103s
  training loss:		2.294673
  validation loss:		2.245370
  validation accuracy:		12.93 %
Epoch 1204 of 2000 took 0.103s
  training loss:		2.295009
  validation loss:		2.245282
  validation accuracy:		12.93 %
Epoch 1205 of 2000 took 0.103s
  training loss:		2.293904
  validation loss:		2.245179
  validation accuracy:		12.93 %
Epoch 1206 of 2000 took 0.103s
  training loss:		2.295208
  validation loss:		2.245085
  validation accuracy:		13.04 %
Epoch 1207 of 2000 took 0.103s
  training loss:		2.294409
  validation loss:		2.245048
  validation accuracy:		12.93 %
Epoch 1208 of 2000 took 0.103s
  training loss:		2.294680
  validation loss:		2.244955
  validation accuracy:		13.04 %
Epoch 1209 of 2000 took 0.103s
  training loss:		2.294744
  validation loss:		2.245243
  validation accuracy:		13.04 %
Epoch 1210 of 2000 took 0.103s
  training loss:		2.293627
  validation loss:		2.244854
  validation accuracy:		13.04 %
Epoch 1211 of 2000 took 0.103s
  training loss:		2.294762
  validation loss:		2.244392
  validation accuracy:		13.04 %
Epoch 1212 of 2000 took 0.104s
  training loss:		2.294272
  validation loss:		2.244650
  validation accuracy:		12.93 %
Epoch 1213 of 2000 took 0.103s
  training loss:		2.293706
  validation loss:		2.243977
  validation accuracy:		12.93 %
Epoch 1214 of 2000 took 0.103s
  training loss:		2.294016
  validation loss:		2.243558
  validation accuracy:		12.93 %
Epoch 1215 of 2000 took 0.103s
  training loss:		2.294754
  validation loss:		2.243813
  validation accuracy:		13.04 %
Epoch 1216 of 2000 took 0.103s
  training loss:		2.294982
  validation loss:		2.244789
  validation accuracy:		13.04 %
Epoch 1217 of 2000 took 0.103s
  training loss:		2.294504
  validation loss:		2.244807
  validation accuracy:		13.04 %
Epoch 1218 of 2000 took 0.103s
  training loss:		2.294673
  validation loss:		2.244865
  validation accuracy:		13.04 %
Epoch 1219 of 2000 took 0.103s
  training loss:		2.294797
  validation loss:		2.244851
  validation accuracy:		13.04 %
Epoch 1220 of 2000 took 0.103s
  training loss:		2.294410
  validation loss:		2.244745
  validation accuracy:		12.93 %
Epoch 1221 of 2000 took 0.103s
  training loss:		2.294228
  validation loss:		2.244888
  validation accuracy:		12.93 %
Epoch 1222 of 2000 took 0.103s
  training loss:		2.294385
  validation loss:		2.244729
  validation accuracy:		12.93 %
Epoch 1223 of 2000 took 0.103s
  training loss:		2.294991
  validation loss:		2.244394
  validation accuracy:		12.93 %
Epoch 1224 of 2000 took 0.109s
  training loss:		2.295427
  validation loss:		2.244743
  validation accuracy:		12.93 %
Epoch 1225 of 2000 took 0.112s
  training loss:		2.294242
  validation loss:		2.244634
  validation accuracy:		12.93 %
Epoch 1226 of 2000 took 0.144s
  training loss:		2.294643
  validation loss:		2.244122
  validation accuracy:		13.04 %
Epoch 1227 of 2000 took 0.118s
  training loss:		2.293613
  validation loss:		2.244164
  validation accuracy:		13.04 %
Epoch 1228 of 2000 took 0.109s
  training loss:		2.294865
  validation loss:		2.244323
  validation accuracy:		13.04 %
Epoch 1229 of 2000 took 0.109s
  training loss:		2.294035
  validation loss:		2.244362
  validation accuracy:		13.04 %
Epoch 1230 of 2000 took 0.109s
  training loss:		2.294934
  validation loss:		2.244417
  validation accuracy:		13.04 %
Epoch 1231 of 2000 took 0.115s
  training loss:		2.295043
  validation loss:		2.244499
  validation accuracy:		13.04 %
Epoch 1232 of 2000 took 0.110s
  training loss:		2.294612
  validation loss:		2.244898
  validation accuracy:		13.04 %
Epoch 1233 of 2000 took 0.107s
  training loss:		2.294831
  validation loss:		2.244603
  validation accuracy:		13.04 %
Epoch 1234 of 2000 took 0.113s
  training loss:		2.294268
  validation loss:		2.244604
  validation accuracy:		12.93 %
Epoch 1235 of 2000 took 0.111s
  training loss:		2.295126
  validation loss:		2.245149
  validation accuracy:		12.93 %
Epoch 1236 of 2000 took 0.106s
  training loss:		2.294242
  validation loss:		2.245341
  validation accuracy:		12.93 %
Epoch 1237 of 2000 took 0.106s
  training loss:		2.294100
  validation loss:		2.244962
  validation accuracy:		12.93 %
Epoch 1238 of 2000 took 0.105s
  training loss:		2.294436
  validation loss:		2.244471
  validation accuracy:		12.93 %
Epoch 1239 of 2000 took 0.115s
  training loss:		2.294721
  validation loss:		2.245153
  validation accuracy:		12.93 %
Epoch 1240 of 2000 took 0.109s
  training loss:		2.294273
  validation loss:		2.244595
  validation accuracy:		12.93 %
Epoch 1241 of 2000 took 0.106s
  training loss:		2.294267
  validation loss:		2.244280
  validation accuracy:		12.93 %
Epoch 1242 of 2000 took 0.114s
  training loss:		2.294324
  validation loss:		2.244173
  validation accuracy:		12.93 %
Epoch 1243 of 2000 took 0.110s
  training loss:		2.294746
  validation loss:		2.244229
  validation accuracy:		12.93 %
Epoch 1244 of 2000 took 0.104s
  training loss:		2.294805
  validation loss:		2.244411
  validation accuracy:		12.93 %
Epoch 1245 of 2000 took 0.106s
  training loss:		2.293611
  validation loss:		2.244277
  validation accuracy:		12.93 %
Epoch 1246 of 2000 took 0.100s
  training loss:		2.294901
  validation loss:		2.244675
  validation accuracy:		12.93 %
Epoch 1247 of 2000 took 0.107s
  training loss:		2.294302
  validation loss:		2.244616
  validation accuracy:		12.93 %
Epoch 1248 of 2000 took 0.110s
  training loss:		2.293597
  validation loss:		2.243747
  validation accuracy:		12.93 %
Epoch 1249 of 2000 took 0.105s
  training loss:		2.293786
  validation loss:		2.243460
  validation accuracy:		12.93 %
Epoch 1250 of 2000 took 0.110s
  training loss:		2.295473
  validation loss:		2.243509
  validation accuracy:		13.04 %
Epoch 1251 of 2000 took 0.116s
  training loss:		2.294729
  validation loss:		2.243713
  validation accuracy:		12.93 %
Epoch 1252 of 2000 took 0.105s
  training loss:		2.294068
  validation loss:		2.244246
  validation accuracy:		12.93 %
Epoch 1253 of 2000 took 0.109s
  training loss:		2.295007
  validation loss:		2.244423
  validation accuracy:		12.93 %
Epoch 1254 of 2000 took 0.106s
  training loss:		2.294302
  validation loss:		2.243876
  validation accuracy:		12.93 %
Epoch 1255 of 2000 took 0.109s
  training loss:		2.294850
  validation loss:		2.243965
  validation accuracy:		12.93 %
Epoch 1256 of 2000 took 0.113s
  training loss:		2.295094
  validation loss:		2.244298
  validation accuracy:		12.93 %
Epoch 1257 of 2000 took 0.106s
  training loss:		2.295429
  validation loss:		2.244958
  validation accuracy:		12.93 %
Epoch 1258 of 2000 took 0.108s
  training loss:		2.294625
  validation loss:		2.244976
  validation accuracy:		12.93 %
Epoch 1259 of 2000 took 0.119s
  training loss:		2.293807
  validation loss:		2.244600
  validation accuracy:		13.04 %
Epoch 1260 of 2000 took 0.106s
  training loss:		2.294771
  validation loss:		2.244636
  validation accuracy:		13.04 %
Epoch 1261 of 2000 took 0.109s
  training loss:		2.294734
  validation loss:		2.244773
  validation accuracy:		13.04 %
Epoch 1262 of 2000 took 0.105s
  training loss:		2.294323
  validation loss:		2.244945
  validation accuracy:		13.04 %
Epoch 1263 of 2000 took 0.110s
  training loss:		2.294684
  validation loss:		2.244787
  validation accuracy:		13.04 %
Epoch 1264 of 2000 took 0.113s
  training loss:		2.294892
  validation loss:		2.244552
  validation accuracy:		12.93 %
Epoch 1265 of 2000 took 0.106s
  training loss:		2.294581
  validation loss:		2.245315
  validation accuracy:		12.93 %
Epoch 1266 of 2000 took 0.111s
  training loss:		2.293776
  validation loss:		2.245011
  validation accuracy:		12.93 %
Epoch 1267 of 2000 took 0.117s
  training loss:		2.294033
  validation loss:		2.244991
  validation accuracy:		13.04 %
Epoch 1268 of 2000 took 0.106s
  training loss:		2.294152
  validation loss:		2.244494
  validation accuracy:		13.04 %
Epoch 1269 of 2000 took 0.108s
  training loss:		2.295266
  validation loss:		2.244758
  validation accuracy:		12.93 %
Epoch 1270 of 2000 took 0.105s
  training loss:		2.294858
  validation loss:		2.244159
  validation accuracy:		12.93 %
Epoch 1271 of 2000 took 0.106s
  training loss:		2.294072
  validation loss:		2.244559
  validation accuracy:		12.93 %
Epoch 1272 of 2000 took 0.117s
  training loss:		2.294548
  validation loss:		2.244318
  validation accuracy:		12.93 %
Epoch 1273 of 2000 took 0.111s
  training loss:		2.294672
  validation loss:		2.244668
  validation accuracy:		12.93 %
Epoch 1274 of 2000 took 0.107s
  training loss:		2.295296
  validation loss:		2.245470
  validation accuracy:		12.93 %
Epoch 1275 of 2000 took 0.115s
  training loss:		2.294004
  validation loss:		2.245214
  validation accuracy:		12.93 %
Epoch 1276 of 2000 took 0.107s
  training loss:		2.294607
  validation loss:		2.245173
  validation accuracy:		12.93 %
Epoch 1277 of 2000 took 0.107s
  training loss:		2.294680
  validation loss:		2.244547
  validation accuracy:		12.93 %
Epoch 1278 of 2000 took 0.106s
  training loss:		2.293666
  validation loss:		2.244042
  validation accuracy:		12.93 %
Epoch 1279 of 2000 took 0.107s
  training loss:		2.294862
  validation loss:		2.244284
  validation accuracy:		12.93 %
Epoch 1280 of 2000 took 0.114s
  training loss:		2.294669
  validation loss:		2.243956
  validation accuracy:		12.93 %
Epoch 1281 of 2000 took 0.108s
  training loss:		2.293838
  validation loss:		2.243610
  validation accuracy:		12.83 %
Epoch 1282 of 2000 took 0.106s
  training loss:		2.294198
  validation loss:		2.243648
  validation accuracy:		13.04 %
Epoch 1283 of 2000 took 0.113s
  training loss:		2.294320
  validation loss:		2.243625
  validation accuracy:		13.04 %
Epoch 1284 of 2000 took 0.113s
  training loss:		2.294060
  validation loss:		2.243551
  validation accuracy:		12.93 %
Epoch 1285 of 2000 took 0.109s
  training loss:		2.294644
  validation loss:		2.243259
  validation accuracy:		12.93 %
Epoch 1286 of 2000 took 0.107s
  training loss:		2.294847
  validation loss:		2.243711
  validation accuracy:		12.93 %
Epoch 1287 of 2000 took 0.110s
  training loss:		2.294005
  validation loss:		2.243842
  validation accuracy:		12.93 %
Epoch 1288 of 2000 took 0.118s
  training loss:		2.294285
  validation loss:		2.243167
  validation accuracy:		12.93 %
Epoch 1289 of 2000 took 0.113s
  training loss:		2.294380
  validation loss:		2.243502
  validation accuracy:		12.93 %
Epoch 1290 of 2000 took 0.110s
  training loss:		2.294737
  validation loss:		2.244063
  validation accuracy:		13.04 %
Epoch 1291 of 2000 took 0.117s
  training loss:		2.294856
  validation loss:		2.244888
  validation accuracy:		12.93 %
Epoch 1292 of 2000 took 0.114s
  training loss:		2.293755
  validation loss:		2.244771
  validation accuracy:		12.93 %
Epoch 1293 of 2000 took 0.111s
  training loss:		2.294290
  validation loss:		2.244275
  validation accuracy:		12.93 %
Epoch 1294 of 2000 took 0.111s
  training loss:		2.294769
  validation loss:		2.244240
  validation accuracy:		12.93 %
Epoch 1295 of 2000 took 0.112s
  training loss:		2.294668
  validation loss:		2.244479
  validation accuracy:		12.93 %
Epoch 1296 of 2000 took 0.117s
  training loss:		2.293734
  validation loss:		2.244468
  validation accuracy:		12.93 %
Epoch 1297 of 2000 took 0.112s
  training loss:		2.293548
  validation loss:		2.243902
  validation accuracy:		13.37 %
Epoch 1298 of 2000 took 0.111s
  training loss:		2.294293
  validation loss:		2.243231
  validation accuracy:		12.93 %
Epoch 1299 of 2000 took 0.117s
  training loss:		2.294763
  validation loss:		2.243260
  validation accuracy:		12.93 %
Epoch 1300 of 2000 took 0.113s
  training loss:		2.293992
  validation loss:		2.243422
  validation accuracy:		13.04 %
Epoch 1301 of 2000 took 0.112s
  training loss:		2.294752
  validation loss:		2.243780
  validation accuracy:		12.93 %
Epoch 1302 of 2000 took 0.111s
  training loss:		2.294708
  validation loss:		2.244021
  validation accuracy:		13.04 %
Epoch 1303 of 2000 took 0.112s
  training loss:		2.294823
  validation loss:		2.244239
  validation accuracy:		12.93 %
Epoch 1304 of 2000 took 0.117s
  training loss:		2.294533
  validation loss:		2.244201
  validation accuracy:		12.93 %
Epoch 1305 of 2000 took 0.112s
  training loss:		2.294535
  validation loss:		2.244567
  validation accuracy:		12.93 %
Epoch 1306 of 2000 took 0.111s
  training loss:		2.294107
  validation loss:		2.244172
  validation accuracy:		12.93 %
Epoch 1307 of 2000 took 0.116s
  training loss:		2.294414
  validation loss:		2.244227
  validation accuracy:		12.93 %
Epoch 1308 of 2000 took 0.110s
  training loss:		2.293736
  validation loss:		2.244009
  validation accuracy:		12.93 %
Epoch 1309 of 2000 took 0.114s
  training loss:		2.294743
  validation loss:		2.243621
  validation accuracy:		12.93 %
Epoch 1310 of 2000 took 0.109s
  training loss:		2.294458
  validation loss:		2.243415
  validation accuracy:		12.93 %
Epoch 1311 of 2000 took 0.104s
  training loss:		2.294186
  validation loss:		2.243480
  validation accuracy:		13.04 %
Epoch 1312 of 2000 took 0.104s
  training loss:		2.294738
  validation loss:		2.244169
  validation accuracy:		13.04 %
Epoch 1313 of 2000 took 0.104s
  training loss:		2.293436
  validation loss:		2.244015
  validation accuracy:		13.04 %
Epoch 1314 of 2000 took 0.104s
  training loss:		2.294596
  validation loss:		2.243941
  validation accuracy:		12.93 %
Epoch 1315 of 2000 took 0.104s
  training loss:		2.294610
  validation loss:		2.243571
  validation accuracy:		13.04 %
Epoch 1316 of 2000 took 0.104s
  training loss:		2.294978
  validation loss:		2.243613
  validation accuracy:		13.04 %
Epoch 1317 of 2000 took 0.103s
  training loss:		2.294885
  validation loss:		2.243597
  validation accuracy:		13.04 %
Epoch 1318 of 2000 took 0.103s
  training loss:		2.295277
  validation loss:		2.244736
  validation accuracy:		13.04 %
Epoch 1319 of 2000 took 0.103s
  training loss:		2.294854
  validation loss:		2.244930
  validation accuracy:		13.04 %
Epoch 1320 of 2000 took 0.103s
  training loss:		2.294258
  validation loss:		2.245368
  validation accuracy:		13.04 %
Epoch 1321 of 2000 took 0.103s
  training loss:		2.293524
  validation loss:		2.244763
  validation accuracy:		12.93 %
Epoch 1322 of 2000 took 0.104s
  training loss:		2.294983
  validation loss:		2.244358
  validation accuracy:		12.93 %
Epoch 1323 of 2000 took 0.103s
  training loss:		2.294763
  validation loss:		2.244737
  validation accuracy:		12.93 %
Epoch 1324 of 2000 took 0.103s
  training loss:		2.294667
  validation loss:		2.244401
  validation accuracy:		12.93 %
Epoch 1325 of 2000 took 0.103s
  training loss:		2.293670
  validation loss:		2.244447
  validation accuracy:		13.04 %
Epoch 1326 of 2000 took 0.103s
  training loss:		2.294172
  validation loss:		2.244306
  validation accuracy:		13.04 %
Epoch 1327 of 2000 took 0.103s
  training loss:		2.293630
  validation loss:		2.244052
  validation accuracy:		13.04 %
Epoch 1328 of 2000 took 0.103s
  training loss:		2.293908
  validation loss:		2.243359
  validation accuracy:		13.04 %
Epoch 1329 of 2000 took 0.103s
  training loss:		2.295369
  validation loss:		2.243876
  validation accuracy:		13.04 %
Epoch 1330 of 2000 took 0.103s
  training loss:		2.294092
  validation loss:		2.243780
  validation accuracy:		13.04 %
Epoch 1331 of 2000 took 0.103s
  training loss:		2.294781
  validation loss:		2.244360
  validation accuracy:		13.04 %
Epoch 1332 of 2000 took 0.103s
  training loss:		2.294217
  validation loss:		2.244462
  validation accuracy:		12.93 %
Epoch 1333 of 2000 took 0.103s
  training loss:		2.295007
  validation loss:		2.244106
  validation accuracy:		13.04 %
Epoch 1334 of 2000 took 0.103s
  training loss:		2.294755
  validation loss:		2.244505
  validation accuracy:		13.04 %
Epoch 1335 of 2000 took 0.103s
  training loss:		2.294847
  validation loss:		2.244825
  validation accuracy:		13.04 %
Epoch 1336 of 2000 took 0.103s
  training loss:		2.293730
  validation loss:		2.244452
  validation accuracy:		13.04 %
Epoch 1337 of 2000 took 0.103s
  training loss:		2.294984
  validation loss:		2.244625
  validation accuracy:		12.93 %
Epoch 1338 of 2000 took 0.103s
  training loss:		2.294470
  validation loss:		2.244782
  validation accuracy:		12.93 %
Epoch 1339 of 2000 took 0.103s
  training loss:		2.294931
  validation loss:		2.245275
  validation accuracy:		12.93 %
Epoch 1340 of 2000 took 0.103s
  training loss:		2.296160
  validation loss:		2.245806
  validation accuracy:		12.93 %
Epoch 1341 of 2000 took 0.103s
  training loss:		2.294744
  validation loss:		2.245609
  validation accuracy:		12.93 %
Epoch 1342 of 2000 took 0.103s
  training loss:		2.294621
  validation loss:		2.245778
  validation accuracy:		12.93 %
Epoch 1343 of 2000 took 0.103s
  training loss:		2.294040
  validation loss:		2.245760
  validation accuracy:		12.93 %
Epoch 1344 of 2000 took 0.103s
  training loss:		2.293700
  validation loss:		2.245221
  validation accuracy:		12.93 %
Epoch 1345 of 2000 took 0.103s
  training loss:		2.294274
  validation loss:		2.244673
  validation accuracy:		12.93 %
Epoch 1346 of 2000 took 0.103s
  training loss:		2.294414
  validation loss:		2.244431
  validation accuracy:		12.93 %
Epoch 1347 of 2000 took 0.103s
  training loss:		2.294698
  validation loss:		2.244972
  validation accuracy:		12.93 %
Epoch 1348 of 2000 took 0.103s
  training loss:		2.294727
  validation loss:		2.244774
  validation accuracy:		13.04 %
Epoch 1349 of 2000 took 0.103s
  training loss:		2.293604
  validation loss:		2.243687
  validation accuracy:		13.04 %
Epoch 1350 of 2000 took 0.103s
  training loss:		2.294494
  validation loss:		2.243560
  validation accuracy:		15.00 %
Epoch 1351 of 2000 took 0.104s
  training loss:		2.293492
  validation loss:		2.243219
  validation accuracy:		13.04 %
Epoch 1352 of 2000 took 0.104s
  training loss:		2.293886
  validation loss:		2.242515
  validation accuracy:		12.93 %
Epoch 1353 of 2000 took 0.103s
  training loss:		2.294270
  validation loss:		2.242661
  validation accuracy:		13.04 %
Epoch 1354 of 2000 took 0.103s
  training loss:		2.293784
  validation loss:		2.242938
  validation accuracy:		13.04 %
Epoch 1355 of 2000 took 0.103s
  training loss:		2.293845
  validation loss:		2.242973
  validation accuracy:		12.93 %
Epoch 1356 of 2000 took 0.103s
  training loss:		2.295195
  validation loss:		2.242878
  validation accuracy:		13.04 %
Epoch 1357 of 2000 took 0.103s
  training loss:		2.294401
  validation loss:		2.243210
  validation accuracy:		13.04 %
Epoch 1358 of 2000 took 0.103s
  training loss:		2.294606
  validation loss:		2.244010
  validation accuracy:		12.83 %
Epoch 1359 of 2000 took 0.103s
  training loss:		2.294871
  validation loss:		2.244647
  validation accuracy:		12.83 %
Epoch 1360 of 2000 took 0.103s
  training loss:		2.293873
  validation loss:		2.244631
  validation accuracy:		12.83 %
Epoch 1361 of 2000 took 0.103s
  training loss:		2.294081
  validation loss:		2.244885
  validation accuracy:		12.83 %
Epoch 1362 of 2000 took 0.103s
  training loss:		2.293498
  validation loss:		2.244182
  validation accuracy:		12.83 %
Epoch 1363 of 2000 took 0.103s
  training loss:		2.293819
  validation loss:		2.243187
  validation accuracy:		12.83 %
Epoch 1364 of 2000 took 0.103s
  training loss:		2.294879
  validation loss:		2.243396
  validation accuracy:		12.83 %
Epoch 1365 of 2000 took 0.103s
  training loss:		2.294901
  validation loss:		2.243240
  validation accuracy:		12.83 %
Epoch 1366 of 2000 took 0.104s
  training loss:		2.294091
  validation loss:		2.243599
  validation accuracy:		12.83 %
Epoch 1367 of 2000 took 0.103s
  training loss:		2.294637
  validation loss:		2.243878
  validation accuracy:		12.83 %
Epoch 1368 of 2000 took 0.103s
  training loss:		2.295041
  validation loss:		2.244244
  validation accuracy:		12.93 %
Epoch 1369 of 2000 took 0.103s
  training loss:		2.294359
  validation loss:		2.245045
  validation accuracy:		12.93 %
Epoch 1370 of 2000 took 0.103s
  training loss:		2.294977
  validation loss:		2.245033
  validation accuracy:		12.93 %
Epoch 1371 of 2000 took 0.103s
  training loss:		2.293649
  validation loss:		2.244803
  validation accuracy:		12.93 %
Epoch 1372 of 2000 took 0.103s
  training loss:		2.294691
  validation loss:		2.244437
  validation accuracy:		12.93 %
Epoch 1373 of 2000 took 0.103s
  training loss:		2.294294
  validation loss:		2.244286
  validation accuracy:		13.04 %
Epoch 1374 of 2000 took 0.103s
  training loss:		2.294230
  validation loss:		2.244461
  validation accuracy:		13.04 %
Epoch 1375 of 2000 took 0.103s
  training loss:		2.294530
  validation loss:		2.244674
  validation accuracy:		12.93 %
Epoch 1376 of 2000 took 0.103s
  training loss:		2.294837
  validation loss:		2.244489
  validation accuracy:		12.93 %
Epoch 1377 of 2000 took 0.103s
  training loss:		2.294433
  validation loss:		2.244537
  validation accuracy:		12.93 %
Epoch 1378 of 2000 took 0.103s
  training loss:		2.293645
  validation loss:		2.244212
  validation accuracy:		12.93 %
Epoch 1379 of 2000 took 0.103s
  training loss:		2.294906
  validation loss:		2.244028
  validation accuracy:		12.93 %
Epoch 1380 of 2000 took 0.103s
  training loss:		2.294425
  validation loss:		2.243934
  validation accuracy:		13.04 %
Epoch 1381 of 2000 took 0.104s
  training loss:		2.295235
  validation loss:		2.244358
  validation accuracy:		13.04 %
Epoch 1382 of 2000 took 0.103s
  training loss:		2.294055
  validation loss:		2.244881
  validation accuracy:		12.93 %
Epoch 1383 of 2000 took 0.103s
  training loss:		2.294100
  validation loss:		2.244885
  validation accuracy:		13.04 %
Epoch 1384 of 2000 took 0.103s
  training loss:		2.294193
  validation loss:		2.245002
  validation accuracy:		13.04 %
Epoch 1385 of 2000 took 0.103s
  training loss:		2.294449
  validation loss:		2.244907
  validation accuracy:		13.04 %
Epoch 1386 of 2000 took 0.103s
  training loss:		2.294688
  validation loss:		2.244592
  validation accuracy:		12.93 %
Epoch 1387 of 2000 took 0.103s
  training loss:		2.293504
  validation loss:		2.243605
  validation accuracy:		12.93 %
Epoch 1388 of 2000 took 0.103s
  training loss:		2.294006
  validation loss:		2.243529
  validation accuracy:		12.93 %
Epoch 1389 of 2000 took 0.103s
  training loss:		2.294071
  validation loss:		2.243226
  validation accuracy:		12.93 %
Epoch 1390 of 2000 took 0.103s
  training loss:		2.294200
  validation loss:		2.242951
  validation accuracy:		13.04 %
Epoch 1391 of 2000 took 0.103s
  training loss:		2.294220
  validation loss:		2.243213
  validation accuracy:		13.04 %
Epoch 1392 of 2000 took 0.103s
  training loss:		2.294606
  validation loss:		2.243102
  validation accuracy:		13.04 %
Epoch 1393 of 2000 took 0.105s
  training loss:		2.295205
  validation loss:		2.243749
  validation accuracy:		13.04 %
Epoch 1394 of 2000 took 0.103s
  training loss:		2.293192
  validation loss:		2.243708
  validation accuracy:		12.93 %
Epoch 1395 of 2000 took 0.103s
  training loss:		2.294197
  validation loss:		2.243516
  validation accuracy:		12.93 %
Epoch 1396 of 2000 took 0.103s
  training loss:		2.293373
  validation loss:		2.243305
  validation accuracy:		12.93 %
Epoch 1397 of 2000 took 0.103s
  training loss:		2.293628
  validation loss:		2.242987
  validation accuracy:		12.93 %
Epoch 1398 of 2000 took 0.103s
  training loss:		2.293926
  validation loss:		2.242598
  validation accuracy:		12.93 %
Epoch 1399 of 2000 took 0.103s
  training loss:		2.294402
  validation loss:		2.242895
  validation accuracy:		12.83 %
Epoch 1400 of 2000 took 0.103s
  training loss:		2.294165
  validation loss:		2.242929
  validation accuracy:		12.93 %
Epoch 1401 of 2000 took 0.103s
  training loss:		2.294913
  validation loss:		2.242733
  validation accuracy:		12.93 %
Epoch 1402 of 2000 took 0.103s
  training loss:		2.294950
  validation loss:		2.243669
  validation accuracy:		12.93 %
Epoch 1403 of 2000 took 0.103s
  training loss:		2.293346
  validation loss:		2.243953
  validation accuracy:		13.04 %
Epoch 1404 of 2000 took 0.103s
  training loss:		2.294214
  validation loss:		2.243256
  validation accuracy:		13.04 %
Epoch 1405 of 2000 took 0.103s
  training loss:		2.294519
  validation loss:		2.243977
  validation accuracy:		13.04 %
Epoch 1406 of 2000 took 0.103s
  training loss:		2.294290
  validation loss:		2.244089
  validation accuracy:		13.04 %
Epoch 1407 of 2000 took 0.103s
  training loss:		2.295027
  validation loss:		2.244669
  validation accuracy:		13.04 %
Epoch 1408 of 2000 took 0.103s
  training loss:		2.294811
  validation loss:		2.244607
  validation accuracy:		12.93 %
Epoch 1409 of 2000 took 0.103s
  training loss:		2.294861
  validation loss:		2.245214
  validation accuracy:		13.04 %
Epoch 1410 of 2000 took 0.104s
  training loss:		2.294721
  validation loss:		2.245551
  validation accuracy:		12.93 %
Epoch 1411 of 2000 took 0.103s
  training loss:		2.293462
  validation loss:		2.244800
  validation accuracy:		12.93 %
Epoch 1412 of 2000 took 0.103s
  training loss:		2.293901
  validation loss:		2.244767
  validation accuracy:		12.93 %
Epoch 1413 of 2000 took 0.103s
  training loss:		2.294427
  validation loss:		2.244367
  validation accuracy:		12.93 %
Epoch 1414 of 2000 took 0.103s
  training loss:		2.294784
  validation loss:		2.244262
  validation accuracy:		13.15 %
Epoch 1415 of 2000 took 0.103s
  training loss:		2.294872
  validation loss:		2.243952
  validation accuracy:		13.04 %
Epoch 1416 of 2000 took 0.103s
  training loss:		2.293638
  validation loss:		2.244010
  validation accuracy:		13.04 %
Epoch 1417 of 2000 took 0.103s
  training loss:		2.294428
  validation loss:		2.243896
  validation accuracy:		13.04 %
Epoch 1418 of 2000 took 0.103s
  training loss:		2.294731
  validation loss:		2.243984
  validation accuracy:		13.04 %
Epoch 1419 of 2000 took 0.103s
  training loss:		2.294042
  validation loss:		2.244333
  validation accuracy:		13.04 %
Epoch 1420 of 2000 took 0.103s
  training loss:		2.293907
  validation loss:		2.244460
  validation accuracy:		13.04 %
Epoch 1421 of 2000 took 0.103s
  training loss:		2.294223
  validation loss:		2.244167
  validation accuracy:		12.93 %
Epoch 1422 of 2000 took 0.103s
  training loss:		2.294151
  validation loss:		2.243902
  validation accuracy:		13.04 %
Epoch 1423 of 2000 took 0.103s
  training loss:		2.294812
  validation loss:		2.243734
  validation accuracy:		12.83 %
Epoch 1424 of 2000 took 0.103s
  training loss:		2.294420
  validation loss:		2.244002
  validation accuracy:		13.04 %
Epoch 1425 of 2000 took 0.103s
  training loss:		2.294048
  validation loss:		2.243937
  validation accuracy:		13.04 %
Epoch 1426 of 2000 took 0.103s
  training loss:		2.294020
  validation loss:		2.244009
  validation accuracy:		13.04 %
Epoch 1427 of 2000 took 0.103s
  training loss:		2.293832
  validation loss:		2.244157
  validation accuracy:		12.93 %
Epoch 1428 of 2000 took 0.103s
  training loss:		2.294358
  validation loss:		2.243495
  validation accuracy:		13.04 %
Epoch 1429 of 2000 took 0.103s
  training loss:		2.295073
  validation loss:		2.243690
  validation accuracy:		12.83 %
Epoch 1430 of 2000 took 0.103s
  training loss:		2.293707
  validation loss:		2.243919
  validation accuracy:		13.04 %
Epoch 1431 of 2000 took 0.103s
  training loss:		2.294922
  validation loss:		2.243909
  validation accuracy:		12.93 %
Epoch 1432 of 2000 took 0.103s
  training loss:		2.295218
  validation loss:		2.244343
  validation accuracy:		12.93 %
Epoch 1433 of 2000 took 0.103s
  training loss:		2.294748
  validation loss:		2.244874
  validation accuracy:		13.04 %
Epoch 1434 of 2000 took 0.103s
  training loss:		2.295812
  validation loss:		2.245190
  validation accuracy:		13.04 %
Epoch 1435 of 2000 took 0.103s
  training loss:		2.294473
  validation loss:		2.246083
  validation accuracy:		13.04 %
Epoch 1436 of 2000 took 0.103s
  training loss:		2.294587
  validation loss:		2.245560
  validation accuracy:		13.04 %
Epoch 1437 of 2000 took 0.103s
  training loss:		2.293874
  validation loss:		2.245110
  validation accuracy:		12.93 %
Epoch 1438 of 2000 took 0.103s
  training loss:		2.294199
  validation loss:		2.244898
  validation accuracy:		12.93 %
Epoch 1439 of 2000 took 0.104s
  training loss:		2.294630
  validation loss:		2.244136
  validation accuracy:		12.93 %
Epoch 1440 of 2000 took 0.103s
  training loss:		2.294948
  validation loss:		2.244557
  validation accuracy:		13.04 %
Epoch 1441 of 2000 took 0.103s
  training loss:		2.295281
  validation loss:		2.244806
  validation accuracy:		13.04 %
Epoch 1442 of 2000 took 0.103s
  training loss:		2.294678
  validation loss:		2.245557
  validation accuracy:		13.04 %
Epoch 1443 of 2000 took 0.103s
  training loss:		2.294004
  validation loss:		2.245266
  validation accuracy:		12.93 %
Epoch 1444 of 2000 took 0.103s
  training loss:		2.295120
  validation loss:		2.245619
  validation accuracy:		13.04 %
Epoch 1445 of 2000 took 0.103s
  training loss:		2.293591
  validation loss:		2.244947
  validation accuracy:		13.04 %
Epoch 1446 of 2000 took 0.103s
  training loss:		2.294086
  validation loss:		2.244776
  validation accuracy:		13.04 %
Epoch 1447 of 2000 took 0.103s
  training loss:		2.294346
  validation loss:		2.244601
  validation accuracy:		13.04 %
Epoch 1448 of 2000 took 0.103s
  training loss:		2.293844
  validation loss:		2.244560
  validation accuracy:		13.04 %
Epoch 1449 of 2000 took 0.103s
  training loss:		2.294379
  validation loss:		2.243855
  validation accuracy:		13.04 %
Epoch 1450 of 2000 took 0.103s
  training loss:		2.294484
  validation loss:		2.244026
  validation accuracy:		13.04 %
Epoch 1451 of 2000 took 0.103s
  training loss:		2.293579
  validation loss:		2.243661
  validation accuracy:		12.93 %
Epoch 1452 of 2000 took 0.103s
  training loss:		2.293974
  validation loss:		2.243885
  validation accuracy:		12.93 %
Epoch 1453 of 2000 took 0.103s
  training loss:		2.294521
  validation loss:		2.243260
  validation accuracy:		12.93 %
Epoch 1454 of 2000 took 0.103s
  training loss:		2.294806
  validation loss:		2.242824
  validation accuracy:		13.04 %
Epoch 1455 of 2000 took 0.103s
  training loss:		2.293991
  validation loss:		2.242920
  validation accuracy:		12.93 %
Epoch 1456 of 2000 took 0.103s
  training loss:		2.294026
  validation loss:		2.243633
  validation accuracy:		12.93 %
Epoch 1457 of 2000 took 0.103s
  training loss:		2.294638
  validation loss:		2.244166
  validation accuracy:		12.93 %
Epoch 1458 of 2000 took 0.103s
  training loss:		2.294017
  validation loss:		2.244410
  validation accuracy:		12.93 %
Epoch 1459 of 2000 took 0.103s
  training loss:		2.293486
  validation loss:		2.243194
  validation accuracy:		12.93 %
Epoch 1460 of 2000 took 0.103s
  training loss:		2.294999
  validation loss:		2.243128
  validation accuracy:		13.04 %
Epoch 1461 of 2000 took 0.103s
  training loss:		2.294812
  validation loss:		2.243634
  validation accuracy:		12.93 %
Epoch 1462 of 2000 took 0.103s
  training loss:		2.294479
  validation loss:		2.244037
  validation accuracy:		12.93 %
Epoch 1463 of 2000 took 0.104s
  training loss:		2.294571
  validation loss:		2.244339
  validation accuracy:		12.93 %
Epoch 1464 of 2000 took 0.103s
  training loss:		2.294457
  validation loss:		2.243807
  validation accuracy:		13.04 %
Epoch 1465 of 2000 took 0.104s
  training loss:		2.294124
  validation loss:		2.244534
  validation accuracy:		12.93 %
Epoch 1466 of 2000 took 0.103s
  training loss:		2.294727
  validation loss:		2.244682
  validation accuracy:		12.93 %
Epoch 1467 of 2000 took 0.103s
  training loss:		2.294079
  validation loss:		2.244400
  validation accuracy:		12.93 %
Epoch 1468 of 2000 took 0.105s
  training loss:		2.294706
  validation loss:		2.244124
  validation accuracy:		12.93 %
Epoch 1469 of 2000 took 0.103s
  training loss:		2.294176
  validation loss:		2.244351
  validation accuracy:		12.93 %
Epoch 1470 of 2000 took 0.103s
  training loss:		2.294365
  validation loss:		2.244118
  validation accuracy:		12.83 %
Epoch 1471 of 2000 took 0.103s
  training loss:		2.294018
  validation loss:		2.243306
  validation accuracy:		12.83 %
Epoch 1472 of 2000 took 0.103s
  training loss:		2.293776
  validation loss:		2.243896
  validation accuracy:		12.83 %
Epoch 1473 of 2000 took 0.103s
  training loss:		2.294700
  validation loss:		2.243880
  validation accuracy:		12.83 %
Epoch 1474 of 2000 took 0.103s
  training loss:		2.294276
  validation loss:		2.243762
  validation accuracy:		12.83 %
Epoch 1475 of 2000 took 0.103s
  training loss:		2.294434
  validation loss:		2.243897
  validation accuracy:		12.83 %
Epoch 1476 of 2000 took 0.103s
  training loss:		2.294287
  validation loss:		2.244237
  validation accuracy:		12.93 %
Epoch 1477 of 2000 took 0.103s
  training loss:		2.294239
  validation loss:		2.243578
  validation accuracy:		12.93 %
Epoch 1478 of 2000 took 0.103s
  training loss:		2.294227
  validation loss:		2.244164
  validation accuracy:		12.93 %
Epoch 1479 of 2000 took 0.103s
  training loss:		2.293858
  validation loss:		2.243396
  validation accuracy:		12.93 %
Epoch 1480 of 2000 took 0.103s
  training loss:		2.293158
  validation loss:		2.242529
  validation accuracy:		12.93 %
Epoch 1481 of 2000 took 0.103s
  training loss:		2.294815
  validation loss:		2.243032
  validation accuracy:		12.93 %
Epoch 1482 of 2000 took 0.103s
  training loss:		2.294707
  validation loss:		2.243456
  validation accuracy:		12.93 %
Epoch 1483 of 2000 took 0.103s
  training loss:		2.293968
  validation loss:		2.243180
  validation accuracy:		13.04 %
Epoch 1484 of 2000 took 0.103s
  training loss:		2.293661
  validation loss:		2.243605
  validation accuracy:		12.93 %
Epoch 1485 of 2000 took 0.103s
  training loss:		2.294672
  validation loss:		2.244105
  validation accuracy:		13.04 %
Epoch 1486 of 2000 took 0.103s
  training loss:		2.294419
  validation loss:		2.244172
  validation accuracy:		13.04 %
Epoch 1487 of 2000 took 0.103s
  training loss:		2.294322
  validation loss:		2.244488
  validation accuracy:		13.04 %
Epoch 1488 of 2000 took 0.103s
  training loss:		2.294143
  validation loss:		2.244322
  validation accuracy:		13.04 %
Epoch 1489 of 2000 took 0.103s
  training loss:		2.293687
  validation loss:		2.243921
  validation accuracy:		13.04 %
Epoch 1490 of 2000 took 0.103s
  training loss:		2.295123
  validation loss:		2.244269
  validation accuracy:		12.93 %
Epoch 1491 of 2000 took 0.103s
  training loss:		2.294713
  validation loss:		2.245018
  validation accuracy:		12.93 %
Epoch 1492 of 2000 took 0.103s
  training loss:		2.294206
  validation loss:		2.245037
  validation accuracy:		12.93 %
Epoch 1493 of 2000 took 0.103s
  training loss:		2.294967
  validation loss:		2.245472
  validation accuracy:		12.93 %
Epoch 1494 of 2000 took 0.103s
  training loss:		2.294144
  validation loss:		2.245295
  validation accuracy:		12.93 %
Epoch 1495 of 2000 took 0.103s
  training loss:		2.294432
  validation loss:		2.245032
  validation accuracy:		12.93 %
Epoch 1496 of 2000 took 0.103s
  training loss:		2.293769
  validation loss:		2.244300
  validation accuracy:		12.93 %
Epoch 1497 of 2000 took 0.104s
  training loss:		2.293663
  validation loss:		2.243657
  validation accuracy:		12.93 %
Epoch 1498 of 2000 took 0.103s
  training loss:		2.294524
  validation loss:		2.243606
  validation accuracy:		13.04 %
Epoch 1499 of 2000 took 0.103s
  training loss:		2.294680
  validation loss:		2.243792
  validation accuracy:		13.04 %
Epoch 1500 of 2000 took 0.103s
  training loss:		2.294213
  validation loss:		2.244394
  validation accuracy:		13.04 %
Epoch 1501 of 2000 took 0.103s
  training loss:		2.294853
  validation loss:		2.244528
  validation accuracy:		13.04 %
Epoch 1502 of 2000 took 0.103s
  training loss:		2.294688
  validation loss:		2.243839
  validation accuracy:		13.04 %
Epoch 1503 of 2000 took 0.104s
  training loss:		2.294266
  validation loss:		2.244169
  validation accuracy:		13.04 %
Epoch 1504 of 2000 took 0.103s
  training loss:		2.294211
  validation loss:		2.244093
  validation accuracy:		13.04 %
Epoch 1505 of 2000 took 0.103s
  training loss:		2.295201
  validation loss:		2.244570
  validation accuracy:		13.04 %
Epoch 1506 of 2000 took 0.104s
  training loss:		2.295009
  validation loss:		2.245094
  validation accuracy:		13.04 %
Epoch 1507 of 2000 took 0.102s
  training loss:		2.293897
  validation loss:		2.245253
  validation accuracy:		13.04 %
Epoch 1508 of 2000 took 0.099s
  training loss:		2.293842
  validation loss:		2.243843
  validation accuracy:		13.04 %
Epoch 1509 of 2000 took 0.099s
  training loss:		2.292611
  validation loss:		2.243524
  validation accuracy:		13.04 %
Epoch 1510 of 2000 took 0.099s
  training loss:		2.294403
  validation loss:		2.243204
  validation accuracy:		12.83 %
Epoch 1511 of 2000 took 0.099s
  training loss:		2.294076
  validation loss:		2.243544
  validation accuracy:		12.93 %
Epoch 1512 of 2000 took 0.099s
  training loss:		2.294539
  validation loss:		2.243796
  validation accuracy:		12.93 %
Epoch 1513 of 2000 took 0.099s
  training loss:		2.294750
  validation loss:		2.243499
  validation accuracy:		13.04 %
Epoch 1514 of 2000 took 0.099s
  training loss:		2.293973
  validation loss:		2.243420
  validation accuracy:		13.04 %
Epoch 1515 of 2000 took 0.099s
  training loss:		2.294417
  validation loss:		2.244029
  validation accuracy:		13.04 %
Epoch 1516 of 2000 took 0.099s
  training loss:		2.293951
  validation loss:		2.244012
  validation accuracy:		13.04 %
Epoch 1517 of 2000 took 0.099s
  training loss:		2.293938
  validation loss:		2.244015
  validation accuracy:		13.04 %
Epoch 1518 of 2000 took 0.099s
  training loss:		2.294353
  validation loss:		2.244412
  validation accuracy:		13.04 %
Epoch 1519 of 2000 took 0.099s
  training loss:		2.294320
  validation loss:		2.244493
  validation accuracy:		12.93 %
Epoch 1520 of 2000 took 0.100s
  training loss:		2.294443
  validation loss:		2.243820
  validation accuracy:		12.93 %
Epoch 1521 of 2000 took 0.099s
  training loss:		2.294049
  validation loss:		2.243816
  validation accuracy:		12.93 %
Epoch 1522 of 2000 took 0.099s
  training loss:		2.293126
  validation loss:		2.243485
  validation accuracy:		12.93 %
Epoch 1523 of 2000 took 0.099s
  training loss:		2.294354
  validation loss:		2.243261
  validation accuracy:		12.93 %
Epoch 1524 of 2000 took 0.099s
  training loss:		2.294300
  validation loss:		2.243449
  validation accuracy:		12.93 %
Epoch 1525 of 2000 took 0.099s
  training loss:		2.293488
  validation loss:		2.243285
  validation accuracy:		12.93 %
Epoch 1526 of 2000 took 0.099s
  training loss:		2.294422
  validation loss:		2.243334
  validation accuracy:		12.83 %
Epoch 1527 of 2000 took 0.100s
  training loss:		2.293147
  validation loss:		2.243560
  validation accuracy:		12.93 %
Epoch 1528 of 2000 took 0.099s
  training loss:		2.294476
  validation loss:		2.243643
  validation accuracy:		13.04 %
Epoch 1529 of 2000 took 0.099s
  training loss:		2.294574
  validation loss:		2.243726
  validation accuracy:		12.93 %
Epoch 1530 of 2000 took 0.099s
  training loss:		2.294314
  validation loss:		2.244368
  validation accuracy:		12.93 %
Epoch 1531 of 2000 took 0.099s
  training loss:		2.294657
  validation loss:		2.244210
  validation accuracy:		12.93 %
Epoch 1532 of 2000 took 0.099s
  training loss:		2.294490
  validation loss:		2.244378
  validation accuracy:		12.93 %
Epoch 1533 of 2000 took 0.099s
  training loss:		2.293917
  validation loss:		2.244410
  validation accuracy:		12.93 %
Epoch 1534 of 2000 took 0.099s
  training loss:		2.294718
  validation loss:		2.244684
  validation accuracy:		12.93 %
Epoch 1535 of 2000 took 0.099s
  training loss:		2.293609
  validation loss:		2.243773
  validation accuracy:		12.93 %
Epoch 1536 of 2000 took 0.099s
  training loss:		2.294203
  validation loss:		2.243917
  validation accuracy:		12.93 %
Epoch 1537 of 2000 took 0.099s
  training loss:		2.293704
  validation loss:		2.243361
  validation accuracy:		12.83 %
Epoch 1538 of 2000 took 0.099s
  training loss:		2.294698
  validation loss:		2.243085
  validation accuracy:		12.93 %
Epoch 1539 of 2000 took 0.099s
  training loss:		2.294801
  validation loss:		2.244336
  validation accuracy:		12.93 %
Epoch 1540 of 2000 took 0.099s
  training loss:		2.294552
  validation loss:		2.244630
  validation accuracy:		12.93 %
Epoch 1541 of 2000 took 0.099s
  training loss:		2.294343
  validation loss:		2.244505
  validation accuracy:		12.93 %
Epoch 1542 of 2000 took 0.099s
  training loss:		2.294247
  validation loss:		2.244385
  validation accuracy:		12.93 %
Epoch 1543 of 2000 took 0.099s
  training loss:		2.295102
  validation loss:		2.244181
  validation accuracy:		12.93 %
Epoch 1544 of 2000 took 0.100s
  training loss:		2.295001
  validation loss:		2.244648
  validation accuracy:		12.93 %
Epoch 1545 of 2000 took 0.099s
  training loss:		2.294531
  validation loss:		2.245348
  validation accuracy:		12.93 %
Epoch 1546 of 2000 took 0.101s
  training loss:		2.294238
  validation loss:		2.244742
  validation accuracy:		12.93 %
Epoch 1547 of 2000 took 0.100s
  training loss:		2.294679
  validation loss:		2.244768
  validation accuracy:		12.93 %
Epoch 1548 of 2000 took 0.099s
  training loss:		2.293344
  validation loss:		2.244035
  validation accuracy:		12.93 %
Epoch 1549 of 2000 took 0.099s
  training loss:		2.293508
  validation loss:		2.243388
  validation accuracy:		13.04 %
Epoch 1550 of 2000 took 0.099s
  training loss:		2.293482
  validation loss:		2.243130
  validation accuracy:		13.04 %
Epoch 1551 of 2000 took 0.099s
  training loss:		2.294883
  validation loss:		2.243409
  validation accuracy:		12.93 %
Epoch 1552 of 2000 took 0.099s
  training loss:		2.294167
  validation loss:		2.243773
  validation accuracy:		12.93 %
Epoch 1553 of 2000 took 0.099s
  training loss:		2.293884
  validation loss:		2.243342
  validation accuracy:		12.93 %
Epoch 1554 of 2000 took 0.099s
  training loss:		2.294607
  validation loss:		2.243403
  validation accuracy:		12.93 %
Epoch 1555 of 2000 took 0.099s
  training loss:		2.294283
  validation loss:		2.243627
  validation accuracy:		12.93 %
Epoch 1556 of 2000 took 0.099s
  training loss:		2.294227
  validation loss:		2.243285
  validation accuracy:		12.93 %
Epoch 1557 of 2000 took 0.099s
  training loss:		2.295106
  validation loss:		2.243504
  validation accuracy:		12.93 %
Epoch 1558 of 2000 took 0.100s
  training loss:		2.294165
  validation loss:		2.243720
  validation accuracy:		12.93 %
Epoch 1559 of 2000 took 0.099s
  training loss:		2.294336
  validation loss:		2.243929
  validation accuracy:		13.04 %
Epoch 1560 of 2000 took 0.099s
  training loss:		2.294635
  validation loss:		2.243998
  validation accuracy:		13.04 %
Epoch 1561 of 2000 took 0.099s
  training loss:		2.294413
  validation loss:		2.243733
  validation accuracy:		12.93 %
Epoch 1562 of 2000 took 0.099s
  training loss:		2.294788
  validation loss:		2.244347
  validation accuracy:		12.93 %
Epoch 1563 of 2000 took 0.099s
  training loss:		2.294375
  validation loss:		2.244826
  validation accuracy:		12.83 %
Epoch 1564 of 2000 took 0.099s
  training loss:		2.294035
  validation loss:		2.244352
  validation accuracy:		12.93 %
Epoch 1565 of 2000 took 0.099s
  training loss:		2.293874
  validation loss:		2.243975
  validation accuracy:		12.93 %
Epoch 1566 of 2000 took 0.099s
  training loss:		2.293771
  validation loss:		2.243783
  validation accuracy:		12.93 %
Epoch 1567 of 2000 took 0.099s
  training loss:		2.294645
  validation loss:		2.243503
  validation accuracy:		13.04 %
Epoch 1568 of 2000 took 0.099s
  training loss:		2.294356
  validation loss:		2.243403
  validation accuracy:		13.04 %
Epoch 1569 of 2000 took 0.099s
  training loss:		2.294060
  validation loss:		2.243560
  validation accuracy:		13.04 %
Epoch 1570 of 2000 took 0.099s
  training loss:		2.294643
  validation loss:		2.243594
  validation accuracy:		13.04 %
Epoch 1571 of 2000 took 0.099s
  training loss:		2.294451
  validation loss:		2.243940
  validation accuracy:		13.04 %
Epoch 1572 of 2000 took 0.099s
  training loss:		2.294937
  validation loss:		2.244235
  validation accuracy:		13.04 %
Epoch 1573 of 2000 took 0.099s
  training loss:		2.294888
  validation loss:		2.244412
  validation accuracy:		12.93 %
Epoch 1574 of 2000 took 0.099s
  training loss:		2.294948
  validation loss:		2.244705
  validation accuracy:		12.93 %
Epoch 1575 of 2000 took 0.099s
  training loss:		2.294616
  validation loss:		2.245124
  validation accuracy:		12.93 %
Epoch 1576 of 2000 took 0.099s
  training loss:		2.293806
  validation loss:		2.244811
  validation accuracy:		13.04 %
Epoch 1577 of 2000 took 0.099s
  training loss:		2.294302
  validation loss:		2.244879
  validation accuracy:		13.04 %
Epoch 1578 of 2000 took 0.099s
  training loss:		2.294584
  validation loss:		2.244541
  validation accuracy:		13.04 %
Epoch 1579 of 2000 took 0.099s
  training loss:		2.294953
  validation loss:		2.244597
  validation accuracy:		12.93 %
Epoch 1580 of 2000 took 0.099s
  training loss:		2.293805
  validation loss:		2.244923
  validation accuracy:		12.93 %
Epoch 1581 of 2000 took 0.099s
  training loss:		2.293841
  validation loss:		2.244299
  validation accuracy:		12.93 %
Epoch 1582 of 2000 took 0.099s
  training loss:		2.294393
  validation loss:		2.244110
  validation accuracy:		12.93 %
Epoch 1583 of 2000 took 0.099s
  training loss:		2.294685
  validation loss:		2.244378
  validation accuracy:		12.93 %
Epoch 1584 of 2000 took 0.099s
  training loss:		2.293676
  validation loss:		2.243746
  validation accuracy:		13.04 %
Epoch 1585 of 2000 took 0.099s
  training loss:		2.293538
  validation loss:		2.243641
  validation accuracy:		13.04 %
Epoch 1586 of 2000 took 0.099s
  training loss:		2.295079
  validation loss:		2.244369
  validation accuracy:		13.04 %
Epoch 1587 of 2000 took 0.099s
  training loss:		2.294746
  validation loss:		2.245069
  validation accuracy:		13.04 %
Epoch 1588 of 2000 took 0.100s
  training loss:		2.295093
  validation loss:		2.245405
  validation accuracy:		13.04 %
Epoch 1589 of 2000 took 0.099s
  training loss:		2.294389
  validation loss:		2.245575
  validation accuracy:		13.04 %
Epoch 1590 of 2000 took 0.099s
  training loss:		2.294029
  validation loss:		2.245447
  validation accuracy:		13.04 %
Epoch 1591 of 2000 took 0.099s
  training loss:		2.294142
  validation loss:		2.245140
  validation accuracy:		12.93 %
Epoch 1592 of 2000 took 0.099s
  training loss:		2.294699
  validation loss:		2.244538
  validation accuracy:		12.93 %
Epoch 1593 of 2000 took 0.099s
  training loss:		2.293682
  validation loss:		2.244434
  validation accuracy:		12.93 %
Epoch 1594 of 2000 took 0.099s
  training loss:		2.295257
  validation loss:		2.244444
  validation accuracy:		12.93 %
Epoch 1595 of 2000 took 0.099s
  training loss:		2.294431
  validation loss:		2.244595
  validation accuracy:		12.93 %
Epoch 1596 of 2000 took 0.099s
  training loss:		2.295100
  validation loss:		2.244522
  validation accuracy:		13.04 %
Epoch 1597 of 2000 took 0.099s
  training loss:		2.293899
  validation loss:		2.244755
  validation accuracy:		13.04 %
Epoch 1598 of 2000 took 0.099s
  training loss:		2.293181
  validation loss:		2.244303
  validation accuracy:		13.04 %
Epoch 1599 of 2000 took 0.099s
  training loss:		2.293797
  validation loss:		2.243372
  validation accuracy:		12.93 %
Epoch 1600 of 2000 took 0.099s
  training loss:		2.293910
  validation loss:		2.243197
  validation accuracy:		12.93 %
Epoch 1601 of 2000 took 0.099s
  training loss:		2.294414
  validation loss:		2.243062
  validation accuracy:		12.93 %
Epoch 1602 of 2000 took 0.099s
  training loss:		2.294778
  validation loss:		2.243524
  validation accuracy:		13.04 %
Epoch 1603 of 2000 took 0.099s
  training loss:		2.293109
  validation loss:		2.243975
  validation accuracy:		13.04 %
Epoch 1604 of 2000 took 0.099s
  training loss:		2.294320
  validation loss:		2.243461
  validation accuracy:		13.04 %
Epoch 1605 of 2000 took 0.099s
  training loss:		2.292908
  validation loss:		2.243103
  validation accuracy:		13.04 %
Epoch 1606 of 2000 took 0.099s
  training loss:		2.294458
  validation loss:		2.242942
  validation accuracy:		12.93 %
Epoch 1607 of 2000 took 0.099s
  training loss:		2.294413
  validation loss:		2.243764
  validation accuracy:		12.93 %
Epoch 1608 of 2000 took 0.099s
  training loss:		2.294369
  validation loss:		2.243191
  validation accuracy:		13.04 %
Epoch 1609 of 2000 took 0.099s
  training loss:		2.293981
  validation loss:		2.243289
  validation accuracy:		13.04 %
Epoch 1610 of 2000 took 0.099s
  training loss:		2.294776
  validation loss:		2.243519
  validation accuracy:		13.04 %
Epoch 1611 of 2000 took 0.099s
  training loss:		2.293019
  validation loss:		2.243035
  validation accuracy:		13.04 %
Epoch 1612 of 2000 took 0.099s
  training loss:		2.293720
  validation loss:		2.242948
  validation accuracy:		13.04 %
Epoch 1613 of 2000 took 0.099s
  training loss:		2.294753
  validation loss:		2.243610
  validation accuracy:		13.04 %
Epoch 1614 of 2000 took 0.099s
  training loss:		2.293377
  validation loss:		2.243191
  validation accuracy:		13.04 %
Epoch 1615 of 2000 took 0.099s
  training loss:		2.294183
  validation loss:		2.242856
  validation accuracy:		13.04 %
Epoch 1616 of 2000 took 0.099s
  training loss:		2.294150
  validation loss:		2.243748
  validation accuracy:		13.04 %
Epoch 1617 of 2000 took 0.099s
  training loss:		2.293925
  validation loss:		2.243562
  validation accuracy:		12.93 %
Epoch 1618 of 2000 took 0.100s
  training loss:		2.294344
  validation loss:		2.243749
  validation accuracy:		12.93 %
Epoch 1619 of 2000 took 0.099s
  training loss:		2.293721
  validation loss:		2.243584
  validation accuracy:		12.93 %
Epoch 1620 of 2000 took 0.099s
  training loss:		2.294523
  validation loss:		2.243213
  validation accuracy:		12.83 %
Epoch 1621 of 2000 took 0.099s
  training loss:		2.294122
  validation loss:		2.243031
  validation accuracy:		13.04 %
Epoch 1622 of 2000 took 0.099s
  training loss:		2.294471
  validation loss:		2.243401
  validation accuracy:		13.04 %
Epoch 1623 of 2000 took 0.099s
  training loss:		2.294560
  validation loss:		2.243971
  validation accuracy:		13.04 %
Epoch 1624 of 2000 took 0.099s
  training loss:		2.294737
  validation loss:		2.243841
  validation accuracy:		13.04 %
Epoch 1625 of 2000 took 0.099s
  training loss:		2.293700
  validation loss:		2.243648
  validation accuracy:		13.04 %
Epoch 1626 of 2000 took 0.099s
  training loss:		2.293984
  validation loss:		2.243716
  validation accuracy:		12.93 %
Epoch 1627 of 2000 took 0.098s
  training loss:		2.294809
  validation loss:		2.243991
  validation accuracy:		12.93 %
Epoch 1628 of 2000 took 0.096s
  training loss:		2.294106
  validation loss:		2.244515
  validation accuracy:		12.93 %
Epoch 1629 of 2000 took 0.095s
  training loss:		2.294619
  validation loss:		2.243964
  validation accuracy:		12.93 %
Epoch 1630 of 2000 took 0.095s
  training loss:		2.293883
  validation loss:		2.243762
  validation accuracy:		13.04 %
Epoch 1631 of 2000 took 0.095s
  training loss:		2.293405
  validation loss:		2.243221
  validation accuracy:		13.04 %
Epoch 1632 of 2000 took 0.095s
  training loss:		2.293982
  validation loss:		2.243580
  validation accuracy:		13.04 %
Epoch 1633 of 2000 took 0.095s
  training loss:		2.294989
  validation loss:		2.244291
  validation accuracy:		12.93 %
Epoch 1634 of 2000 took 0.095s
  training loss:		2.294557
  validation loss:		2.244990
  validation accuracy:		13.04 %
Epoch 1635 of 2000 took 0.095s
  training loss:		2.293745
  validation loss:		2.244605
  validation accuracy:		13.04 %
Epoch 1636 of 2000 took 0.095s
  training loss:		2.293696
  validation loss:		2.244136
  validation accuracy:		13.04 %
Epoch 1637 of 2000 took 0.095s
  training loss:		2.293953
  validation loss:		2.244158
  validation accuracy:		13.04 %
Epoch 1638 of 2000 took 0.095s
  training loss:		2.294148
  validation loss:		2.243911
  validation accuracy:		13.04 %
Epoch 1639 of 2000 took 0.095s
  training loss:		2.294758
  validation loss:		2.243863
  validation accuracy:		12.93 %
Epoch 1640 of 2000 took 0.096s
  training loss:		2.293569
  validation loss:		2.243788
  validation accuracy:		12.93 %
Epoch 1641 of 2000 took 0.096s
  training loss:		2.293838
  validation loss:		2.243142
  validation accuracy:		12.93 %
Epoch 1642 of 2000 took 0.096s
  training loss:		2.294585
  validation loss:		2.243116
  validation accuracy:		13.04 %
Epoch 1643 of 2000 took 0.096s
  training loss:		2.294684
  validation loss:		2.243548
  validation accuracy:		13.04 %
Epoch 1644 of 2000 took 0.096s
  training loss:		2.294332
  validation loss:		2.244136
  validation accuracy:		13.04 %
Epoch 1645 of 2000 took 0.096s
  training loss:		2.294768
  validation loss:		2.244614
  validation accuracy:		12.93 %
Epoch 1646 of 2000 took 0.096s
  training loss:		2.294253
  validation loss:		2.244479
  validation accuracy:		12.93 %
Epoch 1647 of 2000 took 0.096s
  training loss:		2.294597
  validation loss:		2.244320
  validation accuracy:		12.93 %
Epoch 1648 of 2000 took 0.096s
  training loss:		2.294212
  validation loss:		2.244371
  validation accuracy:		12.93 %
Epoch 1649 of 2000 took 0.096s
  training loss:		2.294179
  validation loss:		2.244950
  validation accuracy:		12.93 %
Epoch 1650 of 2000 took 0.097s
  training loss:		2.294913
  validation loss:		2.245202
  validation accuracy:		12.93 %
Epoch 1651 of 2000 took 0.096s
  training loss:		2.293585
  validation loss:		2.244526
  validation accuracy:		12.93 %
Epoch 1652 of 2000 took 0.096s
  training loss:		2.294047
  validation loss:		2.244004
  validation accuracy:		12.93 %
Epoch 1653 of 2000 took 0.096s
  training loss:		2.295126
  validation loss:		2.243612
  validation accuracy:		12.93 %
Epoch 1654 of 2000 took 0.096s
  training loss:		2.293370
  validation loss:		2.243438
  validation accuracy:		12.93 %
Epoch 1655 of 2000 took 0.096s
  training loss:		2.294969
  validation loss:		2.243279
  validation accuracy:		13.04 %
Epoch 1656 of 2000 took 0.096s
  training loss:		2.294127
  validation loss:		2.244074
  validation accuracy:		13.04 %
Epoch 1657 of 2000 took 0.096s
  training loss:		2.294147
  validation loss:		2.244561
  validation accuracy:		12.93 %
Epoch 1658 of 2000 took 0.096s
  training loss:		2.294770
  validation loss:		2.244318
  validation accuracy:		12.83 %
Epoch 1659 of 2000 took 0.096s
  training loss:		2.294951
  validation loss:		2.244979
  validation accuracy:		12.83 %
Epoch 1660 of 2000 took 0.096s
  training loss:		2.294463
  validation loss:		2.244951
  validation accuracy:		12.83 %
Epoch 1661 of 2000 took 0.096s
  training loss:		2.294022
  validation loss:		2.244803
  validation accuracy:		13.04 %
Epoch 1662 of 2000 took 0.096s
  training loss:		2.294379
  validation loss:		2.244747
  validation accuracy:		13.04 %
Epoch 1663 of 2000 took 0.096s
  training loss:		2.294672
  validation loss:		2.244738
  validation accuracy:		13.04 %
Epoch 1664 of 2000 took 0.096s
  training loss:		2.294356
  validation loss:		2.244730
  validation accuracy:		13.04 %
Epoch 1665 of 2000 took 0.096s
  training loss:		2.295491
  validation loss:		2.245089
  validation accuracy:		13.15 %
Epoch 1666 of 2000 took 0.096s
  training loss:		2.294182
  validation loss:		2.244952
  validation accuracy:		13.04 %
Epoch 1667 of 2000 took 0.096s
  training loss:		2.294072
  validation loss:		2.245049
  validation accuracy:		13.04 %
Epoch 1668 of 2000 took 0.096s
  training loss:		2.293858
  validation loss:		2.244885
  validation accuracy:		13.04 %
Epoch 1669 of 2000 took 0.096s
  training loss:		2.293543
  validation loss:		2.243657
  validation accuracy:		12.93 %
Epoch 1670 of 2000 took 0.096s
  training loss:		2.294281
  validation loss:		2.242871
  validation accuracy:		13.04 %
Epoch 1671 of 2000 took 0.096s
  training loss:		2.293261
  validation loss:		2.242986
  validation accuracy:		12.93 %
Epoch 1672 of 2000 took 0.096s
  training loss:		2.294498
  validation loss:		2.243217
  validation accuracy:		12.93 %
Epoch 1673 of 2000 took 0.096s
  training loss:		2.294792
  validation loss:		2.243252
  validation accuracy:		12.93 %
Epoch 1674 of 2000 took 0.096s
  training loss:		2.295147
  validation loss:		2.243712
  validation accuracy:		12.93 %
Epoch 1675 of 2000 took 0.096s
  training loss:		2.295011
  validation loss:		2.244683
  validation accuracy:		12.93 %
Epoch 1676 of 2000 took 0.096s
  training loss:		2.294445
  validation loss:		2.244581
  validation accuracy:		12.93 %
Epoch 1677 of 2000 took 0.096s
  training loss:		2.294003
  validation loss:		2.244822
  validation accuracy:		12.93 %
Epoch 1678 of 2000 took 0.096s
  training loss:		2.293380
  validation loss:		2.244437
  validation accuracy:		12.93 %
Epoch 1679 of 2000 took 0.096s
  training loss:		2.294147
  validation loss:		2.244122
  validation accuracy:		12.93 %
Epoch 1680 of 2000 took 0.096s
  training loss:		2.294475
  validation loss:		2.244252
  validation accuracy:		13.04 %
Epoch 1681 of 2000 took 0.097s
  training loss:		2.293346
  validation loss:		2.244077
  validation accuracy:		13.04 %
Epoch 1682 of 2000 took 0.096s
  training loss:		2.295165
  validation loss:		2.243460
  validation accuracy:		13.04 %
Epoch 1683 of 2000 took 0.096s
  training loss:		2.294737
  validation loss:		2.244412
  validation accuracy:		13.04 %
Epoch 1684 of 2000 took 0.096s
  training loss:		2.293865
  validation loss:		2.244050
  validation accuracy:		13.04 %
Epoch 1685 of 2000 took 0.096s
  training loss:		2.293966
  validation loss:		2.243843
  validation accuracy:		12.93 %
Epoch 1686 of 2000 took 0.096s
  training loss:		2.294062
  validation loss:		2.244305
  validation accuracy:		12.93 %
Epoch 1687 of 2000 took 0.096s
  training loss:		2.293829
  validation loss:		2.243837
  validation accuracy:		12.93 %
Epoch 1688 of 2000 took 0.096s
  training loss:		2.294286
  validation loss:		2.243837
  validation accuracy:		12.93 %
Epoch 1689 of 2000 took 0.096s
  training loss:		2.294344
  validation loss:		2.243893
  validation accuracy:		13.04 %
Epoch 1690 of 2000 took 0.096s
  training loss:		2.294106
  validation loss:		2.243611
  validation accuracy:		13.04 %
Epoch 1691 of 2000 took 0.096s
  training loss:		2.293822
  validation loss:		2.243648
  validation accuracy:		13.04 %
Epoch 1692 of 2000 took 0.096s
  training loss:		2.294225
  validation loss:		2.243606
  validation accuracy:		13.04 %
Epoch 1693 of 2000 took 0.096s
  training loss:		2.295006
  validation loss:		2.244301
  validation accuracy:		13.04 %
Epoch 1694 of 2000 took 0.096s
  training loss:		2.293398
  validation loss:		2.244225
  validation accuracy:		13.04 %
Epoch 1695 of 2000 took 0.096s
  training loss:		2.294915
  validation loss:		2.244312
  validation accuracy:		12.93 %
Epoch 1696 of 2000 took 0.096s
  training loss:		2.293797
  validation loss:		2.244182
  validation accuracy:		12.93 %
Epoch 1697 of 2000 took 0.096s
  training loss:		2.293864
  validation loss:		2.243825
  validation accuracy:		13.04 %
Epoch 1698 of 2000 took 0.096s
  training loss:		2.294870
  validation loss:		2.243898
  validation accuracy:		13.04 %
Epoch 1699 of 2000 took 0.096s
  training loss:		2.294574
  validation loss:		2.244588
  validation accuracy:		13.04 %
Epoch 1700 of 2000 took 0.096s
  training loss:		2.294105
  validation loss:		2.244746
  validation accuracy:		12.93 %
Epoch 1701 of 2000 took 0.096s
  training loss:		2.294063
  validation loss:		2.244620
  validation accuracy:		12.93 %
Epoch 1702 of 2000 took 0.096s
  training loss:		2.294501
  validation loss:		2.244444
  validation accuracy:		12.93 %
Epoch 1703 of 2000 took 0.096s
  training loss:		2.294689
  validation loss:		2.243845
  validation accuracy:		12.83 %
Epoch 1704 of 2000 took 0.096s
  training loss:		2.294978
  validation loss:		2.244188
  validation accuracy:		12.83 %
Epoch 1705 of 2000 took 0.096s
  training loss:		2.293539
  validation loss:		2.244545
  validation accuracy:		13.04 %
Epoch 1706 of 2000 took 0.096s
  training loss:		2.295035
  validation loss:		2.244491
  validation accuracy:		13.04 %
Epoch 1707 of 2000 took 0.096s
  training loss:		2.294462
  validation loss:		2.244685
  validation accuracy:		12.93 %
Epoch 1708 of 2000 took 0.096s
  training loss:		2.294217
  validation loss:		2.245105
  validation accuracy:		12.83 %
Epoch 1709 of 2000 took 0.096s
  training loss:		2.293667
  validation loss:		2.244753
  validation accuracy:		12.83 %
Epoch 1710 of 2000 took 0.096s
  training loss:		2.294237
  validation loss:		2.244200
  validation accuracy:		13.04 %
Epoch 1711 of 2000 took 0.096s
  training loss:		2.294297
  validation loss:		2.244021
  validation accuracy:		12.93 %
Epoch 1712 of 2000 took 0.097s
  training loss:		2.294165
  validation loss:		2.244201
  validation accuracy:		12.93 %
Epoch 1713 of 2000 took 0.096s
  training loss:		2.293740
  validation loss:		2.243867
  validation accuracy:		12.93 %
Epoch 1714 of 2000 took 0.096s
  training loss:		2.294519
  validation loss:		2.243953
  validation accuracy:		13.04 %
Epoch 1715 of 2000 took 0.096s
  training loss:		2.294499
  validation loss:		2.244309
  validation accuracy:		12.93 %
Epoch 1716 of 2000 took 0.096s
  training loss:		2.294872
  validation loss:		2.245173
  validation accuracy:		13.04 %
Epoch 1717 of 2000 took 0.096s
  training loss:		2.294225
  validation loss:		2.244755
  validation accuracy:		13.04 %
Epoch 1718 of 2000 took 0.096s
  training loss:		2.293490
  validation loss:		2.244025
  validation accuracy:		13.04 %
Epoch 1719 of 2000 took 0.096s
  training loss:		2.294907
  validation loss:		2.244572
  validation accuracy:		12.93 %
Epoch 1720 of 2000 took 0.096s
  training loss:		2.294747
  validation loss:		2.245143
  validation accuracy:		12.93 %
Epoch 1721 of 2000 took 0.096s
  training loss:		2.293999
  validation loss:		2.244603
  validation accuracy:		12.93 %
Epoch 1722 of 2000 took 0.098s
  training loss:		2.294061
  validation loss:		2.244000
  validation accuracy:		12.93 %
Epoch 1723 of 2000 took 0.096s
  training loss:		2.293996
  validation loss:		2.244205
  validation accuracy:		12.93 %
Epoch 1724 of 2000 took 0.096s
  training loss:		2.293855
  validation loss:		2.243703
  validation accuracy:		12.93 %
Epoch 1725 of 2000 took 0.097s
  training loss:		2.292984
  validation loss:		2.243954
  validation accuracy:		12.93 %
Epoch 1726 of 2000 took 0.096s
  training loss:		2.294358
  validation loss:		2.243246
  validation accuracy:		12.93 %
Epoch 1727 of 2000 took 0.096s
  training loss:		2.294532
  validation loss:		2.243595
  validation accuracy:		12.93 %
Epoch 1728 of 2000 took 0.096s
  training loss:		2.294496
  validation loss:		2.243905
  validation accuracy:		12.93 %
Epoch 1729 of 2000 took 0.096s
  training loss:		2.294681
  validation loss:		2.243453
  validation accuracy:		12.93 %
Epoch 1730 of 2000 took 0.096s
  training loss:		2.294311
  validation loss:		2.243816
  validation accuracy:		12.93 %
Epoch 1731 of 2000 took 0.096s
  training loss:		2.294203
  validation loss:		2.243491
  validation accuracy:		12.93 %
Epoch 1732 of 2000 took 0.096s
  training loss:		2.294116
  validation loss:		2.244060
  validation accuracy:		13.04 %
Epoch 1733 of 2000 took 0.096s
  training loss:		2.293505
  validation loss:		2.243325
  validation accuracy:		13.04 %
Epoch 1734 of 2000 took 0.096s
  training loss:		2.293785
  validation loss:		2.243319
  validation accuracy:		13.04 %
Epoch 1735 of 2000 took 0.096s
  training loss:		2.294707
  validation loss:		2.243226
  validation accuracy:		13.04 %
Epoch 1736 of 2000 took 0.096s
  training loss:		2.294848
  validation loss:		2.243696
  validation accuracy:		13.04 %
Epoch 1737 of 2000 took 0.096s
  training loss:		2.294081
  validation loss:		2.244256
  validation accuracy:		13.04 %
Epoch 1738 of 2000 took 0.096s
  training loss:		2.294255
  validation loss:		2.244165
  validation accuracy:		13.04 %
Epoch 1739 of 2000 took 0.096s
  training loss:		2.294931
  validation loss:		2.244504
  validation accuracy:		13.04 %
Epoch 1740 of 2000 took 0.096s
  training loss:		2.295316
  validation loss:		2.244882
  validation accuracy:		12.93 %
Epoch 1741 of 2000 took 0.096s
  training loss:		2.294785
  validation loss:		2.245044
  validation accuracy:		13.04 %
Epoch 1742 of 2000 took 0.096s
  training loss:		2.293424
  validation loss:		2.244721
  validation accuracy:		13.04 %
Epoch 1743 of 2000 took 0.096s
  training loss:		2.293697
  validation loss:		2.244013
  validation accuracy:		13.04 %
Epoch 1744 of 2000 took 0.097s
  training loss:		2.294083
  validation loss:		2.244180
  validation accuracy:		13.04 %
Epoch 1745 of 2000 took 0.096s
  training loss:		2.294274
  validation loss:		2.244092
  validation accuracy:		12.93 %
Epoch 1746 of 2000 took 0.096s
  training loss:		2.294236
  validation loss:		2.244188
  validation accuracy:		12.93 %
Epoch 1747 of 2000 took 0.096s
  training loss:		2.294367
  validation loss:		2.244279
  validation accuracy:		12.93 %
Epoch 1748 of 2000 took 0.096s
  training loss:		2.294329
  validation loss:		2.244512
  validation accuracy:		12.93 %
Epoch 1749 of 2000 took 0.096s
  training loss:		2.294079
  validation loss:		2.244769
  validation accuracy:		12.93 %
Epoch 1750 of 2000 took 0.096s
  training loss:		2.293480
  validation loss:		2.244584
  validation accuracy:		12.93 %
Epoch 1751 of 2000 took 0.096s
  training loss:		2.294262
  validation loss:		2.243501
  validation accuracy:		12.93 %
Epoch 1752 of 2000 took 0.096s
  training loss:		2.293953
  validation loss:		2.244374
  validation accuracy:		12.93 %
Epoch 1753 of 2000 took 0.096s
  training loss:		2.292891
  validation loss:		2.242701
  validation accuracy:		13.04 %
Epoch 1754 of 2000 took 0.096s
  training loss:		2.294860
  validation loss:		2.242709
  validation accuracy:		12.93 %
Epoch 1755 of 2000 took 0.096s
  training loss:		2.294569
  validation loss:		2.242867
  validation accuracy:		13.04 %
Epoch 1756 of 2000 took 0.096s
  training loss:		2.293275
  validation loss:		2.243082
  validation accuracy:		13.04 %
Epoch 1757 of 2000 took 0.096s
  training loss:		2.293984
  validation loss:		2.242914
  validation accuracy:		13.04 %
Epoch 1758 of 2000 took 0.096s
  training loss:		2.294973
  validation loss:		2.243532
  validation accuracy:		13.04 %
Epoch 1759 of 2000 took 0.096s
  training loss:		2.294792
  validation loss:		2.244372
  validation accuracy:		13.04 %
Epoch 1760 of 2000 took 0.096s
  training loss:		2.294168
  validation loss:		2.244637
  validation accuracy:		13.04 %
Epoch 1761 of 2000 took 0.096s
  training loss:		2.292911
  validation loss:		2.244396
  validation accuracy:		13.04 %
Epoch 1762 of 2000 took 0.096s
  training loss:		2.294612
  validation loss:		2.243594
  validation accuracy:		13.04 %
Epoch 1763 of 2000 took 0.096s
  training loss:		2.293699
  validation loss:		2.243578
  validation accuracy:		13.04 %
Epoch 1764 of 2000 took 0.096s
  training loss:		2.294352
  validation loss:		2.243161
  validation accuracy:		13.04 %
Epoch 1765 of 2000 took 0.096s
  training loss:		2.295138
  validation loss:		2.243381
  validation accuracy:		13.04 %
Epoch 1766 of 2000 took 0.096s
  training loss:		2.294485
  validation loss:		2.243305
  validation accuracy:		12.93 %
Epoch 1767 of 2000 took 0.096s
  training loss:		2.294169
  validation loss:		2.243745
  validation accuracy:		12.93 %
Epoch 1768 of 2000 took 0.096s
  training loss:		2.295003
  validation loss:		2.244264
  validation accuracy:		12.93 %
Epoch 1769 of 2000 took 0.096s
  training loss:		2.294205
  validation loss:		2.244126
  validation accuracy:		13.04 %
Epoch 1770 of 2000 took 0.096s
  training loss:		2.294322
  validation loss:		2.243649
  validation accuracy:		13.04 %
Epoch 1771 of 2000 took 0.096s
  training loss:		2.294568
  validation loss:		2.244619
  validation accuracy:		13.04 %
Epoch 1772 of 2000 took 0.096s
  training loss:		2.294464
  validation loss:		2.244801
  validation accuracy:		13.04 %
Epoch 1773 of 2000 took 0.096s
  training loss:		2.294066
  validation loss:		2.244225
  validation accuracy:		13.04 %
Epoch 1774 of 2000 took 0.096s
  training loss:		2.294673
  validation loss:		2.244430
  validation accuracy:		12.83 %
Epoch 1775 of 2000 took 0.097s
  training loss:		2.294885
  validation loss:		2.245281
  validation accuracy:		12.93 %
Epoch 1776 of 2000 took 0.096s
  training loss:		2.294726
  validation loss:		2.245496
  validation accuracy:		12.93 %
Epoch 1777 of 2000 took 0.096s
  training loss:		2.293888
  validation loss:		2.244741
  validation accuracy:		12.93 %
Epoch 1778 of 2000 took 0.096s
  training loss:		2.293899
  validation loss:		2.244130
  validation accuracy:		12.93 %
Epoch 1779 of 2000 took 0.096s
  training loss:		2.294011
  validation loss:		2.243972
  validation accuracy:		12.83 %
Epoch 1780 of 2000 took 0.096s
  training loss:		2.293641
  validation loss:		2.243204
  validation accuracy:		12.83 %
Epoch 1781 of 2000 took 0.096s
  training loss:		2.294153
  validation loss:		2.243285
  validation accuracy:		13.04 %
Epoch 1782 of 2000 took 0.096s
  training loss:		2.293556
  validation loss:		2.243259
  validation accuracy:		13.04 %
Epoch 1783 of 2000 took 0.096s
  training loss:		2.295138
  validation loss:		2.243583
  validation accuracy:		12.93 %
Epoch 1784 of 2000 took 0.096s
  training loss:		2.294959
  validation loss:		2.243971
  validation accuracy:		12.93 %
Epoch 1785 of 2000 took 0.096s
  training loss:		2.294926
  validation loss:		2.244875
  validation accuracy:		12.93 %
Epoch 1786 of 2000 took 0.096s
  training loss:		2.294604
  validation loss:		2.245238
  validation accuracy:		13.04 %
Epoch 1787 of 2000 took 0.096s
  training loss:		2.293927
  validation loss:		2.245357
  validation accuracy:		13.04 %
Epoch 1788 of 2000 took 0.096s
  training loss:		2.294519
  validation loss:		2.245716
  validation accuracy:		13.04 %
Epoch 1789 of 2000 took 0.096s
  training loss:		2.294341
  validation loss:		2.245440
  validation accuracy:		13.04 %
Epoch 1790 of 2000 took 0.096s
  training loss:		2.295166
  validation loss:		2.244989
  validation accuracy:		12.93 %
Epoch 1791 of 2000 took 0.096s
  training loss:		2.293613
  validation loss:		2.245377
  validation accuracy:		12.93 %
Epoch 1792 of 2000 took 0.096s
  training loss:		2.294418
  validation loss:		2.245032
  validation accuracy:		12.93 %
Epoch 1793 of 2000 took 0.096s
  training loss:		2.294457
  validation loss:		2.245033
  validation accuracy:		12.93 %
Epoch 1794 of 2000 took 0.096s
  training loss:		2.293888
  validation loss:		2.244704
  validation accuracy:		12.93 %
Epoch 1795 of 2000 took 0.096s
  training loss:		2.294818
  validation loss:		2.244531
  validation accuracy:		12.93 %
Epoch 1796 of 2000 took 0.096s
  training loss:		2.293536
  validation loss:		2.243835
  validation accuracy:		12.93 %
Epoch 1797 of 2000 took 0.096s
  training loss:		2.293874
  validation loss:		2.244034
  validation accuracy:		13.04 %
Epoch 1798 of 2000 took 0.096s
  training loss:		2.294978
  validation loss:		2.244143
  validation accuracy:		13.04 %
Epoch 1799 of 2000 took 0.096s
  training loss:		2.293637
  validation loss:		2.244215
  validation accuracy:		13.04 %
Epoch 1800 of 2000 took 0.096s
  training loss:		2.293918
  validation loss:		2.244059
  validation accuracy:		13.04 %
Epoch 1801 of 2000 took 0.096s
  training loss:		2.294205
  validation loss:		2.244054
  validation accuracy:		13.04 %
Epoch 1802 of 2000 took 0.096s
  training loss:		2.294520
  validation loss:		2.244531
  validation accuracy:		13.04 %
Epoch 1803 of 2000 took 0.096s
  training loss:		2.294634
  validation loss:		2.244712
  validation accuracy:		13.04 %
Epoch 1804 of 2000 took 0.096s
  training loss:		2.294798
  validation loss:		2.244544
  validation accuracy:		13.04 %
Epoch 1805 of 2000 took 0.096s
  training loss:		2.294939
  validation loss:		2.245203
  validation accuracy:		13.04 %
Epoch 1806 of 2000 took 0.097s
  training loss:		2.294294
  validation loss:		2.245192
  validation accuracy:		13.04 %
Epoch 1807 of 2000 took 0.096s
  training loss:		2.293647
  validation loss:		2.245022
  validation accuracy:		12.93 %
Epoch 1808 of 2000 took 0.096s
  training loss:		2.294170
  validation loss:		2.244621
  validation accuracy:		12.93 %
Epoch 1809 of 2000 took 0.096s
  training loss:		2.293866
  validation loss:		2.243668
  validation accuracy:		12.93 %
Epoch 1810 of 2000 took 0.096s
  training loss:		2.294740
  validation loss:		2.244192
  validation accuracy:		12.93 %
Epoch 1811 of 2000 took 0.096s
  training loss:		2.294266
  validation loss:		2.244094
  validation accuracy:		12.93 %
Epoch 1812 of 2000 took 0.096s
  training loss:		2.294514
  validation loss:		2.243750
  validation accuracy:		12.93 %
Epoch 1813 of 2000 took 0.096s
  training loss:		2.293521
  validation loss:		2.243651
  validation accuracy:		12.93 %
Epoch 1814 of 2000 took 0.096s
  training loss:		2.294056
  validation loss:		2.243565
  validation accuracy:		12.93 %
Epoch 1815 of 2000 took 0.096s
  training loss:		2.294523
  validation loss:		2.244014
  validation accuracy:		13.04 %
Epoch 1816 of 2000 took 0.096s
  training loss:		2.294339
  validation loss:		2.244168
  validation accuracy:		13.04 %
Epoch 1817 of 2000 took 0.096s
  training loss:		2.294284
  validation loss:		2.244092
  validation accuracy:		13.04 %
Epoch 1818 of 2000 took 0.096s
  training loss:		2.293843
  validation loss:		2.243801
  validation accuracy:		13.04 %
Epoch 1819 of 2000 took 0.096s
  training loss:		2.293610
  validation loss:		2.243113
  validation accuracy:		13.04 %
Epoch 1820 of 2000 took 0.096s
  training loss:		2.294172
  validation loss:		2.243173
  validation accuracy:		13.04 %
Epoch 1821 of 2000 took 0.096s
  training loss:		2.294106
  validation loss:		2.243628
  validation accuracy:		12.93 %
Epoch 1822 of 2000 took 0.096s
  training loss:		2.294068
  validation loss:		2.243491
  validation accuracy:		12.93 %
Epoch 1823 of 2000 took 0.096s
  training loss:		2.294482
  validation loss:		2.243223
  validation accuracy:		13.04 %
Epoch 1824 of 2000 took 0.096s
  training loss:		2.294496
  validation loss:		2.243315
  validation accuracy:		13.04 %
Epoch 1825 of 2000 took 0.096s
  training loss:		2.293629
  validation loss:		2.243793
  validation accuracy:		12.93 %
Epoch 1826 of 2000 took 0.096s
  training loss:		2.293730
  validation loss:		2.243333
  validation accuracy:		12.93 %
Epoch 1827 of 2000 took 0.096s
  training loss:		2.294728
  validation loss:		2.243655
  validation accuracy:		12.93 %
Epoch 1828 of 2000 took 0.096s
  training loss:		2.294240
  validation loss:		2.243955
  validation accuracy:		13.04 %
Epoch 1829 of 2000 took 0.096s
  training loss:		2.294474
  validation loss:		2.244609
  validation accuracy:		13.04 %
Epoch 1830 of 2000 took 0.096s
  training loss:		2.293996
  validation loss:		2.244485
  validation accuracy:		13.04 %
Epoch 1831 of 2000 took 0.096s
  training loss:		2.293695
  validation loss:		2.243888
  validation accuracy:		13.04 %
Epoch 1832 of 2000 took 0.096s
  training loss:		2.294122
  validation loss:		2.243637
  validation accuracy:		13.04 %
Epoch 1833 of 2000 took 0.096s
  training loss:		2.294489
  validation loss:		2.243913
  validation accuracy:		12.93 %
Epoch 1834 of 2000 took 0.096s
  training loss:		2.294136
  validation loss:		2.243887
  validation accuracy:		13.04 %
Epoch 1835 of 2000 took 0.096s
  training loss:		2.293562
  validation loss:		2.243815
  validation accuracy:		13.04 %
Epoch 1836 of 2000 took 0.096s
  training loss:		2.294312
  validation loss:		2.243812
  validation accuracy:		12.93 %
Epoch 1837 of 2000 took 0.096s
  training loss:		2.294705
  validation loss:		2.244259
  validation accuracy:		12.93 %
Epoch 1838 of 2000 took 0.097s
  training loss:		2.294598
  validation loss:		2.244778
  validation accuracy:		12.93 %
Epoch 1839 of 2000 took 0.102s
  training loss:		2.294127
  validation loss:		2.244560
  validation accuracy:		12.93 %
Epoch 1840 of 2000 took 0.102s
  training loss:		2.294915
  validation loss:		2.244959
  validation accuracy:		12.93 %
Epoch 1841 of 2000 took 0.102s
  training loss:		2.294358
  validation loss:		2.244524
  validation accuracy:		12.93 %
Epoch 1842 of 2000 took 0.102s
  training loss:		2.294112
  validation loss:		2.244844
  validation accuracy:		13.04 %
Epoch 1843 of 2000 took 0.102s
  training loss:		2.294170
  validation loss:		2.244433
  validation accuracy:		12.93 %
Epoch 1844 of 2000 took 0.102s
  training loss:		2.294011
  validation loss:		2.243669
  validation accuracy:		13.04 %
Epoch 1845 of 2000 took 0.102s
  training loss:		2.294145
  validation loss:		2.243865
  validation accuracy:		13.04 %
Epoch 1846 of 2000 took 0.102s
  training loss:		2.294891
  validation loss:		2.244058
  validation accuracy:		13.04 %
Epoch 1847 of 2000 took 0.102s
  training loss:		2.294211
  validation loss:		2.245276
  validation accuracy:		13.04 %
Epoch 1848 of 2000 took 0.101s
  training loss:		2.293096
  validation loss:		2.245113
  validation accuracy:		12.93 %
Epoch 1849 of 2000 took 0.099s
  training loss:		2.294021
  validation loss:		2.243930
  validation accuracy:		13.04 %
Epoch 1850 of 2000 took 0.099s
  training loss:		2.294079
  validation loss:		2.243717
  validation accuracy:		13.04 %
Epoch 1851 of 2000 took 0.099s
  training loss:		2.294569
  validation loss:		2.243532
  validation accuracy:		12.93 %
Epoch 1852 of 2000 took 0.096s
  training loss:		2.294442
  validation loss:		2.243641
  validation accuracy:		12.93 %
Epoch 1853 of 2000 took 0.096s
  training loss:		2.293105
  validation loss:		2.243074
  validation accuracy:		13.04 %
Epoch 1854 of 2000 took 0.096s
  training loss:		2.293851
  validation loss:		2.242951
  validation accuracy:		13.04 %
Epoch 1855 of 2000 took 0.096s
  training loss:		2.293737
  validation loss:		2.242540
  validation accuracy:		13.04 %
Epoch 1856 of 2000 took 0.096s
  training loss:		2.293267
  validation loss:		2.242408
  validation accuracy:		13.04 %
Epoch 1857 of 2000 took 0.096s
  training loss:		2.293285
  validation loss:		2.242070
  validation accuracy:		12.93 %
Epoch 1858 of 2000 took 0.096s
  training loss:		2.293621
  validation loss:		2.242537
  validation accuracy:		12.93 %
Epoch 1859 of 2000 took 0.096s
  training loss:		2.294331
  validation loss:		2.242957
  validation accuracy:		12.93 %
Epoch 1860 of 2000 took 0.096s
  training loss:		2.293927
  validation loss:		2.243019
  validation accuracy:		12.93 %
Epoch 1861 of 2000 took 0.096s
  training loss:		2.294235
  validation loss:		2.243535
  validation accuracy:		12.93 %
Epoch 1862 of 2000 took 0.096s
  training loss:		2.295022
  validation loss:		2.243468
  validation accuracy:		12.93 %
Epoch 1863 of 2000 took 0.096s
  training loss:		2.294355
  validation loss:		2.243549
  validation accuracy:		12.93 %
Epoch 1864 of 2000 took 0.096s
  training loss:		2.294153
  validation loss:		2.243309
  validation accuracy:		13.26 %
Epoch 1865 of 2000 took 0.096s
  training loss:		2.294049
  validation loss:		2.243806
  validation accuracy:		13.04 %
Epoch 1866 of 2000 took 0.096s
  training loss:		2.293831
  validation loss:		2.243453
  validation accuracy:		12.93 %
Epoch 1867 of 2000 took 0.096s
  training loss:		2.294764
  validation loss:		2.244026
  validation accuracy:		13.04 %
Epoch 1868 of 2000 took 0.097s
  training loss:		2.294246
  validation loss:		2.243733
  validation accuracy:		13.04 %
Epoch 1869 of 2000 took 0.096s
  training loss:		2.293882
  validation loss:		2.243729
  validation accuracy:		13.04 %
Epoch 1870 of 2000 took 0.096s
  training loss:		2.294847
  validation loss:		2.243677
  validation accuracy:		12.93 %
Epoch 1871 of 2000 took 0.096s
  training loss:		2.293774
  validation loss:		2.243893
  validation accuracy:		12.93 %
Epoch 1872 of 2000 took 0.096s
  training loss:		2.294072
  validation loss:		2.243590
  validation accuracy:		12.93 %
Epoch 1873 of 2000 took 0.096s
  training loss:		2.293941
  validation loss:		2.243978
  validation accuracy:		12.93 %
Epoch 1874 of 2000 took 0.096s
  training loss:		2.294449
  validation loss:		2.243406
  validation accuracy:		13.04 %
Epoch 1875 of 2000 took 0.096s
  training loss:		2.293902
  validation loss:		2.243615
  validation accuracy:		13.04 %
Epoch 1876 of 2000 took 0.096s
  training loss:		2.294224
  validation loss:		2.243712
  validation accuracy:		12.93 %
Epoch 1877 of 2000 took 0.096s
  training loss:		2.293999
  validation loss:		2.243625
  validation accuracy:		12.93 %
Epoch 1878 of 2000 took 0.096s
  training loss:		2.293910
  validation loss:		2.243595
  validation accuracy:		12.93 %
Epoch 1879 of 2000 took 0.096s
  training loss:		2.294487
  validation loss:		2.243467
  validation accuracy:		13.04 %
Epoch 1880 of 2000 took 0.096s
  training loss:		2.295027
  validation loss:		2.243661
  validation accuracy:		13.70 %
Epoch 1881 of 2000 took 0.096s
  training loss:		2.294368
  validation loss:		2.244511
  validation accuracy:		12.93 %
Epoch 1882 of 2000 took 0.096s
  training loss:		2.294100
  validation loss:		2.244733
  validation accuracy:		13.04 %
Epoch 1883 of 2000 took 0.096s
  training loss:		2.294157
  validation loss:		2.244544
  validation accuracy:		13.04 %
Epoch 1884 of 2000 took 0.096s
  training loss:		2.294894
  validation loss:		2.244525
  validation accuracy:		13.04 %
Epoch 1885 of 2000 took 0.096s
  training loss:		2.293863
  validation loss:		2.244455
  validation accuracy:		13.04 %
Epoch 1886 of 2000 took 0.096s
  training loss:		2.293685
  validation loss:		2.244344
  validation accuracy:		12.93 %
Epoch 1887 of 2000 took 0.096s
  training loss:		2.294789
  validation loss:		2.244470
  validation accuracy:		12.83 %
Epoch 1888 of 2000 took 0.096s
  training loss:		2.294899
  validation loss:		2.244556
  validation accuracy:		12.83 %
Epoch 1889 of 2000 took 0.096s
  training loss:		2.294358
  validation loss:		2.245128
  validation accuracy:		12.83 %
Epoch 1890 of 2000 took 0.096s
  training loss:		2.294335
  validation loss:		2.244720
  validation accuracy:		13.04 %
Epoch 1891 of 2000 took 0.096s
  training loss:		2.293971
  validation loss:		2.245041
  validation accuracy:		13.04 %
Epoch 1892 of 2000 took 0.096s
  training loss:		2.294060
  validation loss:		2.244689
  validation accuracy:		12.93 %
Epoch 1893 of 2000 took 0.096s
  training loss:		2.294060
  validation loss:		2.244208
  validation accuracy:		12.93 %
Epoch 1894 of 2000 took 0.096s
  training loss:		2.294480
  validation loss:		2.244244
  validation accuracy:		13.04 %
Epoch 1895 of 2000 took 0.096s
  training loss:		2.294297
  validation loss:		2.243991
  validation accuracy:		12.93 %
Epoch 1896 of 2000 took 0.096s
  training loss:		2.294433
  validation loss:		2.244092
  validation accuracy:		12.93 %
Epoch 1897 of 2000 took 0.096s
  training loss:		2.294701
  validation loss:		2.244881
  validation accuracy:		12.83 %
Epoch 1898 of 2000 took 0.096s
  training loss:		2.293939
  validation loss:		2.245073
  validation accuracy:		12.83 %
Epoch 1899 of 2000 took 0.096s
  training loss:		2.294963
  validation loss:		2.244764
  validation accuracy:		12.83 %
Epoch 1900 of 2000 took 0.097s
  training loss:		2.293461
  validation loss:		2.245285
  validation accuracy:		12.83 %
Epoch 1901 of 2000 took 0.096s
  training loss:		2.294141
  validation loss:		2.244755
  validation accuracy:		12.93 %
Epoch 1902 of 2000 took 0.096s
  training loss:		2.294192
  validation loss:		2.244084
  validation accuracy:		12.93 %
Epoch 1903 of 2000 took 0.096s
  training loss:		2.294758
  validation loss:		2.243939
  validation accuracy:		13.04 %
Epoch 1904 of 2000 took 0.096s
  training loss:		2.294175
  validation loss:		2.244239
  validation accuracy:		13.04 %
Epoch 1905 of 2000 took 0.096s
  training loss:		2.294176
  validation loss:		2.243357
  validation accuracy:		12.83 %
Epoch 1906 of 2000 took 0.096s
  training loss:		2.294681
  validation loss:		2.243877
  validation accuracy:		12.93 %
Epoch 1907 of 2000 took 0.096s
  training loss:		2.294703
  validation loss:		2.244336
  validation accuracy:		13.04 %
Epoch 1908 of 2000 took 0.096s
  training loss:		2.294282
  validation loss:		2.243999
  validation accuracy:		13.04 %
Epoch 1909 of 2000 took 0.096s
  training loss:		2.294588
  validation loss:		2.243997
  validation accuracy:		13.04 %
Epoch 1910 of 2000 took 0.096s
  training loss:		2.294203
  validation loss:		2.245056
  validation accuracy:		13.04 %
Epoch 1911 of 2000 took 0.096s
  training loss:		2.293723
  validation loss:		2.244928
  validation accuracy:		13.04 %
Epoch 1912 of 2000 took 0.096s
  training loss:		2.293787
  validation loss:		2.244566
  validation accuracy:		13.04 %
Epoch 1913 of 2000 took 0.096s
  training loss:		2.294552
  validation loss:		2.244537
  validation accuracy:		13.04 %
Epoch 1914 of 2000 took 0.096s
  training loss:		2.294821
  validation loss:		2.244914
  validation accuracy:		13.04 %
Epoch 1915 of 2000 took 0.096s
  training loss:		2.294212
  validation loss:		2.245081
  validation accuracy:		13.04 %
Epoch 1916 of 2000 took 0.096s
  training loss:		2.294562
  validation loss:		2.245039
  validation accuracy:		13.04 %
Epoch 1917 of 2000 took 0.096s
  training loss:		2.293451
  validation loss:		2.244096
  validation accuracy:		13.04 %
Epoch 1918 of 2000 took 0.096s
  training loss:		2.293600
  validation loss:		2.243305
  validation accuracy:		13.04 %
Epoch 1919 of 2000 took 0.098s
  training loss:		2.294233
  validation loss:		2.243200
  validation accuracy:		13.04 %
Epoch 1920 of 2000 took 0.096s
  training loss:		2.294761
  validation loss:		2.243548
  validation accuracy:		13.04 %
Epoch 1921 of 2000 took 0.096s
  training loss:		2.293338
  validation loss:		2.243653
  validation accuracy:		12.93 %
Epoch 1922 of 2000 took 0.096s
  training loss:		2.294388
  validation loss:		2.243506
  validation accuracy:		13.04 %
Epoch 1923 of 2000 took 0.096s
  training loss:		2.293961
  validation loss:		2.243810
  validation accuracy:		13.04 %
Epoch 1924 of 2000 took 0.096s
  training loss:		2.294315
  validation loss:		2.244180
  validation accuracy:		13.04 %
Epoch 1925 of 2000 took 0.096s
  training loss:		2.294321
  validation loss:		2.244191
  validation accuracy:		13.04 %
Epoch 1926 of 2000 took 0.096s
  training loss:		2.293840
  validation loss:		2.244155
  validation accuracy:		13.04 %
Epoch 1927 of 2000 took 0.096s
  training loss:		2.294021
  validation loss:		2.243431
  validation accuracy:		12.93 %
Epoch 1928 of 2000 took 0.096s
  training loss:		2.294368
  validation loss:		2.243232
  validation accuracy:		12.93 %
Epoch 1929 of 2000 took 0.096s
  training loss:		2.294793
  validation loss:		2.243441
  validation accuracy:		12.93 %
Epoch 1930 of 2000 took 0.096s
  training loss:		2.294294
  validation loss:		2.243294
  validation accuracy:		12.93 %
Epoch 1931 of 2000 took 0.097s
  training loss:		2.293705
  validation loss:		2.243310
  validation accuracy:		12.93 %
Epoch 1932 of 2000 took 0.096s
  training loss:		2.294092
  validation loss:		2.243828
  validation accuracy:		12.93 %
Epoch 1933 of 2000 took 0.096s
  training loss:		2.293375
  validation loss:		2.244411
  validation accuracy:		12.93 %
Epoch 1934 of 2000 took 0.096s
  training loss:		2.294145
  validation loss:		2.244111
  validation accuracy:		12.93 %
Epoch 1935 of 2000 took 0.096s
  training loss:		2.293359
  validation loss:		2.243452
  validation accuracy:		12.93 %
Epoch 1936 of 2000 took 0.096s
  training loss:		2.294372
  validation loss:		2.243365
  validation accuracy:		13.04 %
Epoch 1937 of 2000 took 0.096s
  training loss:		2.293996
  validation loss:		2.242852
  validation accuracy:		12.93 %
Epoch 1938 of 2000 took 0.096s
  training loss:		2.294482
  validation loss:		2.243680
  validation accuracy:		12.93 %
Epoch 1939 of 2000 took 0.096s
  training loss:		2.293810
  validation loss:		2.243730
  validation accuracy:		12.93 %
Epoch 1940 of 2000 took 0.096s
  training loss:		2.293960
  validation loss:		2.243685
  validation accuracy:		12.93 %
Epoch 1941 of 2000 took 0.096s
  training loss:		2.294218
  validation loss:		2.243383
  validation accuracy:		12.93 %
Epoch 1942 of 2000 took 0.096s
  training loss:		2.293500
  validation loss:		2.243262
  validation accuracy:		13.04 %
Epoch 1943 of 2000 took 0.096s
  training loss:		2.294187
  validation loss:		2.242442
  validation accuracy:		13.04 %
Epoch 1944 of 2000 took 0.096s
  training loss:		2.294617
  validation loss:		2.242725
  validation accuracy:		13.04 %
Epoch 1945 of 2000 took 0.096s
  training loss:		2.293550
  validation loss:		2.243020
  validation accuracy:		13.04 %
Epoch 1946 of 2000 took 0.096s
  training loss:		2.294208
  validation loss:		2.242772
  validation accuracy:		13.04 %
Epoch 1947 of 2000 took 0.096s
  training loss:		2.295046
  validation loss:		2.243733
  validation accuracy:		13.04 %
Epoch 1948 of 2000 took 0.096s
  training loss:		2.294363
  validation loss:		2.245020
  validation accuracy:		13.04 %
Epoch 1949 of 2000 took 0.096s
  training loss:		2.294239
  validation loss:		2.245043
  validation accuracy:		12.93 %
Epoch 1950 of 2000 took 0.096s
  training loss:		2.293959
  validation loss:		2.244533
  validation accuracy:		12.93 %
Epoch 1951 of 2000 took 0.096s
  training loss:		2.293571
  validation loss:		2.244139
  validation accuracy:		12.93 %
Epoch 1952 of 2000 took 0.096s
  training loss:		2.293810
  validation loss:		2.244254
  validation accuracy:		13.04 %
Epoch 1953 of 2000 took 0.096s
  training loss:		2.292954
  validation loss:		2.243423
  validation accuracy:		12.93 %
Epoch 1954 of 2000 took 0.096s
  training loss:		2.294078
  validation loss:		2.243400
  validation accuracy:		12.93 %
Epoch 1955 of 2000 took 0.096s
  training loss:		2.294442
  validation loss:		2.242908
  validation accuracy:		12.93 %
Epoch 1956 of 2000 took 0.096s
  training loss:		2.294382
  validation loss:		2.243341
  validation accuracy:		12.83 %
Epoch 1957 of 2000 took 0.096s
  training loss:		2.294300
  validation loss:		2.243264
  validation accuracy:		12.83 %
Epoch 1958 of 2000 took 0.096s
  training loss:		2.294593
  validation loss:		2.243869
  validation accuracy:		13.04 %
Epoch 1959 of 2000 took 0.096s
  training loss:		2.294019
  validation loss:		2.244084
  validation accuracy:		13.04 %
Epoch 1960 of 2000 took 0.096s
  training loss:		2.294537
  validation loss:		2.244214
  validation accuracy:		12.83 %
Epoch 1961 of 2000 took 0.096s
  training loss:		2.294297
  validation loss:		2.244475
  validation accuracy:		12.83 %
Epoch 1962 of 2000 took 0.097s
  training loss:		2.293624
  validation loss:		2.244567
  validation accuracy:		12.93 %
Epoch 1963 of 2000 took 0.096s
  training loss:		2.294795
  validation loss:		2.243492
  validation accuracy:		12.93 %
Epoch 1964 of 2000 took 0.096s
  training loss:		2.295012
  validation loss:		2.244353
  validation accuracy:		12.93 %
Epoch 1965 of 2000 took 0.096s
  training loss:		2.294841
  validation loss:		2.245084
  validation accuracy:		12.93 %
Epoch 1966 of 2000 took 0.096s
  training loss:		2.293644
  validation loss:		2.244216
  validation accuracy:		12.93 %
Epoch 1967 of 2000 took 0.096s
  training loss:		2.293274
  validation loss:		2.243700
  validation accuracy:		12.93 %
Epoch 1968 of 2000 took 0.096s
  training loss:		2.294083
  validation loss:		2.243544
  validation accuracy:		12.93 %
Epoch 1969 of 2000 took 0.096s
  training loss:		2.293826
  validation loss:		2.243336
  validation accuracy:		12.93 %
Epoch 1970 of 2000 took 0.096s
  training loss:		2.294177
  validation loss:		2.243207
  validation accuracy:		13.04 %
Epoch 1971 of 2000 took 0.096s
  training loss:		2.294388
  validation loss:		2.244050
  validation accuracy:		13.04 %
Epoch 1972 of 2000 took 0.096s
  training loss:		2.294062
  validation loss:		2.243656
  validation accuracy:		12.93 %
Epoch 1973 of 2000 took 0.096s
  training loss:		2.294481
  validation loss:		2.243617
  validation accuracy:		13.04 %
Epoch 1974 of 2000 took 0.096s
  training loss:		2.294631
  validation loss:		2.243442
  validation accuracy:		13.04 %
Epoch 1975 of 2000 took 0.096s
  training loss:		2.294125
  validation loss:		2.243893
  validation accuracy:		12.93 %
Epoch 1976 of 2000 took 0.096s
  training loss:		2.294372
  validation loss:		2.244266
  validation accuracy:		12.93 %
Epoch 1977 of 2000 took 0.096s
  training loss:		2.294831
  validation loss:		2.244558
  validation accuracy:		12.93 %
Epoch 1978 of 2000 took 0.096s
  training loss:		2.293715
  validation loss:		2.244576
  validation accuracy:		12.93 %
Epoch 1979 of 2000 took 0.096s
  training loss:		2.294804
  validation loss:		2.244931
  validation accuracy:		12.93 %
Epoch 1980 of 2000 took 0.096s
  training loss:		2.294392
  validation loss:		2.245034
  validation accuracy:		12.93 %
Epoch 1981 of 2000 took 0.096s
  training loss:		2.293956
  validation loss:		2.244793
  validation accuracy:		12.93 %
Epoch 1982 of 2000 took 0.096s
  training loss:		2.294019
  validation loss:		2.245204
  validation accuracy:		12.93 %
Epoch 1983 of 2000 took 0.096s
  training loss:		2.294427
  validation loss:		2.244939
  validation accuracy:		12.93 %
Epoch 1984 of 2000 took 0.096s
  training loss:		2.293782
  validation loss:		2.244070
  validation accuracy:		13.26 %
Epoch 1985 of 2000 took 0.096s
  training loss:		2.293230
  validation loss:		2.243597
  validation accuracy:		13.04 %
Epoch 1986 of 2000 took 0.096s
  training loss:		2.294187
  validation loss:		2.242856
  validation accuracy:		12.93 %
Epoch 1987 of 2000 took 0.096s
  training loss:		2.293938
  validation loss:		2.243257
  validation accuracy:		12.93 %
Epoch 1988 of 2000 took 0.096s
  training loss:		2.295187
  validation loss:		2.244205
  validation accuracy:		12.93 %
Epoch 1989 of 2000 took 0.096s
  training loss:		2.295034
  validation loss:		2.245047
  validation accuracy:		12.93 %
Epoch 1990 of 2000 took 0.096s
  training loss:		2.294039
  validation loss:		2.245121
  validation accuracy:		12.93 %
Epoch 1991 of 2000 took 0.096s
  training loss:		2.294607
  validation loss:		2.245027
  validation accuracy:		13.04 %
Epoch 1992 of 2000 took 0.096s
  training loss:		2.293658
  validation loss:		2.244735
  validation accuracy:		13.04 %
Epoch 1993 of 2000 took 0.096s
  training loss:		2.294044
  validation loss:		2.244607
  validation accuracy:		13.04 %
Epoch 1994 of 2000 took 0.097s
  training loss:		2.293841
  validation loss:		2.243876
  validation accuracy:		13.04 %
Epoch 1995 of 2000 took 0.096s
  training loss:		2.294250
  validation loss:		2.244357
  validation accuracy:		13.04 %
Epoch 1996 of 2000 took 0.096s
  training loss:		2.294041
  validation loss:		2.244025
  validation accuracy:		12.93 %
Epoch 1997 of 2000 took 0.096s
  training loss:		2.293828
  validation loss:		2.243494
  validation accuracy:		12.93 %
Epoch 1998 of 2000 took 0.096s
  training loss:		2.294101
  validation loss:		2.243528
  validation accuracy:		12.93 %
Epoch 1999 of 2000 took 0.096s
  training loss:		2.294041
  validation loss:		2.243224
  validation accuracy:		12.93 %
Epoch 2000 of 2000 took 0.096s
  training loss:		2.294050
  validation loss:		2.242831
  validation accuracy:		12.93 %
Final results:
  test loss:			2.251217
  test accuracy:		12.36 %
