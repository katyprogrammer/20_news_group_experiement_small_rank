Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (4, 50, 50)...
decomposing tensor B of shape (4, 50)...
Starting training...
Epoch 1 of 2000 took 0.108s
  training loss:		2.994451
  validation loss:		2.988076
  validation accuracy:		12.83 %
Epoch 2 of 2000 took 0.104s
  training loss:		2.983789
  validation loss:		2.973418
  validation accuracy:		12.83 %
Epoch 3 of 2000 took 0.105s
  training loss:		2.969862
  validation loss:		2.956660
  validation accuracy:		12.83 %
Epoch 4 of 2000 took 0.105s
  training loss:		2.954018
  validation loss:		2.939252
  validation accuracy:		14.02 %
Epoch 5 of 2000 took 0.105s
  training loss:		2.938830
  validation loss:		2.921674
  validation accuracy:		16.63 %
Epoch 6 of 2000 took 0.105s
  training loss:		2.921956
  validation loss:		2.903851
  validation accuracy:		13.37 %
Epoch 7 of 2000 took 0.105s
  training loss:		2.905906
  validation loss:		2.885238
  validation accuracy:		12.93 %
Epoch 8 of 2000 took 0.106s
  training loss:		2.888628
  validation loss:		2.865879
  validation accuracy:		12.93 %
Epoch 9 of 2000 took 0.106s
  training loss:		2.869869
  validation loss:		2.845289
  validation accuracy:		12.93 %
Epoch 10 of 2000 took 0.106s
  training loss:		2.851823
  validation loss:		2.822961
  validation accuracy:		12.93 %
Epoch 11 of 2000 took 0.105s
  training loss:		2.830352
  validation loss:		2.798571
  validation accuracy:		12.93 %
Epoch 12 of 2000 took 0.106s
  training loss:		2.808049
  validation loss:		2.771767
  validation accuracy:		12.93 %
Epoch 13 of 2000 took 0.105s
  training loss:		2.780929
  validation loss:		2.741523
  validation accuracy:		12.93 %
Epoch 14 of 2000 took 0.105s
  training loss:		2.753033
  validation loss:		2.707355
  validation accuracy:		12.93 %
Epoch 15 of 2000 took 0.105s
  training loss:		2.719656
  validation loss:		2.668751
  validation accuracy:		12.93 %
Epoch 16 of 2000 took 0.105s
  training loss:		2.683142
  validation loss:		2.624979
  validation accuracy:		12.93 %
Epoch 17 of 2000 took 0.105s
  training loss:		2.641690
  validation loss:		2.576690
  validation accuracy:		12.93 %
Epoch 18 of 2000 took 0.110s
  training loss:		2.599463
  validation loss:		2.525439
  validation accuracy:		12.93 %
Epoch 19 of 2000 took 0.105s
  training loss:		2.554449
  validation loss:		2.471786
  validation accuracy:		12.93 %
Epoch 20 of 2000 took 0.106s
  training loss:		2.506669
  validation loss:		2.420429
  validation accuracy:		12.93 %
Epoch 21 of 2000 took 0.106s
  training loss:		2.463677
  validation loss:		2.374338
  validation accuracy:		12.93 %
Epoch 22 of 2000 took 0.106s
  training loss:		2.422298
  validation loss:		2.333193
  validation accuracy:		12.93 %
Epoch 23 of 2000 took 0.105s
  training loss:		2.388508
  validation loss:		2.303995
  validation accuracy:		12.93 %
Epoch 24 of 2000 took 0.105s
  training loss:		2.361849
  validation loss:		2.283066
  validation accuracy:		12.93 %
Epoch 25 of 2000 took 0.105s
  training loss:		2.343748
  validation loss:		2.269250
  validation accuracy:		12.93 %
Epoch 26 of 2000 took 0.105s
  training loss:		2.330155
  validation loss:		2.263503
  validation accuracy:		12.93 %
Epoch 27 of 2000 took 0.105s
  training loss:		2.322245
  validation loss:		2.259638
  validation accuracy:		12.93 %
Epoch 28 of 2000 took 0.105s
  training loss:		2.316049
  validation loss:		2.260850
  validation accuracy:		12.93 %
Epoch 29 of 2000 took 0.105s
  training loss:		2.311459
  validation loss:		2.257282
  validation accuracy:		12.93 %
Epoch 30 of 2000 took 0.105s
  training loss:		2.308456
  validation loss:		2.253881
  validation accuracy:		13.15 %
Epoch 31 of 2000 took 0.105s
  training loss:		2.304837
  validation loss:		2.251808
  validation accuracy:		18.04 %
Epoch 32 of 2000 took 0.105s
  training loss:		2.303345
  validation loss:		2.249867
  validation accuracy:		14.57 %
Epoch 33 of 2000 took 0.105s
  training loss:		2.302257
  validation loss:		2.248857
  validation accuracy:		13.80 %
Epoch 34 of 2000 took 0.105s
  training loss:		2.301732
  validation loss:		2.250705
  validation accuracy:		13.37 %
Epoch 35 of 2000 took 0.105s
  training loss:		2.300569
  validation loss:		2.246193
  validation accuracy:		12.72 %
Epoch 36 of 2000 took 0.105s
  training loss:		2.300132
  validation loss:		2.247001
  validation accuracy:		13.59 %
Epoch 37 of 2000 took 0.106s
  training loss:		2.298562
  validation loss:		2.247678
  validation accuracy:		12.93 %
Epoch 38 of 2000 took 0.105s
  training loss:		2.298672
  validation loss:		2.247360
  validation accuracy:		12.93 %
Epoch 39 of 2000 took 0.105s
  training loss:		2.297692
  validation loss:		2.244483
  validation accuracy:		13.37 %
Epoch 40 of 2000 took 0.106s
  training loss:		2.297987
  validation loss:		2.245430
  validation accuracy:		14.13 %
Epoch 41 of 2000 took 0.105s
  training loss:		2.298033
  validation loss:		2.243434
  validation accuracy:		12.93 %
Epoch 42 of 2000 took 0.105s
  training loss:		2.297251
  validation loss:		2.245394
  validation accuracy:		12.93 %
Epoch 43 of 2000 took 0.105s
  training loss:		2.295320
  validation loss:		2.244571
  validation accuracy:		12.93 %
Epoch 44 of 2000 took 0.105s
  training loss:		2.296710
  validation loss:		2.245728
  validation accuracy:		14.13 %
Epoch 45 of 2000 took 0.105s
  training loss:		2.297106
  validation loss:		2.245958
  validation accuracy:		18.04 %
Epoch 46 of 2000 took 0.106s
  training loss:		2.295182
  validation loss:		2.240484
  validation accuracy:		12.93 %
Epoch 47 of 2000 took 0.105s
  training loss:		2.296145
  validation loss:		2.244300
  validation accuracy:		12.93 %
Epoch 48 of 2000 took 0.105s
  training loss:		2.296463
  validation loss:		2.243928
  validation accuracy:		13.91 %
Epoch 49 of 2000 took 0.106s
  training loss:		2.295420
  validation loss:		2.240965
  validation accuracy:		18.91 %
Epoch 50 of 2000 took 0.110s
  training loss:		2.295537
  validation loss:		2.243400
  validation accuracy:		13.70 %
Epoch 51 of 2000 took 0.105s
  training loss:		2.296020
  validation loss:		2.241048
  validation accuracy:		12.93 %
Epoch 52 of 2000 took 0.105s
  training loss:		2.294184
  validation loss:		2.243134
  validation accuracy:		16.30 %
Epoch 53 of 2000 took 0.105s
  training loss:		2.295013
  validation loss:		2.242980
  validation accuracy:		12.93 %
Epoch 54 of 2000 took 0.105s
  training loss:		2.295743
  validation loss:		2.241497
  validation accuracy:		17.07 %
Epoch 55 of 2000 took 0.105s
  training loss:		2.296030
  validation loss:		2.244969
  validation accuracy:		13.70 %
Epoch 56 of 2000 took 0.105s
  training loss:		2.295359
  validation loss:		2.244635
  validation accuracy:		12.93 %
Epoch 57 of 2000 took 0.105s
  training loss:		2.295325
  validation loss:		2.243355
  validation accuracy:		13.04 %
Epoch 58 of 2000 took 0.105s
  training loss:		2.295438
  validation loss:		2.242804
  validation accuracy:		13.91 %
Epoch 59 of 2000 took 0.105s
  training loss:		2.295655
  validation loss:		2.246040
  validation accuracy:		13.70 %
Epoch 60 of 2000 took 0.105s
  training loss:		2.295174
  validation loss:		2.245882
  validation accuracy:		17.50 %
Epoch 61 of 2000 took 0.105s
  training loss:		2.296046
  validation loss:		2.245523
  validation accuracy:		19.78 %
Epoch 62 of 2000 took 0.105s
  training loss:		2.295411
  validation loss:		2.243041
  validation accuracy:		12.93 %
Epoch 63 of 2000 took 0.105s
  training loss:		2.294809
  validation loss:		2.238900
  validation accuracy:		12.83 %
Epoch 64 of 2000 took 0.105s
  training loss:		2.294640
  validation loss:		2.238156
  validation accuracy:		12.93 %
Epoch 65 of 2000 took 0.105s
  training loss:		2.294811
  validation loss:		2.244676
  validation accuracy:		15.00 %
Epoch 66 of 2000 took 0.105s
  training loss:		2.293719
  validation loss:		2.242183
  validation accuracy:		12.83 %
Epoch 67 of 2000 took 0.105s
  training loss:		2.294916
  validation loss:		2.241351
  validation accuracy:		15.54 %
Epoch 68 of 2000 took 0.105s
  training loss:		2.293340
  validation loss:		2.240769
  validation accuracy:		13.59 %
Epoch 69 of 2000 took 0.108s
  training loss:		2.295054
  validation loss:		2.242142
  validation accuracy:		18.70 %
Epoch 70 of 2000 took 0.108s
  training loss:		2.293385
  validation loss:		2.240177
  validation accuracy:		15.76 %
Epoch 71 of 2000 took 0.108s
  training loss:		2.295390
  validation loss:		2.241849
  validation accuracy:		14.57 %
Epoch 72 of 2000 took 0.108s
  training loss:		2.294532
  validation loss:		2.243864
  validation accuracy:		15.11 %
Epoch 73 of 2000 took 0.108s
  training loss:		2.294435
  validation loss:		2.241719
  validation accuracy:		15.00 %
Epoch 74 of 2000 took 0.108s
  training loss:		2.294693
  validation loss:		2.243085
  validation accuracy:		18.70 %
Epoch 75 of 2000 took 0.108s
  training loss:		2.294486
  validation loss:		2.241544
  validation accuracy:		12.93 %
Epoch 76 of 2000 took 0.108s
  training loss:		2.294641
  validation loss:		2.245797
  validation accuracy:		12.72 %
Epoch 77 of 2000 took 0.108s
  training loss:		2.293580
  validation loss:		2.239651
  validation accuracy:		12.93 %
Epoch 78 of 2000 took 0.108s
  training loss:		2.295048
  validation loss:		2.245563
  validation accuracy:		13.04 %
Epoch 79 of 2000 took 0.108s
  training loss:		2.294453
  validation loss:		2.244761
  validation accuracy:		18.48 %
Epoch 80 of 2000 took 0.108s
  training loss:		2.295352
  validation loss:		2.243420
  validation accuracy:		14.89 %
Epoch 81 of 2000 took 0.108s
  training loss:		2.294161
  validation loss:		2.241145
  validation accuracy:		12.93 %
Epoch 82 of 2000 took 0.108s
  training loss:		2.293605
  validation loss:		2.240255
  validation accuracy:		14.24 %
Epoch 83 of 2000 took 0.109s
  training loss:		2.294014
  validation loss:		2.244808
  validation accuracy:		12.93 %
Epoch 84 of 2000 took 0.108s
  training loss:		2.293428
  validation loss:		2.240538
  validation accuracy:		13.37 %
Epoch 85 of 2000 took 0.108s
  training loss:		2.294451
  validation loss:		2.238546
  validation accuracy:		12.93 %
Epoch 86 of 2000 took 0.108s
  training loss:		2.293510
  validation loss:		2.234995
  validation accuracy:		13.15 %
Epoch 87 of 2000 took 0.110s
  training loss:		2.293976
  validation loss:		2.244378
  validation accuracy:		12.93 %
Epoch 88 of 2000 took 0.106s
  training loss:		2.293640
  validation loss:		2.244059
  validation accuracy:		18.26 %
Epoch 89 of 2000 took 0.105s
  training loss:		2.294299
  validation loss:		2.243042
  validation accuracy:		15.65 %
Epoch 90 of 2000 took 0.105s
  training loss:		2.293692
  validation loss:		2.240213
  validation accuracy:		13.15 %
Epoch 91 of 2000 took 0.105s
  training loss:		2.293168
  validation loss:		2.240433
  validation accuracy:		17.61 %
Epoch 92 of 2000 took 0.105s
  training loss:		2.294189
  validation loss:		2.241166
  validation accuracy:		13.37 %
Epoch 93 of 2000 took 0.105s
  training loss:		2.294642
  validation loss:		2.241651
  validation accuracy:		18.15 %
Epoch 94 of 2000 took 0.105s
  training loss:		2.292982
  validation loss:		2.243314
  validation accuracy:		17.83 %
Epoch 95 of 2000 took 0.105s
  training loss:		2.292624
  validation loss:		2.240273
  validation accuracy:		12.83 %
Epoch 96 of 2000 took 0.105s
  training loss:		2.294213
  validation loss:		2.238057
  validation accuracy:		15.54 %
Epoch 97 of 2000 took 0.106s
  training loss:		2.292736
  validation loss:		2.242025
  validation accuracy:		13.37 %
Epoch 98 of 2000 took 0.105s
  training loss:		2.293923
  validation loss:		2.240509
  validation accuracy:		18.80 %
Epoch 99 of 2000 took 0.105s
  training loss:		2.293660
  validation loss:		2.240754
  validation accuracy:		12.93 %
Epoch 100 of 2000 took 0.105s
  training loss:		2.293583
  validation loss:		2.239899
  validation accuracy:		12.93 %
Epoch 101 of 2000 took 0.105s
  training loss:		2.292511
  validation loss:		2.239231
  validation accuracy:		16.63 %
Epoch 102 of 2000 took 0.105s
  training loss:		2.292322
  validation loss:		2.235682
  validation accuracy:		12.93 %
Epoch 103 of 2000 took 0.105s
  training loss:		2.293406
  validation loss:		2.237565
  validation accuracy:		13.91 %
Epoch 104 of 2000 took 0.105s
  training loss:		2.293718
  validation loss:		2.248071
  validation accuracy:		16.63 %
Epoch 105 of 2000 took 0.105s
  training loss:		2.293135
  validation loss:		2.241210
  validation accuracy:		14.35 %
Epoch 106 of 2000 took 0.105s
  training loss:		2.293600
  validation loss:		2.239676
  validation accuracy:		12.93 %
Epoch 107 of 2000 took 0.105s
  training loss:		2.291937
  validation loss:		2.235997
  validation accuracy:		12.93 %
Epoch 108 of 2000 took 0.105s
  training loss:		2.293032
  validation loss:		2.239926
  validation accuracy:		12.93 %
Epoch 109 of 2000 took 0.105s
  training loss:		2.292959
  validation loss:		2.246566
  validation accuracy:		17.28 %
Epoch 110 of 2000 took 0.105s
  training loss:		2.292944
  validation loss:		2.237646
  validation accuracy:		17.72 %
Epoch 111 of 2000 took 0.105s
  training loss:		2.292984
  validation loss:		2.241057
  validation accuracy:		18.70 %
Epoch 112 of 2000 took 0.105s
  training loss:		2.293306
  validation loss:		2.244227
  validation accuracy:		16.63 %
Epoch 113 of 2000 took 0.105s
  training loss:		2.292748
  validation loss:		2.240663
  validation accuracy:		12.93 %
Epoch 114 of 2000 took 0.110s
  training loss:		2.291866
  validation loss:		2.239892
  validation accuracy:		18.70 %
Epoch 115 of 2000 took 0.106s
  training loss:		2.292640
  validation loss:		2.241055
  validation accuracy:		16.74 %
Epoch 116 of 2000 took 0.105s
  training loss:		2.292150
  validation loss:		2.238446
  validation accuracy:		12.83 %
Epoch 117 of 2000 took 0.105s
  training loss:		2.292360
  validation loss:		2.236081
  validation accuracy:		12.93 %
Epoch 118 of 2000 took 0.105s
  training loss:		2.292643
  validation loss:		2.239274
  validation accuracy:		12.93 %
Epoch 119 of 2000 took 0.105s
  training loss:		2.291782
  validation loss:		2.238580
  validation accuracy:		12.83 %
Epoch 120 of 2000 took 0.105s
  training loss:		2.292783
  validation loss:		2.236516
  validation accuracy:		15.22 %
Epoch 121 of 2000 took 0.105s
  training loss:		2.292148
  validation loss:		2.240634
  validation accuracy:		19.46 %
Epoch 122 of 2000 took 0.105s
  training loss:		2.292204
  validation loss:		2.239491
  validation accuracy:		15.98 %
Epoch 123 of 2000 took 0.105s
  training loss:		2.291238
  validation loss:		2.242013
  validation accuracy:		18.48 %
Epoch 124 of 2000 took 0.105s
  training loss:		2.291359
  validation loss:		2.236115
  validation accuracy:		12.93 %
Epoch 125 of 2000 took 0.106s
  training loss:		2.291909
  validation loss:		2.239746
  validation accuracy:		15.11 %
Epoch 126 of 2000 took 0.105s
  training loss:		2.292274
  validation loss:		2.240739
  validation accuracy:		13.37 %
Epoch 127 of 2000 took 0.105s
  training loss:		2.291484
  validation loss:		2.237329
  validation accuracy:		16.41 %
Epoch 128 of 2000 took 0.105s
  training loss:		2.291450
  validation loss:		2.233661
  validation accuracy:		12.93 %
Epoch 129 of 2000 took 0.105s
  training loss:		2.293412
  validation loss:		2.242203
  validation accuracy:		19.89 %
Epoch 130 of 2000 took 0.105s
  training loss:		2.291871
  validation loss:		2.240700
  validation accuracy:		15.76 %
Epoch 131 of 2000 took 0.105s
  training loss:		2.291860
  validation loss:		2.242078
  validation accuracy:		19.35 %
Epoch 132 of 2000 took 0.105s
  training loss:		2.293365
  validation loss:		2.237804
  validation accuracy:		16.52 %
Epoch 133 of 2000 took 0.109s
  training loss:		2.292467
  validation loss:		2.242735
  validation accuracy:		19.35 %
Epoch 134 of 2000 took 0.105s
  training loss:		2.292137
  validation loss:		2.239293
  validation accuracy:		17.17 %
Epoch 135 of 2000 took 0.105s
  training loss:		2.293214
  validation loss:		2.238661
  validation accuracy:		12.93 %
Epoch 136 of 2000 took 0.105s
  training loss:		2.292339
  validation loss:		2.234412
  validation accuracy:		13.70 %
Epoch 137 of 2000 took 0.105s
  training loss:		2.290831
  validation loss:		2.238501
  validation accuracy:		18.04 %
Epoch 138 of 2000 took 0.105s
  training loss:		2.292084
  validation loss:		2.241795
  validation accuracy:		17.07 %
Epoch 139 of 2000 took 0.105s
  training loss:		2.291538
  validation loss:		2.236355
  validation accuracy:		12.93 %
Epoch 140 of 2000 took 0.105s
  training loss:		2.291332
  validation loss:		2.235394
  validation accuracy:		16.52 %
Epoch 141 of 2000 took 0.105s
  training loss:		2.290605
  validation loss:		2.238184
  validation accuracy:		14.13 %
Epoch 142 of 2000 took 0.105s
  training loss:		2.290839
  validation loss:		2.240442
  validation accuracy:		19.57 %
Epoch 143 of 2000 took 0.105s
  training loss:		2.291325
  validation loss:		2.232870
  validation accuracy:		13.59 %
Epoch 144 of 2000 took 0.105s
  training loss:		2.291275
  validation loss:		2.239396
  validation accuracy:		16.30 %
Epoch 145 of 2000 took 0.105s
  training loss:		2.290924
  validation loss:		2.235390
  validation accuracy:		19.78 %
Epoch 146 of 2000 took 0.110s
  training loss:		2.290684
  validation loss:		2.239993
  validation accuracy:		13.15 %
Epoch 147 of 2000 took 0.105s
  training loss:		2.291193
  validation loss:		2.236707
  validation accuracy:		19.89 %
Epoch 148 of 2000 took 0.105s
  training loss:		2.290592
  validation loss:		2.236068
  validation accuracy:		13.80 %
Epoch 149 of 2000 took 0.105s
  training loss:		2.290307
  validation loss:		2.238583
  validation accuracy:		12.93 %
Epoch 150 of 2000 took 0.105s
  training loss:		2.291728
  validation loss:		2.245779
  validation accuracy:		12.93 %
Epoch 151 of 2000 took 0.105s
  training loss:		2.291090
  validation loss:		2.238835
  validation accuracy:		18.91 %
Epoch 152 of 2000 took 0.105s
  training loss:		2.291245
  validation loss:		2.233819
  validation accuracy:		18.04 %
Epoch 153 of 2000 took 0.105s
  training loss:		2.289340
  validation loss:		2.233203
  validation accuracy:		14.24 %
Epoch 154 of 2000 took 0.106s
  training loss:		2.290797
  validation loss:		2.239970
  validation accuracy:		24.24 %
Epoch 155 of 2000 took 0.108s
  training loss:		2.290570
  validation loss:		2.240696
  validation accuracy:		14.78 %
Epoch 156 of 2000 took 0.106s
  training loss:		2.290807
  validation loss:		2.233176
  validation accuracy:		13.80 %
Epoch 157 of 2000 took 0.105s
  training loss:		2.291083
  validation loss:		2.243151
  validation accuracy:		28.26 %
Epoch 158 of 2000 took 0.105s
  training loss:		2.290352
  validation loss:		2.232350
  validation accuracy:		19.78 %
Epoch 159 of 2000 took 0.105s
  training loss:		2.290530
  validation loss:		2.237439
  validation accuracy:		14.24 %
Epoch 160 of 2000 took 0.105s
  training loss:		2.289968
  validation loss:		2.241568
  validation accuracy:		13.04 %
Epoch 161 of 2000 took 0.105s
  training loss:		2.290409
  validation loss:		2.239878
  validation accuracy:		19.89 %
Epoch 162 of 2000 took 0.105s
  training loss:		2.290119
  validation loss:		2.236618
  validation accuracy:		13.70 %
Epoch 163 of 2000 took 0.105s
  training loss:		2.289677
  validation loss:		2.233909
  validation accuracy:		13.04 %
Epoch 164 of 2000 took 0.105s
  training loss:		2.289210
  validation loss:		2.241719
  validation accuracy:		15.11 %
Epoch 165 of 2000 took 0.109s
  training loss:		2.289305
  validation loss:		2.234376
  validation accuracy:		19.78 %
Epoch 166 of 2000 took 0.105s
  training loss:		2.289649
  validation loss:		2.238669
  validation accuracy:		14.24 %
Epoch 167 of 2000 took 0.105s
  training loss:		2.290209
  validation loss:		2.235201
  validation accuracy:		21.85 %
Epoch 168 of 2000 took 0.105s
  training loss:		2.290015
  validation loss:		2.237557
  validation accuracy:		14.67 %
Epoch 169 of 2000 took 0.105s
  training loss:		2.288936
  validation loss:		2.240555
  validation accuracy:		21.52 %
Epoch 170 of 2000 took 0.105s
  training loss:		2.288143
  validation loss:		2.235460
  validation accuracy:		21.85 %
Epoch 171 of 2000 took 0.105s
  training loss:		2.289924
  validation loss:		2.234096
  validation accuracy:		13.80 %
Epoch 172 of 2000 took 0.105s
  training loss:		2.289817
  validation loss:		2.236200
  validation accuracy:		13.04 %
Epoch 173 of 2000 took 0.105s
  training loss:		2.289219
  validation loss:		2.235044
  validation accuracy:		14.89 %
Epoch 174 of 2000 took 0.109s
  training loss:		2.289487
  validation loss:		2.236196
  validation accuracy:		19.46 %
Epoch 175 of 2000 took 0.106s
  training loss:		2.288516
  validation loss:		2.237924
  validation accuracy:		17.17 %
Epoch 176 of 2000 took 0.105s
  training loss:		2.288917
  validation loss:		2.239132
  validation accuracy:		16.30 %
Epoch 177 of 2000 took 0.105s
  training loss:		2.288209
  validation loss:		2.231196
  validation accuracy:		22.72 %
Epoch 178 of 2000 took 0.105s
  training loss:		2.287933
  validation loss:		2.235039
  validation accuracy:		15.54 %
Epoch 179 of 2000 took 0.105s
  training loss:		2.288494
  validation loss:		2.239879
  validation accuracy:		17.28 %
Epoch 180 of 2000 took 0.105s
  training loss:		2.287133
  validation loss:		2.235808
  validation accuracy:		20.11 %
Epoch 181 of 2000 took 0.105s
  training loss:		2.287461
  validation loss:		2.229612
  validation accuracy:		14.46 %
Epoch 182 of 2000 took 0.105s
  training loss:		2.288547
  validation loss:		2.237788
  validation accuracy:		14.67 %
Epoch 183 of 2000 took 0.106s
  training loss:		2.287056
  validation loss:		2.233837
  validation accuracy:		16.85 %
Epoch 184 of 2000 took 0.109s
  training loss:		2.286998
  validation loss:		2.231180
  validation accuracy:		16.09 %
Epoch 185 of 2000 took 0.105s
  training loss:		2.286667
  validation loss:		2.233512
  validation accuracy:		19.24 %
Epoch 186 of 2000 took 0.105s
  training loss:		2.288705
  validation loss:		2.238069
  validation accuracy:		14.24 %
Epoch 187 of 2000 took 0.105s
  training loss:		2.287663
  validation loss:		2.236803
  validation accuracy:		20.22 %
Epoch 188 of 2000 took 0.105s
  training loss:		2.287427
  validation loss:		2.236822
  validation accuracy:		16.30 %
Epoch 189 of 2000 took 0.105s
  training loss:		2.286552
  validation loss:		2.234146
  validation accuracy:		15.11 %
Epoch 190 of 2000 took 0.105s
  training loss:		2.287158
  validation loss:		2.234265
  validation accuracy:		16.85 %
Epoch 191 of 2000 took 0.105s
  training loss:		2.286326
  validation loss:		2.235624
  validation accuracy:		21.52 %
Epoch 192 of 2000 took 0.105s
  training loss:		2.285599
  validation loss:		2.233489
  validation accuracy:		23.04 %
Epoch 193 of 2000 took 0.109s
  training loss:		2.285801
  validation loss:		2.230356
  validation accuracy:		15.98 %
Epoch 194 of 2000 took 0.106s
  training loss:		2.285499
  validation loss:		2.232882
  validation accuracy:		19.13 %
Epoch 195 of 2000 took 0.106s
  training loss:		2.285625
  validation loss:		2.232241
  validation accuracy:		16.09 %
Epoch 196 of 2000 took 0.105s
  training loss:		2.285839
  validation loss:		2.235387
  validation accuracy:		24.57 %
Epoch 197 of 2000 took 0.105s
  training loss:		2.283433
  validation loss:		2.226442
  validation accuracy:		19.57 %
Epoch 198 of 2000 took 0.105s
  training loss:		2.284310
  validation loss:		2.228007
  validation accuracy:		17.93 %
Epoch 199 of 2000 took 0.105s
  training loss:		2.284944
  validation loss:		2.231045
  validation accuracy:		20.87 %
Epoch 200 of 2000 took 0.106s
  training loss:		2.283212
  validation loss:		2.228559
  validation accuracy:		23.91 %
Epoch 201 of 2000 took 0.105s
  training loss:		2.283781
  validation loss:		2.228085
  validation accuracy:		24.89 %
Epoch 202 of 2000 took 0.105s
  training loss:		2.283737
  validation loss:		2.229516
  validation accuracy:		22.61 %
Epoch 203 of 2000 took 0.109s
  training loss:		2.283044
  validation loss:		2.227624
  validation accuracy:		16.74 %
Epoch 204 of 2000 took 0.105s
  training loss:		2.283560
  validation loss:		2.230642
  validation accuracy:		21.52 %
Epoch 205 of 2000 took 0.105s
  training loss:		2.284022
  validation loss:		2.234642
  validation accuracy:		21.20 %
Epoch 206 of 2000 took 0.105s
  training loss:		2.282099
  validation loss:		2.227682
  validation accuracy:		18.04 %
Epoch 207 of 2000 took 0.105s
  training loss:		2.282376
  validation loss:		2.225767
  validation accuracy:		17.50 %
Epoch 208 of 2000 took 0.105s
  training loss:		2.281115
  validation loss:		2.225586
  validation accuracy:		18.15 %
Epoch 209 of 2000 took 0.105s
  training loss:		2.281379
  validation loss:		2.228814
  validation accuracy:		16.74 %
Epoch 210 of 2000 took 0.105s
  training loss:		2.280466
  validation loss:		2.228530
  validation accuracy:		20.22 %
Epoch 211 of 2000 took 0.106s
  training loss:		2.280831
  validation loss:		2.225056
  validation accuracy:		18.91 %
Epoch 212 of 2000 took 0.109s
  training loss:		2.279219
  validation loss:		2.223911
  validation accuracy:		17.72 %
Epoch 213 of 2000 took 0.106s
  training loss:		2.280326
  validation loss:		2.229199
  validation accuracy:		17.72 %
Epoch 214 of 2000 took 0.105s
  training loss:		2.280160
  validation loss:		2.225268
  validation accuracy:		22.83 %
Epoch 215 of 2000 took 0.105s
  training loss:		2.279376
  validation loss:		2.221932
  validation accuracy:		25.43 %
Epoch 216 of 2000 took 0.105s
  training loss:		2.278895
  validation loss:		2.219882
  validation accuracy:		20.33 %
Epoch 217 of 2000 took 0.105s
  training loss:		2.278563
  validation loss:		2.229211
  validation accuracy:		21.63 %
Epoch 218 of 2000 took 0.105s
  training loss:		2.277949
  validation loss:		2.224989
  validation accuracy:		18.80 %
Epoch 219 of 2000 took 0.105s
  training loss:		2.276874
  validation loss:		2.223010
  validation accuracy:		21.09 %
Epoch 220 of 2000 took 0.105s
  training loss:		2.275865
  validation loss:		2.223076
  validation accuracy:		17.17 %
Epoch 221 of 2000 took 0.105s
  training loss:		2.275180
  validation loss:		2.220100
  validation accuracy:		25.11 %
Epoch 222 of 2000 took 0.109s
  training loss:		2.274326
  validation loss:		2.224344
  validation accuracy:		21.20 %
Epoch 223 of 2000 took 0.105s
  training loss:		2.274822
  validation loss:		2.219755
  validation accuracy:		18.91 %
Epoch 224 of 2000 took 0.105s
  training loss:		2.273004
  validation loss:		2.217406
  validation accuracy:		22.39 %
Epoch 225 of 2000 took 0.105s
  training loss:		2.270887
  validation loss:		2.215213
  validation accuracy:		21.63 %
Epoch 226 of 2000 took 0.105s
  training loss:		2.271754
  validation loss:		2.216867
  validation accuracy:		23.15 %
Epoch 227 of 2000 took 0.105s
  training loss:		2.269914
  validation loss:		2.217882
  validation accuracy:		21.85 %
Epoch 228 of 2000 took 0.105s
  training loss:		2.269113
  validation loss:		2.216334
  validation accuracy:		20.65 %
Epoch 229 of 2000 took 0.105s
  training loss:		2.267894
  validation loss:		2.212397
  validation accuracy:		22.07 %
Epoch 230 of 2000 took 0.105s
  training loss:		2.267939
  validation loss:		2.214383
  validation accuracy:		22.61 %
Epoch 231 of 2000 took 0.109s
  training loss:		2.265776
  validation loss:		2.209949
  validation accuracy:		22.72 %
Epoch 232 of 2000 took 0.105s
  training loss:		2.265542
  validation loss:		2.211053
  validation accuracy:		21.41 %
Epoch 233 of 2000 took 0.105s
  training loss:		2.263976
  validation loss:		2.212956
  validation accuracy:		21.41 %
Epoch 234 of 2000 took 0.105s
  training loss:		2.262448
  validation loss:		2.208351
  validation accuracy:		22.28 %
Epoch 235 of 2000 took 0.106s
  training loss:		2.260437
  validation loss:		2.204842
  validation accuracy:		20.43 %
Epoch 236 of 2000 took 0.105s
  training loss:		2.260332
  validation loss:		2.206178
  validation accuracy:		21.30 %
Epoch 237 of 2000 took 0.105s
  training loss:		2.257769
  validation loss:		2.203535
  validation accuracy:		24.89 %
Epoch 238 of 2000 took 0.105s
  training loss:		2.255164
  validation loss:		2.199532
  validation accuracy:		24.78 %
Epoch 239 of 2000 took 0.105s
  training loss:		2.253769
  validation loss:		2.199286
  validation accuracy:		23.26 %
Epoch 240 of 2000 took 0.106s
  training loss:		2.251120
  validation loss:		2.199453
  validation accuracy:		22.07 %
Epoch 241 of 2000 took 0.110s
  training loss:		2.248765
  validation loss:		2.189075
  validation accuracy:		22.83 %
Epoch 242 of 2000 took 0.105s
  training loss:		2.244228
  validation loss:		2.192335
  validation accuracy:		21.63 %
Epoch 243 of 2000 took 0.105s
  training loss:		2.241920
  validation loss:		2.184287
  validation accuracy:		22.83 %
Epoch 244 of 2000 took 0.105s
  training loss:		2.240341
  validation loss:		2.187701
  validation accuracy:		25.22 %
Epoch 245 of 2000 took 0.105s
  training loss:		2.236195
  validation loss:		2.181039
  validation accuracy:		25.65 %
Epoch 246 of 2000 took 0.105s
  training loss:		2.231736
  validation loss:		2.177896
  validation accuracy:		25.98 %
Epoch 247 of 2000 took 0.105s
  training loss:		2.228744
  validation loss:		2.175494
  validation accuracy:		25.54 %
Epoch 248 of 2000 took 0.105s
  training loss:		2.221038
  validation loss:		2.168188
  validation accuracy:		26.41 %
Epoch 249 of 2000 took 0.105s
  training loss:		2.216554
  validation loss:		2.156093
  validation accuracy:		23.26 %
Epoch 250 of 2000 took 0.110s
  training loss:		2.210270
  validation loss:		2.153609
  validation accuracy:		23.15 %
Epoch 251 of 2000 took 0.106s
  training loss:		2.205318
  validation loss:		2.148065
  validation accuracy:		23.80 %
Epoch 252 of 2000 took 0.105s
  training loss:		2.197807
  validation loss:		2.146176
  validation accuracy:		27.93 %
Epoch 253 of 2000 took 0.105s
  training loss:		2.188523
  validation loss:		2.132887
  validation accuracy:		25.11 %
Epoch 254 of 2000 took 0.105s
  training loss:		2.179315
  validation loss:		2.112268
  validation accuracy:		24.46 %
Epoch 255 of 2000 took 0.105s
  training loss:		2.171486
  validation loss:		2.107490
  validation accuracy:		26.74 %
Epoch 256 of 2000 took 0.105s
  training loss:		2.157819
  validation loss:		2.104287
  validation accuracy:		25.65 %
Epoch 257 of 2000 took 0.105s
  training loss:		2.147192
  validation loss:		2.081443
  validation accuracy:		25.87 %
Epoch 258 of 2000 took 0.105s
  training loss:		2.131210
  validation loss:		2.063717
  validation accuracy:		26.09 %
Epoch 259 of 2000 took 0.105s
  training loss:		2.117046
  validation loss:		2.048414
  validation accuracy:		28.15 %
Epoch 260 of 2000 took 0.109s
  training loss:		2.095950
  validation loss:		2.025193
  validation accuracy:		27.28 %
Epoch 261 of 2000 took 0.105s
  training loss:		2.079594
  validation loss:		2.012886
  validation accuracy:		29.02 %
Epoch 262 of 2000 took 0.105s
  training loss:		2.060414
  validation loss:		1.983026
  validation accuracy:		29.35 %
Epoch 263 of 2000 took 0.105s
  training loss:		2.038300
  validation loss:		1.974685
  validation accuracy:		28.04 %
Epoch 264 of 2000 took 0.105s
  training loss:		2.010829
  validation loss:		1.937296
  validation accuracy:		28.70 %
Epoch 265 of 2000 took 0.105s
  training loss:		1.990988
  validation loss:		1.914774
  validation accuracy:		31.09 %
Epoch 266 of 2000 took 0.105s
  training loss:		1.961577
  validation loss:		1.884748
  validation accuracy:		32.28 %
Epoch 267 of 2000 took 0.106s
  training loss:		1.938220
  validation loss:		1.865586
  validation accuracy:		33.26 %
Epoch 268 of 2000 took 0.106s
  training loss:		1.917130
  validation loss:		1.850446
  validation accuracy:		33.70 %
Epoch 269 of 2000 took 0.109s
  training loss:		1.889774
  validation loss:		1.811936
  validation accuracy:		33.70 %
Epoch 270 of 2000 took 0.105s
  training loss:		1.870055
  validation loss:		1.795912
  validation accuracy:		33.37 %
Epoch 271 of 2000 took 0.105s
  training loss:		1.842709
  validation loss:		1.759385
  validation accuracy:		35.11 %
Epoch 272 of 2000 took 0.105s
  training loss:		1.820256
  validation loss:		1.746159
  validation accuracy:		35.65 %
Epoch 273 of 2000 took 0.105s
  training loss:		1.793857
  validation loss:		1.733069
  validation accuracy:		35.65 %
Epoch 274 of 2000 took 0.105s
  training loss:		1.768320
  validation loss:		1.702522
  validation accuracy:		36.20 %
Epoch 275 of 2000 took 0.105s
  training loss:		1.749200
  validation loss:		1.690912
  validation accuracy:		37.28 %
Epoch 276 of 2000 took 0.106s
  training loss:		1.729084
  validation loss:		1.657927
  validation accuracy:		37.83 %
Epoch 277 of 2000 took 0.105s
  training loss:		1.719621
  validation loss:		1.637833
  validation accuracy:		38.91 %
Epoch 278 of 2000 took 0.105s
  training loss:		1.697253
  validation loss:		1.626459
  validation accuracy:		40.11 %
Epoch 279 of 2000 took 0.109s
  training loss:		1.670142
  validation loss:		1.600358
  validation accuracy:		40.00 %
Epoch 280 of 2000 took 0.105s
  training loss:		1.659440
  validation loss:		1.587994
  validation accuracy:		38.70 %
Epoch 281 of 2000 took 0.105s
  training loss:		1.642612
  validation loss:		1.568238
  validation accuracy:		41.09 %
Epoch 282 of 2000 took 0.105s
  training loss:		1.631199
  validation loss:		1.565440
  validation accuracy:		44.24 %
Epoch 283 of 2000 took 0.105s
  training loss:		1.618874
  validation loss:		1.554019
  validation accuracy:		41.85 %
Epoch 284 of 2000 took 0.105s
  training loss:		1.597962
  validation loss:		1.545487
  validation accuracy:		41.20 %
Epoch 285 of 2000 took 0.105s
  training loss:		1.584087
  validation loss:		1.520423
  validation accuracy:		42.72 %
Epoch 286 of 2000 took 0.105s
  training loss:		1.577329
  validation loss:		1.507452
  validation accuracy:		44.02 %
Epoch 287 of 2000 took 0.105s
  training loss:		1.575760
  validation loss:		1.501309
  validation accuracy:		45.00 %
Epoch 288 of 2000 took 0.109s
  training loss:		1.550492
  validation loss:		1.510459
  validation accuracy:		44.46 %
Epoch 289 of 2000 took 0.105s
  training loss:		1.545636
  validation loss:		1.474770
  validation accuracy:		44.57 %
Epoch 290 of 2000 took 0.105s
  training loss:		1.521363
  validation loss:		1.472345
  validation accuracy:		45.76 %
Epoch 291 of 2000 took 0.105s
  training loss:		1.547998
  validation loss:		1.592277
  validation accuracy:		40.22 %
Epoch 292 of 2000 took 0.105s
  training loss:		1.530344
  validation loss:		1.452845
  validation accuracy:		46.96 %
Epoch 293 of 2000 took 0.105s
  training loss:		1.511664
  validation loss:		1.443416
  validation accuracy:		47.61 %
Epoch 294 of 2000 took 0.105s
  training loss:		1.502328
  validation loss:		1.435449
  validation accuracy:		46.63 %
Epoch 295 of 2000 took 0.105s
  training loss:		1.542719
  validation loss:		1.572899
  validation accuracy:		41.52 %
Epoch 296 of 2000 took 0.105s
  training loss:		1.554227
  validation loss:		1.493625
  validation accuracy:		43.37 %
Epoch 297 of 2000 took 0.107s
  training loss:		1.493396
  validation loss:		1.513752
  validation accuracy:		43.91 %
Epoch 298 of 2000 took 0.108s
  training loss:		1.633146
  validation loss:		1.676799
  validation accuracy:		36.85 %
Epoch 299 of 2000 took 0.105s
  training loss:		1.487011
  validation loss:		1.421391
  validation accuracy:		48.37 %
Epoch 300 of 2000 took 0.105s
  training loss:		1.471679
  validation loss:		1.460996
  validation accuracy:		45.54 %
Epoch 301 of 2000 took 0.105s
  training loss:		1.466702
  validation loss:		1.455654
  validation accuracy:		46.74 %
Epoch 302 of 2000 took 0.105s
  training loss:		1.485478
  validation loss:		1.417287
  validation accuracy:		48.70 %
Epoch 303 of 2000 took 0.105s
  training loss:		1.454269
  validation loss:		1.401859
  validation accuracy:		49.46 %
Epoch 304 of 2000 took 0.105s
  training loss:		1.574478
  validation loss:		1.488810
  validation accuracy:		46.85 %
Epoch 305 of 2000 took 0.105s
  training loss:		1.545887
  validation loss:		1.472389
  validation accuracy:		45.11 %
Epoch 306 of 2000 took 0.105s
  training loss:		1.594388
  validation loss:		1.455952
  validation accuracy:		48.48 %
Epoch 307 of 2000 took 0.109s
  training loss:		1.454286
  validation loss:		1.446252
  validation accuracy:		47.50 %
Epoch 308 of 2000 took 0.105s
  training loss:		1.569496
  validation loss:		1.535664
  validation accuracy:		39.46 %
Epoch 309 of 2000 took 0.105s
  training loss:		1.504573
  validation loss:		1.463794
  validation accuracy:		47.28 %
Epoch 310 of 2000 took 0.105s
  training loss:		1.448996
  validation loss:		1.391777
  validation accuracy:		51.09 %
Epoch 311 of 2000 took 0.105s
  training loss:		1.436732
  validation loss:		1.380281
  validation accuracy:		51.30 %
Epoch 312 of 2000 took 0.105s
  training loss:		1.433077
  validation loss:		1.369385
  validation accuracy:		49.67 %
Epoch 313 of 2000 took 0.105s
  training loss:		1.455761
  validation loss:		1.387927
  validation accuracy:		49.46 %
Epoch 314 of 2000 took 0.105s
  training loss:		1.543934
  validation loss:		1.737530
  validation accuracy:		35.87 %
Epoch 315 of 2000 took 0.105s
  training loss:		1.591542
  validation loss:		1.390084
  validation accuracy:		49.89 %
Epoch 316 of 2000 took 0.107s
  training loss:		1.426265
  validation loss:		1.375720
  validation accuracy:		50.22 %
Epoch 317 of 2000 took 0.107s
  training loss:		1.458535
  validation loss:		1.422938
  validation accuracy:		47.28 %
Epoch 318 of 2000 took 0.105s
  training loss:		1.504793
  validation loss:		1.387475
  validation accuracy:		51.09 %
Epoch 319 of 2000 took 0.105s
  training loss:		1.425798
  validation loss:		1.412620
  validation accuracy:		49.46 %
Epoch 320 of 2000 took 0.105s
  training loss:		1.416566
  validation loss:		1.363452
  validation accuracy:		51.85 %
Epoch 321 of 2000 took 0.105s
  training loss:		1.431344
  validation loss:		1.482140
  validation accuracy:		44.78 %
Epoch 322 of 2000 took 0.105s
  training loss:		1.512229
  validation loss:		1.394776
  validation accuracy:		48.48 %
Epoch 323 of 2000 took 0.105s
  training loss:		1.499151
  validation loss:		1.481302
  validation accuracy:		46.41 %
Epoch 324 of 2000 took 0.105s
  training loss:		1.421024
  validation loss:		1.369376
  validation accuracy:		50.87 %
Epoch 325 of 2000 took 0.106s
  training loss:		1.410951
  validation loss:		1.351034
  validation accuracy:		52.50 %
Epoch 326 of 2000 took 0.110s
  training loss:		1.412070
  validation loss:		1.354756
  validation accuracy:		51.85 %
Epoch 327 of 2000 took 0.105s
  training loss:		1.403112
  validation loss:		1.375716
  validation accuracy:		51.09 %
Epoch 328 of 2000 took 0.105s
  training loss:		1.404798
  validation loss:		1.365389
  validation accuracy:		51.41 %
Epoch 329 of 2000 took 0.105s
  training loss:		1.403006
  validation loss:		1.477996
  validation accuracy:		45.98 %
Epoch 330 of 2000 took 0.105s
  training loss:		1.668473
  validation loss:		1.512348
  validation accuracy:		42.61 %
Epoch 331 of 2000 took 0.105s
  training loss:		1.438808
  validation loss:		1.392929
  validation accuracy:		52.07 %
Epoch 332 of 2000 took 0.105s
  training loss:		1.460647
  validation loss:		1.408153
  validation accuracy:		48.04 %
Epoch 333 of 2000 took 0.105s
  training loss:		1.448309
  validation loss:		1.368681
  validation accuracy:		51.96 %
Epoch 334 of 2000 took 0.105s
  training loss:		1.419063
  validation loss:		1.351946
  validation accuracy:		53.48 %
Epoch 335 of 2000 took 0.108s
  training loss:		1.389887
  validation loss:		1.392938
  validation accuracy:		49.57 %
Epoch 336 of 2000 took 0.106s
  training loss:		1.405506
  validation loss:		1.372646
  validation accuracy:		51.30 %
Epoch 337 of 2000 took 0.105s
  training loss:		1.481988
  validation loss:		1.389359
  validation accuracy:		50.76 %
Epoch 338 of 2000 took 0.105s
  training loss:		1.456047
  validation loss:		1.367391
  validation accuracy:		53.37 %
Epoch 339 of 2000 took 0.105s
  training loss:		1.397842
  validation loss:		1.347967
  validation accuracy:		53.37 %
Epoch 340 of 2000 took 0.105s
  training loss:		1.398603
  validation loss:		1.345947
  validation accuracy:		51.85 %
Epoch 341 of 2000 took 0.105s
  training loss:		1.412397
  validation loss:		1.369737
  validation accuracy:		50.87 %
Epoch 342 of 2000 took 0.105s
  training loss:		1.387451
  validation loss:		1.342695
  validation accuracy:		54.57 %
Epoch 343 of 2000 took 0.105s
  training loss:		1.398292
  validation loss:		1.337199
  validation accuracy:		53.26 %
Epoch 344 of 2000 took 0.105s
  training loss:		1.376820
  validation loss:		1.338727
  validation accuracy:		54.13 %
Epoch 345 of 2000 took 0.110s
  training loss:		1.384657
  validation loss:		1.335801
  validation accuracy:		54.13 %
Epoch 346 of 2000 took 0.105s
  training loss:		1.406076
  validation loss:		1.348468
  validation accuracy:		53.48 %
Epoch 347 of 2000 took 0.104s
  training loss:		1.415269
  validation loss:		1.398052
  validation accuracy:		49.89 %
Epoch 348 of 2000 took 0.102s
  training loss:		1.458452
  validation loss:		1.720371
  validation accuracy:		37.83 %
Epoch 349 of 2000 took 0.104s
  training loss:		1.561982
  validation loss:		1.362873
  validation accuracy:		53.70 %
Epoch 350 of 2000 took 0.110s
  training loss:		1.396983
  validation loss:		1.347385
  validation accuracy:		53.48 %
Epoch 351 of 2000 took 0.103s
  training loss:		1.391060
  validation loss:		1.338283
  validation accuracy:		53.26 %
Epoch 352 of 2000 took 0.103s
  training loss:		1.376558
  validation loss:		1.356547
  validation accuracy:		51.85 %
Epoch 353 of 2000 took 0.106s
  training loss:		1.382089
  validation loss:		1.335520
  validation accuracy:		53.70 %
Epoch 354 of 2000 took 0.112s
  training loss:		1.388536
  validation loss:		1.458612
  validation accuracy:		46.20 %
Epoch 355 of 2000 took 0.109s
  training loss:		1.446112
  validation loss:		1.392204
  validation accuracy:		49.35 %
Epoch 356 of 2000 took 0.108s
  training loss:		1.447443
  validation loss:		1.564783
  validation accuracy:		42.93 %
Epoch 357 of 2000 took 0.108s
  training loss:		1.412103
  validation loss:		1.340059
  validation accuracy:		54.67 %
Epoch 358 of 2000 took 0.108s
  training loss:		1.385034
  validation loss:		1.343356
  validation accuracy:		52.61 %
Epoch 359 of 2000 took 0.109s
  training loss:		1.372821
  validation loss:		1.335324
  validation accuracy:		53.37 %
Epoch 360 of 2000 took 0.115s
  training loss:		1.369074
  validation loss:		1.331667
  validation accuracy:		53.70 %
Epoch 361 of 2000 took 0.104s
  training loss:		1.366319
  validation loss:		1.342859
  validation accuracy:		52.83 %
Epoch 362 of 2000 took 0.108s
  training loss:		1.361457
  validation loss:		1.369803
  validation accuracy:		51.52 %
Epoch 363 of 2000 took 0.105s
  training loss:		1.355690
  validation loss:		1.347051
  validation accuracy:		53.15 %
Epoch 364 of 2000 took 0.112s
  training loss:		1.472028
  validation loss:		1.411681
  validation accuracy:		48.48 %
Epoch 365 of 2000 took 0.112s
  training loss:		1.387233
  validation loss:		1.346994
  validation accuracy:		52.39 %
Epoch 366 of 2000 took 0.104s
  training loss:		1.365018
  validation loss:		1.331545
  validation accuracy:		52.83 %
Epoch 367 of 2000 took 0.108s
  training loss:		1.366723
  validation loss:		1.353936
  validation accuracy:		52.39 %
Epoch 368 of 2000 took 0.114s
  training loss:		1.366009
  validation loss:		1.338602
  validation accuracy:		52.72 %
Epoch 369 of 2000 took 0.104s
  training loss:		1.353970
  validation loss:		1.328258
  validation accuracy:		54.13 %
Epoch 370 of 2000 took 0.108s
  training loss:		1.351079
  validation loss:		1.327502
  validation accuracy:		54.13 %
Epoch 371 of 2000 took 0.104s
  training loss:		1.422985
  validation loss:		1.367452
  validation accuracy:		53.04 %
Epoch 372 of 2000 took 0.111s
  training loss:		1.366819
  validation loss:		1.372168
  validation accuracy:		52.61 %
Epoch 373 of 2000 took 0.114s
  training loss:		1.404992
  validation loss:		1.330622
  validation accuracy:		54.89 %
Epoch 374 of 2000 took 0.104s
  training loss:		1.403668
  validation loss:		1.429970
  validation accuracy:		48.91 %
Epoch 375 of 2000 took 0.110s
  training loss:		1.355797
  validation loss:		1.335198
  validation accuracy:		54.02 %
Epoch 376 of 2000 took 0.111s
  training loss:		1.355129
  validation loss:		1.324163
  validation accuracy:		53.59 %
Epoch 377 of 2000 took 0.104s
  training loss:		1.360674
  validation loss:		1.324914
  validation accuracy:		54.67 %
Epoch 378 of 2000 took 0.107s
  training loss:		1.416038
  validation loss:		1.490403
  validation accuracy:		44.57 %
Epoch 379 of 2000 took 0.104s
  training loss:		1.416456
  validation loss:		1.366259
  validation accuracy:		53.59 %
Epoch 380 of 2000 took 0.113s
  training loss:		1.342448
  validation loss:		1.326208
  validation accuracy:		53.26 %
Epoch 381 of 2000 took 0.108s
  training loss:		1.351188
  validation loss:		1.333912
  validation accuracy:		52.17 %
Epoch 382 of 2000 took 0.109s
  training loss:		1.348896
  validation loss:		1.333381
  validation accuracy:		53.80 %
Epoch 383 of 2000 took 0.112s
  training loss:		1.348465
  validation loss:		1.326925
  validation accuracy:		53.59 %
Epoch 384 of 2000 took 0.110s
  training loss:		1.370019
  validation loss:		1.352382
  validation accuracy:		52.83 %
Epoch 385 of 2000 took 0.106s
  training loss:		1.349956
  validation loss:		1.330132
  validation accuracy:		54.02 %
Epoch 386 of 2000 took 0.106s
  training loss:		1.363792
  validation loss:		1.384963
  validation accuracy:		51.30 %
Epoch 387 of 2000 took 0.105s
  training loss:		1.395249
  validation loss:		1.360909
  validation accuracy:		51.85 %
Epoch 388 of 2000 took 0.114s
  training loss:		1.336519
  validation loss:		1.322659
  validation accuracy:		53.04 %
Epoch 389 of 2000 took 0.105s
  training loss:		1.344147
  validation loss:		1.333595
  validation accuracy:		54.46 %
Epoch 390 of 2000 took 0.101s
  training loss:		1.341888
  validation loss:		1.381123
  validation accuracy:		51.74 %
Epoch 391 of 2000 took 0.114s
  training loss:		1.392156
  validation loss:		1.394429
  validation accuracy:		50.76 %
Epoch 392 of 2000 took 0.105s
  training loss:		1.376257
  validation loss:		1.451318
  validation accuracy:		46.85 %
Epoch 393 of 2000 took 0.103s
  training loss:		1.386227
  validation loss:		1.334507
  validation accuracy:		52.72 %
Epoch 394 of 2000 took 0.105s
  training loss:		1.345782
  validation loss:		1.330510
  validation accuracy:		52.50 %
Epoch 395 of 2000 took 0.105s
  training loss:		1.327127
  validation loss:		1.315778
  validation accuracy:		53.04 %
Epoch 396 of 2000 took 0.105s
  training loss:		1.336314
  validation loss:		1.316952
  validation accuracy:		52.93 %
Epoch 397 of 2000 took 0.105s
  training loss:		1.332008
  validation loss:		1.344959
  validation accuracy:		51.85 %
Epoch 398 of 2000 took 0.105s
  training loss:		1.372376
  validation loss:		1.341350
  validation accuracy:		52.72 %
Epoch 399 of 2000 took 0.105s
  training loss:		1.341285
  validation loss:		1.338449
  validation accuracy:		53.04 %
Epoch 400 of 2000 took 0.105s
  training loss:		1.325741
  validation loss:		1.358974
  validation accuracy:		52.93 %
Epoch 401 of 2000 took 0.109s
  training loss:		1.337226
  validation loss:		1.319089
  validation accuracy:		51.74 %
Epoch 402 of 2000 took 0.105s
  training loss:		1.326820
  validation loss:		1.329859
  validation accuracy:		52.61 %
Epoch 403 of 2000 took 0.105s
  training loss:		1.322410
  validation loss:		1.331904
  validation accuracy:		52.39 %
Epoch 404 of 2000 took 0.105s
  training loss:		1.372928
  validation loss:		1.343962
  validation accuracy:		51.09 %
Epoch 405 of 2000 took 0.105s
  training loss:		1.393567
  validation loss:		1.388370
  validation accuracy:		50.87 %
Epoch 406 of 2000 took 0.105s
  training loss:		1.331193
  validation loss:		1.329386
  validation accuracy:		52.39 %
Epoch 407 of 2000 took 0.105s
  training loss:		1.323229
  validation loss:		1.311095
  validation accuracy:		52.72 %
Epoch 408 of 2000 took 0.105s
  training loss:		1.320719
  validation loss:		1.360843
  validation accuracy:		52.72 %
Epoch 409 of 2000 took 0.105s
  training loss:		1.354051
  validation loss:		1.319138
  validation accuracy:		51.63 %
Epoch 410 of 2000 took 0.110s
  training loss:		1.343720
  validation loss:		1.314756
  validation accuracy:		51.74 %
Epoch 411 of 2000 took 0.105s
  training loss:		1.313645
  validation loss:		1.316764
  validation accuracy:		53.59 %
Epoch 412 of 2000 took 0.105s
  training loss:		1.311685
  validation loss:		1.305315
  validation accuracy:		53.59 %
Epoch 413 of 2000 took 0.105s
  training loss:		1.326368
  validation loss:		1.311394
  validation accuracy:		52.39 %
Epoch 414 of 2000 took 0.105s
  training loss:		1.317811
  validation loss:		1.313164
  validation accuracy:		52.93 %
Epoch 415 of 2000 took 0.105s
  training loss:		1.332442
  validation loss:		1.305541
  validation accuracy:		53.15 %
Epoch 416 of 2000 took 0.105s
  training loss:		1.302859
  validation loss:		1.303692
  validation accuracy:		52.17 %
Epoch 417 of 2000 took 0.109s
  training loss:		1.320856
  validation loss:		1.326560
  validation accuracy:		53.59 %
Epoch 418 of 2000 took 0.101s
  training loss:		1.318861
  validation loss:		1.298824
  validation accuracy:		52.61 %
Epoch 419 of 2000 took 0.104s
  training loss:		1.321580
  validation loss:		1.298959
  validation accuracy:		51.63 %
Epoch 420 of 2000 took 0.116s
  training loss:		1.343156
  validation loss:		1.345957
  validation accuracy:		50.43 %
Epoch 421 of 2000 took 0.101s
  training loss:		1.335174
  validation loss:		1.353260
  validation accuracy:		52.83 %
Epoch 422 of 2000 took 0.104s
  training loss:		1.371922
  validation loss:		1.308126
  validation accuracy:		53.15 %
Epoch 423 of 2000 took 0.105s
  training loss:		1.321298
  validation loss:		1.313048
  validation accuracy:		52.28 %
Epoch 424 of 2000 took 0.105s
  training loss:		1.314020
  validation loss:		1.330824
  validation accuracy:		52.07 %
Epoch 425 of 2000 took 0.105s
  training loss:		1.326950
  validation loss:		1.291903
  validation accuracy:		53.15 %
Epoch 426 of 2000 took 0.105s
  training loss:		1.325676
  validation loss:		1.292246
  validation accuracy:		53.91 %
Epoch 427 of 2000 took 0.106s
  training loss:		1.306196
  validation loss:		1.300683
  validation accuracy:		52.50 %
Epoch 428 of 2000 took 0.105s
  training loss:		1.297415
  validation loss:		1.276388
  validation accuracy:		54.35 %
Epoch 429 of 2000 took 0.109s
  training loss:		1.300448
  validation loss:		1.275683
  validation accuracy:		53.26 %
Epoch 430 of 2000 took 0.105s
  training loss:		1.291368
  validation loss:		1.278915
  validation accuracy:		53.15 %
Epoch 431 of 2000 took 0.105s
  training loss:		1.296541
  validation loss:		1.274196
  validation accuracy:		53.26 %
Epoch 432 of 2000 took 0.105s
  training loss:		1.300218
  validation loss:		1.270495
  validation accuracy:		53.70 %
Epoch 433 of 2000 took 0.105s
  training loss:		1.285564
  validation loss:		1.263102
  validation accuracy:		55.22 %
Epoch 434 of 2000 took 0.105s
  training loss:		1.283166
  validation loss:		1.285830
  validation accuracy:		54.35 %
Epoch 435 of 2000 took 0.105s
  training loss:		1.280163
  validation loss:		1.279838
  validation accuracy:		53.91 %
Epoch 436 of 2000 took 0.105s
  training loss:		1.281128
  validation loss:		1.265341
  validation accuracy:		54.24 %
Epoch 437 of 2000 took 0.105s
  training loss:		1.276654
  validation loss:		1.251135
  validation accuracy:		55.54 %
Epoch 438 of 2000 took 0.105s
  training loss:		1.263706
  validation loss:		1.253075
  validation accuracy:		54.24 %
Epoch 439 of 2000 took 0.110s
  training loss:		1.268122
  validation loss:		1.243557
  validation accuracy:		56.20 %
Epoch 440 of 2000 took 0.105s
  training loss:		1.266850
  validation loss:		1.259233
  validation accuracy:		54.02 %
Epoch 441 of 2000 took 0.105s
  training loss:		1.259563
  validation loss:		1.241286
  validation accuracy:		56.52 %
Epoch 442 of 2000 took 0.105s
  training loss:		1.254074
  validation loss:		1.268987
  validation accuracy:		53.91 %
Epoch 443 of 2000 took 0.105s
  training loss:		1.245809
  validation loss:		1.243777
  validation accuracy:		55.11 %
Epoch 444 of 2000 took 0.105s
  training loss:		1.239679
  validation loss:		1.210598
  validation accuracy:		57.72 %
Epoch 445 of 2000 took 0.105s
  training loss:		1.240141
  validation loss:		1.202898
  validation accuracy:		56.74 %
Epoch 446 of 2000 took 0.105s
  training loss:		1.226781
  validation loss:		1.199124
  validation accuracy:		58.91 %
Epoch 447 of 2000 took 0.105s
  training loss:		1.225954
  validation loss:		1.197388
  validation accuracy:		58.37 %
Epoch 448 of 2000 took 0.109s
  training loss:		1.223924
  validation loss:		1.185433
  validation accuracy:		58.59 %
Epoch 449 of 2000 took 0.105s
  training loss:		1.228737
  validation loss:		1.174941
  validation accuracy:		61.41 %
Epoch 450 of 2000 took 0.105s
  training loss:		1.198947
  validation loss:		1.168734
  validation accuracy:		59.89 %
Epoch 451 of 2000 took 0.105s
  training loss:		1.195992
  validation loss:		1.154117
  validation accuracy:		61.74 %
Epoch 452 of 2000 took 0.105s
  training loss:		1.191995
  validation loss:		1.172799
  validation accuracy:		60.33 %
Epoch 453 of 2000 took 0.105s
  training loss:		1.181302
  validation loss:		1.133337
  validation accuracy:		63.91 %
Epoch 454 of 2000 took 0.105s
  training loss:		1.168698
  validation loss:		1.134073
  validation accuracy:		62.39 %
Epoch 455 of 2000 took 0.106s
  training loss:		1.161144
  validation loss:		1.110054
  validation accuracy:		64.02 %
Epoch 456 of 2000 took 0.101s
  training loss:		1.146164
  validation loss:		1.098746
  validation accuracy:		64.13 %
Epoch 457 of 2000 took 0.104s
  training loss:		1.129409
  validation loss:		1.083433
  validation accuracy:		65.87 %
Epoch 458 of 2000 took 0.105s
  training loss:		1.115938
  validation loss:		1.073863
  validation accuracy:		65.76 %
Epoch 459 of 2000 took 0.106s
  training loss:		1.106135
  validation loss:		1.066554
  validation accuracy:		66.30 %
Epoch 460 of 2000 took 0.109s
  training loss:		1.092036
  validation loss:		1.041830
  validation accuracy:		68.26 %
Epoch 461 of 2000 took 0.101s
  training loss:		1.088236
  validation loss:		1.045607
  validation accuracy:		67.39 %
Epoch 462 of 2000 took 0.104s
  training loss:		1.073556
  validation loss:		1.037176
  validation accuracy:		67.17 %
Epoch 463 of 2000 took 0.105s
  training loss:		1.071654
  validation loss:		1.016874
  validation accuracy:		68.91 %
Epoch 464 of 2000 took 0.105s
  training loss:		1.050289
  validation loss:		1.002731
  validation accuracy:		69.13 %
Epoch 465 of 2000 took 0.103s
  training loss:		1.051889
  validation loss:		1.008106
  validation accuracy:		68.04 %
Epoch 466 of 2000 took 0.102s
  training loss:		1.030099
  validation loss:		0.979061
  validation accuracy:		69.89 %
Epoch 467 of 2000 took 0.114s
  training loss:		1.025849
  validation loss:		0.967266
  validation accuracy:		70.54 %
Epoch 468 of 2000 took 0.104s
  training loss:		1.021130
  validation loss:		0.963430
  validation accuracy:		70.22 %
Epoch 469 of 2000 took 0.102s
  training loss:		1.009172
  validation loss:		0.995677
  validation accuracy:		67.83 %
Epoch 470 of 2000 took 0.111s
  training loss:		1.007941
  validation loss:		0.954149
  validation accuracy:		70.33 %
Epoch 471 of 2000 took 0.104s
  training loss:		0.983255
  validation loss:		0.948087
  validation accuracy:		69.57 %
Epoch 472 of 2000 took 0.103s
  training loss:		0.989893
  validation loss:		0.938604
  validation accuracy:		70.87 %
Epoch 473 of 2000 took 0.105s
  training loss:		0.980805
  validation loss:		0.929145
  validation accuracy:		71.41 %
Epoch 474 of 2000 took 0.106s
  training loss:		0.976931
  validation loss:		0.935564
  validation accuracy:		70.76 %
Epoch 475 of 2000 took 0.109s
  training loss:		0.964277
  validation loss:		0.917195
  validation accuracy:		71.63 %
Epoch 476 of 2000 took 0.101s
  training loss:		0.967136
  validation loss:		0.947637
  validation accuracy:		70.43 %
Epoch 477 of 2000 took 0.109s
  training loss:		0.954457
  validation loss:		0.896856
  validation accuracy:		73.15 %
Epoch 478 of 2000 took 0.111s
  training loss:		0.950261
  validation loss:		0.906348
  validation accuracy:		72.28 %
Epoch 479 of 2000 took 0.101s
  training loss:		0.943394
  validation loss:		0.889865
  validation accuracy:		72.39 %
Epoch 480 of 2000 took 0.104s
  training loss:		0.930264
  validation loss:		0.876431
  validation accuracy:		74.02 %
Epoch 481 of 2000 took 0.101s
  training loss:		0.935781
  validation loss:		0.878152
  validation accuracy:		72.93 %
Epoch 482 of 2000 took 0.104s
  training loss:		0.918150
  validation loss:		0.864925
  validation accuracy:		74.24 %
Epoch 483 of 2000 took 0.105s
  training loss:		0.920215
  validation loss:		0.867326
  validation accuracy:		72.93 %
Epoch 484 of 2000 took 0.102s
  training loss:		0.914270
  validation loss:		0.859076
  validation accuracy:		74.46 %
Epoch 485 of 2000 took 0.114s
  training loss:		0.915927
  validation loss:		0.862552
  validation accuracy:		73.37 %
Epoch 486 of 2000 took 0.116s
  training loss:		0.906709
  validation loss:		0.846144
  validation accuracy:		74.67 %
Epoch 487 of 2000 took 0.106s
  training loss:		0.905456
  validation loss:		0.849075
  validation accuracy:		74.78 %
Epoch 488 of 2000 took 0.106s
  training loss:		0.898916
  validation loss:		0.838247
  validation accuracy:		75.00 %
Epoch 489 of 2000 took 0.105s
  training loss:		0.883171
  validation loss:		0.838458
  validation accuracy:		75.33 %
Epoch 490 of 2000 took 0.114s
  training loss:		0.889540
  validation loss:		0.839800
  validation accuracy:		74.13 %
Epoch 491 of 2000 took 0.110s
  training loss:		0.892318
  validation loss:		0.837370
  validation accuracy:		74.13 %
Epoch 492 of 2000 took 0.107s
  training loss:		0.884463
  validation loss:		0.834019
  validation accuracy:		74.24 %
Epoch 493 of 2000 took 0.111s
  training loss:		0.873204
  validation loss:		0.815847
  validation accuracy:		75.43 %
Epoch 494 of 2000 took 0.102s
  training loss:		0.871931
  validation loss:		0.828638
  validation accuracy:		74.57 %
Epoch 495 of 2000 took 0.109s
  training loss:		0.868062
  validation loss:		0.808309
  validation accuracy:		75.87 %
Epoch 496 of 2000 took 0.102s
  training loss:		0.868050
  validation loss:		0.809922
  validation accuracy:		76.30 %
Epoch 497 of 2000 took 0.105s
  training loss:		0.857037
  validation loss:		0.807241
  validation accuracy:		75.00 %
Epoch 498 of 2000 took 0.109s
  training loss:		0.853057
  validation loss:		0.795608
  validation accuracy:		75.65 %
Epoch 499 of 2000 took 0.102s
  training loss:		0.858253
  validation loss:		0.796475
  validation accuracy:		76.20 %
Epoch 500 of 2000 took 0.104s
  training loss:		0.846046
  validation loss:		0.807645
  validation accuracy:		75.76 %
Epoch 501 of 2000 took 0.106s
  training loss:		0.835656
  validation loss:		0.796983
  validation accuracy:		75.54 %
Epoch 502 of 2000 took 0.105s
  training loss:		0.847240
  validation loss:		0.797759
  validation accuracy:		75.87 %
Epoch 503 of 2000 took 0.105s
  training loss:		0.840646
  validation loss:		0.783069
  validation accuracy:		76.09 %
Epoch 504 of 2000 took 0.101s
  training loss:		0.835448
  validation loss:		0.789026
  validation accuracy:		75.76 %
Epoch 505 of 2000 took 0.114s
  training loss:		0.831898
  validation loss:		0.808481
  validation accuracy:		75.22 %
Epoch 506 of 2000 took 0.106s
  training loss:		0.833645
  validation loss:		0.781387
  validation accuracy:		76.63 %
Epoch 507 of 2000 took 0.101s
  training loss:		0.829113
  validation loss:		0.778062
  validation accuracy:		76.41 %
Epoch 508 of 2000 took 0.107s
  training loss:		0.814953
  validation loss:		0.767029
  validation accuracy:		76.30 %
Epoch 509 of 2000 took 0.108s
  training loss:		0.812303
  validation loss:		0.772387
  validation accuracy:		76.09 %
Epoch 510 of 2000 took 0.101s
  training loss:		0.819473
  validation loss:		0.762351
  validation accuracy:		75.98 %
Epoch 511 of 2000 took 0.105s
  training loss:		0.810577
  validation loss:		0.771520
  validation accuracy:		76.74 %
Epoch 512 of 2000 took 0.104s
  training loss:		0.808713
  validation loss:		0.762132
  validation accuracy:		76.52 %
Epoch 513 of 2000 took 0.110s
  training loss:		0.803429
  validation loss:		0.757745
  validation accuracy:		76.96 %
Epoch 514 of 2000 took 0.108s
  training loss:		0.808113
  validation loss:		0.759556
  validation accuracy:		76.74 %
Epoch 515 of 2000 took 0.102s
  training loss:		0.804823
  validation loss:		0.771626
  validation accuracy:		76.09 %
Epoch 516 of 2000 took 0.110s
  training loss:		0.796603
  validation loss:		0.761649
  validation accuracy:		76.52 %
Epoch 517 of 2000 took 0.104s
  training loss:		0.803658
  validation loss:		0.743246
  validation accuracy:		77.50 %
Epoch 518 of 2000 took 0.104s
  training loss:		0.792567
  validation loss:		0.749738
  validation accuracy:		77.17 %
Epoch 519 of 2000 took 0.102s
  training loss:		0.795322
  validation loss:		0.752292
  validation accuracy:		77.07 %
Epoch 520 of 2000 took 0.102s
  training loss:		0.789706
  validation loss:		0.757035
  validation accuracy:		76.85 %
Epoch 521 of 2000 took 0.107s
  training loss:		0.781516
  validation loss:		0.737771
  validation accuracy:		76.41 %
Epoch 522 of 2000 took 0.104s
  training loss:		0.776797
  validation loss:		0.736907
  validation accuracy:		77.39 %
Epoch 523 of 2000 took 0.102s
  training loss:		0.779735
  validation loss:		0.734751
  validation accuracy:		77.50 %
Epoch 524 of 2000 took 0.116s
  training loss:		0.764838
  validation loss:		0.730046
  validation accuracy:		78.15 %
Epoch 525 of 2000 took 0.103s
  training loss:		0.765683
  validation loss:		0.732741
  validation accuracy:		77.28 %
Epoch 526 of 2000 took 0.103s
  training loss:		0.774445
  validation loss:		0.742250
  validation accuracy:		77.17 %
Epoch 527 of 2000 took 0.102s
  training loss:		0.755124
  validation loss:		0.724958
  validation accuracy:		76.85 %
Epoch 528 of 2000 took 0.103s
  training loss:		0.769891
  validation loss:		0.721256
  validation accuracy:		77.50 %
Epoch 529 of 2000 took 0.110s
  training loss:		0.758526
  validation loss:		0.714807
  validation accuracy:		77.61 %
Epoch 530 of 2000 took 0.104s
  training loss:		0.757163
  validation loss:		0.721485
  validation accuracy:		76.96 %
Epoch 531 of 2000 took 0.102s
  training loss:		0.758313
  validation loss:		0.705207
  validation accuracy:		78.04 %
Epoch 532 of 2000 took 0.105s
  training loss:		0.750764
  validation loss:		0.714253
  validation accuracy:		77.72 %
Epoch 533 of 2000 took 0.108s
  training loss:		0.745708
  validation loss:		0.709142
  validation accuracy:		77.93 %
Epoch 534 of 2000 took 0.106s
  training loss:		0.745275
  validation loss:		0.720068
  validation accuracy:		78.04 %
Epoch 535 of 2000 took 0.105s
  training loss:		0.749258
  validation loss:		0.706595
  validation accuracy:		78.37 %
Epoch 536 of 2000 took 0.105s
  training loss:		0.765060
  validation loss:		0.697786
  validation accuracy:		77.83 %
Epoch 537 of 2000 took 0.105s
  training loss:		0.734544
  validation loss:		0.691245
  validation accuracy:		78.48 %
Epoch 538 of 2000 took 0.105s
  training loss:		0.742447
  validation loss:		0.691950
  validation accuracy:		78.37 %
Epoch 539 of 2000 took 0.105s
  training loss:		0.732981
  validation loss:		0.715780
  validation accuracy:		77.17 %
Epoch 540 of 2000 took 0.105s
  training loss:		0.736277
  validation loss:		0.684536
  validation accuracy:		78.59 %
Epoch 541 of 2000 took 0.105s
  training loss:		0.723028
  validation loss:		0.710407
  validation accuracy:		78.37 %
Epoch 542 of 2000 took 0.105s
  training loss:		0.715034
  validation loss:		0.713583
  validation accuracy:		77.39 %
Epoch 543 of 2000 took 0.110s
  training loss:		0.721447
  validation loss:		0.673854
  validation accuracy:		79.24 %
Epoch 544 of 2000 took 0.105s
  training loss:		0.719689
  validation loss:		0.713719
  validation accuracy:		77.50 %
Epoch 545 of 2000 took 0.105s
  training loss:		0.716074
  validation loss:		0.674904
  validation accuracy:		79.02 %
Epoch 546 of 2000 took 0.105s
  training loss:		0.726932
  validation loss:		0.675444
  validation accuracy:		78.59 %
Epoch 547 of 2000 took 0.105s
  training loss:		0.733688
  validation loss:		0.671554
  validation accuracy:		79.57 %
Epoch 548 of 2000 took 0.105s
  training loss:		0.715465
  validation loss:		0.676546
  validation accuracy:		79.46 %
Epoch 549 of 2000 took 0.105s
  training loss:		0.699783
  validation loss:		0.675134
  validation accuracy:		78.59 %
Epoch 550 of 2000 took 0.108s
  training loss:		0.699714
  validation loss:		0.677123
  validation accuracy:		79.67 %
Epoch 551 of 2000 took 0.109s
  training loss:		0.707504
  validation loss:		0.659707
  validation accuracy:		79.46 %
Epoch 552 of 2000 took 0.109s
  training loss:		0.706469
  validation loss:		0.674229
  validation accuracy:		78.26 %
Epoch 553 of 2000 took 0.107s
  training loss:		0.711397
  validation loss:		0.664768
  validation accuracy:		79.35 %
Epoch 554 of 2000 took 0.105s
  training loss:		0.707447
  validation loss:		0.663031
  validation accuracy:		78.70 %
Epoch 555 of 2000 took 0.105s
  training loss:		0.700528
  validation loss:		0.651469
  validation accuracy:		78.91 %
Epoch 556 of 2000 took 0.105s
  training loss:		0.708889
  validation loss:		0.650238
  validation accuracy:		79.78 %
Epoch 557 of 2000 took 0.105s
  training loss:		0.691317
  validation loss:		0.670007
  validation accuracy:		79.24 %
Epoch 558 of 2000 took 0.105s
  training loss:		0.687979
  validation loss:		0.645747
  validation accuracy:		79.02 %
Epoch 559 of 2000 took 0.105s
  training loss:		0.693646
  validation loss:		0.662915
  validation accuracy:		79.24 %
Epoch 560 of 2000 took 0.102s
  training loss:		0.700134
  validation loss:		0.642245
  validation accuracy:		80.22 %
Epoch 561 of 2000 took 0.105s
  training loss:		0.682015
  validation loss:		0.656136
  validation accuracy:		79.67 %
Epoch 562 of 2000 took 0.109s
  training loss:		0.691315
  validation loss:		0.667316
  validation accuracy:		79.67 %
Epoch 563 of 2000 took 0.105s
  training loss:		0.713865
  validation loss:		0.641973
  validation accuracy:		80.11 %
Epoch 564 of 2000 took 0.105s
  training loss:		0.687092
  validation loss:		0.643569
  validation accuracy:		79.57 %
Epoch 565 of 2000 took 0.105s
  training loss:		0.678154
  validation loss:		0.646220
  validation accuracy:		78.26 %
Epoch 566 of 2000 took 0.105s
  training loss:		0.677851
  validation loss:		0.699547
  validation accuracy:		78.26 %
Epoch 567 of 2000 took 0.105s
  training loss:		0.686565
  validation loss:		0.632516
  validation accuracy:		79.35 %
Epoch 568 of 2000 took 0.105s
  training loss:		0.686595
  validation loss:		0.664957
  validation accuracy:		79.24 %
Epoch 569 of 2000 took 0.105s
  training loss:		0.671675
  validation loss:		0.635431
  validation accuracy:		79.35 %
Epoch 570 of 2000 took 0.101s
  training loss:		0.693014
  validation loss:		0.671424
  validation accuracy:		78.04 %
Epoch 571 of 2000 took 0.109s
  training loss:		0.687868
  validation loss:		0.753847
  validation accuracy:		76.41 %
Epoch 572 of 2000 took 0.106s
  training loss:		0.680204
  validation loss:		0.807881
  validation accuracy:		74.67 %
Epoch 573 of 2000 took 0.105s
  training loss:		0.687402
  validation loss:		0.628637
  validation accuracy:		79.67 %
Epoch 574 of 2000 took 0.105s
  training loss:		0.663306
  validation loss:		0.626863
  validation accuracy:		80.65 %
Epoch 575 of 2000 took 0.105s
  training loss:		0.662153
  validation loss:		0.628881
  validation accuracy:		78.70 %
Epoch 576 of 2000 took 0.105s
  training loss:		0.670220
  validation loss:		0.629514
  validation accuracy:		80.11 %
Epoch 577 of 2000 took 0.105s
  training loss:		0.664242
  validation loss:		0.631559
  validation accuracy:		79.35 %
Epoch 578 of 2000 took 0.105s
  training loss:		0.709173
  validation loss:		0.631269
  validation accuracy:		78.80 %
Epoch 579 of 2000 took 0.101s
  training loss:		0.655588
  validation loss:		0.636824
  validation accuracy:		80.00 %
Epoch 580 of 2000 took 0.105s
  training loss:		0.664259
  validation loss:		0.614539
  validation accuracy:		79.78 %
Epoch 581 of 2000 took 0.110s
  training loss:		0.668556
  validation loss:		0.654120
  validation accuracy:		79.57 %
Epoch 582 of 2000 took 0.105s
  training loss:		0.662831
  validation loss:		0.640913
  validation accuracy:		78.37 %
Epoch 583 of 2000 took 0.105s
  training loss:		0.661646
  validation loss:		0.622259
  validation accuracy:		79.46 %
Epoch 584 of 2000 took 0.105s
  training loss:		0.664281
  validation loss:		0.617154
  validation accuracy:		80.65 %
Epoch 585 of 2000 took 0.105s
  training loss:		0.657662
  validation loss:		0.664500
  validation accuracy:		79.13 %
Epoch 586 of 2000 took 0.105s
  training loss:		0.671127
  validation loss:		0.625799
  validation accuracy:		79.67 %
Epoch 587 of 2000 took 0.105s
  training loss:		0.664393
  validation loss:		0.616640
  validation accuracy:		79.89 %
Epoch 588 of 2000 took 0.106s
  training loss:		0.645428
  validation loss:		0.622620
  validation accuracy:		79.67 %
Epoch 589 of 2000 took 0.101s
  training loss:		0.654466
  validation loss:		0.620404
  validation accuracy:		79.35 %
Epoch 590 of 2000 took 0.109s
  training loss:		0.647198
  validation loss:		0.639455
  validation accuracy:		79.46 %
Epoch 591 of 2000 took 0.106s
  training loss:		0.700782
  validation loss:		0.647794
  validation accuracy:		79.02 %
Epoch 592 of 2000 took 0.105s
  training loss:		0.657978
  validation loss:		0.609237
  validation accuracy:		80.00 %
Epoch 593 of 2000 took 0.105s
  training loss:		0.656699
  validation loss:		0.611818
  validation accuracy:		80.33 %
Epoch 594 of 2000 took 0.105s
  training loss:		0.652071
  validation loss:		0.612181
  validation accuracy:		79.67 %
Epoch 595 of 2000 took 0.105s
  training loss:		0.649341
  validation loss:		0.646820
  validation accuracy:		79.02 %
Epoch 596 of 2000 took 0.105s
  training loss:		0.633748
  validation loss:		0.646609
  validation accuracy:		79.02 %
Epoch 597 of 2000 took 0.105s
  training loss:		0.654193
  validation loss:		0.608011
  validation accuracy:		79.13 %
Epoch 598 of 2000 took 0.107s
  training loss:		0.652451
  validation loss:		0.623666
  validation accuracy:		79.89 %
Epoch 599 of 2000 took 0.101s
  training loss:		0.629554
  validation loss:		0.609426
  validation accuracy:		80.87 %
Epoch 600 of 2000 took 0.109s
  training loss:		0.646197
  validation loss:		0.605521
  validation accuracy:		79.78 %
Epoch 601 of 2000 took 0.105s
  training loss:		0.630604
  validation loss:		0.630499
  validation accuracy:		79.78 %
Epoch 602 of 2000 took 0.105s
  training loss:		0.654686
  validation loss:		0.604880
  validation accuracy:		80.22 %
Epoch 603 of 2000 took 0.105s
  training loss:		0.646436
  validation loss:		0.630690
  validation accuracy:		79.46 %
Epoch 604 of 2000 took 0.105s
  training loss:		0.641783
  validation loss:		0.613539
  validation accuracy:		80.00 %
Epoch 605 of 2000 took 0.106s
  training loss:		0.721746
  validation loss:		0.636041
  validation accuracy:		79.35 %
Epoch 606 of 2000 took 0.105s
  training loss:		0.676896
  validation loss:		0.621729
  validation accuracy:		80.00 %
Epoch 607 of 2000 took 0.103s
  training loss:		0.658821
  validation loss:		0.616868
  validation accuracy:		79.24 %
Epoch 608 of 2000 took 0.103s
  training loss:		0.638987
  validation loss:		0.612803
  validation accuracy:		79.57 %
Epoch 609 of 2000 took 0.111s
  training loss:		0.625241
  validation loss:		0.614613
  validation accuracy:		80.22 %
Epoch 610 of 2000 took 0.107s
  training loss:		0.631348
  validation loss:		0.603040
  validation accuracy:		80.22 %
Epoch 611 of 2000 took 0.105s
  training loss:		0.625529
  validation loss:		0.596629
  validation accuracy:		80.00 %
Epoch 612 of 2000 took 0.105s
  training loss:		0.644945
  validation loss:		0.696093
  validation accuracy:		77.72 %
Epoch 613 of 2000 took 0.106s
  training loss:		0.627324
  validation loss:		0.594206
  validation accuracy:		80.22 %
Epoch 614 of 2000 took 0.105s
  training loss:		0.626595
  validation loss:		0.640725
  validation accuracy:		79.24 %
Epoch 615 of 2000 took 0.105s
  training loss:		0.635845
  validation loss:		0.595417
  validation accuracy:		79.89 %
Epoch 616 of 2000 took 0.106s
  training loss:		0.629956
  validation loss:		0.595182
  validation accuracy:		80.87 %
Epoch 617 of 2000 took 0.103s
  training loss:		0.635015
  validation loss:		0.604101
  validation accuracy:		79.78 %
Epoch 618 of 2000 took 0.103s
  training loss:		0.625270
  validation loss:		0.596144
  validation accuracy:		80.54 %
Epoch 619 of 2000 took 0.110s
  training loss:		0.630557
  validation loss:		0.595748
  validation accuracy:		79.57 %
Epoch 620 of 2000 took 0.105s
  training loss:		0.653836
  validation loss:		0.615745
  validation accuracy:		80.11 %
Epoch 621 of 2000 took 0.105s
  training loss:		0.643245
  validation loss:		0.607592
  validation accuracy:		79.57 %
Epoch 622 of 2000 took 0.105s
  training loss:		0.620178
  validation loss:		0.591580
  validation accuracy:		79.67 %
Epoch 623 of 2000 took 0.105s
  training loss:		0.626473
  validation loss:		0.607334
  validation accuracy:		80.00 %
Epoch 624 of 2000 took 0.105s
  training loss:		0.632040
  validation loss:		0.636033
  validation accuracy:		78.91 %
Epoch 625 of 2000 took 0.105s
  training loss:		0.617693
  validation loss:		0.614822
  validation accuracy:		79.13 %
Epoch 626 of 2000 took 0.109s
  training loss:		0.624072
  validation loss:		0.612075
  validation accuracy:		79.67 %
Epoch 627 of 2000 took 0.101s
  training loss:		0.627474
  validation loss:		0.602885
  validation accuracy:		79.57 %
Epoch 628 of 2000 took 0.108s
  training loss:		0.620439
  validation loss:		0.646460
  validation accuracy:		78.80 %
Epoch 629 of 2000 took 0.101s
  training loss:		0.650508
  validation loss:		0.596115
  validation accuracy:		80.43 %
Epoch 630 of 2000 took 0.105s
  training loss:		0.637968
  validation loss:		0.619959
  validation accuracy:		79.89 %
Epoch 631 of 2000 took 0.105s
  training loss:		0.626901
  validation loss:		0.578681
  validation accuracy:		80.33 %
Epoch 632 of 2000 took 0.105s
  training loss:		0.622907
  validation loss:		0.613079
  validation accuracy:		79.46 %
Epoch 633 of 2000 took 0.105s
  training loss:		0.629410
  validation loss:		0.606689
  validation accuracy:		80.22 %
Epoch 634 of 2000 took 0.105s
  training loss:		0.615137
  validation loss:		0.589806
  validation accuracy:		80.11 %
Epoch 635 of 2000 took 0.105s
  training loss:		0.622454
  validation loss:		0.613947
  validation accuracy:		79.57 %
Epoch 636 of 2000 took 0.101s
  training loss:		0.623040
  validation loss:		0.629627
  validation accuracy:		79.78 %
Epoch 637 of 2000 took 0.105s
  training loss:		0.674026
  validation loss:		0.607133
  validation accuracy:		80.11 %
Epoch 638 of 2000 took 0.115s
  training loss:		0.628981
  validation loss:		0.585871
  validation accuracy:		80.87 %
Epoch 639 of 2000 took 0.101s
  training loss:		0.617748
  validation loss:		0.580283
  validation accuracy:		79.78 %
Epoch 640 of 2000 took 0.105s
  training loss:		0.626411
  validation loss:		0.614380
  validation accuracy:		79.67 %
Epoch 641 of 2000 took 0.105s
  training loss:		0.612943
  validation loss:		0.588958
  validation accuracy:		80.00 %
Epoch 642 of 2000 took 0.105s
  training loss:		0.620687
  validation loss:		0.591797
  validation accuracy:		80.00 %
Epoch 643 of 2000 took 0.105s
  training loss:		0.605808
  validation loss:		0.581284
  validation accuracy:		80.11 %
Epoch 644 of 2000 took 0.105s
  training loss:		0.623742
  validation loss:		0.600311
  validation accuracy:		79.89 %
Epoch 645 of 2000 took 0.106s
  training loss:		0.616061
  validation loss:		0.580007
  validation accuracy:		80.65 %
Epoch 646 of 2000 took 0.103s
  training loss:		0.639762
  validation loss:		0.608560
  validation accuracy:		80.22 %
Epoch 647 of 2000 took 0.106s
  training loss:		0.628117
  validation loss:		0.590355
  validation accuracy:		80.00 %
Epoch 648 of 2000 took 0.112s
  training loss:		0.622198
  validation loss:		0.587365
  validation accuracy:		80.00 %
Epoch 649 of 2000 took 0.102s
  training loss:		0.617016
  validation loss:		0.599161
  validation accuracy:		80.00 %
Epoch 650 of 2000 took 0.103s
  training loss:		0.610481
  validation loss:		0.579826
  validation accuracy:		80.22 %
Epoch 651 of 2000 took 0.105s
  training loss:		0.604608
  validation loss:		0.585106
  validation accuracy:		80.54 %
Epoch 652 of 2000 took 0.105s
  training loss:		0.610538
  validation loss:		0.576920
  validation accuracy:		81.30 %
Epoch 653 of 2000 took 0.105s
  training loss:		0.613814
  validation loss:		0.587817
  validation accuracy:		80.54 %
Epoch 654 of 2000 took 0.105s
  training loss:		0.618377
  validation loss:		0.583577
  validation accuracy:		80.11 %
Epoch 655 of 2000 took 0.109s
  training loss:		0.623141
  validation loss:		0.591455
  validation accuracy:		80.87 %
Epoch 656 of 2000 took 0.107s
  training loss:		0.611047
  validation loss:		0.578083
  validation accuracy:		80.43 %
Epoch 657 of 2000 took 0.106s
  training loss:		0.615085
  validation loss:		0.592869
  validation accuracy:		80.33 %
Epoch 658 of 2000 took 0.103s
  training loss:		0.621752
  validation loss:		0.590340
  validation accuracy:		80.00 %
Epoch 659 of 2000 took 0.101s
  training loss:		0.639946
  validation loss:		0.573402
  validation accuracy:		80.22 %
Epoch 660 of 2000 took 0.105s
  training loss:		0.624257
  validation loss:		0.600650
  validation accuracy:		80.22 %
Epoch 661 of 2000 took 0.105s
  training loss:		0.598942
  validation loss:		0.573231
  validation accuracy:		80.54 %
Epoch 662 of 2000 took 0.105s
  training loss:		0.606284
  validation loss:		0.578652
  validation accuracy:		80.65 %
Epoch 663 of 2000 took 0.105s
  training loss:		0.616404
  validation loss:		0.708095
  validation accuracy:		77.83 %
Epoch 664 of 2000 took 0.104s
  training loss:		0.620891
  validation loss:		0.567716
  validation accuracy:		80.33 %
Epoch 665 of 2000 took 0.104s
  training loss:		0.619007
  validation loss:		0.599016
  validation accuracy:		80.22 %
Epoch 666 of 2000 took 0.104s
  training loss:		0.614077
  validation loss:		0.576043
  validation accuracy:		79.67 %
Epoch 667 of 2000 took 0.109s
  training loss:		0.605543
  validation loss:		0.574429
  validation accuracy:		81.30 %
Epoch 668 of 2000 took 0.107s
  training loss:		0.646568
  validation loss:		0.569184
  validation accuracy:		80.00 %
Epoch 669 of 2000 took 0.101s
  training loss:		0.593484
  validation loss:		0.616962
  validation accuracy:		80.00 %
Epoch 670 of 2000 took 0.104s
  training loss:		0.615170
  validation loss:		0.569079
  validation accuracy:		80.54 %
Epoch 671 of 2000 took 0.105s
  training loss:		0.600699
  validation loss:		0.575832
  validation accuracy:		80.65 %
Epoch 672 of 2000 took 0.105s
  training loss:		0.610547
  validation loss:		0.585296
  validation accuracy:		80.43 %
Epoch 673 of 2000 took 0.105s
  training loss:		0.607967
  validation loss:		0.569404
  validation accuracy:		79.89 %
Epoch 674 of 2000 took 0.106s
  training loss:		0.593875
  validation loss:		0.580202
  validation accuracy:		80.65 %
Epoch 675 of 2000 took 0.109s
  training loss:		0.618211
  validation loss:		0.566833
  validation accuracy:		81.20 %
Epoch 676 of 2000 took 0.106s
  training loss:		0.598270
  validation loss:		0.589896
  validation accuracy:		80.43 %
Epoch 677 of 2000 took 0.104s
  training loss:		0.605144
  validation loss:		0.575788
  validation accuracy:		81.20 %
Epoch 678 of 2000 took 0.107s
  training loss:		0.610578
  validation loss:		0.609117
  validation accuracy:		80.00 %
Epoch 679 of 2000 took 0.101s
  training loss:		0.610523
  validation loss:		0.584376
  validation accuracy:		81.30 %
Epoch 680 of 2000 took 0.105s
  training loss:		0.635075
  validation loss:		0.646362
  validation accuracy:		78.91 %
Epoch 681 of 2000 took 0.105s
  training loss:		0.617624
  validation loss:		0.583936
  validation accuracy:		80.65 %
Epoch 682 of 2000 took 0.105s
  training loss:		0.608653
  validation loss:		0.578205
  validation accuracy:		80.43 %
Epoch 683 of 2000 took 0.104s
  training loss:		0.595644
  validation loss:		0.589953
  validation accuracy:		80.43 %
Epoch 684 of 2000 took 0.105s
  training loss:		0.598466
  validation loss:		0.570254
  validation accuracy:		80.11 %
Epoch 685 of 2000 took 0.110s
  training loss:		0.669911
  validation loss:		0.603764
  validation accuracy:		79.89 %
Epoch 686 of 2000 took 0.103s
  training loss:		0.599006
  validation loss:		0.566823
  validation accuracy:		80.33 %
Epoch 687 of 2000 took 0.108s
  training loss:		0.604153
  validation loss:		0.583319
  validation accuracy:		80.76 %
Epoch 688 of 2000 took 0.101s
  training loss:		0.603917
  validation loss:		0.566831
  validation accuracy:		80.33 %
Epoch 689 of 2000 took 0.106s
  training loss:		0.594917
  validation loss:		0.593640
  validation accuracy:		80.54 %
Epoch 690 of 2000 took 0.104s
  training loss:		0.607201
  validation loss:		0.609653
  validation accuracy:		79.67 %
Epoch 691 of 2000 took 0.102s
  training loss:		0.612904
  validation loss:		0.572309
  validation accuracy:		80.22 %
Epoch 692 of 2000 took 0.105s
  training loss:		0.594973
  validation loss:		0.587250
  validation accuracy:		80.54 %
Epoch 693 of 2000 took 0.103s
  training loss:		0.603355
  validation loss:		0.571781
  validation accuracy:		81.30 %
Epoch 694 of 2000 took 0.109s
  training loss:		0.607658
  validation loss:		0.563302
  validation accuracy:		80.87 %
Epoch 695 of 2000 took 0.105s
  training loss:		0.599395
  validation loss:		0.561351
  validation accuracy:		81.20 %
Epoch 696 of 2000 took 0.106s
  training loss:		0.590355
  validation loss:		0.582076
  validation accuracy:		80.11 %
Epoch 697 of 2000 took 0.106s
  training loss:		0.607629
  validation loss:		0.570017
  validation accuracy:		81.09 %
Epoch 698 of 2000 took 0.102s
  training loss:		0.610438
  validation loss:		0.595455
  validation accuracy:		80.43 %
Epoch 699 of 2000 took 0.108s
  training loss:		0.594306
  validation loss:		0.565270
  validation accuracy:		81.20 %
Epoch 700 of 2000 took 0.101s
  training loss:		0.633566
  validation loss:		0.572906
  validation accuracy:		81.30 %
Epoch 701 of 2000 took 0.105s
  training loss:		0.598746
  validation loss:		0.600909
  validation accuracy:		80.11 %
Epoch 702 of 2000 took 0.105s
  training loss:		0.605017
  validation loss:		0.563690
  validation accuracy:		80.00 %
Epoch 703 of 2000 took 0.104s
  training loss:		0.599397
  validation loss:		0.560890
  validation accuracy:		80.65 %
Epoch 704 of 2000 took 0.115s
  training loss:		0.599306
  validation loss:		0.560994
  validation accuracy:		81.30 %
Epoch 705 of 2000 took 0.101s
  training loss:		0.593497
  validation loss:		0.584972
  validation accuracy:		80.33 %
Epoch 706 of 2000 took 0.104s
  training loss:		0.601635
  validation loss:		0.605280
  validation accuracy:		79.35 %
Epoch 707 of 2000 took 0.101s
  training loss:		0.616333
  validation loss:		0.561600
  validation accuracy:		81.09 %
Epoch 708 of 2000 took 0.107s
  training loss:		0.598000
  validation loss:		0.572189
  validation accuracy:		80.98 %
Epoch 709 of 2000 took 0.108s
  training loss:		0.602210
  validation loss:		0.555043
  validation accuracy:		81.30 %
Epoch 710 of 2000 took 0.101s
  training loss:		0.597189
  validation loss:		0.562236
  validation accuracy:		81.30 %
Epoch 711 of 2000 took 0.104s
  training loss:		0.596419
  validation loss:		0.566141
  validation accuracy:		80.11 %
Epoch 712 of 2000 took 0.104s
  training loss:		0.583817
  validation loss:		0.556922
  validation accuracy:		81.63 %
Epoch 713 of 2000 took 0.102s
  training loss:		0.578543
  validation loss:		0.575081
  validation accuracy:		80.65 %
Epoch 714 of 2000 took 0.108s
  training loss:		0.608723
  validation loss:		0.587579
  validation accuracy:		80.11 %
Epoch 715 of 2000 took 0.101s
  training loss:		0.599757
  validation loss:		0.611184
  validation accuracy:		78.91 %
Epoch 716 of 2000 took 0.111s
  training loss:		0.635929
  validation loss:		0.578152
  validation accuracy:		81.30 %
Epoch 717 of 2000 took 0.104s
  training loss:		0.607554
  validation loss:		0.583716
  validation accuracy:		80.43 %
Epoch 718 of 2000 took 0.101s
  training loss:		0.599197
  validation loss:		0.567078
  validation accuracy:		80.33 %
Epoch 719 of 2000 took 0.109s
  training loss:		0.602901
  validation loss:		0.571137
  validation accuracy:		80.98 %
Epoch 720 of 2000 took 0.106s
  training loss:		0.601259
  validation loss:		0.602495
  validation accuracy:		79.24 %
Epoch 721 of 2000 took 0.105s
  training loss:		0.592284
  validation loss:		0.566786
  validation accuracy:		80.22 %
Epoch 722 of 2000 took 0.102s
  training loss:		0.593521
  validation loss:		0.563211
  validation accuracy:		80.76 %
Epoch 723 of 2000 took 0.106s
  training loss:		0.586113
  validation loss:		0.552641
  validation accuracy:		81.20 %
Epoch 724 of 2000 took 0.112s
  training loss:		0.584342
  validation loss:		0.556831
  validation accuracy:		81.30 %
Epoch 725 of 2000 took 0.103s
  training loss:		0.585887
  validation loss:		0.630736
  validation accuracy:		79.13 %
Epoch 726 of 2000 took 0.102s
  training loss:		0.593968
  validation loss:		0.557750
  validation accuracy:		80.43 %
Epoch 727 of 2000 took 0.112s
  training loss:		0.591069
  validation loss:		0.558463
  validation accuracy:		81.20 %
Epoch 728 of 2000 took 0.102s
  training loss:		0.579355
  validation loss:		0.558784
  validation accuracy:		81.20 %
Epoch 729 of 2000 took 0.103s
  training loss:		0.596642
  validation loss:		0.558413
  validation accuracy:		80.98 %
Epoch 730 of 2000 took 0.102s
  training loss:		0.587646
  validation loss:		0.592571
  validation accuracy:		80.00 %
Epoch 731 of 2000 took 0.104s
  training loss:		0.615958
  validation loss:		0.563546
  validation accuracy:		80.76 %
Epoch 732 of 2000 took 0.109s
  training loss:		0.592773
  validation loss:		0.556617
  validation accuracy:		80.87 %
Epoch 733 of 2000 took 0.106s
  training loss:		0.582602
  validation loss:		0.559596
  validation accuracy:		81.41 %
Epoch 734 of 2000 took 0.104s
  training loss:		0.591106
  validation loss:		0.554377
  validation accuracy:		80.65 %
Epoch 735 of 2000 took 0.112s
  training loss:		0.581928
  validation loss:		0.558638
  validation accuracy:		81.41 %
Epoch 736 of 2000 took 0.101s
  training loss:		0.588430
  validation loss:		0.563417
  validation accuracy:		80.76 %
Epoch 737 of 2000 took 0.105s
  training loss:		0.586970
  validation loss:		0.555517
  validation accuracy:		80.87 %
Epoch 738 of 2000 took 0.101s
  training loss:		0.605952
  validation loss:		0.566562
  validation accuracy:		80.65 %
Epoch 739 of 2000 took 0.105s
  training loss:		0.596030
  validation loss:		0.556624
  validation accuracy:		80.65 %
Epoch 740 of 2000 took 0.109s
  training loss:		0.596331
  validation loss:		0.554903
  validation accuracy:		81.41 %
Epoch 741 of 2000 took 0.102s
  training loss:		0.606971
  validation loss:		0.553530
  validation accuracy:		81.20 %
Epoch 742 of 2000 took 0.107s
  training loss:		0.586113
  validation loss:		0.565202
  validation accuracy:		80.98 %
Epoch 743 of 2000 took 0.114s
  training loss:		0.583897
  validation loss:		0.614046
  validation accuracy:		79.24 %
Epoch 744 of 2000 took 0.101s
  training loss:		0.581858
  validation loss:		0.551622
  validation accuracy:		81.20 %
Epoch 745 of 2000 took 0.105s
  training loss:		0.583819
  validation loss:		0.570783
  validation accuracy:		80.87 %
Epoch 746 of 2000 took 0.101s
  training loss:		0.584607
  validation loss:		0.556048
  validation accuracy:		81.09 %
Epoch 747 of 2000 took 0.105s
  training loss:		0.613297
  validation loss:		0.550191
  validation accuracy:		81.52 %
Epoch 748 of 2000 took 0.109s
  training loss:		0.580496
  validation loss:		0.557435
  validation accuracy:		81.41 %
Epoch 749 of 2000 took 0.101s
  training loss:		0.582878
  validation loss:		0.553517
  validation accuracy:		80.98 %
Epoch 750 of 2000 took 0.105s
  training loss:		0.586996
  validation loss:		0.547811
  validation accuracy:		81.20 %
Epoch 751 of 2000 took 0.106s
  training loss:		0.583317
  validation loss:		0.558407
  validation accuracy:		81.74 %
Epoch 752 of 2000 took 0.110s
  training loss:		0.589371
  validation loss:		0.557933
  validation accuracy:		81.63 %
Epoch 753 of 2000 took 0.106s
  training loss:		0.591639
  validation loss:		0.549732
  validation accuracy:		81.41 %
Epoch 754 of 2000 took 0.105s
  training loss:		0.584513
  validation loss:		0.563677
  validation accuracy:		80.65 %
Epoch 755 of 2000 took 0.105s
  training loss:		0.583490
  validation loss:		0.555250
  validation accuracy:		81.09 %
Epoch 756 of 2000 took 0.105s
  training loss:		0.606844
  validation loss:		0.635642
  validation accuracy:		79.13 %
Epoch 757 of 2000 took 0.105s
  training loss:		0.611154
  validation loss:		0.557133
  validation accuracy:		80.76 %
Epoch 758 of 2000 took 0.105s
  training loss:		0.586250
  validation loss:		0.550351
  validation accuracy:		81.09 %
Epoch 759 of 2000 took 0.105s
  training loss:		0.581091
  validation loss:		0.562885
  validation accuracy:		81.20 %
Epoch 760 of 2000 took 0.104s
  training loss:		0.571638
  validation loss:		0.549837
  validation accuracy:		81.20 %
Epoch 761 of 2000 took 0.109s
  training loss:		0.581975
  validation loss:		0.581722
  validation accuracy:		80.11 %
Epoch 762 of 2000 took 0.106s
  training loss:		0.585501
  validation loss:		0.559476
  validation accuracy:		81.41 %
Epoch 763 of 2000 took 0.106s
  training loss:		0.586624
  validation loss:		0.596233
  validation accuracy:		79.89 %
Epoch 764 of 2000 took 0.105s
  training loss:		0.603655
  validation loss:		0.564096
  validation accuracy:		81.41 %
Epoch 765 of 2000 took 0.105s
  training loss:		0.585152
  validation loss:		0.549547
  validation accuracy:		81.52 %
Epoch 766 of 2000 took 0.105s
  training loss:		0.611728
  validation loss:		0.569534
  validation accuracy:		80.87 %
Epoch 767 of 2000 took 0.105s
  training loss:		0.587956
  validation loss:		0.550813
  validation accuracy:		80.98 %
Epoch 768 of 2000 took 0.105s
  training loss:		0.580693
  validation loss:		0.547634
  validation accuracy:		81.41 %
Epoch 769 of 2000 took 0.105s
  training loss:		0.588463
  validation loss:		0.579606
  validation accuracy:		80.54 %
Epoch 770 of 2000 took 0.105s
  training loss:		0.618793
  validation loss:		0.556784
  validation accuracy:		81.63 %
Epoch 771 of 2000 took 0.109s
  training loss:		0.589778
  validation loss:		0.550227
  validation accuracy:		81.63 %
Epoch 772 of 2000 took 0.105s
  training loss:		0.585885
  validation loss:		0.576492
  validation accuracy:		80.43 %
Epoch 773 of 2000 took 0.105s
  training loss:		0.585445
  validation loss:		0.548749
  validation accuracy:		81.41 %
Epoch 774 of 2000 took 0.103s
  training loss:		0.596493
  validation loss:		0.571917
  validation accuracy:		80.98 %
Epoch 775 of 2000 took 0.105s
  training loss:		0.624051
  validation loss:		0.561668
  validation accuracy:		80.76 %
Epoch 776 of 2000 took 0.101s
  training loss:		0.585078
  validation loss:		0.567558
  validation accuracy:		80.76 %
Epoch 777 of 2000 took 0.108s
  training loss:		0.582850
  validation loss:		0.561290
  validation accuracy:		80.87 %
Epoch 778 of 2000 took 0.107s
  training loss:		0.580917
  validation loss:		0.547393
  validation accuracy:		81.96 %
Epoch 779 of 2000 took 0.101s
  training loss:		0.585336
  validation loss:		0.560650
  validation accuracy:		81.52 %
Epoch 780 of 2000 took 0.108s
  training loss:		0.580532
  validation loss:		0.549870
  validation accuracy:		81.41 %
Epoch 781 of 2000 took 0.106s
  training loss:		0.591308
  validation loss:		0.547973
  validation accuracy:		81.52 %
Epoch 782 of 2000 took 0.106s
  training loss:		0.580008
  validation loss:		0.571591
  validation accuracy:		80.54 %
Epoch 783 of 2000 took 0.105s
  training loss:		0.587684
  validation loss:		0.553505
  validation accuracy:		81.09 %
Epoch 784 of 2000 took 0.105s
  training loss:		0.583854
  validation loss:		0.557230
  validation accuracy:		80.87 %
Epoch 785 of 2000 took 0.109s
  training loss:		0.588790
  validation loss:		0.549250
  validation accuracy:		81.52 %
Epoch 786 of 2000 took 0.102s
  training loss:		0.570551
  validation loss:		0.545340
  validation accuracy:		81.74 %
Epoch 787 of 2000 took 0.103s
  training loss:		0.584305
  validation loss:		0.566961
  validation accuracy:		80.87 %
Epoch 788 of 2000 took 0.112s
  training loss:		0.589437
  validation loss:		0.552906
  validation accuracy:		81.30 %
Epoch 789 of 2000 took 0.102s
  training loss:		0.573652
  validation loss:		0.549828
  validation accuracy:		81.41 %
Epoch 790 of 2000 took 0.109s
  training loss:		0.580206
  validation loss:		0.560126
  validation accuracy:		80.98 %
Epoch 791 of 2000 took 0.105s
  training loss:		0.585560
  validation loss:		0.551220
  validation accuracy:		81.63 %
Epoch 792 of 2000 took 0.105s
  training loss:		0.583202
  validation loss:		0.565422
  validation accuracy:		80.65 %
Epoch 793 of 2000 took 0.105s
  training loss:		0.592435
  validation loss:		0.558204
  validation accuracy:		81.41 %
Epoch 794 of 2000 took 0.101s
  training loss:		0.583875
  validation loss:		0.557858
  validation accuracy:		81.41 %
Epoch 795 of 2000 took 0.106s
  training loss:		0.586478
  validation loss:		0.544636
  validation accuracy:		82.39 %
Epoch 796 of 2000 took 0.110s
  training loss:		0.591338
  validation loss:		0.570116
  validation accuracy:		80.76 %
Epoch 797 of 2000 took 0.101s
  training loss:		0.587094
  validation loss:		0.552620
  validation accuracy:		81.52 %
Epoch 798 of 2000 took 0.104s
  training loss:		0.583752
  validation loss:		0.552248
  validation accuracy:		81.30 %
Epoch 799 of 2000 took 0.104s
  training loss:		0.588025
  validation loss:		0.566995
  validation accuracy:		81.09 %
Epoch 800 of 2000 took 0.111s
  training loss:		0.576465
  validation loss:		0.559244
  validation accuracy:		82.28 %
Epoch 801 of 2000 took 0.105s
  training loss:		0.581647
  validation loss:		0.545890
  validation accuracy:		81.63 %
Epoch 802 of 2000 took 0.105s
  training loss:		0.580803
  validation loss:		0.551450
  validation accuracy:		81.09 %
Epoch 803 of 2000 took 0.111s
  training loss:		0.581158
  validation loss:		0.544895
  validation accuracy:		81.41 %
Epoch 804 of 2000 took 0.105s
  training loss:		0.587261
  validation loss:		0.553445
  validation accuracy:		80.98 %
Epoch 805 of 2000 took 0.103s
  training loss:		0.585010
  validation loss:		0.542677
  validation accuracy:		81.52 %
Epoch 806 of 2000 took 0.103s
  training loss:		0.575436
  validation loss:		0.546332
  validation accuracy:		81.41 %
Epoch 807 of 2000 took 0.102s
  training loss:		0.571689
  validation loss:		0.543192
  validation accuracy:		81.20 %
Epoch 808 of 2000 took 0.110s
  training loss:		0.590439
  validation loss:		0.594661
  validation accuracy:		80.00 %
Epoch 809 of 2000 took 0.108s
  training loss:		0.578508
  validation loss:		0.552978
  validation accuracy:		82.17 %
Epoch 810 of 2000 took 0.103s
  training loss:		0.583228
  validation loss:		0.558296
  validation accuracy:		81.20 %
Epoch 811 of 2000 took 0.106s
  training loss:		0.593632
  validation loss:		0.558478
  validation accuracy:		81.85 %
Epoch 812 of 2000 took 0.103s
  training loss:		0.573211
  validation loss:		0.544438
  validation accuracy:		82.07 %
Epoch 813 of 2000 took 0.104s
  training loss:		0.587440
  validation loss:		0.592459
  validation accuracy:		80.00 %
Epoch 814 of 2000 took 0.101s
  training loss:		0.602857
  validation loss:		0.558397
  validation accuracy:		81.63 %
Epoch 815 of 2000 took 0.104s
  training loss:		0.581123
  validation loss:		0.554810
  validation accuracy:		81.20 %
Epoch 816 of 2000 took 0.109s
  training loss:		0.594618
  validation loss:		0.538430
  validation accuracy:		82.17 %
Epoch 817 of 2000 took 0.102s
  training loss:		0.582250
  validation loss:		0.543594
  validation accuracy:		82.39 %
Epoch 818 of 2000 took 0.107s
  training loss:		0.573364
  validation loss:		0.590400
  validation accuracy:		80.00 %
Epoch 819 of 2000 took 0.113s
  training loss:		0.567045
  validation loss:		0.541605
  validation accuracy:		82.17 %
Epoch 820 of 2000 took 0.101s
  training loss:		0.573461
  validation loss:		0.541721
  validation accuracy:		82.50 %
Epoch 821 of 2000 took 0.104s
  training loss:		0.572584
  validation loss:		0.585015
  validation accuracy:		80.33 %
Epoch 822 of 2000 took 0.101s
  training loss:		0.587365
  validation loss:		0.544626
  validation accuracy:		82.17 %
Epoch 823 of 2000 took 0.106s
  training loss:		0.583779
  validation loss:		0.556616
  validation accuracy:		81.30 %
Epoch 824 of 2000 took 0.109s
  training loss:		0.576815
  validation loss:		0.552377
  validation accuracy:		81.63 %
Epoch 825 of 2000 took 0.101s
  training loss:		0.583917
  validation loss:		0.564392
  validation accuracy:		82.07 %
Epoch 826 of 2000 took 0.105s
  training loss:		0.582470
  validation loss:		0.544926
  validation accuracy:		82.07 %
Epoch 827 of 2000 took 0.111s
  training loss:		0.586031
  validation loss:		0.550139
  validation accuracy:		82.28 %
Epoch 828 of 2000 took 0.105s
  training loss:		0.583304
  validation loss:		0.543419
  validation accuracy:		82.28 %
Epoch 829 of 2000 took 0.105s
  training loss:		0.588656
  validation loss:		0.544225
  validation accuracy:		82.17 %
Epoch 830 of 2000 took 0.101s
  training loss:		0.579005
  validation loss:		0.607494
  validation accuracy:		79.89 %
Epoch 831 of 2000 took 0.106s
  training loss:		0.581414
  validation loss:		0.550404
  validation accuracy:		81.09 %
Epoch 832 of 2000 took 0.107s
  training loss:		0.581828
  validation loss:		0.552460
  validation accuracy:		81.52 %
Epoch 833 of 2000 took 0.101s
  training loss:		0.573460
  validation loss:		0.557172
  validation accuracy:		81.41 %
Epoch 834 of 2000 took 0.106s
  training loss:		0.578454
  validation loss:		0.541966
  validation accuracy:		82.72 %
Epoch 835 of 2000 took 0.110s
  training loss:		0.584200
  validation loss:		0.561460
  validation accuracy:		82.39 %
Epoch 836 of 2000 took 0.101s
  training loss:		0.579147
  validation loss:		0.555620
  validation accuracy:		81.74 %
Epoch 837 of 2000 took 0.108s
  training loss:		0.570433
  validation loss:		0.541407
  validation accuracy:		82.07 %
Epoch 838 of 2000 took 0.101s
  training loss:		0.587594
  validation loss:		0.538240
  validation accuracy:		82.07 %
Epoch 839 of 2000 took 0.110s
  training loss:		0.573752
  validation loss:		0.552091
  validation accuracy:		81.63 %
Epoch 840 of 2000 took 0.107s
  training loss:		0.587599
  validation loss:		0.553160
  validation accuracy:		82.07 %
Epoch 841 of 2000 took 0.101s
  training loss:		0.584276
  validation loss:		0.546296
  validation accuracy:		82.07 %
Epoch 842 of 2000 took 0.107s
  training loss:		0.581451
  validation loss:		0.563196
  validation accuracy:		81.96 %
Epoch 843 of 2000 took 0.109s
  training loss:		0.575197
  validation loss:		0.537269
  validation accuracy:		82.28 %
Epoch 844 of 2000 took 0.102s
  training loss:		0.588304
  validation loss:		0.587408
  validation accuracy:		80.22 %
Epoch 845 of 2000 took 0.104s
  training loss:		0.581783
  validation loss:		0.545726
  validation accuracy:		82.17 %
Epoch 846 of 2000 took 0.101s
  training loss:		0.577063
  validation loss:		0.539532
  validation accuracy:		81.63 %
Epoch 847 of 2000 took 0.115s
  training loss:		0.577933
  validation loss:		0.540201
  validation accuracy:		82.39 %
Epoch 848 of 2000 took 0.105s
  training loss:		0.577288
  validation loss:		0.543356
  validation accuracy:		82.17 %
Epoch 849 of 2000 took 0.101s
  training loss:		0.571809
  validation loss:		0.536718
  validation accuracy:		81.96 %
Epoch 850 of 2000 took 0.106s
  training loss:		0.562251
  validation loss:		0.547497
  validation accuracy:		81.85 %
Epoch 851 of 2000 took 0.105s
  training loss:		0.574293
  validation loss:		0.538119
  validation accuracy:		82.39 %
Epoch 852 of 2000 took 0.105s
  training loss:		0.570063
  validation loss:		0.543109
  validation accuracy:		82.61 %
Epoch 853 of 2000 took 0.105s
  training loss:		0.569527
  validation loss:		0.537614
  validation accuracy:		82.50 %
Epoch 854 of 2000 took 0.105s
  training loss:		0.575407
  validation loss:		0.535688
  validation accuracy:		82.07 %
Epoch 855 of 2000 took 0.105s
  training loss:		0.570811
  validation loss:		0.541093
  validation accuracy:		82.07 %
Epoch 856 of 2000 took 0.109s
  training loss:		0.562820
  validation loss:		0.542051
  validation accuracy:		82.83 %
Epoch 857 of 2000 took 0.106s
  training loss:		0.567904
  validation loss:		0.546919
  validation accuracy:		82.61 %
Epoch 858 of 2000 took 0.106s
  training loss:		0.571896
  validation loss:		0.541371
  validation accuracy:		81.96 %
Epoch 859 of 2000 took 0.105s
  training loss:		0.578915
  validation loss:		0.536146
  validation accuracy:		82.39 %
Epoch 860 of 2000 took 0.105s
  training loss:		0.570181
  validation loss:		0.537831
  validation accuracy:		82.61 %
Epoch 861 of 2000 took 0.105s
  training loss:		0.566110
  validation loss:		0.551705
  validation accuracy:		82.93 %
Epoch 862 of 2000 took 0.105s
  training loss:		0.571530
  validation loss:		0.534637
  validation accuracy:		83.04 %
Epoch 863 of 2000 took 0.105s
  training loss:		0.571651
  validation loss:		0.542931
  validation accuracy:		82.39 %
Epoch 864 of 2000 took 0.105s
  training loss:		0.577579
  validation loss:		0.545009
  validation accuracy:		82.17 %
Epoch 865 of 2000 took 0.105s
  training loss:		0.571542
  validation loss:		0.550536
  validation accuracy:		82.07 %
Epoch 866 of 2000 took 0.109s
  training loss:		0.584616
  validation loss:		0.536663
  validation accuracy:		81.96 %
Epoch 867 of 2000 took 0.106s
  training loss:		0.567987
  validation loss:		0.542658
  validation accuracy:		82.72 %
Epoch 868 of 2000 took 0.105s
  training loss:		0.569905
  validation loss:		0.569658
  validation accuracy:		81.09 %
Epoch 869 of 2000 took 0.107s
  training loss:		0.571049
  validation loss:		0.545234
  validation accuracy:		82.72 %
Epoch 870 of 2000 took 0.105s
  training loss:		0.570403
  validation loss:		0.568295
  validation accuracy:		82.17 %
Epoch 871 of 2000 took 0.105s
  training loss:		0.569787
  validation loss:		0.533286
  validation accuracy:		82.61 %
Epoch 872 of 2000 took 0.105s
  training loss:		0.574056
  validation loss:		0.534357
  validation accuracy:		82.50 %
Epoch 873 of 2000 took 0.105s
  training loss:		0.586135
  validation loss:		0.547469
  validation accuracy:		82.93 %
Epoch 874 of 2000 took 0.105s
  training loss:		0.562835
  validation loss:		0.533529
  validation accuracy:		82.93 %
Epoch 875 of 2000 took 0.109s
  training loss:		0.573525
  validation loss:		0.534421
  validation accuracy:		82.61 %
Epoch 876 of 2000 took 0.105s
  training loss:		0.568609
  validation loss:		0.538101
  validation accuracy:		82.72 %
Epoch 877 of 2000 took 0.105s
  training loss:		0.561399
  validation loss:		0.537614
  validation accuracy:		83.37 %
Epoch 878 of 2000 took 0.105s
  training loss:		0.559541
  validation loss:		0.533939
  validation accuracy:		82.83 %
Epoch 879 of 2000 took 0.102s
  training loss:		0.559523
  validation loss:		0.552046
  validation accuracy:		82.17 %
Epoch 880 of 2000 took 0.105s
  training loss:		0.554239
  validation loss:		0.551537
  validation accuracy:		83.26 %
Epoch 881 of 2000 took 0.105s
  training loss:		0.559064
  validation loss:		0.531945
  validation accuracy:		82.83 %
Epoch 882 of 2000 took 0.105s
  training loss:		0.566240
  validation loss:		0.538535
  validation accuracy:		83.04 %
Epoch 883 of 2000 took 0.105s
  training loss:		0.564965
  validation loss:		0.559021
  validation accuracy:		81.74 %
Epoch 884 of 2000 took 0.105s
  training loss:		0.565653
  validation loss:		0.528853
  validation accuracy:		83.15 %
Epoch 885 of 2000 took 0.109s
  training loss:		0.561746
  validation loss:		0.534535
  validation accuracy:		83.15 %
Epoch 886 of 2000 took 0.105s
  training loss:		0.555767
  validation loss:		0.536193
  validation accuracy:		83.04 %
Epoch 887 of 2000 took 0.105s
  training loss:		0.568289
  validation loss:		0.533712
  validation accuracy:		82.93 %
Epoch 888 of 2000 took 0.103s
  training loss:		0.568219
  validation loss:		0.532711
  validation accuracy:		82.07 %
Epoch 889 of 2000 took 0.102s
  training loss:		0.566900
  validation loss:		0.536743
  validation accuracy:		83.15 %
Epoch 890 of 2000 took 0.105s
  training loss:		0.567350
  validation loss:		0.527319
  validation accuracy:		83.37 %
Epoch 891 of 2000 took 0.105s
  training loss:		0.561363
  validation loss:		0.541175
  validation accuracy:		82.72 %
Epoch 892 of 2000 took 0.105s
  training loss:		0.552904
  validation loss:		0.527248
  validation accuracy:		82.28 %
Epoch 893 of 2000 took 0.105s
  training loss:		0.560361
  validation loss:		0.526900
  validation accuracy:		82.83 %
Epoch 894 of 2000 took 0.109s
  training loss:		0.565087
  validation loss:		0.527282
  validation accuracy:		81.85 %
Epoch 895 of 2000 took 0.106s
  training loss:		0.555181
  validation loss:		0.542349
  validation accuracy:		82.61 %
Epoch 896 of 2000 took 0.106s
  training loss:		0.554506
  validation loss:		0.561330
  validation accuracy:		81.85 %
Epoch 897 of 2000 took 0.105s
  training loss:		0.571930
  validation loss:		0.525615
  validation accuracy:		82.39 %
Epoch 898 of 2000 took 0.101s
  training loss:		0.558798
  validation loss:		0.524196
  validation accuracy:		83.48 %
Epoch 899 of 2000 took 0.105s
  training loss:		0.556833
  validation loss:		0.540507
  validation accuracy:		83.04 %
Epoch 900 of 2000 took 0.105s
  training loss:		0.554503
  validation loss:		0.526296
  validation accuracy:		83.15 %
Epoch 901 of 2000 took 0.105s
  training loss:		0.561555
  validation loss:		0.527059
  validation accuracy:		83.15 %
Epoch 902 of 2000 took 0.105s
  training loss:		0.557954
  validation loss:		0.521736
  validation accuracy:		82.93 %
Epoch 903 of 2000 took 0.105s
  training loss:		0.556513
  validation loss:		0.522064
  validation accuracy:		83.26 %
Epoch 904 of 2000 took 0.109s
  training loss:		0.548170
  validation loss:		0.540847
  validation accuracy:		82.83 %
Epoch 905 of 2000 took 0.105s
  training loss:		0.558076
  validation loss:		0.528122
  validation accuracy:		82.61 %
Epoch 906 of 2000 took 0.105s
  training loss:		0.553595
  validation loss:		0.557667
  validation accuracy:		82.72 %
Epoch 907 of 2000 took 0.106s
  training loss:		0.543741
  validation loss:		0.583831
  validation accuracy:		82.39 %
Epoch 908 of 2000 took 0.101s
  training loss:		0.560583
  validation loss:		0.524775
  validation accuracy:		83.48 %
Epoch 909 of 2000 took 0.103s
  training loss:		0.547715
  validation loss:		0.521870
  validation accuracy:		82.50 %
Epoch 910 of 2000 took 0.105s
  training loss:		0.558216
  validation loss:		0.521579
  validation accuracy:		82.61 %
Epoch 911 of 2000 took 0.105s
  training loss:		0.551537
  validation loss:		0.518858
  validation accuracy:		83.37 %
Epoch 912 of 2000 took 0.105s
  training loss:		0.551321
  validation loss:		0.528083
  validation accuracy:		83.37 %
Epoch 913 of 2000 took 0.109s
  training loss:		0.549436
  validation loss:		0.538156
  validation accuracy:		82.50 %
Epoch 914 of 2000 took 0.106s
  training loss:		0.557121
  validation loss:		0.523857
  validation accuracy:		83.91 %
Epoch 915 of 2000 took 0.105s
  training loss:		0.556123
  validation loss:		0.523642
  validation accuracy:		83.04 %
Epoch 916 of 2000 took 0.105s
  training loss:		0.539560
  validation loss:		0.513594
  validation accuracy:		82.83 %
Epoch 917 of 2000 took 0.111s
  training loss:		0.553169
  validation loss:		0.514110
  validation accuracy:		83.48 %
Epoch 918 of 2000 took 0.103s
  training loss:		0.534593
  validation loss:		0.526928
  validation accuracy:		83.26 %
Epoch 919 of 2000 took 0.105s
  training loss:		0.540096
  validation loss:		0.520054
  validation accuracy:		83.59 %
Epoch 920 of 2000 took 0.108s
  training loss:		0.536088
  validation loss:		0.510169
  validation accuracy:		82.83 %
Epoch 921 of 2000 took 0.107s
  training loss:		0.548270
  validation loss:		0.537455
  validation accuracy:		83.15 %
Epoch 922 of 2000 took 0.106s
  training loss:		0.558855
  validation loss:		0.524662
  validation accuracy:		83.26 %
Epoch 923 of 2000 took 0.110s
  training loss:		0.543005
  validation loss:		0.514473
  validation accuracy:		83.15 %
Epoch 924 of 2000 took 0.106s
  training loss:		0.551520
  validation loss:		0.582668
  validation accuracy:		81.85 %
Epoch 925 of 2000 took 0.106s
  training loss:		0.552886
  validation loss:		0.519318
  validation accuracy:		83.15 %
Epoch 926 of 2000 took 0.104s
  training loss:		0.544679
  validation loss:		0.514121
  validation accuracy:		83.37 %
Epoch 927 of 2000 took 0.104s
  training loss:		0.550032
  validation loss:		0.515434
  validation accuracy:		83.15 %
Epoch 928 of 2000 took 0.102s
  training loss:		0.545354
  validation loss:		0.512018
  validation accuracy:		84.13 %
Epoch 929 of 2000 took 0.103s
  training loss:		0.542130
  validation loss:		0.509840
  validation accuracy:		83.26 %
Epoch 930 of 2000 took 0.106s
  training loss:		0.537434
  validation loss:		0.508991
  validation accuracy:		84.02 %
Epoch 931 of 2000 took 0.106s
  training loss:		0.531023
  validation loss:		0.536816
  validation accuracy:		82.72 %
Epoch 932 of 2000 took 0.110s
  training loss:		0.555162
  validation loss:		0.505438
  validation accuracy:		84.13 %
Epoch 933 of 2000 took 0.106s
  training loss:		0.536228
  validation loss:		0.507913
  validation accuracy:		82.83 %
Epoch 934 of 2000 took 0.106s
  training loss:		0.534487
  validation loss:		0.499060
  validation accuracy:		84.24 %
Epoch 935 of 2000 took 0.106s
  training loss:		0.539515
  validation loss:		0.506924
  validation accuracy:		83.70 %
Epoch 936 of 2000 took 0.105s
  training loss:		0.536988
  validation loss:		0.506622
  validation accuracy:		84.24 %
Epoch 937 of 2000 took 0.101s
  training loss:		0.534117
  validation loss:		0.509217
  validation accuracy:		83.15 %
Epoch 938 of 2000 took 0.107s
  training loss:		0.531063
  validation loss:		0.512155
  validation accuracy:		83.80 %
Epoch 939 of 2000 took 0.106s
  training loss:		0.531529
  validation loss:		0.516620
  validation accuracy:		83.15 %
Epoch 940 of 2000 took 0.106s
  training loss:		0.534232
  validation loss:		0.506061
  validation accuracy:		83.59 %
Epoch 941 of 2000 took 0.107s
  training loss:		0.522657
  validation loss:		0.508350
  validation accuracy:		84.46 %
Epoch 942 of 2000 took 0.109s
  training loss:		0.520424
  validation loss:		0.503868
  validation accuracy:		83.59 %
Epoch 943 of 2000 took 0.106s
  training loss:		0.530999
  validation loss:		0.517514
  validation accuracy:		83.91 %
Epoch 944 of 2000 took 0.106s
  training loss:		0.526240
  validation loss:		0.497513
  validation accuracy:		84.35 %
Epoch 945 of 2000 took 0.104s
  training loss:		0.525058
  validation loss:		0.497126
  validation accuracy:		84.57 %
Epoch 946 of 2000 took 0.101s
  training loss:		0.514768
  validation loss:		0.497954
  validation accuracy:		84.02 %
Epoch 947 of 2000 took 0.111s
  training loss:		0.522816
  validation loss:		0.490315
  validation accuracy:		84.89 %
Epoch 948 of 2000 took 0.107s
  training loss:		0.524107
  validation loss:		0.512237
  validation accuracy:		83.59 %
Epoch 949 of 2000 took 0.106s
  training loss:		0.513683
  validation loss:		0.489275
  validation accuracy:		83.59 %
Epoch 950 of 2000 took 0.106s
  training loss:		0.524004
  validation loss:		0.488533
  validation accuracy:		83.48 %
Epoch 951 of 2000 took 0.110s
  training loss:		0.521792
  validation loss:		0.488188
  validation accuracy:		83.70 %
Epoch 952 of 2000 took 0.106s
  training loss:		0.517459
  validation loss:		0.484928
  validation accuracy:		84.67 %
Epoch 953 of 2000 took 0.107s
  training loss:		0.514350
  validation loss:		0.485033
  validation accuracy:		84.67 %
Epoch 954 of 2000 took 0.106s
  training loss:		0.506544
  validation loss:		0.480616
  validation accuracy:		85.22 %
Epoch 955 of 2000 took 0.104s
  training loss:		0.503818
  validation loss:		0.479716
  validation accuracy:		85.22 %
Epoch 956 of 2000 took 0.111s
  training loss:		0.502476
  validation loss:		0.500149
  validation accuracy:		84.46 %
Epoch 957 of 2000 took 0.104s
  training loss:		0.507344
  validation loss:		0.475357
  validation accuracy:		85.00 %
Epoch 958 of 2000 took 0.103s
  training loss:		0.499567
  validation loss:		0.492036
  validation accuracy:		85.33 %
Epoch 959 of 2000 took 0.106s
  training loss:		0.499173
  validation loss:		0.480910
  validation accuracy:		84.57 %
Epoch 960 of 2000 took 0.109s
  training loss:		0.499751
  validation loss:		0.485487
  validation accuracy:		85.43 %
Epoch 961 of 2000 took 0.107s
  training loss:		0.501631
  validation loss:		0.473612
  validation accuracy:		85.33 %
Epoch 962 of 2000 took 0.106s
  training loss:		0.492767
  validation loss:		0.480071
  validation accuracy:		84.78 %
Epoch 963 of 2000 took 0.106s
  training loss:		0.504885
  validation loss:		0.474976
  validation accuracy:		85.22 %
Epoch 964 of 2000 took 0.109s
  training loss:		0.493783
  validation loss:		0.474543
  validation accuracy:		85.98 %
Epoch 965 of 2000 took 0.108s
  training loss:		0.491684
  validation loss:		0.475782
  validation accuracy:		85.11 %
Epoch 966 of 2000 took 0.101s
  training loss:		0.496321
  validation loss:		0.469308
  validation accuracy:		85.11 %
Epoch 967 of 2000 took 0.106s
  training loss:		0.480920
  validation loss:		0.471695
  validation accuracy:		85.54 %
Epoch 968 of 2000 took 0.109s
  training loss:		0.489026
  validation loss:		0.459992
  validation accuracy:		84.89 %
Epoch 969 of 2000 took 0.106s
  training loss:		0.489671
  validation loss:		0.468871
  validation accuracy:		85.76 %
Epoch 970 of 2000 took 0.110s
  training loss:		0.487337
  validation loss:		0.459649
  validation accuracy:		85.11 %
Epoch 971 of 2000 took 0.105s
  training loss:		0.493018
  validation loss:		0.459081
  validation accuracy:		85.65 %
Epoch 972 of 2000 took 0.105s
  training loss:		0.475125
  validation loss:		0.448692
  validation accuracy:		86.30 %
Epoch 973 of 2000 took 0.106s
  training loss:		0.477022
  validation loss:		0.452173
  validation accuracy:		85.11 %
Epoch 974 of 2000 took 0.101s
  training loss:		0.468223
  validation loss:		0.458964
  validation accuracy:		85.87 %
Epoch 975 of 2000 took 0.106s
  training loss:		0.465683
  validation loss:		0.449111
  validation accuracy:		86.30 %
Epoch 976 of 2000 took 0.110s
  training loss:		0.470460
  validation loss:		0.446093
  validation accuracy:		86.63 %
Epoch 977 of 2000 took 0.101s
  training loss:		0.476062
  validation loss:		0.467430
  validation accuracy:		85.43 %
Epoch 978 of 2000 took 0.105s
  training loss:		0.477642
  validation loss:		0.447097
  validation accuracy:		86.09 %
Epoch 979 of 2000 took 0.109s
  training loss:		0.471245
  validation loss:		0.445149
  validation accuracy:		86.63 %
Epoch 980 of 2000 took 0.106s
  training loss:		0.469750
  validation loss:		0.469671
  validation accuracy:		85.33 %
Epoch 981 of 2000 took 0.106s
  training loss:		0.465294
  validation loss:		0.440308
  validation accuracy:		86.74 %
Epoch 982 of 2000 took 0.105s
  training loss:		0.459868
  validation loss:		0.452010
  validation accuracy:		86.52 %
Epoch 983 of 2000 took 0.111s
  training loss:		0.460502
  validation loss:		0.455692
  validation accuracy:		86.20 %
Epoch 984 of 2000 took 0.101s
  training loss:		0.457610
  validation loss:		0.461253
  validation accuracy:		85.87 %
Epoch 985 of 2000 took 0.104s
  training loss:		0.469967
  validation loss:		0.438386
  validation accuracy:		87.17 %
Epoch 986 of 2000 took 0.101s
  training loss:		0.450785
  validation loss:		0.428714
  validation accuracy:		87.28 %
Epoch 987 of 2000 took 0.104s
  training loss:		0.453186
  validation loss:		0.434197
  validation accuracy:		87.07 %
Epoch 988 of 2000 took 0.105s
  training loss:		0.452724
  validation loss:		0.456280
  validation accuracy:		86.09 %
Epoch 989 of 2000 took 0.109s
  training loss:		0.453918
  validation loss:		0.431916
  validation accuracy:		87.17 %
Epoch 990 of 2000 took 0.105s
  training loss:		0.453225
  validation loss:		0.434753
  validation accuracy:		87.28 %
Epoch 991 of 2000 took 0.105s
  training loss:		0.453668
  validation loss:		0.428073
  validation accuracy:		86.41 %
Epoch 992 of 2000 took 0.106s
  training loss:		0.451873
  validation loss:		0.432446
  validation accuracy:		86.96 %
Epoch 993 of 2000 took 0.102s
  training loss:		0.451077
  validation loss:		0.452042
  validation accuracy:		86.30 %
Epoch 994 of 2000 took 0.104s
  training loss:		0.446277
  validation loss:		0.421772
  validation accuracy:		87.39 %
Epoch 995 of 2000 took 0.110s
  training loss:		0.444248
  validation loss:		0.424128
  validation accuracy:		87.39 %
Epoch 996 of 2000 took 0.102s
  training loss:		0.442531
  validation loss:		0.425321
  validation accuracy:		88.04 %
Epoch 997 of 2000 took 0.103s
  training loss:		0.447777
  validation loss:		0.421032
  validation accuracy:		87.50 %
Epoch 998 of 2000 took 0.109s
  training loss:		0.440078
  validation loss:		0.421582
  validation accuracy:		87.17 %
Epoch 999 of 2000 took 0.105s
  training loss:		0.444900
  validation loss:		0.451453
  validation accuracy:		86.41 %
Epoch 1000 of 2000 took 0.105s
  training loss:		0.428252
  validation loss:		0.414635
  validation accuracy:		87.83 %
Epoch 1001 of 2000 took 0.105s
  training loss:		0.434649
  validation loss:		0.419733
  validation accuracy:		87.28 %
Epoch 1002 of 2000 took 0.109s
  training loss:		0.435222
  validation loss:		0.415044
  validation accuracy:		87.72 %
Epoch 1003 of 2000 took 0.104s
  training loss:		0.431108
  validation loss:		0.411175
  validation accuracy:		88.04 %
Epoch 1004 of 2000 took 0.102s
  training loss:		0.439794
  validation loss:		0.428014
  validation accuracy:		87.50 %
Epoch 1005 of 2000 took 0.111s
  training loss:		0.428346
  validation loss:		0.416647
  validation accuracy:		87.39 %
Epoch 1006 of 2000 took 0.103s
  training loss:		0.427170
  validation loss:		0.426275
  validation accuracy:		87.39 %
Epoch 1007 of 2000 took 0.102s
  training loss:		0.424455
  validation loss:		0.420002
  validation accuracy:		87.93 %
Epoch 1008 of 2000 took 0.109s
  training loss:		0.431470
  validation loss:		0.408919
  validation accuracy:		88.70 %
Epoch 1009 of 2000 took 0.105s
  training loss:		0.427770
  validation loss:		0.406659
  validation accuracy:		88.26 %
Epoch 1010 of 2000 took 0.106s
  training loss:		0.430937
  validation loss:		0.413527
  validation accuracy:		87.83 %
Epoch 1011 of 2000 took 0.104s
  training loss:		0.431619
  validation loss:		0.401609
  validation accuracy:		88.48 %
Epoch 1012 of 2000 took 0.108s
  training loss:		0.425851
  validation loss:		0.413919
  validation accuracy:		87.72 %
Epoch 1013 of 2000 took 0.108s
  training loss:		0.418982
  validation loss:		0.409884
  validation accuracy:		88.15 %
Epoch 1014 of 2000 took 0.101s
  training loss:		0.422102
  validation loss:		0.405004
  validation accuracy:		88.37 %
Epoch 1015 of 2000 took 0.104s
  training loss:		0.417199
  validation loss:		0.413278
  validation accuracy:		88.26 %
Epoch 1016 of 2000 took 0.101s
  training loss:		0.417246
  validation loss:		0.405473
  validation accuracy:		88.04 %
Epoch 1017 of 2000 took 0.110s
  training loss:		0.416736
  validation loss:		0.427198
  validation accuracy:		87.17 %
Epoch 1018 of 2000 took 0.106s
  training loss:		0.410593
  validation loss:		0.405513
  validation accuracy:		88.59 %
Epoch 1019 of 2000 took 0.105s
  training loss:		0.419960
  validation loss:		0.420989
  validation accuracy:		87.17 %
Epoch 1020 of 2000 took 0.105s
  training loss:		0.427239
  validation loss:		0.409684
  validation accuracy:		88.37 %
Epoch 1021 of 2000 took 0.103s
  training loss:		0.418006
  validation loss:		0.404131
  validation accuracy:		88.70 %
Epoch 1022 of 2000 took 0.104s
  training loss:		0.413771
  validation loss:		0.402781
  validation accuracy:		88.15 %
Epoch 1023 of 2000 took 0.101s
  training loss:		0.415169
  validation loss:		0.417860
  validation accuracy:		87.28 %
Epoch 1024 of 2000 took 0.106s
  training loss:		0.420625
  validation loss:		0.411641
  validation accuracy:		88.26 %
Epoch 1025 of 2000 took 0.109s
  training loss:		0.408696
  validation loss:		0.399795
  validation accuracy:		88.37 %
Epoch 1026 of 2000 took 0.102s
  training loss:		0.411867
  validation loss:		0.402632
  validation accuracy:		88.70 %
Epoch 1027 of 2000 took 0.109s
  training loss:		0.412808
  validation loss:		0.409725
  validation accuracy:		88.26 %
Epoch 1028 of 2000 took 0.105s
  training loss:		0.412593
  validation loss:		0.401689
  validation accuracy:		88.37 %
Epoch 1029 of 2000 took 0.105s
  training loss:		0.405564
  validation loss:		0.400501
  validation accuracy:		88.59 %
Epoch 1030 of 2000 took 0.102s
  training loss:		0.407734
  validation loss:		0.399716
  validation accuracy:		88.48 %
Epoch 1031 of 2000 took 0.098s
  training loss:		0.403324
  validation loss:		0.395285
  validation accuracy:		89.13 %
Epoch 1032 of 2000 took 0.097s
  training loss:		0.402882
  validation loss:		0.395949
  validation accuracy:		88.70 %
Epoch 1033 of 2000 took 0.097s
  training loss:		0.400216
  validation loss:		0.399852
  validation accuracy:		88.70 %
Epoch 1034 of 2000 took 0.097s
  training loss:		0.403696
  validation loss:		0.400231
  validation accuracy:		88.59 %
Epoch 1035 of 2000 took 0.097s
  training loss:		0.399662
  validation loss:		0.391243
  validation accuracy:		89.24 %
Epoch 1036 of 2000 took 0.097s
  training loss:		0.401987
  validation loss:		0.398944
  validation accuracy:		88.59 %
Epoch 1037 of 2000 took 0.101s
  training loss:		0.395482
  validation loss:		0.406705
  validation accuracy:		88.15 %
Epoch 1038 of 2000 took 0.097s
  training loss:		0.401139
  validation loss:		0.386200
  validation accuracy:		88.91 %
Epoch 1039 of 2000 took 0.098s
  training loss:		0.397432
  validation loss:		0.400700
  validation accuracy:		88.80 %
Epoch 1040 of 2000 took 0.097s
  training loss:		0.403164
  validation loss:		0.396721
  validation accuracy:		88.80 %
Epoch 1041 of 2000 took 0.097s
  training loss:		0.392163
  validation loss:		0.399814
  validation accuracy:		88.59 %
Epoch 1042 of 2000 took 0.097s
  training loss:		0.393054
  validation loss:		0.385333
  validation accuracy:		89.02 %
Epoch 1043 of 2000 took 0.097s
  training loss:		0.384714
  validation loss:		0.401165
  validation accuracy:		88.26 %
Epoch 1044 of 2000 took 0.097s
  training loss:		0.388972
  validation loss:		0.391950
  validation accuracy:		89.02 %
Epoch 1045 of 2000 took 0.097s
  training loss:		0.395791
  validation loss:		0.387175
  validation accuracy:		89.13 %
Epoch 1046 of 2000 took 0.097s
  training loss:		0.387132
  validation loss:		0.388587
  validation accuracy:		88.80 %
Epoch 1047 of 2000 took 0.101s
  training loss:		0.390907
  validation loss:		0.387783
  validation accuracy:		89.02 %
Epoch 1048 of 2000 took 0.097s
  training loss:		0.388533
  validation loss:		0.394325
  validation accuracy:		88.48 %
Epoch 1049 of 2000 took 0.097s
  training loss:		0.384888
  validation loss:		0.410070
  validation accuracy:		87.61 %
Epoch 1050 of 2000 took 0.097s
  training loss:		0.385976
  validation loss:		0.388890
  validation accuracy:		88.91 %
Epoch 1051 of 2000 took 0.097s
  training loss:		0.389680
  validation loss:		0.381633
  validation accuracy:		88.59 %
Epoch 1052 of 2000 took 0.097s
  training loss:		0.391076
  validation loss:		0.390819
  validation accuracy:		88.70 %
Epoch 1053 of 2000 took 0.097s
  training loss:		0.387779
  validation loss:		0.379173
  validation accuracy:		89.02 %
Epoch 1054 of 2000 took 0.097s
  training loss:		0.384501
  validation loss:		0.389028
  validation accuracy:		88.80 %
Epoch 1055 of 2000 took 0.097s
  training loss:		0.380962
  validation loss:		0.391856
  validation accuracy:		88.04 %
Epoch 1056 of 2000 took 0.097s
  training loss:		0.376833
  validation loss:		0.381348
  validation accuracy:		88.59 %
Epoch 1057 of 2000 took 0.104s
  training loss:		0.388996
  validation loss:		0.399176
  validation accuracy:		87.83 %
Epoch 1058 of 2000 took 0.104s
  training loss:		0.381210
  validation loss:		0.383833
  validation accuracy:		88.80 %
Epoch 1059 of 2000 took 0.103s
  training loss:		0.381631
  validation loss:		0.392501
  validation accuracy:		87.93 %
Epoch 1060 of 2000 took 0.103s
  training loss:		0.378990
  validation loss:		0.380669
  validation accuracy:		88.59 %
Epoch 1061 of 2000 took 0.103s
  training loss:		0.383673
  validation loss:		0.388632
  validation accuracy:		88.70 %
Epoch 1062 of 2000 took 0.103s
  training loss:		0.383312
  validation loss:		0.379519
  validation accuracy:		88.59 %
Epoch 1063 of 2000 took 0.103s
  training loss:		0.383662
  validation loss:		0.389873
  validation accuracy:		88.15 %
Epoch 1064 of 2000 took 0.103s
  training loss:		0.381779
  validation loss:		0.378777
  validation accuracy:		88.91 %
Epoch 1065 of 2000 took 0.103s
  training loss:		0.373075
  validation loss:		0.383675
  validation accuracy:		88.59 %
Epoch 1066 of 2000 took 0.097s
  training loss:		0.373316
  validation loss:		0.383050
  validation accuracy:		88.48 %
Epoch 1067 of 2000 took 0.100s
  training loss:		0.373093
  validation loss:		0.375527
  validation accuracy:		89.02 %
Epoch 1068 of 2000 took 0.097s
  training loss:		0.374264
  validation loss:		0.382662
  validation accuracy:		88.91 %
Epoch 1069 of 2000 took 0.097s
  training loss:		0.369848
  validation loss:		0.376779
  validation accuracy:		89.24 %
Epoch 1070 of 2000 took 0.098s
  training loss:		0.369880
  validation loss:		0.375107
  validation accuracy:		88.59 %
Epoch 1071 of 2000 took 0.097s
  training loss:		0.374469
  validation loss:		0.373816
  validation accuracy:		89.46 %
Epoch 1072 of 2000 took 0.097s
  training loss:		0.371380
  validation loss:		0.374834
  validation accuracy:		88.59 %
Epoch 1073 of 2000 took 0.097s
  training loss:		0.371125
  validation loss:		0.382867
  validation accuracy:		88.70 %
Epoch 1074 of 2000 took 0.097s
  training loss:		0.373056
  validation loss:		0.379303
  validation accuracy:		88.37 %
Epoch 1075 of 2000 took 0.097s
  training loss:		0.370649
  validation loss:		0.377010
  validation accuracy:		88.91 %
Epoch 1076 of 2000 took 0.097s
  training loss:		0.367296
  validation loss:		0.368834
  validation accuracy:		89.02 %
Epoch 1077 of 2000 took 0.100s
  training loss:		0.364919
  validation loss:		0.380809
  validation accuracy:		88.48 %
Epoch 1078 of 2000 took 0.097s
  training loss:		0.359478
  validation loss:		0.378973
  validation accuracy:		88.70 %
Epoch 1079 of 2000 took 0.097s
  training loss:		0.364164
  validation loss:		0.368150
  validation accuracy:		89.35 %
Epoch 1080 of 2000 took 0.097s
  training loss:		0.368192
  validation loss:		0.400130
  validation accuracy:		87.93 %
Epoch 1081 of 2000 took 0.097s
  training loss:		0.364043
  validation loss:		0.377270
  validation accuracy:		88.70 %
Epoch 1082 of 2000 took 0.097s
  training loss:		0.369532
  validation loss:		0.377015
  validation accuracy:		88.91 %
Epoch 1083 of 2000 took 0.097s
  training loss:		0.359942
  validation loss:		0.371838
  validation accuracy:		89.13 %
Epoch 1084 of 2000 took 0.097s
  training loss:		0.363438
  validation loss:		0.374643
  validation accuracy:		89.24 %
Epoch 1085 of 2000 took 0.097s
  training loss:		0.354772
  validation loss:		0.389125
  validation accuracy:		88.37 %
Epoch 1086 of 2000 took 0.097s
  training loss:		0.357615
  validation loss:		0.372979
  validation accuracy:		88.70 %
Epoch 1087 of 2000 took 0.099s
  training loss:		0.365862
  validation loss:		0.400339
  validation accuracy:		88.15 %
Epoch 1088 of 2000 took 0.098s
  training loss:		0.360362
  validation loss:		0.371667
  validation accuracy:		89.02 %
Epoch 1089 of 2000 took 0.097s
  training loss:		0.358784
  validation loss:		0.364347
  validation accuracy:		88.59 %
Epoch 1090 of 2000 took 0.097s
  training loss:		0.349969
  validation loss:		0.369188
  validation accuracy:		88.80 %
Epoch 1091 of 2000 took 0.097s
  training loss:		0.362069
  validation loss:		0.374587
  validation accuracy:		89.02 %
Epoch 1092 of 2000 took 0.098s
  training loss:		0.360185
  validation loss:		0.372282
  validation accuracy:		89.35 %
Epoch 1093 of 2000 took 0.097s
  training loss:		0.359516
  validation loss:		0.365882
  validation accuracy:		89.13 %
Epoch 1094 of 2000 took 0.097s
  training loss:		0.353822
  validation loss:		0.371226
  validation accuracy:		89.35 %
Epoch 1095 of 2000 took 0.097s
  training loss:		0.356314
  validation loss:		0.367913
  validation accuracy:		89.24 %
Epoch 1096 of 2000 took 0.097s
  training loss:		0.352035
  validation loss:		0.367970
  validation accuracy:		89.13 %
Epoch 1097 of 2000 took 0.097s
  training loss:		0.356480
  validation loss:		0.361593
  validation accuracy:		89.46 %
Epoch 1098 of 2000 took 0.100s
  training loss:		0.353096
  validation loss:		0.363584
  validation accuracy:		88.80 %
Epoch 1099 of 2000 took 0.097s
  training loss:		0.352458
  validation loss:		0.374068
  validation accuracy:		89.35 %
Epoch 1100 of 2000 took 0.097s
  training loss:		0.352006
  validation loss:		0.371174
  validation accuracy:		89.57 %
Epoch 1101 of 2000 took 0.098s
  training loss:		0.350445
  validation loss:		0.367508
  validation accuracy:		89.02 %
Epoch 1102 of 2000 took 0.097s
  training loss:		0.351671
  validation loss:		0.378217
  validation accuracy:		89.24 %
Epoch 1103 of 2000 took 0.097s
  training loss:		0.355954
  validation loss:		0.393248
  validation accuracy:		88.48 %
Epoch 1104 of 2000 took 0.097s
  training loss:		0.351780
  validation loss:		0.370097
  validation accuracy:		88.80 %
Epoch 1105 of 2000 took 0.097s
  training loss:		0.349623
  validation loss:		0.360265
  validation accuracy:		89.57 %
Epoch 1106 of 2000 took 0.097s
  training loss:		0.351808
  validation loss:		0.374899
  validation accuracy:		89.02 %
Epoch 1107 of 2000 took 0.097s
  training loss:		0.344333
  validation loss:		0.374822
  validation accuracy:		88.91 %
Epoch 1108 of 2000 took 0.100s
  training loss:		0.338100
  validation loss:		0.361069
  validation accuracy:		88.70 %
Epoch 1109 of 2000 took 0.097s
  training loss:		0.349137
  validation loss:		0.369389
  validation accuracy:		89.02 %
Epoch 1110 of 2000 took 0.097s
  training loss:		0.348107
  validation loss:		0.362971
  validation accuracy:		89.46 %
Epoch 1111 of 2000 took 0.097s
  training loss:		0.342736
  validation loss:		0.372059
  validation accuracy:		89.24 %
Epoch 1112 of 2000 took 0.097s
  training loss:		0.347749
  validation loss:		0.367508
  validation accuracy:		89.24 %
Epoch 1113 of 2000 took 0.097s
  training loss:		0.339463
  validation loss:		0.358704
  validation accuracy:		88.59 %
Epoch 1114 of 2000 took 0.097s
  training loss:		0.342195
  validation loss:		0.369089
  validation accuracy:		89.13 %
Epoch 1115 of 2000 took 0.097s
  training loss:		0.343231
  validation loss:		0.361670
  validation accuracy:		89.35 %
Epoch 1116 of 2000 took 0.097s
  training loss:		0.343653
  validation loss:		0.367912
  validation accuracy:		88.59 %
Epoch 1117 of 2000 took 0.097s
  training loss:		0.348279
  validation loss:		0.365109
  validation accuracy:		89.24 %
Epoch 1118 of 2000 took 0.100s
  training loss:		0.347843
  validation loss:		0.379861
  validation accuracy:		89.13 %
Epoch 1119 of 2000 took 0.097s
  training loss:		0.342539
  validation loss:		0.376670
  validation accuracy:		88.26 %
Epoch 1120 of 2000 took 0.097s
  training loss:		0.345062
  validation loss:		0.360534
  validation accuracy:		89.46 %
Epoch 1121 of 2000 took 0.097s
  training loss:		0.338668
  validation loss:		0.364091
  validation accuracy:		89.35 %
Epoch 1122 of 2000 took 0.097s
  training loss:		0.338389
  validation loss:		0.366705
  validation accuracy:		89.35 %
Epoch 1123 of 2000 took 0.097s
  training loss:		0.336738
  validation loss:		0.357926
  validation accuracy:		89.57 %
Epoch 1124 of 2000 took 0.099s
  training loss:		0.343622
  validation loss:		0.363871
  validation accuracy:		89.67 %
Epoch 1125 of 2000 took 0.100s
  training loss:		0.328246
  validation loss:		0.366764
  validation accuracy:		88.91 %
Epoch 1126 of 2000 took 0.100s
  training loss:		0.342469
  validation loss:		0.360003
  validation accuracy:		88.80 %
Epoch 1127 of 2000 took 0.098s
  training loss:		0.337627
  validation loss:		0.358898
  validation accuracy:		89.46 %
Epoch 1128 of 2000 took 0.098s
  training loss:		0.335753
  validation loss:		0.373129
  validation accuracy:		88.48 %
Epoch 1129 of 2000 took 0.099s
  training loss:		0.338579
  validation loss:		0.377013
  validation accuracy:		89.13 %
Epoch 1130 of 2000 took 0.097s
  training loss:		0.334857
  validation loss:		0.362703
  validation accuracy:		88.59 %
Epoch 1131 of 2000 took 0.097s
  training loss:		0.339251
  validation loss:		0.353276
  validation accuracy:		89.78 %
Epoch 1132 of 2000 took 0.097s
  training loss:		0.334643
  validation loss:		0.361486
  validation accuracy:		89.24 %
Epoch 1133 of 2000 took 0.097s
  training loss:		0.341122
  validation loss:		0.367561
  validation accuracy:		89.02 %
Epoch 1134 of 2000 took 0.097s
  training loss:		0.335816
  validation loss:		0.357128
  validation accuracy:		89.89 %
Epoch 1135 of 2000 took 0.097s
  training loss:		0.330609
  validation loss:		0.394102
  validation accuracy:		88.59 %
Epoch 1136 of 2000 took 0.097s
  training loss:		0.336481
  validation loss:		0.355757
  validation accuracy:		89.24 %
Epoch 1137 of 2000 took 0.097s
  training loss:		0.337647
  validation loss:		0.354770
  validation accuracy:		89.46 %
Epoch 1138 of 2000 took 0.097s
  training loss:		0.332250
  validation loss:		0.369060
  validation accuracy:		89.13 %
Epoch 1139 of 2000 took 0.101s
  training loss:		0.327678
  validation loss:		0.368832
  validation accuracy:		88.80 %
Epoch 1140 of 2000 took 0.097s
  training loss:		0.331412
  validation loss:		0.363507
  validation accuracy:		88.91 %
Epoch 1141 of 2000 took 0.097s
  training loss:		0.326806
  validation loss:		0.356436
  validation accuracy:		89.02 %
Epoch 1142 of 2000 took 0.097s
  training loss:		0.331586
  validation loss:		0.363136
  validation accuracy:		88.80 %
Epoch 1143 of 2000 took 0.097s
  training loss:		0.332633
  validation loss:		0.366829
  validation accuracy:		88.91 %
Epoch 1144 of 2000 took 0.097s
  training loss:		0.332383
  validation loss:		0.362356
  validation accuracy:		88.80 %
Epoch 1145 of 2000 took 0.097s
  training loss:		0.336251
  validation loss:		0.360614
  validation accuracy:		88.70 %
Epoch 1146 of 2000 took 0.097s
  training loss:		0.333109
  validation loss:		0.390149
  validation accuracy:		88.70 %
Epoch 1147 of 2000 took 0.097s
  training loss:		0.332150
  validation loss:		0.376630
  validation accuracy:		88.91 %
Epoch 1148 of 2000 took 0.097s
  training loss:		0.326797
  validation loss:		0.363353
  validation accuracy:		88.91 %
Epoch 1149 of 2000 took 0.100s
  training loss:		0.328416
  validation loss:		0.389816
  validation accuracy:		88.37 %
Epoch 1150 of 2000 took 0.097s
  training loss:		0.332683
  validation loss:		0.347331
  validation accuracy:		89.67 %
Epoch 1151 of 2000 took 0.097s
  training loss:		0.330660
  validation loss:		0.362080
  validation accuracy:		89.24 %
Epoch 1152 of 2000 took 0.097s
  training loss:		0.328834
  validation loss:		0.349943
  validation accuracy:		89.46 %
Epoch 1153 of 2000 took 0.097s
  training loss:		0.332388
  validation loss:		0.360205
  validation accuracy:		89.57 %
Epoch 1154 of 2000 took 0.097s
  training loss:		0.328659
  validation loss:		0.355571
  validation accuracy:		89.46 %
Epoch 1155 of 2000 took 0.097s
  training loss:		0.327044
  validation loss:		0.368779
  validation accuracy:		88.91 %
Epoch 1156 of 2000 took 0.097s
  training loss:		0.324319
  validation loss:		0.383904
  validation accuracy:		88.70 %
Epoch 1157 of 2000 took 0.097s
  training loss:		0.328719
  validation loss:		0.377677
  validation accuracy:		88.04 %
Epoch 1158 of 2000 took 0.097s
  training loss:		0.331087
  validation loss:		0.362764
  validation accuracy:		88.80 %
Epoch 1159 of 2000 took 0.100s
  training loss:		0.324618
  validation loss:		0.361678
  validation accuracy:		88.80 %
Epoch 1160 of 2000 took 0.098s
  training loss:		0.332628
  validation loss:		0.359309
  validation accuracy:		89.46 %
Epoch 1161 of 2000 took 0.097s
  training loss:		0.329586
  validation loss:		0.353042
  validation accuracy:		89.57 %
Epoch 1162 of 2000 took 0.097s
  training loss:		0.324581
  validation loss:		0.373846
  validation accuracy:		88.37 %
Epoch 1163 of 2000 took 0.097s
  training loss:		0.324012
  validation loss:		0.358497
  validation accuracy:		89.24 %
Epoch 1164 of 2000 took 0.097s
  training loss:		0.330525
  validation loss:		0.353597
  validation accuracy:		89.13 %
Epoch 1165 of 2000 took 0.097s
  training loss:		0.325788
  validation loss:		0.358845
  validation accuracy:		89.13 %
Epoch 1166 of 2000 took 0.097s
  training loss:		0.320265
  validation loss:		0.351473
  validation accuracy:		89.46 %
Epoch 1167 of 2000 took 0.097s
  training loss:		0.319925
  validation loss:		0.356779
  validation accuracy:		88.91 %
Epoch 1168 of 2000 took 0.097s
  training loss:		0.326466
  validation loss:		0.366418
  validation accuracy:		89.02 %
Epoch 1169 of 2000 took 0.097s
  training loss:		0.328854
  validation loss:		0.365523
  validation accuracy:		89.13 %
Epoch 1170 of 2000 took 0.101s
  training loss:		0.324221
  validation loss:		0.353659
  validation accuracy:		89.57 %
Epoch 1171 of 2000 took 0.097s
  training loss:		0.327556
  validation loss:		0.353629
  validation accuracy:		89.57 %
Epoch 1172 of 2000 took 0.097s
  training loss:		0.322704
  validation loss:		0.354166
  validation accuracy:		89.35 %
Epoch 1173 of 2000 took 0.097s
  training loss:		0.318109
  validation loss:		0.353842
  validation accuracy:		89.67 %
Epoch 1174 of 2000 took 0.097s
  training loss:		0.320208
  validation loss:		0.360302
  validation accuracy:		89.24 %
Epoch 1175 of 2000 took 0.097s
  training loss:		0.324581
  validation loss:		0.352661
  validation accuracy:		89.24 %
Epoch 1176 of 2000 took 0.097s
  training loss:		0.320083
  validation loss:		0.371893
  validation accuracy:		89.13 %
Epoch 1177 of 2000 took 0.097s
  training loss:		0.322772
  validation loss:		0.372295
  validation accuracy:		89.13 %
Epoch 1178 of 2000 took 0.097s
  training loss:		0.322941
  validation loss:		0.369118
  validation accuracy:		89.24 %
Epoch 1179 of 2000 took 0.097s
  training loss:		0.334278
  validation loss:		0.360631
  validation accuracy:		89.24 %
Epoch 1180 of 2000 took 0.100s
  training loss:		0.318546
  validation loss:		0.370985
  validation accuracy:		89.24 %
Epoch 1181 of 2000 took 0.097s
  training loss:		0.323101
  validation loss:		0.351861
  validation accuracy:		89.46 %
Epoch 1182 of 2000 took 0.097s
  training loss:		0.319196
  validation loss:		0.352194
  validation accuracy:		90.00 %
Epoch 1183 of 2000 took 0.097s
  training loss:		0.322563
  validation loss:		0.347745
  validation accuracy:		90.00 %
Epoch 1184 of 2000 took 0.097s
  training loss:		0.323195
  validation loss:		0.361971
  validation accuracy:		89.24 %
Epoch 1185 of 2000 took 0.097s
  training loss:		0.324082
  validation loss:		0.352639
  validation accuracy:		89.78 %
Epoch 1186 of 2000 took 0.097s
  training loss:		0.324670
  validation loss:		0.359535
  validation accuracy:		89.02 %
Epoch 1187 of 2000 took 0.097s
  training loss:		0.318732
  validation loss:		0.356373
  validation accuracy:		89.89 %
Epoch 1188 of 2000 took 0.097s
  training loss:		0.324388
  validation loss:		0.363671
  validation accuracy:		89.24 %
Epoch 1189 of 2000 took 0.097s
  training loss:		0.318198
  validation loss:		0.369052
  validation accuracy:		89.24 %
Epoch 1190 of 2000 took 0.100s
  training loss:		0.320092
  validation loss:		0.367144
  validation accuracy:		89.46 %
Epoch 1191 of 2000 took 0.097s
  training loss:		0.317347
  validation loss:		0.360328
  validation accuracy:		89.35 %
Epoch 1192 of 2000 took 0.097s
  training loss:		0.328623
  validation loss:		0.347783
  validation accuracy:		89.57 %
Epoch 1193 of 2000 took 0.097s
  training loss:		0.315695
  validation loss:		0.353478
  validation accuracy:		89.89 %
Epoch 1194 of 2000 took 0.098s
  training loss:		0.315756
  validation loss:		0.355426
  validation accuracy:		89.13 %
Epoch 1195 of 2000 took 0.097s
  training loss:		0.321604
  validation loss:		0.370472
  validation accuracy:		87.93 %
Epoch 1196 of 2000 took 0.097s
  training loss:		0.321983
  validation loss:		0.353426
  validation accuracy:		89.35 %
Epoch 1197 of 2000 took 0.097s
  training loss:		0.318193
  validation loss:		0.346247
  validation accuracy:		89.89 %
Epoch 1198 of 2000 took 0.100s
  training loss:		0.317291
  validation loss:		0.361080
  validation accuracy:		89.46 %
Epoch 1199 of 2000 took 0.100s
  training loss:		0.313616
  validation loss:		0.353274
  validation accuracy:		89.78 %
Epoch 1200 of 2000 took 0.102s
  training loss:		0.315576
  validation loss:		0.360063
  validation accuracy:		89.46 %
Epoch 1201 of 2000 took 0.101s
  training loss:		0.314006
  validation loss:		0.346979
  validation accuracy:		90.00 %
Epoch 1202 of 2000 took 0.100s
  training loss:		0.313310
  validation loss:		0.346282
  validation accuracy:		90.00 %
Epoch 1203 of 2000 took 0.100s
  training loss:		0.312126
  validation loss:		0.361804
  validation accuracy:		89.13 %
Epoch 1204 of 2000 took 0.100s
  training loss:		0.319553
  validation loss:		0.357207
  validation accuracy:		89.57 %
Epoch 1205 of 2000 took 0.100s
  training loss:		0.314353
  validation loss:		0.347160
  validation accuracy:		89.89 %
Epoch 1206 of 2000 took 0.100s
  training loss:		0.318987
  validation loss:		0.347353
  validation accuracy:		89.35 %
Epoch 1207 of 2000 took 0.100s
  training loss:		0.317534
  validation loss:		0.358343
  validation accuracy:		89.35 %
Epoch 1208 of 2000 took 0.100s
  training loss:		0.310649
  validation loss:		0.357980
  validation accuracy:		89.02 %
Epoch 1209 of 2000 took 0.100s
  training loss:		0.317859
  validation loss:		0.354751
  validation accuracy:		89.67 %
Epoch 1210 of 2000 took 0.103s
  training loss:		0.314698
  validation loss:		0.349225
  validation accuracy:		89.57 %
Epoch 1211 of 2000 took 0.101s
  training loss:		0.311944
  validation loss:		0.374431
  validation accuracy:		88.91 %
Epoch 1212 of 2000 took 0.100s
  training loss:		0.312313
  validation loss:		0.347820
  validation accuracy:		89.89 %
Epoch 1213 of 2000 took 0.100s
  training loss:		0.309270
  validation loss:		0.350547
  validation accuracy:		89.67 %
Epoch 1214 of 2000 took 0.100s
  training loss:		0.308904
  validation loss:		0.370315
  validation accuracy:		89.57 %
Epoch 1215 of 2000 took 0.100s
  training loss:		0.317974
  validation loss:		0.363076
  validation accuracy:		89.24 %
Epoch 1216 of 2000 took 0.100s
  training loss:		0.313676
  validation loss:		0.400181
  validation accuracy:		88.04 %
Epoch 1217 of 2000 took 0.100s
  training loss:		0.314258
  validation loss:		0.362389
  validation accuracy:		89.67 %
Epoch 1218 of 2000 took 0.100s
  training loss:		0.307341
  validation loss:		0.359076
  validation accuracy:		89.13 %
Epoch 1219 of 2000 took 0.100s
  training loss:		0.314450
  validation loss:		0.394855
  validation accuracy:		87.61 %
Epoch 1220 of 2000 took 0.103s
  training loss:		0.322407
  validation loss:		0.354983
  validation accuracy:		89.78 %
Epoch 1221 of 2000 took 0.101s
  training loss:		0.310396
  validation loss:		0.360333
  validation accuracy:		89.35 %
Epoch 1222 of 2000 took 0.100s
  training loss:		0.315047
  validation loss:		0.364186
  validation accuracy:		88.91 %
Epoch 1223 of 2000 took 0.100s
  training loss:		0.308539
  validation loss:		0.360442
  validation accuracy:		89.02 %
Epoch 1224 of 2000 took 0.101s
  training loss:		0.310052
  validation loss:		0.346494
  validation accuracy:		89.67 %
Epoch 1225 of 2000 took 0.100s
  training loss:		0.310999
  validation loss:		0.375329
  validation accuracy:		89.13 %
Epoch 1226 of 2000 took 0.100s
  training loss:		0.307103
  validation loss:		0.352829
  validation accuracy:		89.24 %
Epoch 1227 of 2000 took 0.100s
  training loss:		0.311907
  validation loss:		0.356811
  validation accuracy:		89.02 %
Epoch 1228 of 2000 took 0.100s
  training loss:		0.307874
  validation loss:		0.355315
  validation accuracy:		89.89 %
Epoch 1229 of 2000 took 0.100s
  training loss:		0.320886
  validation loss:		0.349616
  validation accuracy:		89.46 %
Epoch 1230 of 2000 took 0.103s
  training loss:		0.305705
  validation loss:		0.361534
  validation accuracy:		89.67 %
Epoch 1231 of 2000 took 0.101s
  training loss:		0.312638
  validation loss:		0.360506
  validation accuracy:		89.24 %
Epoch 1232 of 2000 took 0.100s
  training loss:		0.315808
  validation loss:		0.357294
  validation accuracy:		89.13 %
Epoch 1233 of 2000 took 0.100s
  training loss:		0.311659
  validation loss:		0.365538
  validation accuracy:		89.24 %
Epoch 1234 of 2000 took 0.100s
  training loss:		0.309542
  validation loss:		0.347621
  validation accuracy:		89.35 %
Epoch 1235 of 2000 took 0.100s
  training loss:		0.314290
  validation loss:		0.352787
  validation accuracy:		89.02 %
Epoch 1236 of 2000 took 0.100s
  training loss:		0.300680
  validation loss:		0.359540
  validation accuracy:		89.78 %
Epoch 1237 of 2000 took 0.100s
  training loss:		0.317620
  validation loss:		0.354203
  validation accuracy:		89.67 %
Epoch 1238 of 2000 took 0.100s
  training loss:		0.311310
  validation loss:		0.355711
  validation accuracy:		90.43 %
Epoch 1239 of 2000 took 0.100s
  training loss:		0.313536
  validation loss:		0.354586
  validation accuracy:		89.02 %
Epoch 1240 of 2000 took 0.103s
  training loss:		0.314520
  validation loss:		0.365834
  validation accuracy:		89.13 %
Epoch 1241 of 2000 took 0.100s
  training loss:		0.313042
  validation loss:		0.362385
  validation accuracy:		89.02 %
Epoch 1242 of 2000 took 0.100s
  training loss:		0.303178
  validation loss:		0.358753
  validation accuracy:		89.57 %
Epoch 1243 of 2000 took 0.100s
  training loss:		0.305854
  validation loss:		0.361272
  validation accuracy:		89.02 %
Epoch 1244 of 2000 took 0.100s
  training loss:		0.311821
  validation loss:		0.349534
  validation accuracy:		89.24 %
Epoch 1245 of 2000 took 0.100s
  training loss:		0.312835
  validation loss:		0.387778
  validation accuracy:		88.37 %
Epoch 1246 of 2000 took 0.100s
  training loss:		0.314536
  validation loss:		0.362307
  validation accuracy:		88.70 %
Epoch 1247 of 2000 took 0.100s
  training loss:		0.308764
  validation loss:		0.362185
  validation accuracy:		89.67 %
Epoch 1248 of 2000 took 0.100s
  training loss:		0.313741
  validation loss:		0.351332
  validation accuracy:		89.78 %
Epoch 1249 of 2000 took 0.100s
  training loss:		0.312775
  validation loss:		0.353025
  validation accuracy:		89.67 %
Epoch 1250 of 2000 took 0.103s
  training loss:		0.309480
  validation loss:		0.351715
  validation accuracy:		88.70 %
Epoch 1251 of 2000 took 0.100s
  training loss:		0.306510
  validation loss:		0.347376
  validation accuracy:		90.11 %
Epoch 1252 of 2000 took 0.100s
  training loss:		0.302807
  validation loss:		0.371276
  validation accuracy:		89.13 %
Epoch 1253 of 2000 took 0.100s
  training loss:		0.304526
  validation loss:		0.353684
  validation accuracy:		89.46 %
Epoch 1254 of 2000 took 0.101s
  training loss:		0.302504
  validation loss:		0.346889
  validation accuracy:		89.57 %
Epoch 1255 of 2000 took 0.100s
  training loss:		0.306724
  validation loss:		0.351945
  validation accuracy:		89.67 %
Epoch 1256 of 2000 took 0.100s
  training loss:		0.310350
  validation loss:		0.350944
  validation accuracy:		89.89 %
Epoch 1257 of 2000 took 0.100s
  training loss:		0.307369
  validation loss:		0.382151
  validation accuracy:		88.37 %
Epoch 1258 of 2000 took 0.100s
  training loss:		0.307715
  validation loss:		0.367421
  validation accuracy:		89.35 %
Epoch 1259 of 2000 took 0.100s
  training loss:		0.310814
  validation loss:		0.350555
  validation accuracy:		89.24 %
Epoch 1260 of 2000 took 0.103s
  training loss:		0.307336
  validation loss:		0.361984
  validation accuracy:		89.67 %
Epoch 1261 of 2000 took 0.100s
  training loss:		0.306116
  validation loss:		0.354337
  validation accuracy:		88.91 %
Epoch 1262 of 2000 took 0.100s
  training loss:		0.310387
  validation loss:		0.350791
  validation accuracy:		89.46 %
Epoch 1263 of 2000 took 0.100s
  training loss:		0.308838
  validation loss:		0.349310
  validation accuracy:		90.11 %
Epoch 1264 of 2000 took 0.100s
  training loss:		0.301965
  validation loss:		0.352167
  validation accuracy:		89.13 %
Epoch 1265 of 2000 took 0.100s
  training loss:		0.304783
  validation loss:		0.362093
  validation accuracy:		89.46 %
Epoch 1266 of 2000 took 0.100s
  training loss:		0.306565
  validation loss:		0.350715
  validation accuracy:		89.46 %
Epoch 1267 of 2000 took 0.100s
  training loss:		0.309330
  validation loss:		0.345492
  validation accuracy:		90.11 %
Epoch 1268 of 2000 took 0.100s
  training loss:		0.312079
  validation loss:		0.359757
  validation accuracy:		89.35 %
Epoch 1269 of 2000 took 0.100s
  training loss:		0.311194
  validation loss:		0.348056
  validation accuracy:		90.00 %
Epoch 1270 of 2000 took 0.103s
  training loss:		0.307662
  validation loss:		0.351110
  validation accuracy:		90.00 %
Epoch 1271 of 2000 took 0.100s
  training loss:		0.302960
  validation loss:		0.354688
  validation accuracy:		89.57 %
Epoch 1272 of 2000 took 0.100s
  training loss:		0.303781
  validation loss:		0.368552
  validation accuracy:		88.70 %
Epoch 1273 of 2000 took 0.100s
  training loss:		0.301795
  validation loss:		0.347655
  validation accuracy:		89.89 %
Epoch 1274 of 2000 took 0.100s
  training loss:		0.307478
  validation loss:		0.353726
  validation accuracy:		89.24 %
Epoch 1275 of 2000 took 0.100s
  training loss:		0.312128
  validation loss:		0.367727
  validation accuracy:		89.02 %
Epoch 1276 of 2000 took 0.101s
  training loss:		0.303689
  validation loss:		0.349157
  validation accuracy:		89.24 %
Epoch 1277 of 2000 took 0.103s
  training loss:		0.303456
  validation loss:		0.357883
  validation accuracy:		89.46 %
Epoch 1278 of 2000 took 0.103s
  training loss:		0.298937
  validation loss:		0.352285
  validation accuracy:		90.22 %
Epoch 1279 of 2000 took 0.103s
  training loss:		0.302314
  validation loss:		0.347632
  validation accuracy:		89.67 %
Epoch 1280 of 2000 took 0.107s
  training loss:		0.308030
  validation loss:		0.352450
  validation accuracy:		89.57 %
Epoch 1281 of 2000 took 0.103s
  training loss:		0.307822
  validation loss:		0.356535
  validation accuracy:		89.02 %
Epoch 1282 of 2000 took 0.103s
  training loss:		0.309603
  validation loss:		0.366061
  validation accuracy:		88.70 %
Epoch 1283 of 2000 took 0.103s
  training loss:		0.306344
  validation loss:		0.377947
  validation accuracy:		88.59 %
Epoch 1284 of 2000 took 0.104s
  training loss:		0.301798
  validation loss:		0.361594
  validation accuracy:		90.22 %
Epoch 1285 of 2000 took 0.103s
  training loss:		0.298441
  validation loss:		0.348504
  validation accuracy:		89.46 %
Epoch 1286 of 2000 took 0.103s
  training loss:		0.309696
  validation loss:		0.361699
  validation accuracy:		89.24 %
Epoch 1287 of 2000 took 0.103s
  training loss:		0.298533
  validation loss:		0.347084
  validation accuracy:		89.67 %
Epoch 1288 of 2000 took 0.103s
  training loss:		0.305561
  validation loss:		0.354271
  validation accuracy:		89.67 %
Epoch 1289 of 2000 took 0.103s
  training loss:		0.303095
  validation loss:		0.346649
  validation accuracy:		90.00 %
Epoch 1290 of 2000 took 0.107s
  training loss:		0.301810
  validation loss:		0.345485
  validation accuracy:		89.78 %
Epoch 1291 of 2000 took 0.103s
  training loss:		0.296469
  validation loss:		0.394034
  validation accuracy:		88.70 %
Epoch 1292 of 2000 took 0.103s
  training loss:		0.305657
  validation loss:		0.355304
  validation accuracy:		89.57 %
Epoch 1293 of 2000 took 0.103s
  training loss:		0.306543
  validation loss:		0.349158
  validation accuracy:		89.67 %
Epoch 1294 of 2000 took 0.103s
  training loss:		0.292132
  validation loss:		0.347494
  validation accuracy:		89.35 %
Epoch 1295 of 2000 took 0.103s
  training loss:		0.305876
  validation loss:		0.382874
  validation accuracy:		88.80 %
Epoch 1296 of 2000 took 0.103s
  training loss:		0.305343
  validation loss:		0.364265
  validation accuracy:		89.35 %
Epoch 1297 of 2000 took 0.103s
  training loss:		0.305239
  validation loss:		0.363865
  validation accuracy:		89.02 %
Epoch 1298 of 2000 took 0.103s
  training loss:		0.297320
  validation loss:		0.354434
  validation accuracy:		89.46 %
Epoch 1299 of 2000 took 0.106s
  training loss:		0.300039
  validation loss:		0.346346
  validation accuracy:		90.33 %
Epoch 1300 of 2000 took 0.104s
  training loss:		0.313959
  validation loss:		0.348455
  validation accuracy:		89.24 %
Epoch 1301 of 2000 took 0.103s
  training loss:		0.308311
  validation loss:		0.352102
  validation accuracy:		89.57 %
Epoch 1302 of 2000 took 0.103s
  training loss:		0.299772
  validation loss:		0.357179
  validation accuracy:		89.24 %
Epoch 1303 of 2000 took 0.103s
  training loss:		0.306720
  validation loss:		0.353666
  validation accuracy:		89.24 %
Epoch 1304 of 2000 took 0.103s
  training loss:		0.298433
  validation loss:		0.376705
  validation accuracy:		88.80 %
Epoch 1305 of 2000 took 0.103s
  training loss:		0.306716
  validation loss:		0.365671
  validation accuracy:		89.35 %
Epoch 1306 of 2000 took 0.103s
  training loss:		0.302640
  validation loss:		0.367411
  validation accuracy:		88.48 %
Epoch 1307 of 2000 took 0.103s
  training loss:		0.302843
  validation loss:		0.355184
  validation accuracy:		89.46 %
Epoch 1308 of 2000 took 0.105s
  training loss:		0.300354
  validation loss:		0.355746
  validation accuracy:		89.46 %
Epoch 1309 of 2000 took 0.115s
  training loss:		0.305376
  validation loss:		0.352583
  validation accuracy:		89.46 %
Epoch 1310 of 2000 took 0.119s
  training loss:		0.306477
  validation loss:		0.358061
  validation accuracy:		89.24 %
Epoch 1311 of 2000 took 0.139s
  training loss:		0.299234
  validation loss:		0.378427
  validation accuracy:		88.91 %
Epoch 1312 of 2000 took 0.104s
  training loss:		0.313828
  validation loss:		0.349964
  validation accuracy:		89.78 %
Epoch 1313 of 2000 took 0.104s
  training loss:		0.303013
  validation loss:		0.344304
  validation accuracy:		89.67 %
Epoch 1314 of 2000 took 0.105s
  training loss:		0.309842
  validation loss:		0.345391
  validation accuracy:		90.22 %
Epoch 1315 of 2000 took 0.105s
  training loss:		0.297386
  validation loss:		0.355006
  validation accuracy:		89.57 %
Epoch 1316 of 2000 took 0.105s
  training loss:		0.302308
  validation loss:		0.377755
  validation accuracy:		88.70 %
Epoch 1317 of 2000 took 0.102s
  training loss:		0.298512
  validation loss:		0.349067
  validation accuracy:		89.78 %
Epoch 1318 of 2000 took 0.105s
  training loss:		0.299953
  validation loss:		0.389326
  validation accuracy:		88.70 %
Epoch 1319 of 2000 took 0.101s
  training loss:		0.299113
  validation loss:		0.348219
  validation accuracy:		90.00 %
Epoch 1320 of 2000 took 0.101s
  training loss:		0.301375
  validation loss:		0.358387
  validation accuracy:		89.24 %
Epoch 1321 of 2000 took 0.100s
  training loss:		0.299861
  validation loss:		0.352191
  validation accuracy:		89.67 %
Epoch 1322 of 2000 took 0.101s
  training loss:		0.300008
  validation loss:		0.351732
  validation accuracy:		89.57 %
Epoch 1323 of 2000 took 0.101s
  training loss:		0.302453
  validation loss:		0.354292
  validation accuracy:		89.35 %
Epoch 1324 of 2000 took 0.101s
  training loss:		0.301008
  validation loss:		0.363560
  validation accuracy:		89.24 %
Epoch 1325 of 2000 took 0.101s
  training loss:		0.300670
  validation loss:		0.379229
  validation accuracy:		88.48 %
Epoch 1326 of 2000 took 0.101s
  training loss:		0.300858
  validation loss:		0.380726
  validation accuracy:		88.59 %
Epoch 1327 of 2000 took 0.101s
  training loss:		0.306529
  validation loss:		0.365504
  validation accuracy:		88.91 %
Epoch 1328 of 2000 took 0.103s
  training loss:		0.302388
  validation loss:		0.346063
  validation accuracy:		90.00 %
Epoch 1329 of 2000 took 0.101s
  training loss:		0.294797
  validation loss:		0.348219
  validation accuracy:		89.67 %
Epoch 1330 of 2000 took 0.101s
  training loss:		0.298018
  validation loss:		0.347115
  validation accuracy:		90.00 %
Epoch 1331 of 2000 took 0.101s
  training loss:		0.302716
  validation loss:		0.349324
  validation accuracy:		90.00 %
Epoch 1332 of 2000 took 0.100s
  training loss:		0.297416
  validation loss:		0.357846
  validation accuracy:		89.89 %
Epoch 1333 of 2000 took 0.101s
  training loss:		0.300281
  validation loss:		0.353236
  validation accuracy:		89.46 %
Epoch 1334 of 2000 took 0.101s
  training loss:		0.298575
  validation loss:		0.355478
  validation accuracy:		89.78 %
Epoch 1335 of 2000 took 0.101s
  training loss:		0.302660
  validation loss:		0.348458
  validation accuracy:		89.46 %
Epoch 1336 of 2000 took 0.101s
  training loss:		0.300401
  validation loss:		0.349012
  validation accuracy:		89.78 %
Epoch 1337 of 2000 took 0.101s
  training loss:		0.296098
  validation loss:		0.350863
  validation accuracy:		89.02 %
Epoch 1338 of 2000 took 0.104s
  training loss:		0.303354
  validation loss:		0.360124
  validation accuracy:		88.91 %
Epoch 1339 of 2000 took 0.101s
  training loss:		0.302524
  validation loss:		0.368406
  validation accuracy:		88.80 %
Epoch 1340 of 2000 took 0.100s
  training loss:		0.292484
  validation loss:		0.365246
  validation accuracy:		89.02 %
Epoch 1341 of 2000 took 0.101s
  training loss:		0.301812
  validation loss:		0.354815
  validation accuracy:		89.46 %
Epoch 1342 of 2000 took 0.101s
  training loss:		0.294166
  validation loss:		0.355371
  validation accuracy:		89.35 %
Epoch 1343 of 2000 took 0.101s
  training loss:		0.299816
  validation loss:		0.344155
  validation accuracy:		89.89 %
Epoch 1344 of 2000 took 0.101s
  training loss:		0.303473
  validation loss:		0.372496
  validation accuracy:		88.48 %
Epoch 1345 of 2000 took 0.101s
  training loss:		0.299389
  validation loss:		0.367733
  validation accuracy:		89.13 %
Epoch 1346 of 2000 took 0.101s
  training loss:		0.295064
  validation loss:		0.345945
  validation accuracy:		90.00 %
Epoch 1347 of 2000 took 0.101s
  training loss:		0.301387
  validation loss:		0.357449
  validation accuracy:		88.91 %
Epoch 1348 of 2000 took 0.104s
  training loss:		0.298911
  validation loss:		0.362846
  validation accuracy:		89.13 %
Epoch 1349 of 2000 took 0.101s
  training loss:		0.296637
  validation loss:		0.356419
  validation accuracy:		89.35 %
Epoch 1350 of 2000 took 0.103s
  training loss:		0.295052
  validation loss:		0.351504
  validation accuracy:		89.67 %
Epoch 1351 of 2000 took 0.103s
  training loss:		0.297817
  validation loss:		0.359555
  validation accuracy:		89.13 %
Epoch 1352 of 2000 took 0.101s
  training loss:		0.298852
  validation loss:		0.361063
  validation accuracy:		89.57 %
Epoch 1353 of 2000 took 0.100s
  training loss:		0.306113
  validation loss:		0.374334
  validation accuracy:		89.13 %
Epoch 1354 of 2000 took 0.101s
  training loss:		0.301466
  validation loss:		0.351464
  validation accuracy:		89.46 %
Epoch 1355 of 2000 took 0.101s
  training loss:		0.296501
  validation loss:		0.360007
  validation accuracy:		89.24 %
Epoch 1356 of 2000 took 0.101s
  training loss:		0.299264
  validation loss:		0.394534
  validation accuracy:		88.15 %
Epoch 1357 of 2000 took 0.100s
  training loss:		0.295301
  validation loss:		0.348310
  validation accuracy:		89.46 %
Epoch 1358 of 2000 took 0.102s
  training loss:		0.297611
  validation loss:		0.358059
  validation accuracy:		89.78 %
Epoch 1359 of 2000 took 0.102s
  training loss:		0.295051
  validation loss:		0.357037
  validation accuracy:		89.46 %
Epoch 1360 of 2000 took 0.100s
  training loss:		0.293402
  validation loss:		0.354697
  validation accuracy:		89.89 %
Epoch 1361 of 2000 took 0.100s
  training loss:		0.294000
  validation loss:		0.369182
  validation accuracy:		89.13 %
Epoch 1362 of 2000 took 0.101s
  training loss:		0.299829
  validation loss:		0.361874
  validation accuracy:		89.02 %
Epoch 1363 of 2000 took 0.100s
  training loss:		0.301335
  validation loss:		0.357137
  validation accuracy:		89.24 %
Epoch 1364 of 2000 took 0.100s
  training loss:		0.295499
  validation loss:		0.371078
  validation accuracy:		89.24 %
Epoch 1365 of 2000 took 0.100s
  training loss:		0.294636
  validation loss:		0.359550
  validation accuracy:		89.24 %
Epoch 1366 of 2000 took 0.101s
  training loss:		0.296431
  validation loss:		0.368727
  validation accuracy:		88.80 %
Epoch 1367 of 2000 took 0.100s
  training loss:		0.299248
  validation loss:		0.354038
  validation accuracy:		89.67 %
Epoch 1368 of 2000 took 0.100s
  training loss:		0.299051
  validation loss:		0.356318
  validation accuracy:		89.67 %
Epoch 1369 of 2000 took 0.100s
  training loss:		0.300079
  validation loss:		0.362686
  validation accuracy:		89.13 %
Epoch 1370 of 2000 took 0.102s
  training loss:		0.302795
  validation loss:		0.350526
  validation accuracy:		89.89 %
Epoch 1371 of 2000 took 0.103s
  training loss:		0.294030
  validation loss:		0.362035
  validation accuracy:		89.35 %
Epoch 1372 of 2000 took 0.102s
  training loss:		0.291748
  validation loss:		0.349990
  validation accuracy:		89.78 %
Epoch 1373 of 2000 took 0.107s
  training loss:		0.304529
  validation loss:		0.354418
  validation accuracy:		89.57 %
Epoch 1374 of 2000 took 0.107s
  training loss:		0.293798
  validation loss:		0.366991
  validation accuracy:		89.02 %
Epoch 1375 of 2000 took 0.108s
  training loss:		0.297222
  validation loss:		0.369223
  validation accuracy:		89.57 %
Epoch 1376 of 2000 took 0.098s
  training loss:		0.301008
  validation loss:		0.383563
  validation accuracy:		88.59 %
Epoch 1377 of 2000 took 0.097s
  training loss:		0.296807
  validation loss:		0.347594
  validation accuracy:		90.11 %
Epoch 1378 of 2000 took 0.097s
  training loss:		0.293125
  validation loss:		0.355093
  validation accuracy:		89.46 %
Epoch 1379 of 2000 took 0.097s
  training loss:		0.296146
  validation loss:		0.375656
  validation accuracy:		89.13 %
Epoch 1380 of 2000 took 0.097s
  training loss:		0.291452
  validation loss:		0.353850
  validation accuracy:		89.78 %
Epoch 1381 of 2000 took 0.096s
  training loss:		0.299238
  validation loss:		0.356654
  validation accuracy:		90.11 %
Epoch 1382 of 2000 took 0.097s
  training loss:		0.292634
  validation loss:		0.353999
  validation accuracy:		89.67 %
Epoch 1383 of 2000 took 0.096s
  training loss:		0.295950
  validation loss:		0.360415
  validation accuracy:		89.46 %
Epoch 1384 of 2000 took 0.100s
  training loss:		0.297830
  validation loss:		0.363545
  validation accuracy:		89.46 %
Epoch 1385 of 2000 took 0.097s
  training loss:		0.299075
  validation loss:		0.348770
  validation accuracy:		89.78 %
Epoch 1386 of 2000 took 0.097s
  training loss:		0.295054
  validation loss:		0.352372
  validation accuracy:		89.67 %
Epoch 1387 of 2000 took 0.097s
  training loss:		0.295285
  validation loss:		0.386925
  validation accuracy:		89.35 %
Epoch 1388 of 2000 took 0.097s
  training loss:		0.302851
  validation loss:		0.357100
  validation accuracy:		89.24 %
Epoch 1389 of 2000 took 0.096s
  training loss:		0.288857
  validation loss:		0.347427
  validation accuracy:		89.78 %
Epoch 1390 of 2000 took 0.097s
  training loss:		0.298703
  validation loss:		0.360278
  validation accuracy:		90.11 %
Epoch 1391 of 2000 took 0.097s
  training loss:		0.298951
  validation loss:		0.369555
  validation accuracy:		88.91 %
Epoch 1392 of 2000 took 0.097s
  training loss:		0.299723
  validation loss:		0.350482
  validation accuracy:		90.11 %
Epoch 1393 of 2000 took 0.097s
  training loss:		0.292659
  validation loss:		0.349350
  validation accuracy:		89.67 %
Epoch 1394 of 2000 took 0.097s
  training loss:		0.296222
  validation loss:		0.376692
  validation accuracy:		89.24 %
Epoch 1395 of 2000 took 0.097s
  training loss:		0.290007
  validation loss:		0.355986
  validation accuracy:		90.11 %
Epoch 1396 of 2000 took 0.097s
  training loss:		0.298034
  validation loss:		0.392866
  validation accuracy:		88.70 %
Epoch 1397 of 2000 took 0.097s
  training loss:		0.294316
  validation loss:		0.369458
  validation accuracy:		89.24 %
Epoch 1398 of 2000 took 0.097s
  training loss:		0.293943
  validation loss:		0.353942
  validation accuracy:		89.57 %
Epoch 1399 of 2000 took 0.100s
  training loss:		0.297525
  validation loss:		0.358099
  validation accuracy:		89.46 %
Epoch 1400 of 2000 took 0.097s
  training loss:		0.298394
  validation loss:		0.376934
  validation accuracy:		88.37 %
Epoch 1401 of 2000 took 0.097s
  training loss:		0.297798
  validation loss:		0.364051
  validation accuracy:		89.46 %
Epoch 1402 of 2000 took 0.098s
  training loss:		0.292142
  validation loss:		0.350994
  validation accuracy:		90.22 %
Epoch 1403 of 2000 took 0.097s
  training loss:		0.293519
  validation loss:		0.353730
  validation accuracy:		89.67 %
Epoch 1404 of 2000 took 0.097s
  training loss:		0.292763
  validation loss:		0.352155
  validation accuracy:		89.35 %
Epoch 1405 of 2000 took 0.096s
  training loss:		0.295604
  validation loss:		0.348148
  validation accuracy:		90.11 %
Epoch 1406 of 2000 took 0.097s
  training loss:		0.294052
  validation loss:		0.360290
  validation accuracy:		88.70 %
Epoch 1407 of 2000 took 0.097s
  training loss:		0.290484
  validation loss:		0.366160
  validation accuracy:		88.80 %
Epoch 1408 of 2000 took 0.097s
  training loss:		0.292449
  validation loss:		0.354260
  validation accuracy:		89.35 %
Epoch 1409 of 2000 took 0.097s
  training loss:		0.291014
  validation loss:		0.369870
  validation accuracy:		89.02 %
Epoch 1410 of 2000 took 0.097s
  training loss:		0.292003
  validation loss:		0.349526
  validation accuracy:		90.11 %
Epoch 1411 of 2000 took 0.097s
  training loss:		0.294443
  validation loss:		0.364516
  validation accuracy:		89.02 %
Epoch 1412 of 2000 took 0.096s
  training loss:		0.292815
  validation loss:		0.358155
  validation accuracy:		89.13 %
Epoch 1413 of 2000 took 0.097s
  training loss:		0.289266
  validation loss:		0.386602
  validation accuracy:		88.80 %
Epoch 1414 of 2000 took 0.097s
  training loss:		0.298072
  validation loss:		0.357818
  validation accuracy:		89.35 %
Epoch 1415 of 2000 took 0.098s
  training loss:		0.295117
  validation loss:		0.361302
  validation accuracy:		88.80 %
Epoch 1416 of 2000 took 0.099s
  training loss:		0.295696
  validation loss:		0.373752
  validation accuracy:		89.24 %
Epoch 1417 of 2000 took 0.097s
  training loss:		0.294218
  validation loss:		0.366707
  validation accuracy:		89.57 %
Epoch 1418 of 2000 took 0.096s
  training loss:		0.290978
  validation loss:		0.367195
  validation accuracy:		89.02 %
Epoch 1419 of 2000 took 0.097s
  training loss:		0.294098
  validation loss:		0.351568
  validation accuracy:		89.78 %
Epoch 1420 of 2000 took 0.097s
  training loss:		0.292435
  validation loss:		0.396613
  validation accuracy:		88.37 %
Epoch 1421 of 2000 took 0.097s
  training loss:		0.299549
  validation loss:		0.362359
  validation accuracy:		88.80 %
Epoch 1422 of 2000 took 0.096s
  training loss:		0.290920
  validation loss:		0.362914
  validation accuracy:		89.46 %
Epoch 1423 of 2000 took 0.097s
  training loss:		0.288234
  validation loss:		0.364460
  validation accuracy:		89.02 %
Epoch 1424 of 2000 took 0.097s
  training loss:		0.287218
  validation loss:		0.354445
  validation accuracy:		89.89 %
Epoch 1425 of 2000 took 0.097s
  training loss:		0.292439
  validation loss:		0.358165
  validation accuracy:		89.13 %
Epoch 1426 of 2000 took 0.096s
  training loss:		0.292511
  validation loss:		0.360993
  validation accuracy:		88.80 %
Epoch 1427 of 2000 took 0.097s
  training loss:		0.288148
  validation loss:		0.348206
  validation accuracy:		90.22 %
Epoch 1428 of 2000 took 0.096s
  training loss:		0.301121
  validation loss:		0.360157
  validation accuracy:		89.67 %
Epoch 1429 of 2000 took 0.097s
  training loss:		0.287537
  validation loss:		0.349885
  validation accuracy:		89.57 %
Epoch 1430 of 2000 took 0.097s
  training loss:		0.292298
  validation loss:		0.345561
  validation accuracy:		90.00 %
Epoch 1431 of 2000 took 0.097s
  training loss:		0.294022
  validation loss:		0.354537
  validation accuracy:		90.11 %
Epoch 1432 of 2000 took 0.097s
  training loss:		0.297728
  validation loss:		0.367260
  validation accuracy:		88.70 %
Epoch 1433 of 2000 took 0.098s
  training loss:		0.294474
  validation loss:		0.356003
  validation accuracy:		90.22 %
Epoch 1434 of 2000 took 0.099s
  training loss:		0.290915
  validation loss:		0.354735
  validation accuracy:		89.35 %
Epoch 1435 of 2000 took 0.097s
  training loss:		0.295104
  validation loss:		0.355060
  validation accuracy:		90.11 %
Epoch 1436 of 2000 took 0.097s
  training loss:		0.294996
  validation loss:		0.361335
  validation accuracy:		89.67 %
Epoch 1437 of 2000 took 0.097s
  training loss:		0.291013
  validation loss:		0.356548
  validation accuracy:		89.46 %
Epoch 1438 of 2000 took 0.097s
  training loss:		0.298661
  validation loss:		0.364120
  validation accuracy:		88.80 %
Epoch 1439 of 2000 took 0.097s
  training loss:		0.294405
  validation loss:		0.347859
  validation accuracy:		90.00 %
Epoch 1440 of 2000 took 0.097s
  training loss:		0.293934
  validation loss:		0.361183
  validation accuracy:		89.35 %
Epoch 1441 of 2000 took 0.097s
  training loss:		0.288882
  validation loss:		0.365070
  validation accuracy:		89.57 %
Epoch 1442 of 2000 took 0.097s
  training loss:		0.291520
  validation loss:		0.354997
  validation accuracy:		89.67 %
Epoch 1443 of 2000 took 0.096s
  training loss:		0.289824
  validation loss:		0.390569
  validation accuracy:		88.48 %
Epoch 1444 of 2000 took 0.097s
  training loss:		0.293448
  validation loss:		0.353369
  validation accuracy:		89.35 %
Epoch 1445 of 2000 took 0.097s
  training loss:		0.290357
  validation loss:		0.356159
  validation accuracy:		90.00 %
Epoch 1446 of 2000 took 0.097s
  training loss:		0.286690
  validation loss:		0.366152
  validation accuracy:		89.67 %
Epoch 1447 of 2000 took 0.097s
  training loss:		0.288657
  validation loss:		0.355207
  validation accuracy:		90.11 %
Epoch 1448 of 2000 took 0.097s
  training loss:		0.288195
  validation loss:		0.358337
  validation accuracy:		90.00 %
Epoch 1449 of 2000 took 0.096s
  training loss:		0.295581
  validation loss:		0.372077
  validation accuracy:		89.13 %
Epoch 1450 of 2000 took 0.097s
  training loss:		0.293949
  validation loss:		0.363486
  validation accuracy:		89.46 %
Epoch 1451 of 2000 took 0.097s
  training loss:		0.288759
  validation loss:		0.356439
  validation accuracy:		89.67 %
Epoch 1452 of 2000 took 0.097s
  training loss:		0.283732
  validation loss:		0.359999
  validation accuracy:		89.78 %
Epoch 1453 of 2000 took 0.097s
  training loss:		0.286412
  validation loss:		0.358979
  validation accuracy:		89.02 %
Epoch 1454 of 2000 took 0.099s
  training loss:		0.294987
  validation loss:		0.364920
  validation accuracy:		89.35 %
Epoch 1455 of 2000 took 0.097s
  training loss:		0.295308
  validation loss:		0.347563
  validation accuracy:		89.78 %
Epoch 1456 of 2000 took 0.097s
  training loss:		0.297517
  validation loss:		0.348896
  validation accuracy:		90.54 %
Epoch 1457 of 2000 took 0.096s
  training loss:		0.292668
  validation loss:		0.356993
  validation accuracy:		89.57 %
Epoch 1458 of 2000 took 0.097s
  training loss:		0.287270
  validation loss:		0.358522
  validation accuracy:		89.78 %
Epoch 1459 of 2000 took 0.097s
  training loss:		0.295104
  validation loss:		0.352724
  validation accuracy:		90.11 %
Epoch 1460 of 2000 took 0.097s
  training loss:		0.290654
  validation loss:		0.352375
  validation accuracy:		90.22 %
Epoch 1461 of 2000 took 0.096s
  training loss:		0.288117
  validation loss:		0.344142
  validation accuracy:		90.22 %
Epoch 1462 of 2000 took 0.097s
  training loss:		0.287667
  validation loss:		0.379072
  validation accuracy:		88.91 %
Epoch 1463 of 2000 took 0.096s
  training loss:		0.283391
  validation loss:		0.374111
  validation accuracy:		89.35 %
Epoch 1464 of 2000 took 0.098s
  training loss:		0.293450
  validation loss:		0.378067
  validation accuracy:		88.80 %
Epoch 1465 of 2000 took 0.097s
  training loss:		0.294012
  validation loss:		0.374908
  validation accuracy:		89.57 %
Epoch 1466 of 2000 took 0.097s
  training loss:		0.296562
  validation loss:		0.346957
  validation accuracy:		90.11 %
Epoch 1467 of 2000 took 0.096s
  training loss:		0.293475
  validation loss:		0.370316
  validation accuracy:		89.13 %
Epoch 1468 of 2000 took 0.097s
  training loss:		0.280221
  validation loss:		0.376861
  validation accuracy:		88.70 %
Epoch 1469 of 2000 took 0.097s
  training loss:		0.284225
  validation loss:		0.368687
  validation accuracy:		89.02 %
Epoch 1470 of 2000 took 0.097s
  training loss:		0.286324
  validation loss:		0.352516
  validation accuracy:		90.00 %
Epoch 1471 of 2000 took 0.097s
  training loss:		0.288536
  validation loss:		0.345121
  validation accuracy:		90.33 %
Epoch 1472 of 2000 took 0.097s
  training loss:		0.290312
  validation loss:		0.367343
  validation accuracy:		89.02 %
Epoch 1473 of 2000 took 0.097s
  training loss:		0.294164
  validation loss:		0.349227
  validation accuracy:		89.78 %
Epoch 1474 of 2000 took 0.097s
  training loss:		0.294700
  validation loss:		0.358181
  validation accuracy:		90.00 %
Epoch 1475 of 2000 took 0.097s
  training loss:		0.286528
  validation loss:		0.348729
  validation accuracy:		90.22 %
Epoch 1476 of 2000 took 0.099s
  training loss:		0.291778
  validation loss:		0.364073
  validation accuracy:		88.91 %
Epoch 1477 of 2000 took 0.097s
  training loss:		0.291207
  validation loss:		0.365574
  validation accuracy:		89.78 %
Epoch 1478 of 2000 took 0.097s
  training loss:		0.286259
  validation loss:		0.358605
  validation accuracy:		89.89 %
Epoch 1479 of 2000 took 0.097s
  training loss:		0.293274
  validation loss:		0.358465
  validation accuracy:		89.57 %
Epoch 1480 of 2000 took 0.100s
  training loss:		0.287889
  validation loss:		0.372242
  validation accuracy:		89.02 %
Epoch 1481 of 2000 took 0.106s
  training loss:		0.295864
  validation loss:		0.353873
  validation accuracy:		89.24 %
Epoch 1482 of 2000 took 0.110s
  training loss:		0.291468
  validation loss:		0.349931
  validation accuracy:		89.78 %
Epoch 1483 of 2000 took 0.100s
  training loss:		0.289813
  validation loss:		0.346264
  validation accuracy:		89.67 %
Epoch 1484 of 2000 took 0.100s
  training loss:		0.286583
  validation loss:		0.348616
  validation accuracy:		90.33 %
Epoch 1485 of 2000 took 0.101s
  training loss:		0.290747
  validation loss:		0.354206
  validation accuracy:		89.13 %
Epoch 1486 of 2000 took 0.100s
  training loss:		0.290679
  validation loss:		0.358422
  validation accuracy:		89.02 %
Epoch 1487 of 2000 took 0.100s
  training loss:		0.285557
  validation loss:		0.355199
  validation accuracy:		89.13 %
Epoch 1488 of 2000 took 0.100s
  training loss:		0.291745
  validation loss:		0.365981
  validation accuracy:		89.13 %
Epoch 1489 of 2000 took 0.101s
  training loss:		0.287550
  validation loss:		0.362936
  validation accuracy:		89.67 %
Epoch 1490 of 2000 took 0.100s
  training loss:		0.288880
  validation loss:		0.379556
  validation accuracy:		88.80 %
Epoch 1491 of 2000 took 0.100s
  training loss:		0.291244
  validation loss:		0.364067
  validation accuracy:		89.46 %
Epoch 1492 of 2000 took 0.100s
  training loss:		0.293520
  validation loss:		0.374195
  validation accuracy:		88.80 %
Epoch 1493 of 2000 took 0.107s
  training loss:		0.287725
  validation loss:		0.357013
  validation accuracy:		89.57 %
Epoch 1494 of 2000 took 0.108s
  training loss:		0.292638
  validation loss:		0.370992
  validation accuracy:		89.35 %
Epoch 1495 of 2000 took 0.104s
  training loss:		0.287556
  validation loss:		0.383552
  validation accuracy:		88.48 %
Epoch 1496 of 2000 took 0.106s
  training loss:		0.287737
  validation loss:		0.351485
  validation accuracy:		90.00 %
Epoch 1497 of 2000 took 0.107s
  training loss:		0.289788
  validation loss:		0.356947
  validation accuracy:		89.67 %
Epoch 1498 of 2000 took 0.105s
  training loss:		0.285207
  validation loss:		0.349310
  validation accuracy:		89.78 %
Epoch 1499 of 2000 took 0.106s
  training loss:		0.286033
  validation loss:		0.352445
  validation accuracy:		89.24 %
Epoch 1500 of 2000 took 0.104s
  training loss:		0.285094
  validation loss:		0.350113
  validation accuracy:		90.11 %
Epoch 1501 of 2000 took 0.104s
  training loss:		0.290934
  validation loss:		0.359806
  validation accuracy:		89.13 %
Epoch 1502 of 2000 took 0.103s
  training loss:		0.285552
  validation loss:		0.359439
  validation accuracy:		89.78 %
Epoch 1503 of 2000 took 0.103s
  training loss:		0.296655
  validation loss:		0.351243
  validation accuracy:		90.11 %
Epoch 1504 of 2000 took 0.104s
  training loss:		0.289449
  validation loss:		0.355615
  validation accuracy:		90.33 %
Epoch 1505 of 2000 took 0.104s
  training loss:		0.281150
  validation loss:		0.347868
  validation accuracy:		89.89 %
Epoch 1506 of 2000 took 0.104s
  training loss:		0.288470
  validation loss:		0.360849
  validation accuracy:		89.13 %
Epoch 1507 of 2000 took 0.103s
  training loss:		0.284672
  validation loss:		0.356761
  validation accuracy:		89.78 %
Epoch 1508 of 2000 took 0.104s
  training loss:		0.285887
  validation loss:		0.367746
  validation accuracy:		89.57 %
Epoch 1509 of 2000 took 0.104s
  training loss:		0.285336
  validation loss:		0.383866
  validation accuracy:		89.46 %
Epoch 1510 of 2000 took 0.104s
  training loss:		0.292031
  validation loss:		0.353193
  validation accuracy:		90.00 %
Epoch 1511 of 2000 took 0.104s
  training loss:		0.286605
  validation loss:		0.376392
  validation accuracy:		89.02 %
Epoch 1512 of 2000 took 0.104s
  training loss:		0.284315
  validation loss:		0.408718
  validation accuracy:		88.80 %
Epoch 1513 of 2000 took 0.104s
  training loss:		0.291675
  validation loss:		0.358175
  validation accuracy:		89.67 %
Epoch 1514 of 2000 took 0.104s
  training loss:		0.283278
  validation loss:		0.350617
  validation accuracy:		90.11 %
Epoch 1515 of 2000 took 0.104s
  training loss:		0.287336
  validation loss:		0.363863
  validation accuracy:		89.57 %
Epoch 1516 of 2000 took 0.104s
  training loss:		0.282519
  validation loss:		0.360441
  validation accuracy:		89.24 %
Epoch 1517 of 2000 took 0.104s
  training loss:		0.292798
  validation loss:		0.355101
  validation accuracy:		90.11 %
Epoch 1518 of 2000 took 0.104s
  training loss:		0.292672
  validation loss:		0.350367
  validation accuracy:		89.89 %
Epoch 1519 of 2000 took 0.104s
  training loss:		0.284520
  validation loss:		0.356944
  validation accuracy:		90.11 %
Epoch 1520 of 2000 took 0.103s
  training loss:		0.286093
  validation loss:		0.360404
  validation accuracy:		89.13 %
Epoch 1521 of 2000 took 0.104s
  training loss:		0.291288
  validation loss:		0.364943
  validation accuracy:		89.46 %
Epoch 1522 of 2000 took 0.104s
  training loss:		0.287851
  validation loss:		0.347806
  validation accuracy:		90.11 %
Epoch 1523 of 2000 took 0.104s
  training loss:		0.286012
  validation loss:		0.368200
  validation accuracy:		89.46 %
Epoch 1524 of 2000 took 0.106s
  training loss:		0.291785
  validation loss:		0.343459
  validation accuracy:		90.43 %
Epoch 1525 of 2000 took 0.104s
  training loss:		0.286895
  validation loss:		0.351154
  validation accuracy:		89.57 %
Epoch 1526 of 2000 took 0.103s
  training loss:		0.285328
  validation loss:		0.368009
  validation accuracy:		89.24 %
Epoch 1527 of 2000 took 0.104s
  training loss:		0.279003
  validation loss:		0.351901
  validation accuracy:		89.57 %
Epoch 1528 of 2000 took 0.103s
  training loss:		0.282775
  validation loss:		0.373445
  validation accuracy:		89.24 %
Epoch 1529 of 2000 took 0.104s
  training loss:		0.288411
  validation loss:		0.369370
  validation accuracy:		89.24 %
Epoch 1530 of 2000 took 0.103s
  training loss:		0.286740
  validation loss:		0.346256
  validation accuracy:		89.78 %
Epoch 1531 of 2000 took 0.104s
  training loss:		0.285741
  validation loss:		0.358509
  validation accuracy:		89.24 %
Epoch 1532 of 2000 took 0.104s
  training loss:		0.279704
  validation loss:		0.344437
  validation accuracy:		90.98 %
Epoch 1533 of 2000 took 0.104s
  training loss:		0.282337
  validation loss:		0.389719
  validation accuracy:		88.70 %
Epoch 1534 of 2000 took 0.103s
  training loss:		0.282318
  validation loss:		0.346133
  validation accuracy:		89.57 %
Epoch 1535 of 2000 took 0.104s
  training loss:		0.279321
  validation loss:		0.369365
  validation accuracy:		88.91 %
Epoch 1536 of 2000 took 0.104s
  training loss:		0.301070
  validation loss:		0.349078
  validation accuracy:		89.89 %
Epoch 1537 of 2000 took 0.104s
  training loss:		0.286341
  validation loss:		0.353890
  validation accuracy:		89.89 %
Epoch 1538 of 2000 took 0.103s
  training loss:		0.286783
  validation loss:		0.357981
  validation accuracy:		90.00 %
Epoch 1539 of 2000 took 0.104s
  training loss:		0.284838
  validation loss:		0.363027
  validation accuracy:		89.24 %
Epoch 1540 of 2000 took 0.104s
  training loss:		0.290541
  validation loss:		0.363142
  validation accuracy:		89.67 %
Epoch 1541 of 2000 took 0.104s
  training loss:		0.284027
  validation loss:		0.361932
  validation accuracy:		89.13 %
Epoch 1542 of 2000 took 0.104s
  training loss:		0.284704
  validation loss:		0.347727
  validation accuracy:		90.00 %
Epoch 1543 of 2000 took 0.104s
  training loss:		0.287717
  validation loss:		0.380480
  validation accuracy:		89.57 %
Epoch 1544 of 2000 took 0.103s
  training loss:		0.284753
  validation loss:		0.366468
  validation accuracy:		89.57 %
Epoch 1545 of 2000 took 0.103s
  training loss:		0.294290
  validation loss:		0.349281
  validation accuracy:		89.57 %
Epoch 1546 of 2000 took 0.104s
  training loss:		0.287936
  validation loss:		0.365420
  validation accuracy:		89.24 %
Epoch 1547 of 2000 took 0.104s
  training loss:		0.277052
  validation loss:		0.352169
  validation accuracy:		89.46 %
Epoch 1548 of 2000 took 0.104s
  training loss:		0.283775
  validation loss:		0.374941
  validation accuracy:		88.37 %
Epoch 1549 of 2000 took 0.104s
  training loss:		0.288065
  validation loss:		0.352670
  validation accuracy:		90.54 %
Epoch 1550 of 2000 took 0.104s
  training loss:		0.289460
  validation loss:		0.348248
  validation accuracy:		90.00 %
Epoch 1551 of 2000 took 0.104s
  training loss:		0.288884
  validation loss:		0.351979
  validation accuracy:		90.00 %
Epoch 1552 of 2000 took 0.107s
  training loss:		0.283750
  validation loss:		0.357985
  validation accuracy:		90.11 %
Epoch 1553 of 2000 took 0.104s
  training loss:		0.285512
  validation loss:		0.357057
  validation accuracy:		90.33 %
Epoch 1554 of 2000 took 0.104s
  training loss:		0.286987
  validation loss:		0.349343
  validation accuracy:		90.43 %
Epoch 1555 of 2000 took 0.104s
  training loss:		0.282481
  validation loss:		0.359933
  validation accuracy:		89.24 %
Epoch 1556 of 2000 took 0.104s
  training loss:		0.285864
  validation loss:		0.400028
  validation accuracy:		88.48 %
Epoch 1557 of 2000 took 0.104s
  training loss:		0.289200
  validation loss:		0.365723
  validation accuracy:		89.78 %
Epoch 1558 of 2000 took 0.104s
  training loss:		0.289644
  validation loss:		0.378649
  validation accuracy:		89.35 %
Epoch 1559 of 2000 took 0.103s
  training loss:		0.283982
  validation loss:		0.363067
  validation accuracy:		89.24 %
Epoch 1560 of 2000 took 0.104s
  training loss:		0.283958
  validation loss:		0.370527
  validation accuracy:		89.02 %
Epoch 1561 of 2000 took 0.103s
  training loss:		0.289212
  validation loss:		0.362175
  validation accuracy:		89.78 %
Epoch 1562 of 2000 took 0.104s
  training loss:		0.279317
  validation loss:		0.342615
  validation accuracy:		90.22 %
Epoch 1563 of 2000 took 0.103s
  training loss:		0.281184
  validation loss:		0.341639
  validation accuracy:		90.87 %
Epoch 1564 of 2000 took 0.104s
  training loss:		0.291135
  validation loss:		0.349393
  validation accuracy:		90.11 %
Epoch 1565 of 2000 took 0.104s
  training loss:		0.275670
  validation loss:		0.376887
  validation accuracy:		88.91 %
Epoch 1566 of 2000 took 0.104s
  training loss:		0.288313
  validation loss:		0.361094
  validation accuracy:		89.89 %
Epoch 1567 of 2000 took 0.104s
  training loss:		0.283623
  validation loss:		0.356931
  validation accuracy:		89.78 %
Epoch 1568 of 2000 took 0.104s
  training loss:		0.277093
  validation loss:		0.356264
  validation accuracy:		89.57 %
Epoch 1569 of 2000 took 0.104s
  training loss:		0.286027
  validation loss:		0.380104
  validation accuracy:		89.02 %
Epoch 1570 of 2000 took 0.103s
  training loss:		0.287763
  validation loss:		0.356691
  validation accuracy:		89.57 %
Epoch 1571 of 2000 took 0.104s
  training loss:		0.287448
  validation loss:		0.350265
  validation accuracy:		89.57 %
Epoch 1572 of 2000 took 0.104s
  training loss:		0.283200
  validation loss:		0.358587
  validation accuracy:		89.67 %
Epoch 1573 of 2000 took 0.104s
  training loss:		0.283736
  validation loss:		0.366557
  validation accuracy:		89.24 %
Epoch 1574 of 2000 took 0.104s
  training loss:		0.282944
  validation loss:		0.364300
  validation accuracy:		89.89 %
Epoch 1575 of 2000 took 0.104s
  training loss:		0.280971
  validation loss:		0.344905
  validation accuracy:		89.89 %
Epoch 1576 of 2000 took 0.104s
  training loss:		0.282911
  validation loss:		0.385574
  validation accuracy:		88.91 %
Epoch 1577 of 2000 took 0.104s
  training loss:		0.289733
  validation loss:		0.358056
  validation accuracy:		89.24 %
Epoch 1578 of 2000 took 0.104s
  training loss:		0.284449
  validation loss:		0.347678
  validation accuracy:		89.78 %
Epoch 1579 of 2000 took 0.104s
  training loss:		0.286485
  validation loss:		0.355995
  validation accuracy:		89.78 %
Epoch 1580 of 2000 took 0.104s
  training loss:		0.277429
  validation loss:		0.350085
  validation accuracy:		89.89 %
Epoch 1581 of 2000 took 0.104s
  training loss:		0.287951
  validation loss:		0.365860
  validation accuracy:		89.24 %
Epoch 1582 of 2000 took 0.106s
  training loss:		0.284609
  validation loss:		0.348909
  validation accuracy:		90.33 %
Epoch 1583 of 2000 took 0.104s
  training loss:		0.282564
  validation loss:		0.345167
  validation accuracy:		90.22 %
Epoch 1584 of 2000 took 0.103s
  training loss:		0.283284
  validation loss:		0.356794
  validation accuracy:		90.00 %
Epoch 1585 of 2000 took 0.104s
  training loss:		0.279488
  validation loss:		0.358526
  validation accuracy:		89.78 %
Epoch 1586 of 2000 took 0.104s
  training loss:		0.290915
  validation loss:		0.346243
  validation accuracy:		91.09 %
Epoch 1587 of 2000 took 0.104s
  training loss:		0.279172
  validation loss:		0.361476
  validation accuracy:		89.46 %
Epoch 1588 of 2000 took 0.104s
  training loss:		0.286226
  validation loss:		0.359087
  validation accuracy:		89.78 %
Epoch 1589 of 2000 took 0.104s
  training loss:		0.284496
  validation loss:		0.348788
  validation accuracy:		89.67 %
Epoch 1590 of 2000 took 0.104s
  training loss:		0.288304
  validation loss:		0.351611
  validation accuracy:		89.46 %
Epoch 1591 of 2000 took 0.104s
  training loss:		0.280476
  validation loss:		0.351975
  validation accuracy:		89.78 %
Epoch 1592 of 2000 took 0.104s
  training loss:		0.284926
  validation loss:		0.349254
  validation accuracy:		89.78 %
Epoch 1593 of 2000 took 0.104s
  training loss:		0.282419
  validation loss:		0.348462
  validation accuracy:		89.89 %
Epoch 1594 of 2000 took 0.104s
  training loss:		0.283602
  validation loss:		0.352204
  validation accuracy:		89.67 %
Epoch 1595 of 2000 took 0.104s
  training loss:		0.278391
  validation loss:		0.360734
  validation accuracy:		90.00 %
Epoch 1596 of 2000 took 0.104s
  training loss:		0.288742
  validation loss:		0.350355
  validation accuracy:		90.33 %
Epoch 1597 of 2000 took 0.104s
  training loss:		0.285890
  validation loss:		0.359277
  validation accuracy:		89.67 %
Epoch 1598 of 2000 took 0.104s
  training loss:		0.282172
  validation loss:		0.357929
  validation accuracy:		89.67 %
Epoch 1599 of 2000 took 0.104s
  training loss:		0.285120
  validation loss:		0.360221
  validation accuracy:		90.22 %
Epoch 1600 of 2000 took 0.104s
  training loss:		0.286608
  validation loss:		0.352007
  validation accuracy:		89.89 %
Epoch 1601 of 2000 took 0.103s
  training loss:		0.284999
  validation loss:		0.358903
  validation accuracy:		89.35 %
Epoch 1602 of 2000 took 0.104s
  training loss:		0.285365
  validation loss:		0.359029
  validation accuracy:		89.89 %
Epoch 1603 of 2000 took 0.104s
  training loss:		0.287492
  validation loss:		0.371017
  validation accuracy:		88.59 %
Epoch 1604 of 2000 took 0.104s
  training loss:		0.279841
  validation loss:		0.355526
  validation accuracy:		90.00 %
Epoch 1605 of 2000 took 0.104s
  training loss:		0.273807
  validation loss:		0.366424
  validation accuracy:		89.78 %
Epoch 1606 of 2000 took 0.104s
  training loss:		0.282994
  validation loss:		0.344387
  validation accuracy:		90.65 %
Epoch 1607 of 2000 took 0.104s
  training loss:		0.284492
  validation loss:		0.348106
  validation accuracy:		90.11 %
Epoch 1608 of 2000 took 0.104s
  training loss:		0.280145
  validation loss:		0.379775
  validation accuracy:		89.02 %
Epoch 1609 of 2000 took 0.104s
  training loss:		0.277414
  validation loss:		0.355141
  validation accuracy:		89.78 %
Epoch 1610 of 2000 took 0.104s
  training loss:		0.283061
  validation loss:		0.376379
  validation accuracy:		89.57 %
Epoch 1611 of 2000 took 0.104s
  training loss:		0.287762
  validation loss:		0.357252
  validation accuracy:		89.46 %
Epoch 1612 of 2000 took 0.104s
  training loss:		0.285453
  validation loss:		0.357736
  validation accuracy:		89.67 %
Epoch 1613 of 2000 took 0.104s
  training loss:		0.283095
  validation loss:		0.376651
  validation accuracy:		89.35 %
Epoch 1614 of 2000 took 0.104s
  training loss:		0.285997
  validation loss:		0.355601
  validation accuracy:		89.89 %
Epoch 1615 of 2000 took 0.105s
  training loss:		0.273794
  validation loss:		0.349577
  validation accuracy:		89.78 %
Epoch 1616 of 2000 took 0.105s
  training loss:		0.284400
  validation loss:		0.348255
  validation accuracy:		90.43 %
Epoch 1617 of 2000 took 0.104s
  training loss:		0.279737
  validation loss:		0.353160
  validation accuracy:		90.43 %
Epoch 1618 of 2000 took 0.104s
  training loss:		0.281142
  validation loss:		0.369751
  validation accuracy:		89.13 %
Epoch 1619 of 2000 took 0.104s
  training loss:		0.287173
  validation loss:		0.349690
  validation accuracy:		89.89 %
Epoch 1620 of 2000 took 0.104s
  training loss:		0.283617
  validation loss:		0.381714
  validation accuracy:		89.02 %
Epoch 1621 of 2000 took 0.104s
  training loss:		0.283490
  validation loss:		0.348754
  validation accuracy:		89.78 %
Epoch 1622 of 2000 took 0.104s
  training loss:		0.275860
  validation loss:		0.358756
  validation accuracy:		89.89 %
Epoch 1623 of 2000 took 0.104s
  training loss:		0.277579
  validation loss:		0.362314
  validation accuracy:		89.78 %
Epoch 1624 of 2000 took 0.104s
  training loss:		0.279263
  validation loss:		0.357310
  validation accuracy:		89.78 %
Epoch 1625 of 2000 took 0.104s
  training loss:		0.279410
  validation loss:		0.367069
  validation accuracy:		89.35 %
Epoch 1626 of 2000 took 0.103s
  training loss:		0.289040
  validation loss:		0.372285
  validation accuracy:		89.02 %
Epoch 1627 of 2000 took 0.104s
  training loss:		0.284233
  validation loss:		0.354554
  validation accuracy:		89.57 %
Epoch 1628 of 2000 took 0.104s
  training loss:		0.283308
  validation loss:		0.368049
  validation accuracy:		89.13 %
Epoch 1629 of 2000 took 0.095s
  training loss:		0.282358
  validation loss:		0.357521
  validation accuracy:		89.89 %
Epoch 1630 of 2000 took 0.097s
  training loss:		0.280465
  validation loss:		0.373412
  validation accuracy:		88.80 %
Epoch 1631 of 2000 took 0.102s
  training loss:		0.285704
  validation loss:		0.357522
  validation accuracy:		89.78 %
Epoch 1632 of 2000 took 0.095s
  training loss:		0.275759
  validation loss:		0.343880
  validation accuracy:		90.76 %
Epoch 1633 of 2000 took 0.098s
  training loss:		0.284528
  validation loss:		0.354357
  validation accuracy:		89.67 %
Epoch 1634 of 2000 took 0.096s
  training loss:		0.281024
  validation loss:		0.360616
  validation accuracy:		90.22 %
Epoch 1635 of 2000 took 0.102s
  training loss:		0.280080
  validation loss:		0.356990
  validation accuracy:		89.78 %
Epoch 1636 of 2000 took 0.099s
  training loss:		0.284644
  validation loss:		0.369518
  validation accuracy:		89.35 %
Epoch 1637 of 2000 took 0.096s
  training loss:		0.277980
  validation loss:		0.357702
  validation accuracy:		89.57 %
Epoch 1638 of 2000 took 0.102s
  training loss:		0.280970
  validation loss:		0.372434
  validation accuracy:		90.11 %
Epoch 1639 of 2000 took 0.099s
  training loss:		0.288346
  validation loss:		0.362652
  validation accuracy:		89.89 %
Epoch 1640 of 2000 took 0.097s
  training loss:		0.283089
  validation loss:		0.348802
  validation accuracy:		90.33 %
Epoch 1641 of 2000 took 0.097s
  training loss:		0.276615
  validation loss:		0.363394
  validation accuracy:		89.57 %
Epoch 1642 of 2000 took 0.097s
  training loss:		0.278841
  validation loss:		0.361278
  validation accuracy:		89.67 %
Epoch 1643 of 2000 took 0.102s
  training loss:		0.279184
  validation loss:		0.361493
  validation accuracy:		90.11 %
Epoch 1644 of 2000 took 0.098s
  training loss:		0.278129
  validation loss:		0.348355
  validation accuracy:		90.33 %
Epoch 1645 of 2000 took 0.097s
  training loss:		0.285002
  validation loss:		0.369472
  validation accuracy:		89.24 %
Epoch 1646 of 2000 took 0.104s
  training loss:		0.271878
  validation loss:		0.400696
  validation accuracy:		88.91 %
Epoch 1647 of 2000 took 0.096s
  training loss:		0.277400
  validation loss:		0.352343
  validation accuracy:		90.00 %
Epoch 1648 of 2000 took 0.097s
  training loss:		0.281995
  validation loss:		0.359331
  validation accuracy:		90.00 %
Epoch 1649 of 2000 took 0.096s
  training loss:		0.278051
  validation loss:		0.356927
  validation accuracy:		90.11 %
Epoch 1650 of 2000 took 0.099s
  training loss:		0.279795
  validation loss:		0.367642
  validation accuracy:		90.00 %
Epoch 1651 of 2000 took 0.102s
  training loss:		0.280708
  validation loss:		0.370127
  validation accuracy:		89.46 %
Epoch 1652 of 2000 took 0.096s
  training loss:		0.281599
  validation loss:		0.365534
  validation accuracy:		89.67 %
Epoch 1653 of 2000 took 0.099s
  training loss:		0.287521
  validation loss:		0.353054
  validation accuracy:		90.00 %
Epoch 1654 of 2000 took 0.104s
  training loss:		0.285931
  validation loss:		0.355119
  validation accuracy:		90.11 %
Epoch 1655 of 2000 took 0.096s
  training loss:		0.286086
  validation loss:		0.353294
  validation accuracy:		90.22 %
Epoch 1656 of 2000 took 0.098s
  training loss:		0.285264
  validation loss:		0.368861
  validation accuracy:		89.46 %
Epoch 1657 of 2000 took 0.096s
  training loss:		0.274621
  validation loss:		0.362072
  validation accuracy:		89.89 %
Epoch 1658 of 2000 took 0.102s
  training loss:		0.282630
  validation loss:		0.356544
  validation accuracy:		90.00 %
Epoch 1659 of 2000 took 0.098s
  training loss:		0.278580
  validation loss:		0.347130
  validation accuracy:		90.54 %
Epoch 1660 of 2000 took 0.096s
  training loss:		0.281774
  validation loss:		0.366610
  validation accuracy:		89.67 %
Epoch 1661 of 2000 took 0.103s
  training loss:		0.276195
  validation loss:		0.357395
  validation accuracy:		89.78 %
Epoch 1662 of 2000 took 0.098s
  training loss:		0.279438
  validation loss:		0.352982
  validation accuracy:		89.67 %
Epoch 1663 of 2000 took 0.097s
  training loss:		0.280354
  validation loss:		0.355127
  validation accuracy:		90.00 %
Epoch 1664 of 2000 took 0.097s
  training loss:		0.280767
  validation loss:		0.355097
  validation accuracy:		90.22 %
Epoch 1665 of 2000 took 0.097s
  training loss:		0.281866
  validation loss:		0.373908
  validation accuracy:		89.67 %
Epoch 1666 of 2000 took 0.101s
  training loss:		0.284522
  validation loss:		0.350079
  validation accuracy:		90.22 %
Epoch 1667 of 2000 took 0.097s
  training loss:		0.276037
  validation loss:		0.375678
  validation accuracy:		89.24 %
Epoch 1668 of 2000 took 0.097s
  training loss:		0.290426
  validation loss:		0.362614
  validation accuracy:		89.89 %
Epoch 1669 of 2000 took 0.104s
  training loss:		0.280678
  validation loss:		0.348188
  validation accuracy:		90.33 %
Epoch 1670 of 2000 took 0.096s
  training loss:		0.274967
  validation loss:		0.354481
  validation accuracy:		90.22 %
Epoch 1671 of 2000 took 0.098s
  training loss:		0.283739
  validation loss:		0.360860
  validation accuracy:		89.67 %
Epoch 1672 of 2000 took 0.096s
  training loss:		0.276153
  validation loss:		0.351440
  validation accuracy:		90.33 %
Epoch 1673 of 2000 took 0.099s
  training loss:		0.279982
  validation loss:		0.360076
  validation accuracy:		90.43 %
Epoch 1674 of 2000 took 0.103s
  training loss:		0.273888
  validation loss:		0.359451
  validation accuracy:		89.57 %
Epoch 1675 of 2000 took 0.099s
  training loss:		0.275975
  validation loss:		0.363418
  validation accuracy:		89.46 %
Epoch 1676 of 2000 took 0.102s
  training loss:		0.277919
  validation loss:		0.358803
  validation accuracy:		89.67 %
Epoch 1677 of 2000 took 0.105s
  training loss:		0.280374
  validation loss:		0.351176
  validation accuracy:		90.11 %
Epoch 1678 of 2000 took 0.099s
  training loss:		0.280026
  validation loss:		0.354903
  validation accuracy:		90.22 %
Epoch 1679 of 2000 took 0.100s
  training loss:		0.279055
  validation loss:		0.371166
  validation accuracy:		89.46 %
Epoch 1680 of 2000 took 0.099s
  training loss:		0.274904
  validation loss:		0.353882
  validation accuracy:		89.78 %
Epoch 1681 of 2000 took 0.105s
  training loss:		0.277565
  validation loss:		0.360859
  validation accuracy:		89.46 %
Epoch 1682 of 2000 took 0.101s
  training loss:		0.278160
  validation loss:		0.371362
  validation accuracy:		89.24 %
Epoch 1683 of 2000 took 0.099s
  training loss:		0.276025
  validation loss:		0.351092
  validation accuracy:		90.22 %
Epoch 1684 of 2000 took 0.107s
  training loss:		0.278453
  validation loss:		0.354412
  validation accuracy:		90.00 %
Epoch 1685 of 2000 took 0.100s
  training loss:		0.277959
  validation loss:		0.352210
  validation accuracy:		89.57 %
Epoch 1686 of 2000 took 0.100s
  training loss:		0.286818
  validation loss:		0.363363
  validation accuracy:		89.78 %
Epoch 1687 of 2000 took 0.100s
  training loss:		0.281659
  validation loss:		0.359513
  validation accuracy:		90.00 %
Epoch 1688 of 2000 took 0.101s
  training loss:		0.283286
  validation loss:		0.356160
  validation accuracy:		90.22 %
Epoch 1689 of 2000 took 0.098s
  training loss:		0.277983
  validation loss:		0.350656
  validation accuracy:		89.89 %
Epoch 1690 of 2000 took 0.098s
  training loss:		0.279617
  validation loss:		0.356010
  validation accuracy:		89.78 %
Epoch 1691 of 2000 took 0.096s
  training loss:		0.281029
  validation loss:		0.364385
  validation accuracy:		89.46 %
Epoch 1692 of 2000 took 0.097s
  training loss:		0.281105
  validation loss:		0.350219
  validation accuracy:		89.67 %
Epoch 1693 of 2000 took 0.096s
  training loss:		0.283554
  validation loss:		0.352829
  validation accuracy:		90.00 %
Epoch 1694 of 2000 took 0.097s
  training loss:		0.274020
  validation loss:		0.357180
  validation accuracy:		89.57 %
Epoch 1695 of 2000 took 0.096s
  training loss:		0.278263
  validation loss:		0.357116
  validation accuracy:		90.00 %
Epoch 1696 of 2000 took 0.099s
  training loss:		0.270849
  validation loss:		0.345203
  validation accuracy:		90.54 %
Epoch 1697 of 2000 took 0.097s
  training loss:		0.280148
  validation loss:		0.352932
  validation accuracy:		89.78 %
Epoch 1698 of 2000 took 0.096s
  training loss:		0.283663
  validation loss:		0.355327
  validation accuracy:		90.22 %
Epoch 1699 of 2000 took 0.097s
  training loss:		0.279074
  validation loss:		0.361695
  validation accuracy:		89.46 %
Epoch 1700 of 2000 took 0.097s
  training loss:		0.286914
  validation loss:		0.363228
  validation accuracy:		89.89 %
Epoch 1701 of 2000 took 0.097s
  training loss:		0.276354
  validation loss:		0.349792
  validation accuracy:		90.11 %
Epoch 1702 of 2000 took 0.097s
  training loss:		0.277989
  validation loss:		0.368597
  validation accuracy:		89.57 %
Epoch 1703 of 2000 took 0.096s
  training loss:		0.281319
  validation loss:		0.361844
  validation accuracy:		90.00 %
Epoch 1704 of 2000 took 0.097s
  training loss:		0.276388
  validation loss:		0.380989
  validation accuracy:		89.89 %
Epoch 1705 of 2000 took 0.097s
  training loss:		0.280158
  validation loss:		0.345933
  validation accuracy:		90.54 %
Epoch 1706 of 2000 took 0.097s
  training loss:		0.279688
  validation loss:		0.357574
  validation accuracy:		89.57 %
Epoch 1707 of 2000 took 0.096s
  training loss:		0.272702
  validation loss:		0.344761
  validation accuracy:		90.43 %
Epoch 1708 of 2000 took 0.097s
  training loss:		0.275754
  validation loss:		0.351929
  validation accuracy:		89.78 %
Epoch 1709 of 2000 took 0.097s
  training loss:		0.282585
  validation loss:		0.353638
  validation accuracy:		90.00 %
Epoch 1710 of 2000 took 0.097s
  training loss:		0.278834
  validation loss:		0.368132
  validation accuracy:		89.57 %
Epoch 1711 of 2000 took 0.096s
  training loss:		0.280387
  validation loss:		0.348954
  validation accuracy:		90.00 %
Epoch 1712 of 2000 took 0.097s
  training loss:		0.272543
  validation loss:		0.349026
  validation accuracy:		90.43 %
Epoch 1713 of 2000 took 0.097s
  training loss:		0.286278
  validation loss:		0.368373
  validation accuracy:		88.91 %
Epoch 1714 of 2000 took 0.096s
  training loss:		0.278607
  validation loss:		0.382228
  validation accuracy:		89.02 %
Epoch 1715 of 2000 took 0.097s
  training loss:		0.282338
  validation loss:		0.348763
  validation accuracy:		90.33 %
Epoch 1716 of 2000 took 0.096s
  training loss:		0.273828
  validation loss:		0.344330
  validation accuracy:		90.54 %
Epoch 1717 of 2000 took 0.096s
  training loss:		0.279343
  validation loss:		0.354882
  validation accuracy:		90.00 %
Epoch 1718 of 2000 took 0.096s
  training loss:		0.278392
  validation loss:		0.350893
  validation accuracy:		90.00 %
Epoch 1719 of 2000 took 0.096s
  training loss:		0.278932
  validation loss:		0.347626
  validation accuracy:		90.11 %
Epoch 1720 of 2000 took 0.097s
  training loss:		0.273945
  validation loss:		0.352923
  validation accuracy:		90.11 %
Epoch 1721 of 2000 took 0.097s
  training loss:		0.284294
  validation loss:		0.354950
  validation accuracy:		89.89 %
Epoch 1722 of 2000 took 0.096s
  training loss:		0.281906
  validation loss:		0.377859
  validation accuracy:		88.80 %
Epoch 1723 of 2000 took 0.096s
  training loss:		0.280904
  validation loss:		0.365248
  validation accuracy:		89.89 %
Epoch 1724 of 2000 took 0.096s
  training loss:		0.281980
  validation loss:		0.348965
  validation accuracy:		90.22 %
Epoch 1725 of 2000 took 0.097s
  training loss:		0.279545
  validation loss:		0.358177
  validation accuracy:		90.43 %
Epoch 1726 of 2000 took 0.096s
  training loss:		0.275127
  validation loss:		0.351255
  validation accuracy:		89.67 %
Epoch 1727 of 2000 took 0.097s
  training loss:		0.280159
  validation loss:		0.343966
  validation accuracy:		90.33 %
Epoch 1728 of 2000 took 0.096s
  training loss:		0.283404
  validation loss:		0.351237
  validation accuracy:		90.11 %
Epoch 1729 of 2000 took 0.097s
  training loss:		0.279815
  validation loss:		0.349945
  validation accuracy:		90.00 %
Epoch 1730 of 2000 took 0.096s
  training loss:		0.282046
  validation loss:		0.355402
  validation accuracy:		90.00 %
Epoch 1731 of 2000 took 0.097s
  training loss:		0.282174
  validation loss:		0.356671
  validation accuracy:		89.57 %
Epoch 1732 of 2000 took 0.098s
  training loss:		0.281124
  validation loss:		0.364879
  validation accuracy:		89.57 %
Epoch 1733 of 2000 took 0.097s
  training loss:		0.269524
  validation loss:		0.350449
  validation accuracy:		90.22 %
Epoch 1734 of 2000 took 0.096s
  training loss:		0.282902
  validation loss:		0.347536
  validation accuracy:		89.89 %
Epoch 1735 of 2000 took 0.097s
  training loss:		0.279019
  validation loss:		0.365067
  validation accuracy:		89.78 %
Epoch 1736 of 2000 took 0.097s
  training loss:		0.276933
  validation loss:		0.362640
  validation accuracy:		89.57 %
Epoch 1737 of 2000 took 0.097s
  training loss:		0.279502
  validation loss:		0.348929
  validation accuracy:		90.22 %
Epoch 1738 of 2000 took 0.096s
  training loss:		0.279040
  validation loss:		0.385215
  validation accuracy:		89.13 %
Epoch 1739 of 2000 took 0.097s
  training loss:		0.276850
  validation loss:		0.355507
  validation accuracy:		90.22 %
Epoch 1740 of 2000 took 0.096s
  training loss:		0.279906
  validation loss:		0.358351
  validation accuracy:		90.00 %
Epoch 1741 of 2000 took 0.097s
  training loss:		0.275117
  validation loss:		0.369510
  validation accuracy:		89.13 %
Epoch 1742 of 2000 took 0.097s
  training loss:		0.276212
  validation loss:		0.351535
  validation accuracy:		90.22 %
Epoch 1743 of 2000 took 0.098s
  training loss:		0.279954
  validation loss:		0.367049
  validation accuracy:		89.35 %
Epoch 1744 of 2000 took 0.097s
  training loss:		0.279833
  validation loss:		0.354217
  validation accuracy:		89.67 %
Epoch 1745 of 2000 took 0.096s
  training loss:		0.280576
  validation loss:		0.356671
  validation accuracy:		90.22 %
Epoch 1746 of 2000 took 0.097s
  training loss:		0.265568
  validation loss:		0.358029
  validation accuracy:		90.00 %
Epoch 1747 of 2000 took 0.096s
  training loss:		0.279178
  validation loss:		0.355181
  validation accuracy:		90.11 %
Epoch 1748 of 2000 took 0.097s
  training loss:		0.278064
  validation loss:		0.349876
  validation accuracy:		90.33 %
Epoch 1749 of 2000 took 0.097s
  training loss:		0.280230
  validation loss:		0.345030
  validation accuracy:		90.65 %
Epoch 1750 of 2000 took 0.096s
  training loss:		0.280919
  validation loss:		0.370615
  validation accuracy:		89.57 %
Epoch 1751 of 2000 took 0.097s
  training loss:		0.278702
  validation loss:		0.350619
  validation accuracy:		90.00 %
Epoch 1752 of 2000 took 0.097s
  training loss:		0.275945
  validation loss:		0.383287
  validation accuracy:		89.24 %
Epoch 1753 of 2000 took 0.096s
  training loss:		0.272517
  validation loss:		0.357421
  validation accuracy:		89.67 %
Epoch 1754 of 2000 took 0.097s
  training loss:		0.274468
  validation loss:		0.345462
  validation accuracy:		90.43 %
Epoch 1755 of 2000 took 0.097s
  training loss:		0.277626
  validation loss:		0.366552
  validation accuracy:		90.00 %
Epoch 1756 of 2000 took 0.097s
  training loss:		0.287489
  validation loss:		0.354711
  validation accuracy:		89.35 %
Epoch 1757 of 2000 took 0.097s
  training loss:		0.278353
  validation loss:		0.363606
  validation accuracy:		90.11 %
Epoch 1758 of 2000 took 0.096s
  training loss:		0.280387
  validation loss:		0.351214
  validation accuracy:		89.89 %
Epoch 1759 of 2000 took 0.096s
  training loss:		0.279240
  validation loss:		0.359373
  validation accuracy:		90.33 %
Epoch 1760 of 2000 took 0.097s
  training loss:		0.278010
  validation loss:		0.353248
  validation accuracy:		90.11 %
Epoch 1761 of 2000 took 0.096s
  training loss:		0.273890
  validation loss:		0.347537
  validation accuracy:		90.22 %
Epoch 1762 of 2000 took 0.097s
  training loss:		0.275866
  validation loss:		0.372102
  validation accuracy:		89.24 %
Epoch 1763 of 2000 took 0.097s
  training loss:		0.273476
  validation loss:		0.348213
  validation accuracy:		90.43 %
Epoch 1764 of 2000 took 0.097s
  training loss:		0.281769
  validation loss:		0.348671
  validation accuracy:		90.00 %
Epoch 1765 of 2000 took 0.096s
  training loss:		0.277132
  validation loss:		0.351170
  validation accuracy:		89.89 %
Epoch 1766 of 2000 took 0.097s
  training loss:		0.273793
  validation loss:		0.353251
  validation accuracy:		90.11 %
Epoch 1767 of 2000 took 0.097s
  training loss:		0.274846
  validation loss:		0.341980
  validation accuracy:		90.76 %
Epoch 1768 of 2000 took 0.097s
  training loss:		0.276063
  validation loss:		0.349261
  validation accuracy:		89.46 %
Epoch 1769 of 2000 took 0.096s
  training loss:		0.279861
  validation loss:		0.350845
  validation accuracy:		89.57 %
Epoch 1770 of 2000 took 0.096s
  training loss:		0.271397
  validation loss:		0.359385
  validation accuracy:		90.11 %
Epoch 1771 of 2000 took 0.097s
  training loss:		0.282557
  validation loss:		0.361897
  validation accuracy:		89.35 %
Epoch 1772 of 2000 took 0.097s
  training loss:		0.276182
  validation loss:		0.373625
  validation accuracy:		89.57 %
Epoch 1773 of 2000 took 0.097s
  training loss:		0.277748
  validation loss:		0.362900
  validation accuracy:		89.67 %
Epoch 1774 of 2000 took 0.096s
  training loss:		0.283531
  validation loss:		0.351457
  validation accuracy:		90.00 %
Epoch 1775 of 2000 took 0.097s
  training loss:		0.278736
  validation loss:		0.353918
  validation accuracy:		90.65 %
Epoch 1776 of 2000 took 0.096s
  training loss:		0.279917
  validation loss:		0.360532
  validation accuracy:		89.89 %
Epoch 1777 of 2000 took 0.097s
  training loss:		0.277719
  validation loss:		0.350421
  validation accuracy:		90.33 %
Epoch 1778 of 2000 took 0.097s
  training loss:		0.274920
  validation loss:		0.347476
  validation accuracy:		90.43 %
Epoch 1779 of 2000 took 0.097s
  training loss:		0.274321
  validation loss:		0.349684
  validation accuracy:		90.43 %
Epoch 1780 of 2000 took 0.096s
  training loss:		0.281583
  validation loss:		0.356229
  validation accuracy:		90.00 %
Epoch 1781 of 2000 took 0.097s
  training loss:		0.277642
  validation loss:		0.347921
  validation accuracy:		90.00 %
Epoch 1782 of 2000 took 0.096s
  training loss:		0.278446
  validation loss:		0.370159
  validation accuracy:		88.91 %
Epoch 1783 of 2000 took 0.097s
  training loss:		0.281865
  validation loss:		0.359088
  validation accuracy:		90.00 %
Epoch 1784 of 2000 took 0.096s
  training loss:		0.273283
  validation loss:		0.358261
  validation accuracy:		89.13 %
Epoch 1785 of 2000 took 0.096s
  training loss:		0.283693
  validation loss:		0.350035
  validation accuracy:		90.11 %
Epoch 1786 of 2000 took 0.096s
  training loss:		0.281642
  validation loss:		0.344731
  validation accuracy:		90.22 %
Epoch 1787 of 2000 took 0.097s
  training loss:		0.274955
  validation loss:		0.356713
  validation accuracy:		89.35 %
Epoch 1788 of 2000 took 0.097s
  training loss:		0.282282
  validation loss:		0.346441
  validation accuracy:		90.43 %
Epoch 1789 of 2000 took 0.097s
  training loss:		0.270825
  validation loss:		0.349185
  validation accuracy:		90.22 %
Epoch 1790 of 2000 took 0.096s
  training loss:		0.276624
  validation loss:		0.368385
  validation accuracy:		89.57 %
Epoch 1791 of 2000 took 0.097s
  training loss:		0.278757
  validation loss:		0.350919
  validation accuracy:		90.43 %
Epoch 1792 of 2000 took 0.097s
  training loss:		0.280855
  validation loss:		0.346148
  validation accuracy:		90.11 %
Epoch 1793 of 2000 took 0.097s
  training loss:		0.277456
  validation loss:		0.354676
  validation accuracy:		90.00 %
Epoch 1794 of 2000 took 0.097s
  training loss:		0.279535
  validation loss:		0.353830
  validation accuracy:		89.35 %
Epoch 1795 of 2000 took 0.098s
  training loss:		0.274119
  validation loss:		0.354902
  validation accuracy:		89.89 %
Epoch 1796 of 2000 took 0.097s
  training loss:		0.281544
  validation loss:		0.347388
  validation accuracy:		90.11 %
Epoch 1797 of 2000 took 0.097s
  training loss:		0.280951
  validation loss:		0.362859
  validation accuracy:		89.67 %
Epoch 1798 of 2000 took 0.097s
  training loss:		0.273951
  validation loss:		0.371181
  validation accuracy:		90.22 %
Epoch 1799 of 2000 took 0.096s
  training loss:		0.283309
  validation loss:		0.346922
  validation accuracy:		90.43 %
Epoch 1800 of 2000 took 0.097s
  training loss:		0.276671
  validation loss:		0.364166
  validation accuracy:		89.46 %
Epoch 1801 of 2000 took 0.099s
  training loss:		0.279788
  validation loss:		0.357038
  validation accuracy:		90.43 %
Epoch 1802 of 2000 took 0.097s
  training loss:		0.279927
  validation loss:		0.361078
  validation accuracy:		89.46 %
Epoch 1803 of 2000 took 0.096s
  training loss:		0.273351
  validation loss:		0.349302
  validation accuracy:		90.11 %
Epoch 1804 of 2000 took 0.097s
  training loss:		0.272769
  validation loss:		0.356083
  validation accuracy:		90.11 %
Epoch 1805 of 2000 took 0.096s
  training loss:		0.278407
  validation loss:		0.360128
  validation accuracy:		89.02 %
Epoch 1806 of 2000 took 0.097s
  training loss:		0.278429
  validation loss:		0.350589
  validation accuracy:		90.00 %
Epoch 1807 of 2000 took 0.097s
  training loss:		0.278490
  validation loss:		0.376800
  validation accuracy:		89.02 %
Epoch 1808 of 2000 took 0.097s
  training loss:		0.278971
  validation loss:		0.355940
  validation accuracy:		90.33 %
Epoch 1809 of 2000 took 0.096s
  training loss:		0.279861
  validation loss:		0.347934
  validation accuracy:		89.89 %
Epoch 1810 of 2000 took 0.096s
  training loss:		0.280072
  validation loss:		0.354483
  validation accuracy:		90.11 %
Epoch 1811 of 2000 took 0.096s
  training loss:		0.274162
  validation loss:		0.348018
  validation accuracy:		90.11 %
Epoch 1812 of 2000 took 0.096s
  training loss:		0.273183
  validation loss:		0.359132
  validation accuracy:		90.22 %
Epoch 1813 of 2000 took 0.097s
  training loss:		0.277163
  validation loss:		0.357702
  validation accuracy:		90.22 %
Epoch 1814 of 2000 took 0.097s
  training loss:		0.272984
  validation loss:		0.350728
  validation accuracy:		89.89 %
Epoch 1815 of 2000 took 0.096s
  training loss:		0.273973
  validation loss:		0.355679
  validation accuracy:		90.00 %
Epoch 1816 of 2000 took 0.096s
  training loss:		0.273745
  validation loss:		0.353789
  validation accuracy:		90.54 %
Epoch 1817 of 2000 took 0.096s
  training loss:		0.275623
  validation loss:		0.354137
  validation accuracy:		89.67 %
Epoch 1818 of 2000 took 0.097s
  training loss:		0.275000
  validation loss:		0.352195
  validation accuracy:		89.46 %
Epoch 1819 of 2000 took 0.096s
  training loss:		0.274406
  validation loss:		0.350384
  validation accuracy:		90.00 %
Epoch 1820 of 2000 took 0.097s
  training loss:		0.275587
  validation loss:		0.350831
  validation accuracy:		90.22 %
Epoch 1821 of 2000 took 0.096s
  training loss:		0.282540
  validation loss:		0.357858
  validation accuracy:		90.11 %
Epoch 1822 of 2000 took 0.097s
  training loss:		0.273491
  validation loss:		0.358199
  validation accuracy:		90.11 %
Epoch 1823 of 2000 took 0.099s
  training loss:		0.273503
  validation loss:		0.351234
  validation accuracy:		90.00 %
Epoch 1824 of 2000 took 0.100s
  training loss:		0.279003
  validation loss:		0.352338
  validation accuracy:		89.89 %
Epoch 1825 of 2000 took 0.100s
  training loss:		0.272195
  validation loss:		0.349015
  validation accuracy:		90.11 %
Epoch 1826 of 2000 took 0.100s
  training loss:		0.276322
  validation loss:		0.365261
  validation accuracy:		89.46 %
Epoch 1827 of 2000 took 0.100s
  training loss:		0.277644
  validation loss:		0.352133
  validation accuracy:		90.11 %
Epoch 1828 of 2000 took 0.100s
  training loss:		0.282406
  validation loss:		0.347790
  validation accuracy:		90.33 %
Epoch 1829 of 2000 took 0.100s
  training loss:		0.275077
  validation loss:		0.353851
  validation accuracy:		90.11 %
Epoch 1830 of 2000 took 0.100s
  training loss:		0.278547
  validation loss:		0.349635
  validation accuracy:		90.11 %
Epoch 1831 of 2000 took 0.100s
  training loss:		0.275726
  validation loss:		0.355038
  validation accuracy:		90.11 %
Epoch 1832 of 2000 took 0.100s
  training loss:		0.278721
  validation loss:		0.348080
  validation accuracy:		90.22 %
Epoch 1833 of 2000 took 0.100s
  training loss:		0.280584
  validation loss:		0.343346
  validation accuracy:		90.11 %
Epoch 1834 of 2000 took 0.100s
  training loss:		0.277526
  validation loss:		0.362135
  validation accuracy:		89.67 %
Epoch 1835 of 2000 took 0.100s
  training loss:		0.277733
  validation loss:		0.372095
  validation accuracy:		89.67 %
Epoch 1836 of 2000 took 0.100s
  training loss:		0.278542
  validation loss:		0.370591
  validation accuracy:		89.67 %
Epoch 1837 of 2000 took 0.098s
  training loss:		0.277459
  validation loss:		0.358612
  validation accuracy:		90.00 %
Epoch 1838 of 2000 took 0.097s
  training loss:		0.278694
  validation loss:		0.350032
  validation accuracy:		90.11 %
Epoch 1839 of 2000 took 0.097s
  training loss:		0.280894
  validation loss:		0.353127
  validation accuracy:		90.00 %
Epoch 1840 of 2000 took 0.097s
  training loss:		0.273996
  validation loss:		0.353715
  validation accuracy:		90.00 %
Epoch 1841 of 2000 took 0.096s
  training loss:		0.274855
  validation loss:		0.356238
  validation accuracy:		90.00 %
Epoch 1842 of 2000 took 0.097s
  training loss:		0.278296
  validation loss:		0.356224
  validation accuracy:		90.00 %
Epoch 1843 of 2000 took 0.099s
  training loss:		0.278346
  validation loss:		0.350228
  validation accuracy:		89.89 %
Epoch 1844 of 2000 took 0.105s
  training loss:		0.277447
  validation loss:		0.350049
  validation accuracy:		89.89 %
Epoch 1845 of 2000 took 0.134s
  training loss:		0.272569
  validation loss:		0.369621
  validation accuracy:		89.57 %
Epoch 1846 of 2000 took 0.137s
  training loss:		0.276470
  validation loss:		0.356938
  validation accuracy:		89.78 %
Epoch 1847 of 2000 took 0.099s
  training loss:		0.274605
  validation loss:		0.358297
  validation accuracy:		89.89 %
Epoch 1848 of 2000 took 0.100s
  training loss:		0.276354
  validation loss:		0.361369
  validation accuracy:		89.89 %
Epoch 1849 of 2000 took 0.101s
  training loss:		0.276593
  validation loss:		0.353788
  validation accuracy:		90.00 %
Epoch 1850 of 2000 took 0.099s
  training loss:		0.272964
  validation loss:		0.346816
  validation accuracy:		90.00 %
Epoch 1851 of 2000 took 0.098s
  training loss:		0.274293
  validation loss:		0.352578
  validation accuracy:		89.35 %
Epoch 1852 of 2000 took 0.096s
  training loss:		0.277404
  validation loss:		0.368437
  validation accuracy:		89.46 %
Epoch 1853 of 2000 took 0.096s
  training loss:		0.277621
  validation loss:		0.373786
  validation accuracy:		89.35 %
Epoch 1854 of 2000 took 0.096s
  training loss:		0.269556
  validation loss:		0.382048
  validation accuracy:		89.13 %
Epoch 1855 of 2000 took 0.096s
  training loss:		0.274128
  validation loss:		0.346098
  validation accuracy:		90.33 %
Epoch 1856 of 2000 took 0.098s
  training loss:		0.282704
  validation loss:		0.349948
  validation accuracy:		90.00 %
Epoch 1857 of 2000 took 0.096s
  training loss:		0.270043
  validation loss:		0.350243
  validation accuracy:		89.78 %
Epoch 1858 of 2000 took 0.097s
  training loss:		0.279144
  validation loss:		0.348188
  validation accuracy:		90.22 %
Epoch 1859 of 2000 took 0.096s
  training loss:		0.281508
  validation loss:		0.344108
  validation accuracy:		90.33 %
Epoch 1860 of 2000 took 0.096s
  training loss:		0.276092
  validation loss:		0.366150
  validation accuracy:		89.57 %
Epoch 1861 of 2000 took 0.096s
  training loss:		0.281704
  validation loss:		0.366994
  validation accuracy:		89.24 %
Epoch 1862 of 2000 took 0.098s
  training loss:		0.269619
  validation loss:		0.357981
  validation accuracy:		90.11 %
Epoch 1863 of 2000 took 0.097s
  training loss:		0.278106
  validation loss:		0.353159
  validation accuracy:		90.22 %
Epoch 1864 of 2000 took 0.096s
  training loss:		0.278813
  validation loss:		0.350898
  validation accuracy:		90.00 %
Epoch 1865 of 2000 took 0.096s
  training loss:		0.274247
  validation loss:		0.365042
  validation accuracy:		89.67 %
Epoch 1866 of 2000 took 0.096s
  training loss:		0.282311
  validation loss:		0.367494
  validation accuracy:		89.67 %
Epoch 1867 of 2000 took 0.096s
  training loss:		0.270182
  validation loss:		0.357754
  validation accuracy:		90.22 %
Epoch 1868 of 2000 took 0.097s
  training loss:		0.273878
  validation loss:		0.359014
  validation accuracy:		90.11 %
Epoch 1869 of 2000 took 0.097s
  training loss:		0.276991
  validation loss:		0.343903
  validation accuracy:		90.43 %
Epoch 1870 of 2000 took 0.096s
  training loss:		0.279184
  validation loss:		0.354555
  validation accuracy:		89.67 %
Epoch 1871 of 2000 took 0.096s
  training loss:		0.275132
  validation loss:		0.351668
  validation accuracy:		90.00 %
Epoch 1872 of 2000 took 0.097s
  training loss:		0.274983
  validation loss:		0.351307
  validation accuracy:		89.89 %
Epoch 1873 of 2000 took 0.096s
  training loss:		0.278738
  validation loss:		0.361087
  validation accuracy:		89.78 %
Epoch 1874 of 2000 took 0.096s
  training loss:		0.276196
  validation loss:		0.361867
  validation accuracy:		89.78 %
Epoch 1875 of 2000 took 0.096s
  training loss:		0.270999
  validation loss:		0.357703
  validation accuracy:		90.11 %
Epoch 1876 of 2000 took 0.096s
  training loss:		0.271889
  validation loss:		0.361431
  validation accuracy:		90.11 %
Epoch 1877 of 2000 took 0.097s
  training loss:		0.279407
  validation loss:		0.358142
  validation accuracy:		90.00 %
Epoch 1878 of 2000 took 0.096s
  training loss:		0.275784
  validation loss:		0.364044
  validation accuracy:		89.46 %
Epoch 1879 of 2000 took 0.097s
  training loss:		0.278996
  validation loss:		0.351913
  validation accuracy:		89.89 %
Epoch 1880 of 2000 took 0.096s
  training loss:		0.270217
  validation loss:		0.351641
  validation accuracy:		89.89 %
Epoch 1881 of 2000 took 0.096s
  training loss:		0.280929
  validation loss:		0.355309
  validation accuracy:		90.00 %
Epoch 1882 of 2000 took 0.097s
  training loss:		0.278014
  validation loss:		0.356125
  validation accuracy:		89.89 %
Epoch 1883 of 2000 took 0.096s
  training loss:		0.274297
  validation loss:		0.346000
  validation accuracy:		90.11 %
Epoch 1884 of 2000 took 0.099s
  training loss:		0.274050
  validation loss:		0.354195
  validation accuracy:		89.67 %
Epoch 1885 of 2000 took 0.178s
  training loss:		0.273953
  validation loss:		0.361630
  validation accuracy:		90.00 %
Epoch 1886 of 2000 took 0.234s
  training loss:		0.271593
  validation loss:		0.357049
  validation accuracy:		89.78 %
Epoch 1887 of 2000 took 0.294s
  training loss:		0.276605
  validation loss:		0.351500
  validation accuracy:		90.11 %
Epoch 1888 of 2000 took 0.208s
  training loss:		0.274004
  validation loss:		0.370019
  validation accuracy:		89.57 %
Epoch 1889 of 2000 took 0.326s
  training loss:		0.269445
  validation loss:		0.363791
  validation accuracy:		89.78 %
Epoch 1890 of 2000 took 0.189s
  training loss:		0.272190
  validation loss:		0.364126
  validation accuracy:		89.57 %
Epoch 1891 of 2000 took 0.164s
  training loss:		0.278517
  validation loss:		0.363195
  validation accuracy:		89.46 %
Epoch 1892 of 2000 took 0.139s
  training loss:		0.279574
  validation loss:		0.361930
  validation accuracy:		89.67 %
Epoch 1893 of 2000 took 0.096s
  training loss:		0.274516
  validation loss:		0.388751
  validation accuracy:		88.70 %
Epoch 1894 of 2000 took 0.097s
  training loss:		0.281454
  validation loss:		0.348749
  validation accuracy:		89.89 %
Epoch 1895 of 2000 took 0.103s
  training loss:		0.272867
  validation loss:		0.392939
  validation accuracy:		88.91 %
Epoch 1896 of 2000 took 0.106s
  training loss:		0.273754
  validation loss:		0.360799
  validation accuracy:		90.22 %
Epoch 1897 of 2000 took 0.102s
  training loss:		0.270315
  validation loss:		0.360168
  validation accuracy:		89.57 %
Epoch 1898 of 2000 took 0.096s
  training loss:		0.275543
  validation loss:		0.346652
  validation accuracy:		90.11 %
Epoch 1899 of 2000 took 0.096s
  training loss:		0.273504
  validation loss:		0.349896
  validation accuracy:		90.00 %
Epoch 1900 of 2000 took 0.097s
  training loss:		0.272550
  validation loss:		0.355390
  validation accuracy:		89.78 %
Epoch 1901 of 2000 took 0.096s
  training loss:		0.276888
  validation loss:		0.356795
  validation accuracy:		90.11 %
Epoch 1902 of 2000 took 0.097s
  training loss:		0.269126
  validation loss:		0.352942
  validation accuracy:		89.78 %
Epoch 1903 of 2000 took 0.096s
  training loss:		0.276141
  validation loss:		0.349509
  validation accuracy:		90.22 %
Epoch 1904 of 2000 took 0.096s
  training loss:		0.279013
  validation loss:		0.356580
  validation accuracy:		90.00 %
Epoch 1905 of 2000 took 0.096s
  training loss:		0.275692
  validation loss:		0.353442
  validation accuracy:		90.00 %
Epoch 1906 of 2000 took 0.096s
  training loss:		0.275474
  validation loss:		0.347588
  validation accuracy:		90.22 %
Epoch 1907 of 2000 took 0.097s
  training loss:		0.274599
  validation loss:		0.377027
  validation accuracy:		89.46 %
Epoch 1908 of 2000 took 0.101s
  training loss:		0.276424
  validation loss:		0.360125
  validation accuracy:		89.24 %
Epoch 1909 of 2000 took 0.096s
  training loss:		0.275540
  validation loss:		0.370873
  validation accuracy:		89.24 %
Epoch 1910 of 2000 took 0.098s
  training loss:		0.275730
  validation loss:		0.351905
  validation accuracy:		90.00 %
Epoch 1911 of 2000 took 0.097s
  training loss:		0.273379
  validation loss:		0.388798
  validation accuracy:		88.91 %
Epoch 1912 of 2000 took 0.097s
  training loss:		0.280484
  validation loss:		0.381291
  validation accuracy:		89.24 %
Epoch 1913 of 2000 took 0.097s
  training loss:		0.273605
  validation loss:		0.356228
  validation accuracy:		89.78 %
Epoch 1914 of 2000 took 0.097s
  training loss:		0.274462
  validation loss:		0.349040
  validation accuracy:		90.00 %
Epoch 1915 of 2000 took 0.097s
  training loss:		0.278286
  validation loss:		0.351834
  validation accuracy:		89.67 %
Epoch 1916 of 2000 took 0.097s
  training loss:		0.274615
  validation loss:		0.352295
  validation accuracy:		90.00 %
Epoch 1917 of 2000 took 0.097s
  training loss:		0.282062
  validation loss:		0.346681
  validation accuracy:		90.43 %
Epoch 1918 of 2000 took 0.097s
  training loss:		0.277163
  validation loss:		0.354679
  validation accuracy:		89.67 %
Epoch 1919 of 2000 took 0.096s
  training loss:		0.276780
  validation loss:		0.370481
  validation accuracy:		89.57 %
Epoch 1920 of 2000 took 0.097s
  training loss:		0.274446
  validation loss:		0.364262
  validation accuracy:		89.67 %
Epoch 1921 of 2000 took 0.099s
  training loss:		0.278585
  validation loss:		0.358083
  validation accuracy:		90.11 %
Epoch 1922 of 2000 took 0.097s
  training loss:		0.268904
  validation loss:		0.359251
  validation accuracy:		90.11 %
Epoch 1923 of 2000 took 0.097s
  training loss:		0.275515
  validation loss:		0.351943
  validation accuracy:		90.00 %
Epoch 1924 of 2000 took 0.097s
  training loss:		0.276171
  validation loss:		0.341491
  validation accuracy:		90.43 %
Epoch 1925 of 2000 took 0.097s
  training loss:		0.280603
  validation loss:		0.369235
  validation accuracy:		89.57 %
Epoch 1926 of 2000 took 0.097s
  training loss:		0.278864
  validation loss:		0.367358
  validation accuracy:		89.46 %
Epoch 1927 of 2000 took 0.097s
  training loss:		0.271763
  validation loss:		0.351921
  validation accuracy:		89.89 %
Epoch 1928 of 2000 took 0.096s
  training loss:		0.278118
  validation loss:		0.359274
  validation accuracy:		89.89 %
Epoch 1929 of 2000 took 0.097s
  training loss:		0.264969
  validation loss:		0.362924
  validation accuracy:		89.46 %
Epoch 1930 of 2000 took 0.097s
  training loss:		0.280258
  validation loss:		0.345266
  validation accuracy:		90.22 %
Epoch 1931 of 2000 took 0.097s
  training loss:		0.276224
  validation loss:		0.348578
  validation accuracy:		90.00 %
Epoch 1932 of 2000 took 0.097s
  training loss:		0.275106
  validation loss:		0.369640
  validation accuracy:		90.11 %
Epoch 1933 of 2000 took 0.097s
  training loss:		0.281595
  validation loss:		0.348357
  validation accuracy:		90.00 %
Epoch 1934 of 2000 took 0.097s
  training loss:		0.277079
  validation loss:		0.356239
  validation accuracy:		90.00 %
Epoch 1935 of 2000 took 0.097s
  training loss:		0.270272
  validation loss:		0.345538
  validation accuracy:		90.11 %
Epoch 1936 of 2000 took 0.097s
  training loss:		0.273349
  validation loss:		0.344768
  validation accuracy:		90.00 %
Epoch 1937 of 2000 took 0.097s
  training loss:		0.276710
  validation loss:		0.366570
  validation accuracy:		89.78 %
Epoch 1938 of 2000 took 0.097s
  training loss:		0.277317
  validation loss:		0.353093
  validation accuracy:		89.89 %
Epoch 1939 of 2000 took 0.098s
  training loss:		0.275486
  validation loss:		0.351781
  validation accuracy:		90.00 %
Epoch 1940 of 2000 took 0.097s
  training loss:		0.274831
  validation loss:		0.354624
  validation accuracy:		89.89 %
Epoch 1941 of 2000 took 0.097s
  training loss:		0.269134
  validation loss:		0.350015
  validation accuracy:		90.22 %
Epoch 1942 of 2000 took 0.097s
  training loss:		0.278317
  validation loss:		0.348663
  validation accuracy:		90.54 %
Epoch 1943 of 2000 took 0.097s
  training loss:		0.275798
  validation loss:		0.345133
  validation accuracy:		90.22 %
Epoch 1944 of 2000 took 0.097s
  training loss:		0.274477
  validation loss:		0.366350
  validation accuracy:		89.78 %
Epoch 1945 of 2000 took 0.097s
  training loss:		0.276270
  validation loss:		0.354900
  validation accuracy:		89.78 %
Epoch 1946 of 2000 took 0.097s
  training loss:		0.279297
  validation loss:		0.350809
  validation accuracy:		90.00 %
Epoch 1947 of 2000 took 0.097s
  training loss:		0.280239
  validation loss:		0.350387
  validation accuracy:		90.00 %
Epoch 1948 of 2000 took 0.097s
  training loss:		0.275234
  validation loss:		0.358579
  validation accuracy:		89.67 %
Epoch 1949 of 2000 took 0.097s
  training loss:		0.275619
  validation loss:		0.354430
  validation accuracy:		90.00 %
Epoch 1950 of 2000 took 0.097s
  training loss:		0.275698
  validation loss:		0.361031
  validation accuracy:		89.46 %
Epoch 1951 of 2000 took 0.097s
  training loss:		0.281506
  validation loss:		0.344534
  validation accuracy:		90.65 %
Epoch 1952 of 2000 took 0.097s
  training loss:		0.272087
  validation loss:		0.348249
  validation accuracy:		90.33 %
Epoch 1953 of 2000 took 0.097s
  training loss:		0.272383
  validation loss:		0.364841
  validation accuracy:		89.67 %
Epoch 1954 of 2000 took 0.096s
  training loss:		0.270993
  validation loss:		0.346411
  validation accuracy:		90.22 %
Epoch 1955 of 2000 took 0.096s
  training loss:		0.275690
  validation loss:		0.352611
  validation accuracy:		89.78 %
Epoch 1956 of 2000 took 0.097s
  training loss:		0.276430
  validation loss:		0.362128
  validation accuracy:		89.57 %
Epoch 1957 of 2000 took 0.097s
  training loss:		0.275206
  validation loss:		0.354158
  validation accuracy:		90.22 %
Epoch 1958 of 2000 took 0.097s
  training loss:		0.278017
  validation loss:		0.348668
  validation accuracy:		89.78 %
Epoch 1959 of 2000 took 0.097s
  training loss:		0.275217
  validation loss:		0.375140
  validation accuracy:		89.35 %
Epoch 1960 of 2000 took 0.097s
  training loss:		0.272209
  validation loss:		0.358551
  validation accuracy:		89.35 %
Epoch 1961 of 2000 took 0.097s
  training loss:		0.273321
  validation loss:		0.356101
  validation accuracy:		89.67 %
Epoch 1962 of 2000 took 0.097s
  training loss:		0.278000
  validation loss:		0.344268
  validation accuracy:		90.11 %
Epoch 1963 of 2000 took 0.097s
  training loss:		0.276329
  validation loss:		0.357150
  validation accuracy:		90.33 %
Epoch 1964 of 2000 took 0.097s
  training loss:		0.274194
  validation loss:		0.366307
  validation accuracy:		89.89 %
Epoch 1965 of 2000 took 0.097s
  training loss:		0.276143
  validation loss:		0.356927
  validation accuracy:		90.22 %
Epoch 1966 of 2000 took 0.097s
  training loss:		0.262566
  validation loss:		0.355813
  validation accuracy:		90.22 %
Epoch 1967 of 2000 took 0.097s
  training loss:		0.275551
  validation loss:		0.346119
  validation accuracy:		90.11 %
Epoch 1968 of 2000 took 0.097s
  training loss:		0.273673
  validation loss:		0.353115
  validation accuracy:		89.67 %
Epoch 1969 of 2000 took 0.097s
  training loss:		0.277143
  validation loss:		0.348625
  validation accuracy:		89.67 %
Epoch 1970 of 2000 took 0.098s
  training loss:		0.278945
  validation loss:		0.354273
  validation accuracy:		90.33 %
Epoch 1971 of 2000 took 0.097s
  training loss:		0.276336
  validation loss:		0.362823
  validation accuracy:		89.67 %
Epoch 1972 of 2000 took 0.097s
  training loss:		0.277183
  validation loss:		0.365907
  validation accuracy:		90.22 %
Epoch 1973 of 2000 took 0.097s
  training loss:		0.276530
  validation loss:		0.344624
  validation accuracy:		90.00 %
Epoch 1974 of 2000 took 0.097s
  training loss:		0.273668
  validation loss:		0.377166
  validation accuracy:		89.24 %
Epoch 1975 of 2000 took 0.097s
  training loss:		0.280017
  validation loss:		0.358875
  validation accuracy:		90.22 %
Epoch 1976 of 2000 took 0.097s
  training loss:		0.275135
  validation loss:		0.357167
  validation accuracy:		90.22 %
Epoch 1977 of 2000 took 0.097s
  training loss:		0.278960
  validation loss:		0.349706
  validation accuracy:		90.00 %
Epoch 1978 of 2000 took 0.097s
  training loss:		0.273737
  validation loss:		0.351913
  validation accuracy:		90.11 %
Epoch 1979 of 2000 took 0.097s
  training loss:		0.277441
  validation loss:		0.350464
  validation accuracy:		90.11 %
Epoch 1980 of 2000 took 0.097s
  training loss:		0.270355
  validation loss:		0.358608
  validation accuracy:		90.00 %
Epoch 1981 of 2000 took 0.097s
  training loss:		0.274485
  validation loss:		0.358295
  validation accuracy:		89.78 %
Epoch 1982 of 2000 took 0.097s
  training loss:		0.271731
  validation loss:		0.348604
  validation accuracy:		90.00 %
Epoch 1983 of 2000 took 0.097s
  training loss:		0.277381
  validation loss:		0.371274
  validation accuracy:		89.67 %
Epoch 1984 of 2000 took 0.097s
  training loss:		0.275682
  validation loss:		0.344098
  validation accuracy:		90.43 %
Epoch 1985 of 2000 took 0.097s
  training loss:		0.278500
  validation loss:		0.350494
  validation accuracy:		90.22 %
Epoch 1986 of 2000 took 0.097s
  training loss:		0.275715
  validation loss:		0.357778
  validation accuracy:		90.11 %
Epoch 1987 of 2000 took 0.097s
  training loss:		0.272566
  validation loss:		0.348131
  validation accuracy:		90.76 %
Epoch 1988 of 2000 took 0.097s
  training loss:		0.275747
  validation loss:		0.370637
  validation accuracy:		89.78 %
Epoch 1989 of 2000 took 0.097s
  training loss:		0.276996
  validation loss:		0.348418
  validation accuracy:		90.33 %
Epoch 1990 of 2000 took 0.097s
  training loss:		0.269167
  validation loss:		0.346559
  validation accuracy:		90.22 %
Epoch 1991 of 2000 took 0.097s
  training loss:		0.274447
  validation loss:		0.356426
  validation accuracy:		90.00 %
Epoch 1992 of 2000 took 0.097s
  training loss:		0.278404
  validation loss:		0.349153
  validation accuracy:		89.89 %
Epoch 1993 of 2000 took 0.097s
  training loss:		0.274787
  validation loss:		0.359675
  validation accuracy:		90.54 %
Epoch 1994 of 2000 took 0.097s
  training loss:		0.271100
  validation loss:		0.356190
  validation accuracy:		89.78 %
Epoch 1995 of 2000 took 0.097s
  training loss:		0.276615
  validation loss:		0.350718
  validation accuracy:		89.78 %
Epoch 1996 of 2000 took 0.097s
  training loss:		0.270807
  validation loss:		0.371972
  validation accuracy:		89.02 %
Epoch 1997 of 2000 took 0.099s
  training loss:		0.283488
  validation loss:		0.360228
  validation accuracy:		90.22 %
Epoch 1998 of 2000 took 0.097s
  training loss:		0.279270
  validation loss:		0.352265
  validation accuracy:		89.57 %
Epoch 1999 of 2000 took 0.097s
  training loss:		0.270668
  validation loss:		0.355108
  validation accuracy:		90.22 %
Epoch 2000 of 2000 took 0.097s
  training loss:		0.271924
  validation loss:		0.352962
  validation accuracy:		90.00 %
Final results:
  test loss:			0.721962
  test accuracy:		80.40 %
