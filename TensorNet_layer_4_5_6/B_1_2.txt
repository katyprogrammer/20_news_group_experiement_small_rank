Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (3, 50, 50)...
decomposing tensor B of shape (3, 50)...
Starting training...
Epoch 1 of 2000 took 0.175s
  training loss:		2.999058
  validation loss:		2.990323
  validation accuracy:		11.09 %
Epoch 2 of 2000 took 0.124s
  training loss:		2.979979
  validation loss:		2.964978
  validation accuracy:		12.83 %
Epoch 3 of 2000 took 0.168s
  training loss:		2.956153
  validation loss:		2.937785
  validation accuracy:		12.83 %
Epoch 4 of 2000 took 0.145s
  training loss:		2.931948
  validation loss:		2.911086
  validation accuracy:		12.83 %
Epoch 5 of 2000 took 0.142s
  training loss:		2.908478
  validation loss:		2.885239
  validation accuracy:		12.83 %
Epoch 6 of 2000 took 0.168s
  training loss:		2.885232
  validation loss:		2.859902
  validation accuracy:		12.83 %
Epoch 7 of 2000 took 0.123s
  training loss:		2.863585
  validation loss:		2.835383
  validation accuracy:		12.83 %
Epoch 8 of 2000 took 0.166s
  training loss:		2.840820
  validation loss:		2.810523
  validation accuracy:		12.83 %
Epoch 9 of 2000 took 0.161s
  training loss:		2.817973
  validation loss:		2.785388
  validation accuracy:		12.83 %
Epoch 10 of 2000 took 0.127s
  training loss:		2.796982
  validation loss:		2.759950
  validation accuracy:		12.83 %
Epoch 11 of 2000 took 0.168s
  training loss:		2.776250
  validation loss:		2.733964
  validation accuracy:		12.83 %
Epoch 12 of 2000 took 0.137s
  training loss:		2.754015
  validation loss:		2.707540
  validation accuracy:		12.83 %
Epoch 13 of 2000 took 0.150s
  training loss:		2.728415
  validation loss:		2.680949
  validation accuracy:		12.83 %
Epoch 14 of 2000 took 0.168s
  training loss:		2.706611
  validation loss:		2.653819
  validation accuracy:		12.83 %
Epoch 15 of 2000 took 0.123s
  training loss:		2.681981
  validation loss:		2.625821
  validation accuracy:		12.83 %
Epoch 16 of 2000 took 0.167s
  training loss:		2.658488
  validation loss:		2.597627
  validation accuracy:		12.83 %
Epoch 17 of 2000 took 0.152s
  training loss:		2.634875
  validation loss:		2.569574
  validation accuracy:		12.83 %
Epoch 18 of 2000 took 0.135s
  training loss:		2.612079
  validation loss:		2.541363
  validation accuracy:		12.83 %
Epoch 19 of 2000 took 0.176s
  training loss:		2.586695
  validation loss:		2.514159
  validation accuracy:		12.83 %
Epoch 20 of 2000 took 0.127s
  training loss:		2.562602
  validation loss:		2.487017
  validation accuracy:		12.83 %
Epoch 21 of 2000 took 0.162s
  training loss:		2.542660
  validation loss:		2.461071
  validation accuracy:		12.83 %
Epoch 22 of 2000 took 0.165s
  training loss:		2.519424
  validation loss:		2.436567
  validation accuracy:		12.83 %
Epoch 23 of 2000 took 0.124s
  training loss:		2.497965
  validation loss:		2.413279
  validation accuracy:		12.83 %
Epoch 24 of 2000 took 0.168s
  training loss:		2.476977
  validation loss:		2.391433
  validation accuracy:		12.83 %
Epoch 25 of 2000 took 0.141s
  training loss:		2.458376
  validation loss:		2.370990
  validation accuracy:		12.83 %
Epoch 26 of 2000 took 0.146s
  training loss:		2.438188
  validation loss:		2.352760
  validation accuracy:		12.93 %
Epoch 27 of 2000 took 0.168s
  training loss:		2.423414
  validation loss:		2.336085
  validation accuracy:		12.93 %
Epoch 28 of 2000 took 0.124s
  training loss:		2.409450
  validation loss:		2.322899
  validation accuracy:		12.93 %
Epoch 29 of 2000 took 0.166s
  training loss:		2.395923
  validation loss:		2.311238
  validation accuracy:		12.93 %
Epoch 30 of 2000 took 0.166s
  training loss:		2.382779
  validation loss:		2.301715
  validation accuracy:		12.93 %
Epoch 31 of 2000 took 0.123s
  training loss:		2.374199
  validation loss:		2.293625
  validation accuracy:		12.93 %
Epoch 32 of 2000 took 0.163s
  training loss:		2.362232
  validation loss:		2.286719
  validation accuracy:		12.93 %
Epoch 33 of 2000 took 0.152s
  training loss:		2.356565
  validation loss:		2.280601
  validation accuracy:		12.93 %
Epoch 34 of 2000 took 0.166s
  training loss:		2.350001
  validation loss:		2.276657
  validation accuracy:		12.93 %
Epoch 35 of 2000 took 0.109s
  training loss:		2.341910
  validation loss:		2.273636
  validation accuracy:		12.93 %
Epoch 36 of 2000 took 0.165s
  training loss:		2.338024
  validation loss:		2.271932
  validation accuracy:		12.93 %
Epoch 37 of 2000 took 0.162s
  training loss:		2.333717
  validation loss:		2.269817
  validation accuracy:		12.93 %
Epoch 38 of 2000 took 0.137s
  training loss:		2.330007
  validation loss:		2.268107
  validation accuracy:		12.93 %
Epoch 39 of 2000 took 0.166s
  training loss:		2.326000
  validation loss:		2.267587
  validation accuracy:		12.93 %
Epoch 40 of 2000 took 0.134s
  training loss:		2.322772
  validation loss:		2.266538
  validation accuracy:		12.93 %
Epoch 41 of 2000 took 0.165s
  training loss:		2.322248
  validation loss:		2.265806
  validation accuracy:		12.93 %
Epoch 42 of 2000 took 0.162s
  training loss:		2.317731
  validation loss:		2.264798
  validation accuracy:		12.93 %
Epoch 43 of 2000 took 0.138s
  training loss:		2.316021
  validation loss:		2.261372
  validation accuracy:		12.93 %
Epoch 44 of 2000 took 0.166s
  training loss:		2.315034
  validation loss:		2.259990
  validation accuracy:		12.93 %
Epoch 45 of 2000 took 0.134s
  training loss:		2.312570
  validation loss:		2.258465
  validation accuracy:		12.28 %
Epoch 46 of 2000 took 0.165s
  training loss:		2.311786
  validation loss:		2.256774
  validation accuracy:		12.93 %
Epoch 47 of 2000 took 0.160s
  training loss:		2.311054
  validation loss:		2.256234
  validation accuracy:		12.72 %
Epoch 48 of 2000 took 0.140s
  training loss:		2.309612
  validation loss:		2.256252
  validation accuracy:		12.93 %
Epoch 49 of 2000 took 0.166s
  training loss:		2.309102
  validation loss:		2.256165
  validation accuracy:		12.83 %
Epoch 50 of 2000 took 0.134s
  training loss:		2.307806
  validation loss:		2.254623
  validation accuracy:		13.04 %
Epoch 51 of 2000 took 0.165s
  training loss:		2.307148
  validation loss:		2.253932
  validation accuracy:		12.83 %
Epoch 52 of 2000 took 0.158s
  training loss:		2.307390
  validation loss:		2.254436
  validation accuracy:		12.93 %
Epoch 53 of 2000 took 0.141s
  training loss:		2.306292
  validation loss:		2.255621
  validation accuracy:		12.93 %
Epoch 54 of 2000 took 0.166s
  training loss:		2.304689
  validation loss:		2.255260
  validation accuracy:		12.93 %
Epoch 55 of 2000 took 0.134s
  training loss:		2.304763
  validation loss:		2.252580
  validation accuracy:		13.04 %
Epoch 56 of 2000 took 0.165s
  training loss:		2.304958
  validation loss:		2.253180
  validation accuracy:		9.78 %
Epoch 57 of 2000 took 0.157s
  training loss:		2.303993
  validation loss:		2.253554
  validation accuracy:		12.93 %
Epoch 58 of 2000 took 0.143s
  training loss:		2.302733
  validation loss:		2.252232
  validation accuracy:		13.04 %
Epoch 59 of 2000 took 0.166s
  training loss:		2.302155
  validation loss:		2.249451
  validation accuracy:		13.04 %
Epoch 60 of 2000 took 0.133s
  training loss:		2.301950
  validation loss:		2.249807
  validation accuracy:		13.04 %
Epoch 61 of 2000 took 0.165s
  training loss:		2.303644
  validation loss:		2.250620
  validation accuracy:		12.93 %
Epoch 62 of 2000 took 0.156s
  training loss:		2.302234
  validation loss:		2.252234
  validation accuracy:		12.93 %
Epoch 63 of 2000 took 0.143s
  training loss:		2.301498
  validation loss:		2.250747
  validation accuracy:		13.04 %
Epoch 64 of 2000 took 0.166s
  training loss:		2.301187
  validation loss:		2.248888
  validation accuracy:		12.83 %
Epoch 65 of 2000 took 0.141s
  training loss:		2.300792
  validation loss:		2.247558
  validation accuracy:		12.83 %
Epoch 66 of 2000 took 0.166s
  training loss:		2.300475
  validation loss:		2.249124
  validation accuracy:		12.83 %
Epoch 67 of 2000 took 0.152s
  training loss:		2.301124
  validation loss:		2.251581
  validation accuracy:		13.04 %
Epoch 68 of 2000 took 0.148s
  training loss:		2.300704
  validation loss:		2.251175
  validation accuracy:		13.04 %
Epoch 69 of 2000 took 0.166s
  training loss:		2.300253
  validation loss:		2.250215
  validation accuracy:		13.04 %
Epoch 70 of 2000 took 0.134s
  training loss:		2.300413
  validation loss:		2.248940
  validation accuracy:		12.93 %
Epoch 71 of 2000 took 0.165s
  training loss:		2.299629
  validation loss:		2.249279
  validation accuracy:		12.93 %
Epoch 72 of 2000 took 0.166s
  training loss:		2.299574
  validation loss:		2.249209
  validation accuracy:		12.93 %
Epoch 73 of 2000 took 0.158s
  training loss:		2.299584
  validation loss:		2.249156
  validation accuracy:		12.83 %
Epoch 74 of 2000 took 0.167s
  training loss:		2.298704
  validation loss:		2.248131
  validation accuracy:		12.83 %
Epoch 75 of 2000 took 0.134s
  training loss:		2.298745
  validation loss:		2.246834
  validation accuracy:		13.04 %
Epoch 76 of 2000 took 0.165s
  training loss:		2.298925
  validation loss:		2.247574
  validation accuracy:		12.93 %
Epoch 77 of 2000 took 0.139s
  training loss:		2.299576
  validation loss:		2.248598
  validation accuracy:		13.04 %
Epoch 78 of 2000 took 0.160s
  training loss:		2.299133
  validation loss:		2.249413
  validation accuracy:		12.93 %
Epoch 79 of 2000 took 0.166s
  training loss:		2.297945
  validation loss:		2.248362
  validation accuracy:		12.93 %
Epoch 80 of 2000 took 0.134s
  training loss:		2.297979
  validation loss:		2.245389
  validation accuracy:		12.83 %
Epoch 81 of 2000 took 0.166s
  training loss:		2.297979
  validation loss:		2.245726
  validation accuracy:		12.83 %
Epoch 82 of 2000 took 0.137s
  training loss:		2.297696
  validation loss:		2.246077
  validation accuracy:		12.93 %
Epoch 83 of 2000 took 0.162s
  training loss:		2.298854
  validation loss:		2.245440
  validation accuracy:		12.83 %
Epoch 84 of 2000 took 0.175s
  training loss:		2.297686
  validation loss:		2.248448
  validation accuracy:		12.83 %
Epoch 85 of 2000 took 0.139s
  training loss:		2.298432
  validation loss:		2.246606
  validation accuracy:		15.43 %
Epoch 86 of 2000 took 0.166s
  training loss:		2.297863
  validation loss:		2.247315
  validation accuracy:		12.93 %
Epoch 87 of 2000 took 0.135s
  training loss:		2.298247
  validation loss:		2.248874
  validation accuracy:		12.93 %
Epoch 88 of 2000 took 0.165s
  training loss:		2.297689
  validation loss:		2.249381
  validation accuracy:		12.93 %
Epoch 89 of 2000 took 0.159s
  training loss:		2.297213
  validation loss:		2.246896
  validation accuracy:		12.93 %
Epoch 90 of 2000 took 0.141s
  training loss:		2.297242
  validation loss:		2.244293
  validation accuracy:		12.93 %
Epoch 91 of 2000 took 0.166s
  training loss:		2.297057
  validation loss:		2.244698
  validation accuracy:		12.93 %
Epoch 92 of 2000 took 0.134s
  training loss:		2.297282
  validation loss:		2.244781
  validation accuracy:		12.83 %
Epoch 93 of 2000 took 0.165s
  training loss:		2.297458
  validation loss:		2.246276
  validation accuracy:		12.83 %
Epoch 94 of 2000 took 0.158s
  training loss:		2.297181
  validation loss:		2.246152
  validation accuracy:		12.83 %
Epoch 95 of 2000 took 0.143s
  training loss:		2.297839
  validation loss:		2.247413
  validation accuracy:		13.04 %
Epoch 96 of 2000 took 0.166s
  training loss:		2.296752
  validation loss:		2.246605
  validation accuracy:		13.04 %
Epoch 97 of 2000 took 0.161s
  training loss:		2.296800
  validation loss:		2.245262
  validation accuracy:		13.04 %
Epoch 98 of 2000 took 0.335s
  training loss:		2.296793
  validation loss:		2.244262
  validation accuracy:		8.91 %
Epoch 99 of 2000 took 0.182s
  training loss:		2.296503
  validation loss:		2.247488
  validation accuracy:		12.93 %
Epoch 100 of 2000 took 0.165s
  training loss:		2.297662
  validation loss:		2.247502
  validation accuracy:		12.93 %
Epoch 101 of 2000 took 0.137s
  training loss:		2.297428
  validation loss:		2.247178
  validation accuracy:		12.83 %
Epoch 102 of 2000 took 0.162s
  training loss:		2.295678
  validation loss:		2.246669
  validation accuracy:		12.83 %
Epoch 103 of 2000 took 0.166s
  training loss:		2.296513
  validation loss:		2.243535
  validation accuracy:		12.83 %
Epoch 104 of 2000 took 0.134s
  training loss:		2.296295
  validation loss:		2.244913
  validation accuracy:		12.93 %
Epoch 105 of 2000 took 0.165s
  training loss:		2.296288
  validation loss:		2.243570
  validation accuracy:		12.93 %
Epoch 106 of 2000 took 0.136s
  training loss:		2.297097
  validation loss:		2.246689
  validation accuracy:		12.93 %
Epoch 107 of 2000 took 0.163s
  training loss:		2.295959
  validation loss:		2.245622
  validation accuracy:		12.93 %
Epoch 108 of 2000 took 0.165s
  training loss:		2.296161
  validation loss:		2.245840
  validation accuracy:		13.04 %
Epoch 109 of 2000 took 0.135s
  training loss:		2.296278
  validation loss:		2.245318
  validation accuracy:		12.83 %
Epoch 110 of 2000 took 0.166s
  training loss:		2.296624
  validation loss:		2.245287
  validation accuracy:		12.39 %
Epoch 111 of 2000 took 0.135s
  training loss:		2.296209
  validation loss:		2.245888
  validation accuracy:		13.04 %
Epoch 112 of 2000 took 0.164s
  training loss:		2.296289
  validation loss:		2.244080
  validation accuracy:		12.93 %
Epoch 113 of 2000 took 0.166s
  training loss:		2.295946
  validation loss:		2.243277
  validation accuracy:		12.83 %
Epoch 114 of 2000 took 0.133s
  training loss:		2.295556
  validation loss:		2.244182
  validation accuracy:		13.04 %
Epoch 115 of 2000 took 0.165s
  training loss:		2.296425
  validation loss:		2.247419
  validation accuracy:		12.93 %
Epoch 116 of 2000 took 0.139s
  training loss:		2.296140
  validation loss:		2.247263
  validation accuracy:		12.83 %
Epoch 117 of 2000 took 0.160s
  training loss:		2.296684
  validation loss:		2.248649
  validation accuracy:		12.93 %
Epoch 118 of 2000 took 0.166s
  training loss:		2.295692
  validation loss:		2.244152
  validation accuracy:		12.83 %
Epoch 119 of 2000 took 0.134s
  training loss:		2.296163
  validation loss:		2.244079
  validation accuracy:		12.83 %
Epoch 120 of 2000 took 0.165s
  training loss:		2.296371
  validation loss:		2.243721
  validation accuracy:		12.83 %
Epoch 121 of 2000 took 0.138s
  training loss:		2.296891
  validation loss:		2.246629
  validation accuracy:		12.83 %
Epoch 122 of 2000 took 0.161s
  training loss:		2.296137
  validation loss:		2.247382
  validation accuracy:		12.83 %
Epoch 123 of 2000 took 0.169s
  training loss:		2.295799
  validation loss:		2.248646
  validation accuracy:		13.04 %
Epoch 124 of 2000 took 0.139s
  training loss:		2.296728
  validation loss:		2.245016
  validation accuracy:		13.04 %
Epoch 125 of 2000 took 0.166s
  training loss:		2.297328
  validation loss:		2.245242
  validation accuracy:		13.04 %
Epoch 126 of 2000 took 0.134s
  training loss:		2.295402
  validation loss:		2.245251
  validation accuracy:		12.93 %
Epoch 127 of 2000 took 0.164s
  training loss:		2.295386
  validation loss:		2.245865
  validation accuracy:		12.83 %
Epoch 128 of 2000 took 0.163s
  training loss:		2.295679
  validation loss:		2.243951
  validation accuracy:		13.04 %
Epoch 129 of 2000 took 0.137s
  training loss:		2.296541
  validation loss:		2.244301
  validation accuracy:		12.83 %
Epoch 130 of 2000 took 0.166s
  training loss:		2.294945
  validation loss:		2.244038
  validation accuracy:		12.93 %
Epoch 131 of 2000 took 0.134s
  training loss:		2.296413
  validation loss:		2.243388
  validation accuracy:		12.93 %
Epoch 132 of 2000 took 0.165s
  training loss:		2.294769
  validation loss:		2.244482
  validation accuracy:		13.91 %
Epoch 133 of 2000 took 0.162s
  training loss:		2.295917
  validation loss:		2.244334
  validation accuracy:		14.67 %
Epoch 134 of 2000 took 0.137s
  training loss:		2.295586
  validation loss:		2.243545
  validation accuracy:		12.93 %
Epoch 135 of 2000 took 0.166s
  training loss:		2.295949
  validation loss:		2.244047
  validation accuracy:		12.93 %
Epoch 136 of 2000 took 0.134s
  training loss:		2.296340
  validation loss:		2.246381
  validation accuracy:		12.93 %
Epoch 137 of 2000 took 0.164s
  training loss:		2.294797
  validation loss:		2.244734
  validation accuracy:		13.04 %
Epoch 138 of 2000 took 0.161s
  training loss:		2.294671
  validation loss:		2.244072
  validation accuracy:		12.93 %
Epoch 139 of 2000 took 0.138s
  training loss:		2.295803
  validation loss:		2.244080
  validation accuracy:		13.04 %
Epoch 140 of 2000 took 0.166s
  training loss:		2.295692
  validation loss:		2.244195
  validation accuracy:		10.65 %
Epoch 141 of 2000 took 0.134s
  training loss:		2.295462
  validation loss:		2.244877
  validation accuracy:		12.93 %
Epoch 142 of 2000 took 0.164s
  training loss:		2.296332
  validation loss:		2.246143
  validation accuracy:		13.04 %
Epoch 143 of 2000 took 0.160s
  training loss:		2.295710
  validation loss:		2.243670
  validation accuracy:		12.83 %
Epoch 144 of 2000 took 0.140s
  training loss:		2.295754
  validation loss:		2.244374
  validation accuracy:		13.04 %
Epoch 145 of 2000 took 0.166s
  training loss:		2.295646
  validation loss:		2.246175
  validation accuracy:		12.93 %
Epoch 146 of 2000 took 0.134s
  training loss:		2.295400
  validation loss:		2.244420
  validation accuracy:		13.04 %
Epoch 147 of 2000 took 0.164s
  training loss:		2.296009
  validation loss:		2.245721
  validation accuracy:		13.04 %
Epoch 148 of 2000 took 0.160s
  training loss:		2.295075
  validation loss:		2.246432
  validation accuracy:		13.04 %
Epoch 149 of 2000 took 0.142s
  training loss:		2.295556
  validation loss:		2.242931
  validation accuracy:		12.83 %
Epoch 150 of 2000 took 0.166s
  training loss:		2.295741
  validation loss:		2.244362
  validation accuracy:		12.93 %
Epoch 151 of 2000 took 0.134s
  training loss:		2.295954
  validation loss:		2.247321
  validation accuracy:		12.93 %
Epoch 152 of 2000 took 0.165s
  training loss:		2.295269
  validation loss:		2.246696
  validation accuracy:		12.83 %
Epoch 153 of 2000 took 0.157s
  training loss:		2.295688
  validation loss:		2.243906
  validation accuracy:		12.83 %
Epoch 154 of 2000 took 0.142s
  training loss:		2.295860
  validation loss:		2.245148
  validation accuracy:		12.83 %
Epoch 155 of 2000 took 0.166s
  training loss:		2.296194
  validation loss:		2.244521
  validation accuracy:		12.93 %
Epoch 156 of 2000 took 0.134s
  training loss:		2.295767
  validation loss:		2.246951
  validation accuracy:		12.93 %
Epoch 157 of 2000 took 0.164s
  training loss:		2.295550
  validation loss:		2.244891
  validation accuracy:		13.04 %
Epoch 158 of 2000 took 0.156s
  training loss:		2.294546
  validation loss:		2.241342
  validation accuracy:		12.93 %
Epoch 159 of 2000 took 0.143s
  training loss:		2.294986
  validation loss:		2.241139
  validation accuracy:		13.04 %
Epoch 160 of 2000 took 0.166s
  training loss:		2.295091
  validation loss:		2.243357
  validation accuracy:		13.04 %
Epoch 161 of 2000 took 0.133s
  training loss:		2.295650
  validation loss:		2.245542
  validation accuracy:		12.93 %
Epoch 162 of 2000 took 0.164s
  training loss:		2.295938
  validation loss:		2.243356
  validation accuracy:		13.04 %
Epoch 163 of 2000 took 0.156s
  training loss:		2.295049
  validation loss:		2.245751
  validation accuracy:		12.83 %
Epoch 164 of 2000 took 0.144s
  training loss:		2.295895
  validation loss:		2.247220
  validation accuracy:		12.93 %
Epoch 165 of 2000 took 0.166s
  training loss:		2.295636
  validation loss:		2.247057
  validation accuracy:		12.83 %
Epoch 166 of 2000 took 0.134s
  training loss:		2.295570
  validation loss:		2.245556
  validation accuracy:		12.83 %
Epoch 167 of 2000 took 0.165s
  training loss:		2.295923
  validation loss:		2.246977
  validation accuracy:		12.93 %
Epoch 168 of 2000 took 0.155s
  training loss:		2.296300
  validation loss:		2.245678
  validation accuracy:		12.93 %
Epoch 169 of 2000 took 0.145s
  training loss:		2.295062
  validation loss:		2.245074
  validation accuracy:		12.93 %
Epoch 170 of 2000 took 0.166s
  training loss:		2.295531
  validation loss:		2.244030
  validation accuracy:		12.83 %
Epoch 171 of 2000 took 0.134s
  training loss:		2.295579
  validation loss:		2.244549
  validation accuracy:		13.04 %
Epoch 172 of 2000 took 0.165s
  training loss:		2.294691
  validation loss:		2.244767
  validation accuracy:		13.04 %
Epoch 173 of 2000 took 0.153s
  training loss:		2.295907
  validation loss:		2.242836
  validation accuracy:		12.83 %
Epoch 174 of 2000 took 0.147s
  training loss:		2.295097
  validation loss:		2.243642
  validation accuracy:		12.83 %
Epoch 175 of 2000 took 0.166s
  training loss:		2.295758
  validation loss:		2.245318
  validation accuracy:		12.93 %
Epoch 176 of 2000 took 0.134s
  training loss:		2.295363
  validation loss:		2.245309
  validation accuracy:		13.04 %
Epoch 177 of 2000 took 0.164s
  training loss:		2.294865
  validation loss:		2.246446
  validation accuracy:		12.83 %
Epoch 178 of 2000 took 0.155s
  training loss:		2.295136
  validation loss:		2.244689
  validation accuracy:		13.04 %
Epoch 179 of 2000 took 0.106s
  training loss:		2.296393
  validation loss:		2.246294
  validation accuracy:		12.93 %
Epoch 180 of 2000 took 0.105s
  training loss:		2.296018
  validation loss:		2.246260
  validation accuracy:		12.83 %
Epoch 181 of 2000 took 0.105s
  training loss:		2.295596
  validation loss:		2.246386
  validation accuracy:		11.63 %
Epoch 182 of 2000 took 0.105s
  training loss:		2.295155
  validation loss:		2.245121
  validation accuracy:		12.93 %
Epoch 183 of 2000 took 0.105s
  training loss:		2.294868
  validation loss:		2.243453
  validation accuracy:		12.83 %
Epoch 184 of 2000 took 0.105s
  training loss:		2.294957
  validation loss:		2.244081
  validation accuracy:		12.93 %
Epoch 185 of 2000 took 0.105s
  training loss:		2.294650
  validation loss:		2.242812
  validation accuracy:		12.93 %
Epoch 186 of 2000 took 0.105s
  training loss:		2.294532
  validation loss:		2.240691
  validation accuracy:		12.83 %
Epoch 187 of 2000 took 0.105s
  training loss:		2.294674
  validation loss:		2.243554
  validation accuracy:		13.04 %
Epoch 188 of 2000 took 0.105s
  training loss:		2.295217
  validation loss:		2.244546
  validation accuracy:		12.93 %
Epoch 189 of 2000 took 0.105s
  training loss:		2.296071
  validation loss:		2.246357
  validation accuracy:		12.93 %
Epoch 190 of 2000 took 0.105s
  training loss:		2.294868
  validation loss:		2.246965
  validation accuracy:		13.04 %
Epoch 191 of 2000 took 0.105s
  training loss:		2.295998
  validation loss:		2.246179
  validation accuracy:		13.04 %
Epoch 192 of 2000 took 0.105s
  training loss:		2.295034
  validation loss:		2.245453
  validation accuracy:		12.93 %
Epoch 193 of 2000 took 0.105s
  training loss:		2.295806
  validation loss:		2.244046
  validation accuracy:		12.83 %
Epoch 194 of 2000 took 0.105s
  training loss:		2.294349
  validation loss:		2.241935
  validation accuracy:		12.83 %
Epoch 195 of 2000 took 0.105s
  training loss:		2.295631
  validation loss:		2.245291
  validation accuracy:		12.93 %
Epoch 196 of 2000 took 0.105s
  training loss:		2.294571
  validation loss:		2.243168
  validation accuracy:		12.83 %
Epoch 197 of 2000 took 0.106s
  training loss:		2.295173
  validation loss:		2.242457
  validation accuracy:		13.04 %
Epoch 198 of 2000 took 0.106s
  training loss:		2.295050
  validation loss:		2.243096
  validation accuracy:		13.04 %
Epoch 199 of 2000 took 0.105s
  training loss:		2.295460
  validation loss:		2.244816
  validation accuracy:		12.83 %
Epoch 200 of 2000 took 0.105s
  training loss:		2.295782
  validation loss:		2.247852
  validation accuracy:		12.93 %
Epoch 201 of 2000 took 0.105s
  training loss:		2.294459
  validation loss:		2.245921
  validation accuracy:		13.04 %
Epoch 202 of 2000 took 0.105s
  training loss:		2.294990
  validation loss:		2.242080
  validation accuracy:		13.04 %
Epoch 203 of 2000 took 0.105s
  training loss:		2.294785
  validation loss:		2.241419
  validation accuracy:		12.83 %
Epoch 204 of 2000 took 0.105s
  training loss:		2.294312
  validation loss:		2.241702
  validation accuracy:		12.93 %
Epoch 205 of 2000 took 0.105s
  training loss:		2.293470
  validation loss:		2.241193
  validation accuracy:		13.04 %
Epoch 206 of 2000 took 0.105s
  training loss:		2.295350
  validation loss:		2.241533
  validation accuracy:		12.93 %
Epoch 207 of 2000 took 0.105s
  training loss:		2.294934
  validation loss:		2.244118
  validation accuracy:		13.04 %
Epoch 208 of 2000 took 0.105s
  training loss:		2.295123
  validation loss:		2.246100
  validation accuracy:		13.04 %
Epoch 209 of 2000 took 0.105s
  training loss:		2.295541
  validation loss:		2.246743
  validation accuracy:		13.04 %
Epoch 210 of 2000 took 0.105s
  training loss:		2.295386
  validation loss:		2.243490
  validation accuracy:		11.96 %
Epoch 211 of 2000 took 0.105s
  training loss:		2.295826
  validation loss:		2.243324
  validation accuracy:		12.83 %
Epoch 212 of 2000 took 0.105s
  training loss:		2.294429
  validation loss:		2.245277
  validation accuracy:		13.04 %
Epoch 213 of 2000 took 0.105s
  training loss:		2.294736
  validation loss:		2.245099
  validation accuracy:		13.04 %
Epoch 214 of 2000 took 0.105s
  training loss:		2.293303
  validation loss:		2.242143
  validation accuracy:		12.83 %
Epoch 215 of 2000 took 0.105s
  training loss:		2.295679
  validation loss:		2.243786
  validation accuracy:		12.83 %
Epoch 216 of 2000 took 0.105s
  training loss:		2.295901
  validation loss:		2.248835
  validation accuracy:		12.93 %
Epoch 217 of 2000 took 0.105s
  training loss:		2.294753
  validation loss:		2.245810
  validation accuracy:		12.93 %
Epoch 218 of 2000 took 0.105s
  training loss:		2.294597
  validation loss:		2.241586
  validation accuracy:		13.04 %
Epoch 219 of 2000 took 0.105s
  training loss:		2.295057
  validation loss:		2.244079
  validation accuracy:		12.93 %
Epoch 220 of 2000 took 0.105s
  training loss:		2.295414
  validation loss:		2.244469
  validation accuracy:		12.93 %
Epoch 221 of 2000 took 0.105s
  training loss:		2.296051
  validation loss:		2.245518
  validation accuracy:		14.35 %
Epoch 222 of 2000 took 0.105s
  training loss:		2.294882
  validation loss:		2.246788
  validation accuracy:		12.93 %
Epoch 223 of 2000 took 0.105s
  training loss:		2.294229
  validation loss:		2.245782
  validation accuracy:		12.93 %
Epoch 224 of 2000 took 0.105s
  training loss:		2.295360
  validation loss:		2.242319
  validation accuracy:		12.83 %
Epoch 225 of 2000 took 0.105s
  training loss:		2.295543
  validation loss:		2.240679
  validation accuracy:		15.11 %
Epoch 226 of 2000 took 0.106s
  training loss:		2.295646
  validation loss:		2.246600
  validation accuracy:		12.93 %
Epoch 227 of 2000 took 0.105s
  training loss:		2.294550
  validation loss:		2.245518
  validation accuracy:		12.93 %
Epoch 228 of 2000 took 0.105s
  training loss:		2.294624
  validation loss:		2.243895
  validation accuracy:		13.04 %
Epoch 229 of 2000 took 0.106s
  training loss:		2.295195
  validation loss:		2.245277
  validation accuracy:		12.83 %
Epoch 230 of 2000 took 0.105s
  training loss:		2.295022
  validation loss:		2.245021
  validation accuracy:		12.93 %
Epoch 231 of 2000 took 0.105s
  training loss:		2.295367
  validation loss:		2.248575
  validation accuracy:		12.93 %
Epoch 232 of 2000 took 0.105s
  training loss:		2.294522
  validation loss:		2.244342
  validation accuracy:		12.93 %
Epoch 233 of 2000 took 0.105s
  training loss:		2.295650
  validation loss:		2.242699
  validation accuracy:		13.04 %
Epoch 234 of 2000 took 0.105s
  training loss:		2.294023
  validation loss:		2.240642
  validation accuracy:		13.04 %
Epoch 235 of 2000 took 0.105s
  training loss:		2.294651
  validation loss:		2.242231
  validation accuracy:		12.93 %
Epoch 236 of 2000 took 0.105s
  training loss:		2.295422
  validation loss:		2.241225
  validation accuracy:		13.04 %
Epoch 237 of 2000 took 0.105s
  training loss:		2.294851
  validation loss:		2.248364
  validation accuracy:		13.04 %
Epoch 238 of 2000 took 0.105s
  training loss:		2.295620
  validation loss:		2.246862
  validation accuracy:		12.93 %
Epoch 239 of 2000 took 0.105s
  training loss:		2.295366
  validation loss:		2.246474
  validation accuracy:		12.93 %
Epoch 240 of 2000 took 0.105s
  training loss:		2.294842
  validation loss:		2.243511
  validation accuracy:		13.04 %
Epoch 241 of 2000 took 0.105s
  training loss:		2.294568
  validation loss:		2.244470
  validation accuracy:		12.93 %
Epoch 242 of 2000 took 0.105s
  training loss:		2.294419
  validation loss:		2.243361
  validation accuracy:		12.93 %
Epoch 243 of 2000 took 0.105s
  training loss:		2.295102
  validation loss:		2.245576
  validation accuracy:		12.83 %
Epoch 244 of 2000 took 0.105s
  training loss:		2.294155
  validation loss:		2.241557
  validation accuracy:		13.04 %
Epoch 245 of 2000 took 0.105s
  training loss:		2.295774
  validation loss:		2.241911
  validation accuracy:		12.93 %
Epoch 246 of 2000 took 0.105s
  training loss:		2.294023
  validation loss:		2.243136
  validation accuracy:		12.83 %
Epoch 247 of 2000 took 0.105s
  training loss:		2.294859
  validation loss:		2.245365
  validation accuracy:		12.83 %
Epoch 248 of 2000 took 0.111s
  training loss:		2.294732
  validation loss:		2.244514
  validation accuracy:		13.04 %
Epoch 249 of 2000 took 0.106s
  training loss:		2.294334
  validation loss:		2.242872
  validation accuracy:		12.83 %
Epoch 250 of 2000 took 0.105s
  training loss:		2.294791
  validation loss:		2.243287
  validation accuracy:		12.93 %
Epoch 251 of 2000 took 0.105s
  training loss:		2.295536
  validation loss:		2.245046
  validation accuracy:		12.83 %
Epoch 252 of 2000 took 0.105s
  training loss:		2.295177
  validation loss:		2.244574
  validation accuracy:		12.93 %
Epoch 253 of 2000 took 0.105s
  training loss:		2.295310
  validation loss:		2.246234
  validation accuracy:		13.04 %
Epoch 254 of 2000 took 0.106s
  training loss:		2.295289
  validation loss:		2.244781
  validation accuracy:		12.83 %
Epoch 255 of 2000 took 0.106s
  training loss:		2.295495
  validation loss:		2.247326
  validation accuracy:		12.83 %
Epoch 256 of 2000 took 0.105s
  training loss:		2.294608
  validation loss:		2.243180
  validation accuracy:		12.93 %
Epoch 257 of 2000 took 0.105s
  training loss:		2.295836
  validation loss:		2.245451
  validation accuracy:		12.93 %
Epoch 258 of 2000 took 0.105s
  training loss:		2.295703
  validation loss:		2.249065
  validation accuracy:		12.93 %
Epoch 259 of 2000 took 0.105s
  training loss:		2.294538
  validation loss:		2.241278
  validation accuracy:		12.83 %
Epoch 260 of 2000 took 0.105s
  training loss:		2.294565
  validation loss:		2.242045
  validation accuracy:		12.83 %
Epoch 261 of 2000 took 0.105s
  training loss:		2.294889
  validation loss:		2.241491
  validation accuracy:		12.83 %
Epoch 262 of 2000 took 0.105s
  training loss:		2.295083
  validation loss:		2.244598
  validation accuracy:		12.93 %
Epoch 263 of 2000 took 0.105s
  training loss:		2.295105
  validation loss:		2.245297
  validation accuracy:		13.04 %
Epoch 264 of 2000 took 0.105s
  training loss:		2.294987
  validation loss:		2.242481
  validation accuracy:		12.83 %
Epoch 265 of 2000 took 0.105s
  training loss:		2.293877
  validation loss:		2.244088
  validation accuracy:		12.93 %
Epoch 266 of 2000 took 0.105s
  training loss:		2.294971
  validation loss:		2.242345
  validation accuracy:		12.93 %
Epoch 267 of 2000 took 0.105s
  training loss:		2.294514
  validation loss:		2.245951
  validation accuracy:		12.93 %
Epoch 268 of 2000 took 0.105s
  training loss:		2.294926
  validation loss:		2.244389
  validation accuracy:		12.83 %
Epoch 269 of 2000 took 0.105s
  training loss:		2.294739
  validation loss:		2.243525
  validation accuracy:		12.83 %
Epoch 270 of 2000 took 0.105s
  training loss:		2.294746
  validation loss:		2.242259
  validation accuracy:		12.93 %
Epoch 271 of 2000 took 0.105s
  training loss:		2.295607
  validation loss:		2.246584
  validation accuracy:		12.93 %
Epoch 272 of 2000 took 0.105s
  training loss:		2.296355
  validation loss:		2.246888
  validation accuracy:		13.04 %
Epoch 273 of 2000 took 0.105s
  training loss:		2.294721
  validation loss:		2.244935
  validation accuracy:		12.83 %
Epoch 274 of 2000 took 0.105s
  training loss:		2.295336
  validation loss:		2.242596
  validation accuracy:		12.83 %
Epoch 275 of 2000 took 0.105s
  training loss:		2.294713
  validation loss:		2.243542
  validation accuracy:		12.93 %
Epoch 276 of 2000 took 0.105s
  training loss:		2.295546
  validation loss:		2.247527
  validation accuracy:		13.04 %
Epoch 277 of 2000 took 0.105s
  training loss:		2.295599
  validation loss:		2.245222
  validation accuracy:		13.04 %
Epoch 278 of 2000 took 0.105s
  training loss:		2.294465
  validation loss:		2.242932
  validation accuracy:		12.93 %
Epoch 279 of 2000 took 0.105s
  training loss:		2.295049
  validation loss:		2.243735
  validation accuracy:		12.93 %
Epoch 280 of 2000 took 0.105s
  training loss:		2.295173
  validation loss:		2.246975
  validation accuracy:		12.93 %
Epoch 281 of 2000 took 0.105s
  training loss:		2.294849
  validation loss:		2.243321
  validation accuracy:		12.83 %
Epoch 282 of 2000 took 0.105s
  training loss:		2.295279
  validation loss:		2.241347
  validation accuracy:		13.04 %
Epoch 283 of 2000 took 0.106s
  training loss:		2.295357
  validation loss:		2.247723
  validation accuracy:		12.83 %
Epoch 284 of 2000 took 0.105s
  training loss:		2.294215
  validation loss:		2.245900
  validation accuracy:		13.04 %
Epoch 285 of 2000 took 0.105s
  training loss:		2.294818
  validation loss:		2.243648
  validation accuracy:		13.04 %
Epoch 286 of 2000 took 0.105s
  training loss:		2.295792
  validation loss:		2.245363
  validation accuracy:		12.93 %
Epoch 287 of 2000 took 0.105s
  training loss:		2.294169
  validation loss:		2.242540
  validation accuracy:		12.83 %
Epoch 288 of 2000 took 0.105s
  training loss:		2.295312
  validation loss:		2.242095
  validation accuracy:		12.83 %
Epoch 289 of 2000 took 0.105s
  training loss:		2.295292
  validation loss:		2.246558
  validation accuracy:		13.04 %
Epoch 290 of 2000 took 0.105s
  training loss:		2.295130
  validation loss:		2.248402
  validation accuracy:		13.04 %
Epoch 291 of 2000 took 0.105s
  training loss:		2.294797
  validation loss:		2.245334
  validation accuracy:		12.93 %
Epoch 292 of 2000 took 0.105s
  training loss:		2.294275
  validation loss:		2.241459
  validation accuracy:		12.83 %
Epoch 293 of 2000 took 0.105s
  training loss:		2.295153
  validation loss:		2.242154
  validation accuracy:		13.04 %
Epoch 294 of 2000 took 0.105s
  training loss:		2.295084
  validation loss:		2.246790
  validation accuracy:		13.04 %
Epoch 295 of 2000 took 0.105s
  training loss:		2.294637
  validation loss:		2.245202
  validation accuracy:		13.04 %
Epoch 296 of 2000 took 0.105s
  training loss:		2.293333
  validation loss:		2.240948
  validation accuracy:		12.83 %
Epoch 297 of 2000 took 0.105s
  training loss:		2.294754
  validation loss:		2.242009
  validation accuracy:		12.83 %
Epoch 298 of 2000 took 0.105s
  training loss:		2.293021
  validation loss:		2.240270
  validation accuracy:		12.83 %
Epoch 299 of 2000 took 0.105s
  training loss:		2.294760
  validation loss:		2.241300
  validation accuracy:		13.04 %
Epoch 300 of 2000 took 0.105s
  training loss:		2.295199
  validation loss:		2.246155
  validation accuracy:		13.04 %
Epoch 301 of 2000 took 0.105s
  training loss:		2.294605
  validation loss:		2.247164
  validation accuracy:		12.93 %
Epoch 302 of 2000 took 0.105s
  training loss:		2.294928
  validation loss:		2.244408
  validation accuracy:		12.83 %
Epoch 303 of 2000 took 0.105s
  training loss:		2.295106
  validation loss:		2.241703
  validation accuracy:		12.93 %
Epoch 304 of 2000 took 0.105s
  training loss:		2.295457
  validation loss:		2.244271
  validation accuracy:		13.04 %
Epoch 305 of 2000 took 0.105s
  training loss:		2.295160
  validation loss:		2.248541
  validation accuracy:		13.04 %
Epoch 306 of 2000 took 0.105s
  training loss:		2.295248
  validation loss:		2.244219
  validation accuracy:		12.93 %
Epoch 307 of 2000 took 0.105s
  training loss:		2.294941
  validation loss:		2.244181
  validation accuracy:		13.04 %
Epoch 308 of 2000 took 0.105s
  training loss:		2.294695
  validation loss:		2.243182
  validation accuracy:		12.93 %
Epoch 309 of 2000 took 0.105s
  training loss:		2.294010
  validation loss:		2.244144
  validation accuracy:		12.93 %
Epoch 310 of 2000 took 0.105s
  training loss:		2.294173
  validation loss:		2.241596
  validation accuracy:		12.83 %
Epoch 311 of 2000 took 0.105s
  training loss:		2.295577
  validation loss:		2.244477
  validation accuracy:		14.67 %
Epoch 312 of 2000 took 0.110s
  training loss:		2.294729
  validation loss:		2.245310
  validation accuracy:		12.83 %
Epoch 313 of 2000 took 0.105s
  training loss:		2.295273
  validation loss:		2.246807
  validation accuracy:		12.93 %
Epoch 314 of 2000 took 0.105s
  training loss:		2.294964
  validation loss:		2.245849
  validation accuracy:		12.93 %
Epoch 315 of 2000 took 0.105s
  training loss:		2.295716
  validation loss:		2.247421
  validation accuracy:		12.83 %
Epoch 316 of 2000 took 0.105s
  training loss:		2.294257
  validation loss:		2.242000
  validation accuracy:		12.83 %
Epoch 317 of 2000 took 0.105s
  training loss:		2.295708
  validation loss:		2.244576
  validation accuracy:		12.83 %
Epoch 318 of 2000 took 0.105s
  training loss:		2.293904
  validation loss:		2.247087
  validation accuracy:		12.93 %
Epoch 319 of 2000 took 0.105s
  training loss:		2.294157
  validation loss:		2.242018
  validation accuracy:		12.93 %
Epoch 320 of 2000 took 0.105s
  training loss:		2.294566
  validation loss:		2.239411
  validation accuracy:		12.83 %
Epoch 321 of 2000 took 0.105s
  training loss:		2.294795
  validation loss:		2.240789
  validation accuracy:		13.04 %
Epoch 322 of 2000 took 0.105s
  training loss:		2.295029
  validation loss:		2.247185
  validation accuracy:		12.83 %
Epoch 323 of 2000 took 0.105s
  training loss:		2.294435
  validation loss:		2.248850
  validation accuracy:		13.04 %
Epoch 324 of 2000 took 0.106s
  training loss:		2.294663
  validation loss:		2.245988
  validation accuracy:		13.04 %
Epoch 325 of 2000 took 0.105s
  training loss:		2.295118
  validation loss:		2.241418
  validation accuracy:		13.04 %
Epoch 326 of 2000 took 0.105s
  training loss:		2.295564
  validation loss:		2.246107
  validation accuracy:		12.93 %
Epoch 327 of 2000 took 0.105s
  training loss:		2.294643
  validation loss:		2.243744
  validation accuracy:		12.83 %
Epoch 328 of 2000 took 0.105s
  training loss:		2.294829
  validation loss:		2.245971
  validation accuracy:		12.93 %
Epoch 329 of 2000 took 0.160s
  training loss:		2.295121
  validation loss:		2.247896
  validation accuracy:		12.83 %
Epoch 330 of 2000 took 0.167s
  training loss:		2.295345
  validation loss:		2.247544
  validation accuracy:		12.93 %
Epoch 331 of 2000 took 0.112s
  training loss:		2.295656
  validation loss:		2.244707
  validation accuracy:		13.04 %
Epoch 332 of 2000 took 0.104s
  training loss:		2.295030
  validation loss:		2.246151
  validation accuracy:		12.93 %
Epoch 333 of 2000 took 0.104s
  training loss:		2.294197
  validation loss:		2.244168
  validation accuracy:		13.91 %
Epoch 334 of 2000 took 0.104s
  training loss:		2.294316
  validation loss:		2.243239
  validation accuracy:		13.04 %
Epoch 335 of 2000 took 0.104s
  training loss:		2.295428
  validation loss:		2.241795
  validation accuracy:		12.83 %
Epoch 336 of 2000 took 0.104s
  training loss:		2.295828
  validation loss:		2.246293
  validation accuracy:		12.83 %
Epoch 337 of 2000 took 0.104s
  training loss:		2.295020
  validation loss:		2.246017
  validation accuracy:		12.93 %
Epoch 338 of 2000 took 0.105s
  training loss:		2.293854
  validation loss:		2.242918
  validation accuracy:		13.04 %
Epoch 339 of 2000 took 0.105s
  training loss:		2.294307
  validation loss:		2.241882
  validation accuracy:		12.83 %
Epoch 340 of 2000 took 0.105s
  training loss:		2.295217
  validation loss:		2.245190
  validation accuracy:		12.93 %
Epoch 341 of 2000 took 0.105s
  training loss:		2.294634
  validation loss:		2.244006
  validation accuracy:		12.93 %
Epoch 342 of 2000 took 0.105s
  training loss:		2.295999
  validation loss:		2.245384
  validation accuracy:		12.93 %
Epoch 343 of 2000 took 0.105s
  training loss:		2.295062
  validation loss:		2.243047
  validation accuracy:		12.83 %
Epoch 344 of 2000 took 0.105s
  training loss:		2.294578
  validation loss:		2.241510
  validation accuracy:		13.04 %
Epoch 345 of 2000 took 0.105s
  training loss:		2.294135
  validation loss:		2.244482
  validation accuracy:		13.04 %
Epoch 346 of 2000 took 0.105s
  training loss:		2.293953
  validation loss:		2.243388
  validation accuracy:		12.93 %
Epoch 347 of 2000 took 0.105s
  training loss:		2.293980
  validation loss:		2.243906
  validation accuracy:		12.93 %
Epoch 348 of 2000 took 0.105s
  training loss:		2.296096
  validation loss:		2.246237
  validation accuracy:		13.04 %
Epoch 349 of 2000 took 0.105s
  training loss:		2.294719
  validation loss:		2.243948
  validation accuracy:		12.83 %
Epoch 350 of 2000 took 0.105s
  training loss:		2.294603
  validation loss:		2.245326
  validation accuracy:		13.04 %
Epoch 351 of 2000 took 0.105s
  training loss:		2.293862
  validation loss:		2.243533
  validation accuracy:		12.83 %
Epoch 352 of 2000 took 0.105s
  training loss:		2.294901
  validation loss:		2.245405
  validation accuracy:		15.22 %
Epoch 353 of 2000 took 0.105s
  training loss:		2.294291
  validation loss:		2.242153
  validation accuracy:		12.93 %
Epoch 354 of 2000 took 0.105s
  training loss:		2.294591
  validation loss:		2.244153
  validation accuracy:		12.93 %
Epoch 355 of 2000 took 0.105s
  training loss:		2.294709
  validation loss:		2.244972
  validation accuracy:		12.93 %
Epoch 356 of 2000 took 0.105s
  training loss:		2.295602
  validation loss:		2.244727
  validation accuracy:		17.50 %
Epoch 357 of 2000 took 0.105s
  training loss:		2.294891
  validation loss:		2.245377
  validation accuracy:		12.83 %
Epoch 358 of 2000 took 0.105s
  training loss:		2.295751
  validation loss:		2.242724
  validation accuracy:		12.93 %
Epoch 359 of 2000 took 0.105s
  training loss:		2.295039
  validation loss:		2.245147
  validation accuracy:		12.93 %
Epoch 360 of 2000 took 0.105s
  training loss:		2.294179
  validation loss:		2.243925
  validation accuracy:		12.93 %
Epoch 361 of 2000 took 0.105s
  training loss:		2.294907
  validation loss:		2.245593
  validation accuracy:		13.04 %
Epoch 362 of 2000 took 0.105s
  training loss:		2.295052
  validation loss:		2.244423
  validation accuracy:		12.93 %
Epoch 363 of 2000 took 0.105s
  training loss:		2.294884
  validation loss:		2.241146
  validation accuracy:		12.83 %
Epoch 364 of 2000 took 0.105s
  training loss:		2.294619
  validation loss:		2.243624
  validation accuracy:		13.04 %
Epoch 365 of 2000 took 0.105s
  training loss:		2.294598
  validation loss:		2.242352
  validation accuracy:		12.93 %
Epoch 366 of 2000 took 0.105s
  training loss:		2.294438
  validation loss:		2.245124
  validation accuracy:		13.04 %
Epoch 367 of 2000 took 0.105s
  training loss:		2.295363
  validation loss:		2.242776
  validation accuracy:		12.83 %
Epoch 368 of 2000 took 0.110s
  training loss:		2.295230
  validation loss:		2.246005
  validation accuracy:		12.83 %
Epoch 369 of 2000 took 0.105s
  training loss:		2.294438
  validation loss:		2.242720
  validation accuracy:		12.83 %
Epoch 370 of 2000 took 0.105s
  training loss:		2.294933
  validation loss:		2.244107
  validation accuracy:		13.04 %
Epoch 371 of 2000 took 0.105s
  training loss:		2.294533
  validation loss:		2.244720
  validation accuracy:		13.04 %
Epoch 372 of 2000 took 0.105s
  training loss:		2.294024
  validation loss:		2.243914
  validation accuracy:		12.93 %
Epoch 373 of 2000 took 0.105s
  training loss:		2.294728
  validation loss:		2.244165
  validation accuracy:		12.83 %
Epoch 374 of 2000 took 0.105s
  training loss:		2.294201
  validation loss:		2.242608
  validation accuracy:		13.04 %
Epoch 375 of 2000 took 0.105s
  training loss:		2.294592
  validation loss:		2.243803
  validation accuracy:		12.93 %
Epoch 376 of 2000 took 0.105s
  training loss:		2.295254
  validation loss:		2.241804
  validation accuracy:		12.93 %
Epoch 377 of 2000 took 0.105s
  training loss:		2.294411
  validation loss:		2.241928
  validation accuracy:		13.04 %
Epoch 378 of 2000 took 0.105s
  training loss:		2.294628
  validation loss:		2.243513
  validation accuracy:		12.83 %
Epoch 379 of 2000 took 0.105s
  training loss:		2.295031
  validation loss:		2.246245
  validation accuracy:		12.93 %
Epoch 380 of 2000 took 0.105s
  training loss:		2.295121
  validation loss:		2.247062
  validation accuracy:		12.93 %
Epoch 381 of 2000 took 0.105s
  training loss:		2.294205
  validation loss:		2.242509
  validation accuracy:		13.04 %
Epoch 382 of 2000 took 0.105s
  training loss:		2.294096
  validation loss:		2.241392
  validation accuracy:		13.04 %
Epoch 383 of 2000 took 0.105s
  training loss:		2.295238
  validation loss:		2.244703
  validation accuracy:		12.83 %
Epoch 384 of 2000 took 0.105s
  training loss:		2.294600
  validation loss:		2.243585
  validation accuracy:		12.83 %
Epoch 385 of 2000 took 0.105s
  training loss:		2.294626
  validation loss:		2.245116
  validation accuracy:		13.04 %
Epoch 386 of 2000 took 0.105s
  training loss:		2.294920
  validation loss:		2.243796
  validation accuracy:		12.83 %
Epoch 387 of 2000 took 0.105s
  training loss:		2.294128
  validation loss:		2.243302
  validation accuracy:		13.04 %
Epoch 388 of 2000 took 0.105s
  training loss:		2.294769
  validation loss:		2.242350
  validation accuracy:		13.04 %
Epoch 389 of 2000 took 0.105s
  training loss:		2.293888
  validation loss:		2.245231
  validation accuracy:		12.93 %
Epoch 390 of 2000 took 0.105s
  training loss:		2.294419
  validation loss:		2.243848
  validation accuracy:		12.83 %
Epoch 391 of 2000 took 0.105s
  training loss:		2.293754
  validation loss:		2.240244
  validation accuracy:		13.04 %
Epoch 392 of 2000 took 0.105s
  training loss:		2.295237
  validation loss:		2.244884
  validation accuracy:		13.04 %
Epoch 393 of 2000 took 0.105s
  training loss:		2.294229
  validation loss:		2.244367
  validation accuracy:		12.83 %
Epoch 394 of 2000 took 0.105s
  training loss:		2.294324
  validation loss:		2.245056
  validation accuracy:		12.83 %
Epoch 395 of 2000 took 0.105s
  training loss:		2.294858
  validation loss:		2.243298
  validation accuracy:		12.83 %
Epoch 396 of 2000 took 0.105s
  training loss:		2.294872
  validation loss:		2.243188
  validation accuracy:		13.04 %
Epoch 397 of 2000 took 0.105s
  training loss:		2.294873
  validation loss:		2.245591
  validation accuracy:		13.04 %
Epoch 398 of 2000 took 0.105s
  training loss:		2.295240
  validation loss:		2.244259
  validation accuracy:		12.93 %
Epoch 399 of 2000 took 0.105s
  training loss:		2.294936
  validation loss:		2.244954
  validation accuracy:		12.83 %
Epoch 400 of 2000 took 0.105s
  training loss:		2.294833
  validation loss:		2.244135
  validation accuracy:		12.93 %
Epoch 401 of 2000 took 0.105s
  training loss:		2.294507
  validation loss:		2.244135
  validation accuracy:		12.93 %
Epoch 402 of 2000 took 0.105s
  training loss:		2.294327
  validation loss:		2.245653
  validation accuracy:		14.13 %
Epoch 403 of 2000 took 0.105s
  training loss:		2.294672
  validation loss:		2.244544
  validation accuracy:		13.04 %
Epoch 404 of 2000 took 0.105s
  training loss:		2.293363
  validation loss:		2.239870
  validation accuracy:		12.93 %
Epoch 405 of 2000 took 0.105s
  training loss:		2.294886
  validation loss:		2.243467
  validation accuracy:		12.93 %
Epoch 406 of 2000 took 0.105s
  training loss:		2.294625
  validation loss:		2.242643
  validation accuracy:		13.04 %
Epoch 407 of 2000 took 0.105s
  training loss:		2.294672
  validation loss:		2.241349
  validation accuracy:		12.93 %
Epoch 408 of 2000 took 0.105s
  training loss:		2.294622
  validation loss:		2.245333
  validation accuracy:		13.04 %
Epoch 409 of 2000 took 0.105s
  training loss:		2.294860
  validation loss:		2.246691
  validation accuracy:		13.04 %
Epoch 410 of 2000 took 0.105s
  training loss:		2.295197
  validation loss:		2.247069
  validation accuracy:		12.93 %
Epoch 411 of 2000 took 0.105s
  training loss:		2.295141
  validation loss:		2.246412
  validation accuracy:		13.04 %
Epoch 412 of 2000 took 0.105s
  training loss:		2.294692
  validation loss:		2.245610
  validation accuracy:		12.83 %
Epoch 413 of 2000 took 0.105s
  training loss:		2.293609
  validation loss:		2.242996
  validation accuracy:		12.93 %
Epoch 414 of 2000 took 0.105s
  training loss:		2.294807
  validation loss:		2.241348
  validation accuracy:		13.04 %
Epoch 415 of 2000 took 0.105s
  training loss:		2.295401
  validation loss:		2.244520
  validation accuracy:		13.15 %
Epoch 416 of 2000 took 0.105s
  training loss:		2.293328
  validation loss:		2.241934
  validation accuracy:		13.04 %
Epoch 417 of 2000 took 0.105s
  training loss:		2.294875
  validation loss:		2.240590
  validation accuracy:		12.93 %
Epoch 418 of 2000 took 0.105s
  training loss:		2.294774
  validation loss:		2.245313
  validation accuracy:		12.93 %
Epoch 419 of 2000 took 0.107s
  training loss:		2.294743
  validation loss:		2.245946
  validation accuracy:		13.04 %
Epoch 420 of 2000 took 0.108s
  training loss:		2.295792
  validation loss:		2.245739
  validation accuracy:		12.83 %
Epoch 421 of 2000 took 0.105s
  training loss:		2.295372
  validation loss:		2.247822
  validation accuracy:		12.83 %
Epoch 422 of 2000 took 0.105s
  training loss:		2.294443
  validation loss:		2.246424
  validation accuracy:		12.93 %
Epoch 423 of 2000 took 0.105s
  training loss:		2.294210
  validation loss:		2.241752
  validation accuracy:		12.93 %
Epoch 424 of 2000 took 0.105s
  training loss:		2.294389
  validation loss:		2.242046
  validation accuracy:		13.04 %
Epoch 425 of 2000 took 0.105s
  training loss:		2.294757
  validation loss:		2.243624
  validation accuracy:		13.04 %
Epoch 426 of 2000 took 0.105s
  training loss:		2.293966
  validation loss:		2.245120
  validation accuracy:		12.93 %
Epoch 427 of 2000 took 0.105s
  training loss:		2.293989
  validation loss:		2.244248
  validation accuracy:		12.83 %
Epoch 428 of 2000 took 0.105s
  training loss:		2.294530
  validation loss:		2.241028
  validation accuracy:		13.04 %
Epoch 429 of 2000 took 0.105s
  training loss:		2.294561
  validation loss:		2.243506
  validation accuracy:		13.04 %
Epoch 430 of 2000 took 0.105s
  training loss:		2.295771
  validation loss:		2.244633
  validation accuracy:		12.93 %
Epoch 431 of 2000 took 0.105s
  training loss:		2.295350
  validation loss:		2.246101
  validation accuracy:		13.04 %
Epoch 432 of 2000 took 0.105s
  training loss:		2.294571
  validation loss:		2.245096
  validation accuracy:		13.04 %
Epoch 433 of 2000 took 0.105s
  training loss:		2.294455
  validation loss:		2.243743
  validation accuracy:		12.83 %
Epoch 434 of 2000 took 0.105s
  training loss:		2.294611
  validation loss:		2.246050
  validation accuracy:		12.93 %
Epoch 435 of 2000 took 0.105s
  training loss:		2.294806
  validation loss:		2.243060
  validation accuracy:		13.04 %
Epoch 436 of 2000 took 0.105s
  training loss:		2.295351
  validation loss:		2.241741
  validation accuracy:		13.04 %
Epoch 437 of 2000 took 0.105s
  training loss:		2.294698
  validation loss:		2.244328
  validation accuracy:		12.83 %
Epoch 438 of 2000 took 0.105s
  training loss:		2.294422
  validation loss:		2.244850
  validation accuracy:		13.37 %
Epoch 439 of 2000 took 0.105s
  training loss:		2.294376
  validation loss:		2.243973
  validation accuracy:		12.93 %
Epoch 440 of 2000 took 0.105s
  training loss:		2.294922
  validation loss:		2.244051
  validation accuracy:		12.83 %
Epoch 441 of 2000 took 0.105s
  training loss:		2.293855
  validation loss:		2.240935
  validation accuracy:		12.83 %
Epoch 442 of 2000 took 0.105s
  training loss:		2.294425
  validation loss:		2.239852
  validation accuracy:		12.93 %
Epoch 443 of 2000 took 0.105s
  training loss:		2.295370
  validation loss:		2.242519
  validation accuracy:		12.93 %
Epoch 444 of 2000 took 0.106s
  training loss:		2.295299
  validation loss:		2.246775
  validation accuracy:		13.80 %
Epoch 445 of 2000 took 0.106s
  training loss:		2.294883
  validation loss:		2.249111
  validation accuracy:		12.83 %
Epoch 446 of 2000 took 0.106s
  training loss:		2.293883
  validation loss:		2.243564
  validation accuracy:		12.83 %
Epoch 447 of 2000 took 0.106s
  training loss:		2.294087
  validation loss:		2.242868
  validation accuracy:		12.93 %
Epoch 448 of 2000 took 0.107s
  training loss:		2.294668
  validation loss:		2.241523
  validation accuracy:		12.93 %
Epoch 449 of 2000 took 0.107s
  training loss:		2.294162
  validation loss:		2.243970
  validation accuracy:		12.93 %
Epoch 450 of 2000 took 0.107s
  training loss:		2.295126
  validation loss:		2.240746
  validation accuracy:		12.83 %
Epoch 451 of 2000 took 0.107s
  training loss:		2.294273
  validation loss:		2.243723
  validation accuracy:		12.93 %
Epoch 452 of 2000 took 0.107s
  training loss:		2.294403
  validation loss:		2.243643
  validation accuracy:		12.83 %
Epoch 453 of 2000 took 0.107s
  training loss:		2.295058
  validation loss:		2.246066
  validation accuracy:		12.93 %
Epoch 454 of 2000 took 0.107s
  training loss:		2.294089
  validation loss:		2.242703
  validation accuracy:		12.83 %
Epoch 455 of 2000 took 0.107s
  training loss:		2.294997
  validation loss:		2.244588
  validation accuracy:		12.83 %
Epoch 456 of 2000 took 0.107s
  training loss:		2.295074
  validation loss:		2.248274
  validation accuracy:		12.93 %
Epoch 457 of 2000 took 0.107s
  training loss:		2.294734
  validation loss:		2.246060
  validation accuracy:		12.93 %
Epoch 458 of 2000 took 0.107s
  training loss:		2.294748
  validation loss:		2.245117
  validation accuracy:		13.04 %
Epoch 459 of 2000 took 0.107s
  training loss:		2.294863
  validation loss:		2.244362
  validation accuracy:		13.04 %
Epoch 460 of 2000 took 0.107s
  training loss:		2.293935
  validation loss:		2.243420
  validation accuracy:		13.04 %
Epoch 461 of 2000 took 0.107s
  training loss:		2.293948
  validation loss:		2.242294
  validation accuracy:		12.93 %
Epoch 462 of 2000 took 0.107s
  training loss:		2.294187
  validation loss:		2.240173
  validation accuracy:		12.83 %
Epoch 463 of 2000 took 0.107s
  training loss:		2.294973
  validation loss:		2.242528
  validation accuracy:		13.04 %
Epoch 464 of 2000 took 0.107s
  training loss:		2.294618
  validation loss:		2.243652
  validation accuracy:		13.04 %
Epoch 465 of 2000 took 0.107s
  training loss:		2.294615
  validation loss:		2.243892
  validation accuracy:		12.93 %
Epoch 466 of 2000 took 0.112s
  training loss:		2.294660
  validation loss:		2.242828
  validation accuracy:		12.93 %
Epoch 467 of 2000 took 0.107s
  training loss:		2.294943
  validation loss:		2.244869
  validation accuracy:		13.04 %
Epoch 468 of 2000 took 0.107s
  training loss:		2.295918
  validation loss:		2.248343
  validation accuracy:		12.93 %
Epoch 469 of 2000 took 0.107s
  training loss:		2.295077
  validation loss:		2.243198
  validation accuracy:		12.83 %
Epoch 470 of 2000 took 0.107s
  training loss:		2.294766
  validation loss:		2.243745
  validation accuracy:		16.74 %
Epoch 471 of 2000 took 0.107s
  training loss:		2.295025
  validation loss:		2.241410
  validation accuracy:		13.04 %
Epoch 472 of 2000 took 0.107s
  training loss:		2.294239
  validation loss:		2.244467
  validation accuracy:		12.93 %
Epoch 473 of 2000 took 0.106s
  training loss:		2.295209
  validation loss:		2.247294
  validation accuracy:		12.83 %
Epoch 474 of 2000 took 0.106s
  training loss:		2.295155
  validation loss:		2.243946
  validation accuracy:		12.83 %
Epoch 475 of 2000 took 0.106s
  training loss:		2.294995
  validation loss:		2.244878
  validation accuracy:		12.93 %
Epoch 476 of 2000 took 0.106s
  training loss:		2.293860
  validation loss:		2.241856
  validation accuracy:		12.83 %
Epoch 477 of 2000 took 0.106s
  training loss:		2.295047
  validation loss:		2.243162
  validation accuracy:		12.93 %
Epoch 478 of 2000 took 0.106s
  training loss:		2.294009
  validation loss:		2.239663
  validation accuracy:		18.15 %
Epoch 479 of 2000 took 0.105s
  training loss:		2.294927
  validation loss:		2.241157
  validation accuracy:		12.93 %
Epoch 480 of 2000 took 0.106s
  training loss:		2.294314
  validation loss:		2.244761
  validation accuracy:		12.83 %
Epoch 481 of 2000 took 0.106s
  training loss:		2.295601
  validation loss:		2.247891
  validation accuracy:		12.83 %
Epoch 482 of 2000 took 0.152s
  training loss:		2.295200
  validation loss:		2.245714
  validation accuracy:		12.93 %
Epoch 483 of 2000 took 0.109s
  training loss:		2.295047
  validation loss:		2.246481
  validation accuracy:		13.04 %
Epoch 484 of 2000 took 0.106s
  training loss:		2.294692
  validation loss:		2.242235
  validation accuracy:		13.04 %
Epoch 485 of 2000 took 0.106s
  training loss:		2.295033
  validation loss:		2.242708
  validation accuracy:		14.89 %
Epoch 486 of 2000 took 0.106s
  training loss:		2.294022
  validation loss:		2.242725
  validation accuracy:		12.83 %
Epoch 487 of 2000 took 0.110s
  training loss:		2.294277
  validation loss:		2.242521
  validation accuracy:		12.83 %
Epoch 488 of 2000 took 0.107s
  training loss:		2.294257
  validation loss:		2.242947
  validation accuracy:		13.04 %
Epoch 489 of 2000 took 0.105s
  training loss:		2.294124
  validation loss:		2.241673
  validation accuracy:		13.04 %
Epoch 490 of 2000 took 0.104s
  training loss:		2.294581
  validation loss:		2.247226
  validation accuracy:		12.93 %
Epoch 491 of 2000 took 0.106s
  training loss:		2.294958
  validation loss:		2.242663
  validation accuracy:		12.93 %
Epoch 492 of 2000 took 0.113s
  training loss:		2.295550
  validation loss:		2.242859
  validation accuracy:		13.04 %
Epoch 493 of 2000 took 0.105s
  training loss:		2.294960
  validation loss:		2.244306
  validation accuracy:		12.83 %
Epoch 494 of 2000 took 0.105s
  training loss:		2.294646
  validation loss:		2.245753
  validation accuracy:		12.93 %
Epoch 495 of 2000 took 0.106s
  training loss:		2.295061
  validation loss:		2.246408
  validation accuracy:		15.00 %
Epoch 496 of 2000 took 0.105s
  training loss:		2.295176
  validation loss:		2.247712
  validation accuracy:		12.83 %
Epoch 497 of 2000 took 0.105s
  training loss:		2.294783
  validation loss:		2.241308
  validation accuracy:		12.93 %
Epoch 498 of 2000 took 0.107s
  training loss:		2.294276
  validation loss:		2.242789
  validation accuracy:		12.93 %
Epoch 499 of 2000 took 0.109s
  training loss:		2.294525
  validation loss:		2.243484
  validation accuracy:		13.04 %
Epoch 500 of 2000 took 0.108s
  training loss:		2.295042
  validation loss:		2.244630
  validation accuracy:		12.83 %
Epoch 501 of 2000 took 0.106s
  training loss:		2.294100
  validation loss:		2.245957
  validation accuracy:		12.93 %
Epoch 502 of 2000 took 0.107s
  training loss:		2.294836
  validation loss:		2.246818
  validation accuracy:		12.93 %
Epoch 503 of 2000 took 0.112s
  training loss:		2.295985
  validation loss:		2.244722
  validation accuracy:		13.37 %
Epoch 504 of 2000 took 0.107s
  training loss:		2.295491
  validation loss:		2.243726
  validation accuracy:		13.04 %
Epoch 505 of 2000 took 0.107s
  training loss:		2.294871
  validation loss:		2.246479
  validation accuracy:		13.04 %
Epoch 506 of 2000 took 0.110s
  training loss:		2.293975
  validation loss:		2.241781
  validation accuracy:		13.26 %
Epoch 507 of 2000 took 0.106s
  training loss:		2.295357
  validation loss:		2.244944
  validation accuracy:		12.83 %
Epoch 508 of 2000 took 0.107s
  training loss:		2.295107
  validation loss:		2.242664
  validation accuracy:		12.93 %
Epoch 509 of 2000 took 0.111s
  training loss:		2.294783
  validation loss:		2.242951
  validation accuracy:		12.83 %
Epoch 510 of 2000 took 0.111s
  training loss:		2.294564
  validation loss:		2.246830
  validation accuracy:		12.93 %
Epoch 511 of 2000 took 0.105s
  training loss:		2.294701
  validation loss:		2.240388
  validation accuracy:		13.04 %
Epoch 512 of 2000 took 0.115s
  training loss:		2.295135
  validation loss:		2.243723
  validation accuracy:		12.93 %
Epoch 513 of 2000 took 0.112s
  training loss:		2.294866
  validation loss:		2.243692
  validation accuracy:		13.04 %
Epoch 514 of 2000 took 0.112s
  training loss:		2.295237
  validation loss:		2.247914
  validation accuracy:		12.93 %
Epoch 515 of 2000 took 0.112s
  training loss:		2.294503
  validation loss:		2.243592
  validation accuracy:		12.93 %
Epoch 516 of 2000 took 0.108s
  training loss:		2.294932
  validation loss:		2.245855
  validation accuracy:		12.83 %
Epoch 517 of 2000 took 0.114s
  training loss:		2.294817
  validation loss:		2.245178
  validation accuracy:		12.93 %
Epoch 518 of 2000 took 0.110s
  training loss:		2.295212
  validation loss:		2.245034
  validation accuracy:		13.04 %
Epoch 519 of 2000 took 0.105s
  training loss:		2.296681
  validation loss:		2.245466
  validation accuracy:		12.83 %
Epoch 520 of 2000 took 0.111s
  training loss:		2.294197
  validation loss:		2.243813
  validation accuracy:		12.83 %
Epoch 521 of 2000 took 0.115s
  training loss:		2.294912
  validation loss:		2.245478
  validation accuracy:		12.93 %
Epoch 522 of 2000 took 0.105s
  training loss:		2.294797
  validation loss:		2.244204
  validation accuracy:		12.93 %
Epoch 523 of 2000 took 0.109s
  training loss:		2.293148
  validation loss:		2.241833
  validation accuracy:		12.93 %
Epoch 524 of 2000 took 0.105s
  training loss:		2.295556
  validation loss:		2.244090
  validation accuracy:		12.83 %
Epoch 525 of 2000 took 0.113s
  training loss:		2.294777
  validation loss:		2.243372
  validation accuracy:		12.83 %
Epoch 526 of 2000 took 0.112s
  training loss:		2.295338
  validation loss:		2.246620
  validation accuracy:		12.83 %
Epoch 527 of 2000 took 0.111s
  training loss:		2.295327
  validation loss:		2.248393
  validation accuracy:		12.93 %
Epoch 528 of 2000 took 0.111s
  training loss:		2.294897
  validation loss:		2.242736
  validation accuracy:		12.83 %
Epoch 529 of 2000 took 0.114s
  training loss:		2.294796
  validation loss:		2.242208
  validation accuracy:		12.83 %
Epoch 530 of 2000 took 0.113s
  training loss:		2.295541
  validation loss:		2.246439
  validation accuracy:		14.46 %
Epoch 531 of 2000 took 0.116s
  training loss:		2.295213
  validation loss:		2.246732
  validation accuracy:		12.93 %
Epoch 532 of 2000 took 0.110s
  training loss:		2.295444
  validation loss:		2.248465
  validation accuracy:		13.04 %
Epoch 533 of 2000 took 0.116s
  training loss:		2.294624
  validation loss:		2.244952
  validation accuracy:		13.04 %
Epoch 534 of 2000 took 0.114s
  training loss:		2.292718
  validation loss:		2.239759
  validation accuracy:		12.83 %
Epoch 535 of 2000 took 0.110s
  training loss:		2.295289
  validation loss:		2.241527
  validation accuracy:		12.93 %
Epoch 536 of 2000 took 0.115s
  training loss:		2.294921
  validation loss:		2.242317
  validation accuracy:		12.93 %
Epoch 537 of 2000 took 0.117s
  training loss:		2.294963
  validation loss:		2.246596
  validation accuracy:		12.93 %
Epoch 538 of 2000 took 0.110s
  training loss:		2.294918
  validation loss:		2.245597
  validation accuracy:		13.15 %
Epoch 539 of 2000 took 0.113s
  training loss:		2.293671
  validation loss:		2.243486
  validation accuracy:		12.83 %
Epoch 540 of 2000 took 0.110s
  training loss:		2.295585
  validation loss:		2.244454
  validation accuracy:		12.83 %
Epoch 541 of 2000 took 0.117s
  training loss:		2.294387
  validation loss:		2.246122
  validation accuracy:		12.83 %
Epoch 542 of 2000 took 0.118s
  training loss:		2.294118
  validation loss:		2.243936
  validation accuracy:		12.83 %
Epoch 543 of 2000 took 0.110s
  training loss:		2.295320
  validation loss:		2.244299
  validation accuracy:		12.93 %
Epoch 544 of 2000 took 0.116s
  training loss:		2.294891
  validation loss:		2.244143
  validation accuracy:		12.93 %
Epoch 545 of 2000 took 0.116s
  training loss:		2.294708
  validation loss:		2.245586
  validation accuracy:		12.93 %
Epoch 546 of 2000 took 0.111s
  training loss:		2.294957
  validation loss:		2.246132
  validation accuracy:		12.93 %
Epoch 547 of 2000 took 0.113s
  training loss:		2.295134
  validation loss:		2.244293
  validation accuracy:		12.83 %
Epoch 548 of 2000 took 0.110s
  training loss:		2.295007
  validation loss:		2.244545
  validation accuracy:		12.83 %
Epoch 549 of 2000 took 0.118s
  training loss:		2.294499
  validation loss:		2.244850
  validation accuracy:		17.72 %
Epoch 550 of 2000 took 0.114s
  training loss:		2.294208
  validation loss:		2.243837
  validation accuracy:		12.83 %
Epoch 551 of 2000 took 0.110s
  training loss:		2.295190
  validation loss:		2.245957
  validation accuracy:		13.04 %
Epoch 552 of 2000 took 0.116s
  training loss:		2.294843
  validation loss:		2.244697
  validation accuracy:		12.93 %
Epoch 553 of 2000 took 0.117s
  training loss:		2.294220
  validation loss:		2.244766
  validation accuracy:		12.93 %
Epoch 554 of 2000 took 0.110s
  training loss:		2.294691
  validation loss:		2.244399
  validation accuracy:		12.83 %
Epoch 555 of 2000 took 0.113s
  training loss:		2.293845
  validation loss:		2.242217
  validation accuracy:		15.43 %
Epoch 556 of 2000 took 0.110s
  training loss:		2.293665
  validation loss:		2.240261
  validation accuracy:		12.93 %
Epoch 557 of 2000 took 0.117s
  training loss:		2.295423
  validation loss:		2.243119
  validation accuracy:		12.93 %
Epoch 558 of 2000 took 0.114s
  training loss:		2.295380
  validation loss:		2.248583
  validation accuracy:		12.93 %
Epoch 559 of 2000 took 0.110s
  training loss:		2.294813
  validation loss:		2.246092
  validation accuracy:		12.93 %
Epoch 560 of 2000 took 0.116s
  training loss:		2.295189
  validation loss:		2.246250
  validation accuracy:		12.83 %
Epoch 561 of 2000 took 0.116s
  training loss:		2.294547
  validation loss:		2.244557
  validation accuracy:		13.04 %
Epoch 562 of 2000 took 0.111s
  training loss:		2.294744
  validation loss:		2.242913
  validation accuracy:		13.04 %
Epoch 563 of 2000 took 0.149s
  training loss:		2.293870
  validation loss:		2.242809
  validation accuracy:		12.93 %
Epoch 564 of 2000 took 0.105s
  training loss:		2.295210
  validation loss:		2.243884
  validation accuracy:		12.83 %
Epoch 565 of 2000 took 0.108s
  training loss:		2.294419
  validation loss:		2.243093
  validation accuracy:		12.83 %
Epoch 566 of 2000 took 0.103s
  training loss:		2.294617
  validation loss:		2.243453
  validation accuracy:		12.93 %
Epoch 567 of 2000 took 0.102s
  training loss:		2.294375
  validation loss:		2.242970
  validation accuracy:		13.04 %
Epoch 568 of 2000 took 0.109s
  training loss:		2.293931
  validation loss:		2.243231
  validation accuracy:		13.04 %
Epoch 569 of 2000 took 0.103s
  training loss:		2.295066
  validation loss:		2.243466
  validation accuracy:		12.83 %
Epoch 570 of 2000 took 0.102s
  training loss:		2.295351
  validation loss:		2.242566
  validation accuracy:		13.04 %
Epoch 571 of 2000 took 0.102s
  training loss:		2.294963
  validation loss:		2.247243
  validation accuracy:		12.93 %
Epoch 572 of 2000 took 0.102s
  training loss:		2.294494
  validation loss:		2.246354
  validation accuracy:		12.93 %
Epoch 573 of 2000 took 0.108s
  training loss:		2.294747
  validation loss:		2.242699
  validation accuracy:		12.93 %
Epoch 574 of 2000 took 0.103s
  training loss:		2.294980
  validation loss:		2.247142
  validation accuracy:		12.93 %
Epoch 575 of 2000 took 0.101s
  training loss:		2.294190
  validation loss:		2.243700
  validation accuracy:		12.93 %
Epoch 576 of 2000 took 0.104s
  training loss:		2.295735
  validation loss:		2.245057
  validation accuracy:		13.04 %
Epoch 577 of 2000 took 0.104s
  training loss:		2.295470
  validation loss:		2.246011
  validation accuracy:		13.04 %
Epoch 578 of 2000 took 0.104s
  training loss:		2.294735
  validation loss:		2.244093
  validation accuracy:		12.93 %
Epoch 579 of 2000 took 0.104s
  training loss:		2.295091
  validation loss:		2.243438
  validation accuracy:		12.83 %
Epoch 580 of 2000 took 0.104s
  training loss:		2.294488
  validation loss:		2.245604
  validation accuracy:		14.78 %
Epoch 581 of 2000 took 0.109s
  training loss:		2.295549
  validation loss:		2.244801
  validation accuracy:		13.04 %
Epoch 582 of 2000 took 0.104s
  training loss:		2.294955
  validation loss:		2.239931
  validation accuracy:		13.04 %
Epoch 583 of 2000 took 0.104s
  training loss:		2.295098
  validation loss:		2.243908
  validation accuracy:		12.93 %
Epoch 584 of 2000 took 0.104s
  training loss:		2.293781
  validation loss:		2.246830
  validation accuracy:		12.93 %
Epoch 585 of 2000 took 0.104s
  training loss:		2.294419
  validation loss:		2.243688
  validation accuracy:		12.83 %
Epoch 586 of 2000 took 0.104s
  training loss:		2.294660
  validation loss:		2.242304
  validation accuracy:		13.04 %
Epoch 587 of 2000 took 0.104s
  training loss:		2.294353
  validation loss:		2.242020
  validation accuracy:		13.04 %
Epoch 588 of 2000 took 0.104s
  training loss:		2.295788
  validation loss:		2.245993
  validation accuracy:		12.83 %
Epoch 589 of 2000 took 0.104s
  training loss:		2.294158
  validation loss:		2.244040
  validation accuracy:		14.57 %
Epoch 590 of 2000 took 0.105s
  training loss:		2.294233
  validation loss:		2.243039
  validation accuracy:		12.93 %
Epoch 591 of 2000 took 0.104s
  training loss:		2.294814
  validation loss:		2.242886
  validation accuracy:		13.04 %
Epoch 592 of 2000 took 0.105s
  training loss:		2.294728
  validation loss:		2.245617
  validation accuracy:		12.93 %
Epoch 593 of 2000 took 0.104s
  training loss:		2.295183
  validation loss:		2.246666
  validation accuracy:		12.93 %
Epoch 594 of 2000 took 0.104s
  training loss:		2.294942
  validation loss:		2.247334
  validation accuracy:		12.83 %
Epoch 595 of 2000 took 0.104s
  training loss:		2.294911
  validation loss:		2.241957
  validation accuracy:		12.83 %
Epoch 596 of 2000 took 0.102s
  training loss:		2.294599
  validation loss:		2.245463
  validation accuracy:		13.04 %
Epoch 597 of 2000 took 0.108s
  training loss:		2.294808
  validation loss:		2.245736
  validation accuracy:		13.04 %
Epoch 598 of 2000 took 0.124s
  training loss:		2.294307
  validation loss:		2.243338
  validation accuracy:		13.04 %
Epoch 599 of 2000 took 0.124s
  training loss:		2.294572
  validation loss:		2.240921
  validation accuracy:		12.83 %
Epoch 600 of 2000 took 0.098s
  training loss:		2.294974
  validation loss:		2.245208
  validation accuracy:		12.93 %
Epoch 601 of 2000 took 0.100s
  training loss:		2.294210
  validation loss:		2.244274
  validation accuracy:		12.93 %
Epoch 602 of 2000 took 0.106s
  training loss:		2.294182
  validation loss:		2.243896
  validation accuracy:		13.80 %
Epoch 603 of 2000 took 0.106s
  training loss:		2.294428
  validation loss:		2.245716
  validation accuracy:		12.93 %
Epoch 604 of 2000 took 0.105s
  training loss:		2.293682
  validation loss:		2.244608
  validation accuracy:		13.59 %
Epoch 605 of 2000 took 0.103s
  training loss:		2.294790
  validation loss:		2.243358
  validation accuracy:		12.93 %
Epoch 606 of 2000 took 0.101s
  training loss:		2.294170
  validation loss:		2.243728
  validation accuracy:		13.15 %
Epoch 607 of 2000 took 0.101s
  training loss:		2.293952
  validation loss:		2.240873
  validation accuracy:		12.93 %
Epoch 608 of 2000 took 0.101s
  training loss:		2.295058
  validation loss:		2.243354
  validation accuracy:		12.83 %
Epoch 609 of 2000 took 0.100s
  training loss:		2.293888
  validation loss:		2.243776
  validation accuracy:		12.83 %
Epoch 610 of 2000 took 0.101s
  training loss:		2.294353
  validation loss:		2.242140
  validation accuracy:		13.04 %
Epoch 611 of 2000 took 0.101s
  training loss:		2.294768
  validation loss:		2.243570
  validation accuracy:		12.83 %
Epoch 612 of 2000 took 0.101s
  training loss:		2.294304
  validation loss:		2.241586
  validation accuracy:		13.80 %
Epoch 613 of 2000 took 0.101s
  training loss:		2.295143
  validation loss:		2.244435
  validation accuracy:		12.93 %
Epoch 614 of 2000 took 0.101s
  training loss:		2.293912
  validation loss:		2.244367
  validation accuracy:		12.93 %
Epoch 615 of 2000 took 0.101s
  training loss:		2.294638
  validation loss:		2.243381
  validation accuracy:		13.04 %
Epoch 616 of 2000 took 0.101s
  training loss:		2.294715
  validation loss:		2.244472
  validation accuracy:		13.04 %
Epoch 617 of 2000 took 0.101s
  training loss:		2.294787
  validation loss:		2.244848
  validation accuracy:		12.93 %
Epoch 618 of 2000 took 0.101s
  training loss:		2.293833
  validation loss:		2.241869
  validation accuracy:		12.93 %
Epoch 619 of 2000 took 0.101s
  training loss:		2.295091
  validation loss:		2.244123
  validation accuracy:		12.83 %
Epoch 620 of 2000 took 0.101s
  training loss:		2.294968
  validation loss:		2.247080
  validation accuracy:		12.93 %
Epoch 621 of 2000 took 0.102s
  training loss:		2.294900
  validation loss:		2.245473
  validation accuracy:		13.04 %
Epoch 622 of 2000 took 0.101s
  training loss:		2.295878
  validation loss:		2.245589
  validation accuracy:		13.04 %
Epoch 623 of 2000 took 0.101s
  training loss:		2.294797
  validation loss:		2.245378
  validation accuracy:		12.83 %
Epoch 624 of 2000 took 0.101s
  training loss:		2.295061
  validation loss:		2.241107
  validation accuracy:		12.93 %
Epoch 625 of 2000 took 0.101s
  training loss:		2.294860
  validation loss:		2.244202
  validation accuracy:		12.93 %
Epoch 626 of 2000 took 0.101s
  training loss:		2.295372
  validation loss:		2.248461
  validation accuracy:		12.93 %
Epoch 627 of 2000 took 0.101s
  training loss:		2.294256
  validation loss:		2.246139
  validation accuracy:		17.39 %
Epoch 628 of 2000 took 0.101s
  training loss:		2.294729
  validation loss:		2.241112
  validation accuracy:		12.93 %
Epoch 629 of 2000 took 0.101s
  training loss:		2.294365
  validation loss:		2.244467
  validation accuracy:		16.20 %
Epoch 630 of 2000 took 0.101s
  training loss:		2.294820
  validation loss:		2.244399
  validation accuracy:		12.93 %
Epoch 631 of 2000 took 0.101s
  training loss:		2.295556
  validation loss:		2.244308
  validation accuracy:		12.83 %
Epoch 632 of 2000 took 0.101s
  training loss:		2.294968
  validation loss:		2.245405
  validation accuracy:		13.04 %
Epoch 633 of 2000 took 0.101s
  training loss:		2.294670
  validation loss:		2.246853
  validation accuracy:		12.93 %
Epoch 634 of 2000 took 0.101s
  training loss:		2.294136
  validation loss:		2.242249
  validation accuracy:		12.83 %
Epoch 635 of 2000 took 0.101s
  training loss:		2.294052
  validation loss:		2.242369
  validation accuracy:		12.93 %
Epoch 636 of 2000 took 0.105s
  training loss:		2.293981
  validation loss:		2.243921
  validation accuracy:		12.93 %
Epoch 637 of 2000 took 0.104s
  training loss:		2.294394
  validation loss:		2.239343
  validation accuracy:		13.04 %
Epoch 638 of 2000 took 0.115s
  training loss:		2.294414
  validation loss:		2.244068
  validation accuracy:		12.83 %
Epoch 639 of 2000 took 0.165s
  training loss:		2.293575
  validation loss:		2.241819
  validation accuracy:		12.93 %
Epoch 640 of 2000 took 0.165s
  training loss:		2.295024
  validation loss:		2.245135
  validation accuracy:		13.04 %
Epoch 641 of 2000 took 0.130s
  training loss:		2.294414
  validation loss:		2.242179
  validation accuracy:		16.85 %
Epoch 642 of 2000 took 0.101s
  training loss:		2.293879
  validation loss:		2.241358
  validation accuracy:		11.85 %
Epoch 643 of 2000 took 0.100s
  training loss:		2.294864
  validation loss:		2.242845
  validation accuracy:		12.83 %
Epoch 644 of 2000 took 0.100s
  training loss:		2.295033
  validation loss:		2.248034
  validation accuracy:		12.83 %
Epoch 645 of 2000 took 0.110s
  training loss:		2.294129
  validation loss:		2.246255
  validation accuracy:		12.83 %
Epoch 646 of 2000 took 0.107s
  training loss:		2.295029
  validation loss:		2.243753
  validation accuracy:		13.04 %
Epoch 647 of 2000 took 0.101s
  training loss:		2.294555
  validation loss:		2.244098
  validation accuracy:		12.93 %
Epoch 648 of 2000 took 0.100s
  training loss:		2.295699
  validation loss:		2.248179
  validation accuracy:		12.93 %
Epoch 649 of 2000 took 0.101s
  training loss:		2.295203
  validation loss:		2.247013
  validation accuracy:		13.04 %
Epoch 650 of 2000 took 0.100s
  training loss:		2.294671
  validation loss:		2.244400
  validation accuracy:		14.57 %
Epoch 651 of 2000 took 0.100s
  training loss:		2.294803
  validation loss:		2.242514
  validation accuracy:		12.93 %
Epoch 652 of 2000 took 0.100s
  training loss:		2.294686
  validation loss:		2.244652
  validation accuracy:		13.04 %
Epoch 653 of 2000 took 0.101s
  training loss:		2.294202
  validation loss:		2.245154
  validation accuracy:		12.93 %
Epoch 654 of 2000 took 0.100s
  training loss:		2.293603
  validation loss:		2.240914
  validation accuracy:		12.93 %
Epoch 655 of 2000 took 0.101s
  training loss:		2.293906
  validation loss:		2.239978
  validation accuracy:		13.04 %
Epoch 656 of 2000 took 0.100s
  training loss:		2.294438
  validation loss:		2.244173
  validation accuracy:		12.93 %
Epoch 657 of 2000 took 0.107s
  training loss:		2.294578
  validation loss:		2.244123
  validation accuracy:		12.83 %
Epoch 658 of 2000 took 0.102s
  training loss:		2.294046
  validation loss:		2.243726
  validation accuracy:		13.04 %
Epoch 659 of 2000 took 0.100s
  training loss:		2.294578
  validation loss:		2.242381
  validation accuracy:		12.93 %
Epoch 660 of 2000 took 0.104s
  training loss:		2.295238
  validation loss:		2.242584
  validation accuracy:		12.93 %
Epoch 661 of 2000 took 0.104s
  training loss:		2.293528
  validation loss:		2.241877
  validation accuracy:		12.93 %
Epoch 662 of 2000 took 0.104s
  training loss:		2.295132
  validation loss:		2.245933
  validation accuracy:		12.93 %
Epoch 663 of 2000 took 0.104s
  training loss:		2.293056
  validation loss:		2.240934
  validation accuracy:		12.93 %
Epoch 664 of 2000 took 0.104s
  training loss:		2.294675
  validation loss:		2.241919
  validation accuracy:		12.83 %
Epoch 665 of 2000 took 0.104s
  training loss:		2.294576
  validation loss:		2.244565
  validation accuracy:		13.04 %
Epoch 666 of 2000 took 0.104s
  training loss:		2.294856
  validation loss:		2.244672
  validation accuracy:		13.04 %
Epoch 667 of 2000 took 0.104s
  training loss:		2.294590
  validation loss:		2.242813
  validation accuracy:		13.26 %
Epoch 668 of 2000 took 0.104s
  training loss:		2.294267
  validation loss:		2.241714
  validation accuracy:		12.83 %
Epoch 669 of 2000 took 0.104s
  training loss:		2.294298
  validation loss:		2.244453
  validation accuracy:		12.93 %
Epoch 670 of 2000 took 0.104s
  training loss:		2.294162
  validation loss:		2.245541
  validation accuracy:		12.93 %
Epoch 671 of 2000 took 0.104s
  training loss:		2.294100
  validation loss:		2.245074
  validation accuracy:		12.93 %
Epoch 672 of 2000 took 0.104s
  training loss:		2.294538
  validation loss:		2.242921
  validation accuracy:		12.93 %
Epoch 673 of 2000 took 0.104s
  training loss:		2.294618
  validation loss:		2.240700
  validation accuracy:		13.04 %
Epoch 674 of 2000 took 0.104s
  training loss:		2.294620
  validation loss:		2.243869
  validation accuracy:		13.04 %
Epoch 675 of 2000 took 0.104s
  training loss:		2.293301
  validation loss:		2.245742
  validation accuracy:		12.93 %
Epoch 676 of 2000 took 0.104s
  training loss:		2.293498
  validation loss:		2.242358
  validation accuracy:		13.26 %
Epoch 677 of 2000 took 0.104s
  training loss:		2.294548
  validation loss:		2.239689
  validation accuracy:		12.93 %
Epoch 678 of 2000 took 0.105s
  training loss:		2.294071
  validation loss:		2.241179
  validation accuracy:		12.83 %
Epoch 679 of 2000 took 0.104s
  training loss:		2.293484
  validation loss:		2.241947
  validation accuracy:		12.93 %
Epoch 680 of 2000 took 0.104s
  training loss:		2.294147
  validation loss:		2.246690
  validation accuracy:		14.24 %
Epoch 681 of 2000 took 0.104s
  training loss:		2.294834
  validation loss:		2.242927
  validation accuracy:		12.83 %
Epoch 682 of 2000 took 0.104s
  training loss:		2.295193
  validation loss:		2.248124
  validation accuracy:		12.83 %
Epoch 683 of 2000 took 0.104s
  training loss:		2.295021
  validation loss:		2.245557
  validation accuracy:		13.04 %
Epoch 684 of 2000 took 0.104s
  training loss:		2.294945
  validation loss:		2.242990
  validation accuracy:		12.93 %
Epoch 685 of 2000 took 0.104s
  training loss:		2.294786
  validation loss:		2.243192
  validation accuracy:		12.83 %
Epoch 686 of 2000 took 0.104s
  training loss:		2.294782
  validation loss:		2.247710
  validation accuracy:		16.74 %
Epoch 687 of 2000 took 0.104s
  training loss:		2.294001
  validation loss:		2.240841
  validation accuracy:		12.93 %
Epoch 688 of 2000 took 0.104s
  training loss:		2.294940
  validation loss:		2.243029
  validation accuracy:		12.83 %
Epoch 689 of 2000 took 0.104s
  training loss:		2.295011
  validation loss:		2.244964
  validation accuracy:		13.04 %
Epoch 690 of 2000 took 0.104s
  training loss:		2.294897
  validation loss:		2.243988
  validation accuracy:		12.93 %
Epoch 691 of 2000 took 0.104s
  training loss:		2.295094
  validation loss:		2.246625
  validation accuracy:		12.93 %
Epoch 692 of 2000 took 0.104s
  training loss:		2.295024
  validation loss:		2.244838
  validation accuracy:		12.93 %
Epoch 693 of 2000 took 0.104s
  training loss:		2.294412
  validation loss:		2.243140
  validation accuracy:		13.48 %
Epoch 694 of 2000 took 0.104s
  training loss:		2.294054
  validation loss:		2.243853
  validation accuracy:		13.04 %
Epoch 695 of 2000 took 0.109s
  training loss:		2.294019
  validation loss:		2.241015
  validation accuracy:		12.83 %
Epoch 696 of 2000 took 0.104s
  training loss:		2.295139
  validation loss:		2.244753
  validation accuracy:		12.83 %
Epoch 697 of 2000 took 0.104s
  training loss:		2.295330
  validation loss:		2.247106
  validation accuracy:		12.93 %
Epoch 698 of 2000 took 0.104s
  training loss:		2.293944
  validation loss:		2.240981
  validation accuracy:		12.83 %
Epoch 699 of 2000 took 0.104s
  training loss:		2.294625
  validation loss:		2.241102
  validation accuracy:		12.83 %
Epoch 700 of 2000 took 0.104s
  training loss:		2.294673
  validation loss:		2.242648
  validation accuracy:		13.04 %
Epoch 701 of 2000 took 0.104s
  training loss:		2.293740
  validation loss:		2.248462
  validation accuracy:		12.83 %
Epoch 702 of 2000 took 0.104s
  training loss:		2.294548
  validation loss:		2.244063
  validation accuracy:		13.04 %
Epoch 703 of 2000 took 0.104s
  training loss:		2.294734
  validation loss:		2.244151
  validation accuracy:		13.04 %
Epoch 704 of 2000 took 0.104s
  training loss:		2.293810
  validation loss:		2.244156
  validation accuracy:		12.93 %
Epoch 705 of 2000 took 0.104s
  training loss:		2.294753
  validation loss:		2.242468
  validation accuracy:		14.02 %
Epoch 706 of 2000 took 0.104s
  training loss:		2.294980
  validation loss:		2.246501
  validation accuracy:		12.83 %
Epoch 707 of 2000 took 0.105s
  training loss:		2.294680
  validation loss:		2.244873
  validation accuracy:		13.04 %
Epoch 708 of 2000 took 0.104s
  training loss:		2.295375
  validation loss:		2.244219
  validation accuracy:		13.48 %
Epoch 709 of 2000 took 0.104s
  training loss:		2.295231
  validation loss:		2.249562
  validation accuracy:		12.93 %
Epoch 710 of 2000 took 0.104s
  training loss:		2.294069
  validation loss:		2.243945
  validation accuracy:		12.83 %
Epoch 711 of 2000 took 0.104s
  training loss:		2.294878
  validation loss:		2.245017
  validation accuracy:		12.93 %
Epoch 712 of 2000 took 0.104s
  training loss:		2.293644
  validation loss:		2.241903
  validation accuracy:		12.93 %
Epoch 713 of 2000 took 0.104s
  training loss:		2.294069
  validation loss:		2.240738
  validation accuracy:		12.83 %
Epoch 714 of 2000 took 0.104s
  training loss:		2.294931
  validation loss:		2.244090
  validation accuracy:		12.83 %
Epoch 715 of 2000 took 0.104s
  training loss:		2.294463
  validation loss:		2.243451
  validation accuracy:		13.04 %
Epoch 716 of 2000 took 0.104s
  training loss:		2.294793
  validation loss:		2.243316
  validation accuracy:		12.83 %
Epoch 717 of 2000 took 0.104s
  training loss:		2.294996
  validation loss:		2.247022
  validation accuracy:		12.93 %
Epoch 718 of 2000 took 0.104s
  training loss:		2.295277
  validation loss:		2.249018
  validation accuracy:		12.93 %
Epoch 719 of 2000 took 0.104s
  training loss:		2.294229
  validation loss:		2.240682
  validation accuracy:		12.83 %
Epoch 720 of 2000 took 0.104s
  training loss:		2.295064
  validation loss:		2.241633
  validation accuracy:		13.04 %
Epoch 721 of 2000 took 0.104s
  training loss:		2.294816
  validation loss:		2.245079
  validation accuracy:		12.83 %
Epoch 722 of 2000 took 0.104s
  training loss:		2.295161
  validation loss:		2.245690
  validation accuracy:		12.83 %
Epoch 723 of 2000 took 0.104s
  training loss:		2.294630
  validation loss:		2.247331
  validation accuracy:		13.48 %
Epoch 724 of 2000 took 0.104s
  training loss:		2.294668
  validation loss:		2.246927
  validation accuracy:		12.83 %
Epoch 725 of 2000 took 0.104s
  training loss:		2.294602
  validation loss:		2.242037
  validation accuracy:		12.83 %
Epoch 726 of 2000 took 0.104s
  training loss:		2.294550
  validation loss:		2.243545
  validation accuracy:		12.83 %
Epoch 727 of 2000 took 0.104s
  training loss:		2.293953
  validation loss:		2.246101
  validation accuracy:		13.04 %
Epoch 728 of 2000 took 0.104s
  training loss:		2.294019
  validation loss:		2.242623
  validation accuracy:		13.04 %
Epoch 729 of 2000 took 0.104s
  training loss:		2.294551
  validation loss:		2.242219
  validation accuracy:		16.09 %
Epoch 730 of 2000 took 0.104s
  training loss:		2.294511
  validation loss:		2.244440
  validation accuracy:		13.04 %
Epoch 731 of 2000 took 0.104s
  training loss:		2.294848
  validation loss:		2.242777
  validation accuracy:		13.80 %
Epoch 732 of 2000 took 0.104s
  training loss:		2.294713
  validation loss:		2.244370
  validation accuracy:		13.04 %
Epoch 733 of 2000 took 0.104s
  training loss:		2.294633
  validation loss:		2.246995
  validation accuracy:		12.93 %
Epoch 734 of 2000 took 0.104s
  training loss:		2.294352
  validation loss:		2.245582
  validation accuracy:		12.93 %
Epoch 735 of 2000 took 0.104s
  training loss:		2.293936
  validation loss:		2.240319
  validation accuracy:		13.04 %
Epoch 736 of 2000 took 0.105s
  training loss:		2.294980
  validation loss:		2.245818
  validation accuracy:		13.04 %
Epoch 737 of 2000 took 0.104s
  training loss:		2.294525
  validation loss:		2.242655
  validation accuracy:		12.83 %
Epoch 738 of 2000 took 0.104s
  training loss:		2.294331
  validation loss:		2.245727
  validation accuracy:		16.85 %
Epoch 739 of 2000 took 0.104s
  training loss:		2.293627
  validation loss:		2.242148
  validation accuracy:		13.04 %
Epoch 740 of 2000 took 0.104s
  training loss:		2.294446
  validation loss:		2.242202
  validation accuracy:		12.93 %
Epoch 741 of 2000 took 0.104s
  training loss:		2.294832
  validation loss:		2.246455
  validation accuracy:		12.83 %
Epoch 742 of 2000 took 0.104s
  training loss:		2.294611
  validation loss:		2.240709
  validation accuracy:		17.39 %
Epoch 743 of 2000 took 0.104s
  training loss:		2.295060
  validation loss:		2.244668
  validation accuracy:		13.04 %
Epoch 744 of 2000 took 0.104s
  training loss:		2.294678
  validation loss:		2.245174
  validation accuracy:		13.04 %
Epoch 745 of 2000 took 0.104s
  training loss:		2.294611
  validation loss:		2.245729
  validation accuracy:		12.83 %
Epoch 746 of 2000 took 0.104s
  training loss:		2.295160
  validation loss:		2.243932
  validation accuracy:		12.93 %
Epoch 747 of 2000 took 0.104s
  training loss:		2.294837
  validation loss:		2.247167
  validation accuracy:		13.04 %
Epoch 748 of 2000 took 0.104s
  training loss:		2.295009
  validation loss:		2.246018
  validation accuracy:		16.74 %
Epoch 749 of 2000 took 0.104s
  training loss:		2.294170
  validation loss:		2.240824
  validation accuracy:		12.83 %
Epoch 750 of 2000 took 0.104s
  training loss:		2.294342
  validation loss:		2.241950
  validation accuracy:		12.83 %
Epoch 751 of 2000 took 0.104s
  training loss:		2.294339
  validation loss:		2.243840
  validation accuracy:		13.04 %
Epoch 752 of 2000 took 0.104s
  training loss:		2.294943
  validation loss:		2.244033
  validation accuracy:		13.04 %
Epoch 753 of 2000 took 0.104s
  training loss:		2.295199
  validation loss:		2.245255
  validation accuracy:		12.83 %
Epoch 754 of 2000 took 0.104s
  training loss:		2.294319
  validation loss:		2.243000
  validation accuracy:		12.93 %
Epoch 755 of 2000 took 0.104s
  training loss:		2.294189
  validation loss:		2.243493
  validation accuracy:		14.89 %
Epoch 756 of 2000 took 0.104s
  training loss:		2.293742
  validation loss:		2.240935
  validation accuracy:		12.93 %
Epoch 757 of 2000 took 0.104s
  training loss:		2.294491
  validation loss:		2.240169
  validation accuracy:		12.93 %
Epoch 758 of 2000 took 0.104s
  training loss:		2.294401
  validation loss:		2.245960
  validation accuracy:		13.04 %
Epoch 759 of 2000 took 0.104s
  training loss:		2.293742
  validation loss:		2.247752
  validation accuracy:		12.93 %
Epoch 760 of 2000 took 0.104s
  training loss:		2.294894
  validation loss:		2.241656
  validation accuracy:		12.83 %
Epoch 761 of 2000 took 0.109s
  training loss:		2.294480
  validation loss:		2.243020
  validation accuracy:		12.83 %
Epoch 762 of 2000 took 0.104s
  training loss:		2.294720
  validation loss:		2.244950
  validation accuracy:		12.93 %
Epoch 763 of 2000 took 0.104s
  training loss:		2.293984
  validation loss:		2.244413
  validation accuracy:		12.93 %
Epoch 764 of 2000 took 0.104s
  training loss:		2.294755
  validation loss:		2.245351
  validation accuracy:		12.83 %
Epoch 765 of 2000 took 0.105s
  training loss:		2.295597
  validation loss:		2.243720
  validation accuracy:		12.83 %
Epoch 766 of 2000 took 0.104s
  training loss:		2.293529
  validation loss:		2.243707
  validation accuracy:		12.93 %
Epoch 767 of 2000 took 0.104s
  training loss:		2.293742
  validation loss:		2.239119
  validation accuracy:		12.93 %
Epoch 768 of 2000 took 0.104s
  training loss:		2.293876
  validation loss:		2.241613
  validation accuracy:		12.93 %
Epoch 769 of 2000 took 0.104s
  training loss:		2.293879
  validation loss:		2.243625
  validation accuracy:		13.04 %
Epoch 770 of 2000 took 0.104s
  training loss:		2.294156
  validation loss:		2.241151
  validation accuracy:		13.04 %
Epoch 771 of 2000 took 0.104s
  training loss:		2.294424
  validation loss:		2.241486
  validation accuracy:		16.52 %
Epoch 772 of 2000 took 0.104s
  training loss:		2.294863
  validation loss:		2.242399
  validation accuracy:		12.93 %
Epoch 773 of 2000 took 0.104s
  training loss:		2.293966
  validation loss:		2.246501
  validation accuracy:		12.83 %
Epoch 774 of 2000 took 0.104s
  training loss:		2.294869
  validation loss:		2.245633
  validation accuracy:		13.04 %
Epoch 775 of 2000 took 0.104s
  training loss:		2.294123
  validation loss:		2.240363
  validation accuracy:		13.15 %
Epoch 776 of 2000 took 0.104s
  training loss:		2.294573
  validation loss:		2.241538
  validation accuracy:		12.93 %
Epoch 777 of 2000 took 0.104s
  training loss:		2.294986
  validation loss:		2.245970
  validation accuracy:		13.26 %
Epoch 778 of 2000 took 0.104s
  training loss:		2.294031
  validation loss:		2.242171
  validation accuracy:		14.57 %
Epoch 779 of 2000 took 0.104s
  training loss:		2.294693
  validation loss:		2.242707
  validation accuracy:		12.93 %
Epoch 780 of 2000 took 0.104s
  training loss:		2.294631
  validation loss:		2.245249
  validation accuracy:		13.48 %
Epoch 781 of 2000 took 0.104s
  training loss:		2.295243
  validation loss:		2.244485
  validation accuracy:		12.93 %
Epoch 782 of 2000 took 0.104s
  training loss:		2.294326
  validation loss:		2.243320
  validation accuracy:		12.83 %
Epoch 783 of 2000 took 0.104s
  training loss:		2.294362
  validation loss:		2.241283
  validation accuracy:		12.93 %
Epoch 784 of 2000 took 0.104s
  training loss:		2.294649
  validation loss:		2.242758
  validation accuracy:		15.11 %
Epoch 785 of 2000 took 0.104s
  training loss:		2.293932
  validation loss:		2.243121
  validation accuracy:		12.93 %
Epoch 786 of 2000 took 0.104s
  training loss:		2.293805
  validation loss:		2.244650
  validation accuracy:		12.93 %
Epoch 787 of 2000 took 0.104s
  training loss:		2.295054
  validation loss:		2.242672
  validation accuracy:		14.02 %
Epoch 788 of 2000 took 0.104s
  training loss:		2.294516
  validation loss:		2.244572
  validation accuracy:		12.93 %
Epoch 789 of 2000 took 0.104s
  training loss:		2.293229
  validation loss:		2.239406
  validation accuracy:		13.04 %
Epoch 790 of 2000 took 0.104s
  training loss:		2.294403
  validation loss:		2.241543
  validation accuracy:		12.93 %
Epoch 791 of 2000 took 0.104s
  training loss:		2.294517
  validation loss:		2.245339
  validation accuracy:		13.04 %
Epoch 792 of 2000 took 0.104s
  training loss:		2.294328
  validation loss:		2.242344
  validation accuracy:		17.50 %
Epoch 793 of 2000 took 0.104s
  training loss:		2.295257
  validation loss:		2.246045
  validation accuracy:		12.93 %
Epoch 794 of 2000 took 0.105s
  training loss:		2.295087
  validation loss:		2.246349
  validation accuracy:		13.26 %
Epoch 795 of 2000 took 0.104s
  training loss:		2.294404
  validation loss:		2.244032
  validation accuracy:		16.41 %
Epoch 796 of 2000 took 0.104s
  training loss:		2.294866
  validation loss:		2.243957
  validation accuracy:		12.93 %
Epoch 797 of 2000 took 0.104s
  training loss:		2.294156
  validation loss:		2.245576
  validation accuracy:		13.04 %
Epoch 798 of 2000 took 0.104s
  training loss:		2.294430
  validation loss:		2.244261
  validation accuracy:		14.13 %
Epoch 799 of 2000 took 0.104s
  training loss:		2.294108
  validation loss:		2.242244
  validation accuracy:		11.85 %
Epoch 800 of 2000 took 0.104s
  training loss:		2.293504
  validation loss:		2.241154
  validation accuracy:		12.83 %
Epoch 801 of 2000 took 0.104s
  training loss:		2.294736
  validation loss:		2.244810
  validation accuracy:		12.93 %
Epoch 802 of 2000 took 0.104s
  training loss:		2.294802
  validation loss:		2.245696
  validation accuracy:		12.93 %
Epoch 803 of 2000 took 0.104s
  training loss:		2.294754
  validation loss:		2.243795
  validation accuracy:		16.41 %
Epoch 804 of 2000 took 0.104s
  training loss:		2.294672
  validation loss:		2.246928
  validation accuracy:		12.83 %
Epoch 805 of 2000 took 0.104s
  training loss:		2.294494
  validation loss:		2.242466
  validation accuracy:		12.83 %
Epoch 806 of 2000 took 0.104s
  training loss:		2.293498
  validation loss:		2.242941
  validation accuracy:		13.04 %
Epoch 807 of 2000 took 0.104s
  training loss:		2.293630
  validation loss:		2.242325
  validation accuracy:		13.04 %
Epoch 808 of 2000 took 0.104s
  training loss:		2.293990
  validation loss:		2.241874
  validation accuracy:		12.93 %
Epoch 809 of 2000 took 0.104s
  training loss:		2.294357
  validation loss:		2.242545
  validation accuracy:		12.93 %
Epoch 810 of 2000 took 0.104s
  training loss:		2.294227
  validation loss:		2.244923
  validation accuracy:		12.83 %
Epoch 811 of 2000 took 0.104s
  training loss:		2.294172
  validation loss:		2.242969
  validation accuracy:		13.04 %
Epoch 812 of 2000 took 0.104s
  training loss:		2.294790
  validation loss:		2.240993
  validation accuracy:		16.85 %
Epoch 813 of 2000 took 0.104s
  training loss:		2.295056
  validation loss:		2.246948
  validation accuracy:		13.04 %
Epoch 814 of 2000 took 0.104s
  training loss:		2.294873
  validation loss:		2.247573
  validation accuracy:		12.93 %
Epoch 815 of 2000 took 0.104s
  training loss:		2.294400
  validation loss:		2.244057
  validation accuracy:		12.93 %
Epoch 816 of 2000 took 0.104s
  training loss:		2.294482
  validation loss:		2.245241
  validation accuracy:		13.48 %
Epoch 817 of 2000 took 0.104s
  training loss:		2.294790
  validation loss:		2.246967
  validation accuracy:		13.04 %
Epoch 818 of 2000 took 0.104s
  training loss:		2.294404
  validation loss:		2.244246
  validation accuracy:		13.04 %
Epoch 819 of 2000 took 0.104s
  training loss:		2.294421
  validation loss:		2.242372
  validation accuracy:		12.83 %
Epoch 820 of 2000 took 0.104s
  training loss:		2.293970
  validation loss:		2.238743
  validation accuracy:		15.33 %
Epoch 821 of 2000 took 0.104s
  training loss:		2.294311
  validation loss:		2.245974
  validation accuracy:		12.50 %
Epoch 822 of 2000 took 0.104s
  training loss:		2.294505
  validation loss:		2.244766
  validation accuracy:		15.87 %
Epoch 823 of 2000 took 0.105s
  training loss:		2.294314
  validation loss:		2.245062
  validation accuracy:		13.04 %
Epoch 824 of 2000 took 0.104s
  training loss:		2.293546
  validation loss:		2.240940
  validation accuracy:		13.04 %
Epoch 825 of 2000 took 0.104s
  training loss:		2.294407
  validation loss:		2.244142
  validation accuracy:		12.93 %
Epoch 826 of 2000 took 0.104s
  training loss:		2.294097
  validation loss:		2.243342
  validation accuracy:		13.04 %
Epoch 827 of 2000 took 0.104s
  training loss:		2.295233
  validation loss:		2.246609
  validation accuracy:		12.93 %
Epoch 828 of 2000 took 0.104s
  training loss:		2.294281
  validation loss:		2.244972
  validation accuracy:		12.83 %
Epoch 829 of 2000 took 0.104s
  training loss:		2.294789
  validation loss:		2.244333
  validation accuracy:		12.83 %
Epoch 830 of 2000 took 0.104s
  training loss:		2.294436
  validation loss:		2.245317
  validation accuracy:		12.93 %
Epoch 831 of 2000 took 0.104s
  training loss:		2.294394
  validation loss:		2.242449
  validation accuracy:		13.04 %
Epoch 832 of 2000 took 0.104s
  training loss:		2.294423
  validation loss:		2.243485
  validation accuracy:		13.70 %
Epoch 833 of 2000 took 0.108s
  training loss:		2.294408
  validation loss:		2.247485
  validation accuracy:		12.83 %
Epoch 834 of 2000 took 0.105s
  training loss:		2.293978
  validation loss:		2.245182
  validation accuracy:		11.96 %
Epoch 835 of 2000 took 0.104s
  training loss:		2.293279
  validation loss:		2.238843
  validation accuracy:		13.70 %
Epoch 836 of 2000 took 0.104s
  training loss:		2.294047
  validation loss:		2.239550
  validation accuracy:		13.04 %
Epoch 837 of 2000 took 0.104s
  training loss:		2.294906
  validation loss:		2.242915
  validation accuracy:		12.93 %
Epoch 838 of 2000 took 0.107s
  training loss:		2.293960
  validation loss:		2.245942
  validation accuracy:		12.93 %
Epoch 839 of 2000 took 0.107s
  training loss:		2.294021
  validation loss:		2.240727
  validation accuracy:		13.04 %
Epoch 840 of 2000 took 0.107s
  training loss:		2.294006
  validation loss:		2.243214
  validation accuracy:		13.04 %
Epoch 841 of 2000 took 0.107s
  training loss:		2.294424
  validation loss:		2.242021
  validation accuracy:		13.48 %
Epoch 842 of 2000 took 0.107s
  training loss:		2.294711
  validation loss:		2.244016
  validation accuracy:		13.59 %
Epoch 843 of 2000 took 0.107s
  training loss:		2.295065
  validation loss:		2.244314
  validation accuracy:		13.04 %
Epoch 844 of 2000 took 0.107s
  training loss:		2.294074
  validation loss:		2.244382
  validation accuracy:		13.04 %
Epoch 845 of 2000 took 0.107s
  training loss:		2.294111
  validation loss:		2.243344
  validation accuracy:		14.46 %
Epoch 846 of 2000 took 0.107s
  training loss:		2.294321
  validation loss:		2.241944
  validation accuracy:		12.93 %
Epoch 847 of 2000 took 0.107s
  training loss:		2.294058
  validation loss:		2.240981
  validation accuracy:		12.83 %
Epoch 848 of 2000 took 0.107s
  training loss:		2.294802
  validation loss:		2.246459
  validation accuracy:		13.04 %
Epoch 849 of 2000 took 0.107s
  training loss:		2.294598
  validation loss:		2.247748
  validation accuracy:		12.93 %
Epoch 850 of 2000 took 0.107s
  training loss:		2.294803
  validation loss:		2.241796
  validation accuracy:		12.93 %
Epoch 851 of 2000 took 0.108s
  training loss:		2.293847
  validation loss:		2.240293
  validation accuracy:		13.04 %
Epoch 852 of 2000 took 0.107s
  training loss:		2.293677
  validation loss:		2.241457
  validation accuracy:		13.04 %
Epoch 853 of 2000 took 0.107s
  training loss:		2.293599
  validation loss:		2.242019
  validation accuracy:		12.93 %
Epoch 854 of 2000 took 0.107s
  training loss:		2.294720
  validation loss:		2.243295
  validation accuracy:		12.83 %
Epoch 855 of 2000 took 0.107s
  training loss:		2.294580
  validation loss:		2.245062
  validation accuracy:		12.93 %
Epoch 856 of 2000 took 0.107s
  training loss:		2.294106
  validation loss:		2.241970
  validation accuracy:		13.04 %
Epoch 857 of 2000 took 0.107s
  training loss:		2.294295
  validation loss:		2.240129
  validation accuracy:		12.93 %
Epoch 858 of 2000 took 0.107s
  training loss:		2.294352
  validation loss:		2.242989
  validation accuracy:		13.15 %
Epoch 859 of 2000 took 0.107s
  training loss:		2.294868
  validation loss:		2.244734
  validation accuracy:		12.93 %
Epoch 860 of 2000 took 0.107s
  training loss:		2.293767
  validation loss:		2.247827
  validation accuracy:		14.46 %
Epoch 861 of 2000 took 0.107s
  training loss:		2.294370
  validation loss:		2.243847
  validation accuracy:		12.93 %
Epoch 862 of 2000 took 0.107s
  training loss:		2.294579
  validation loss:		2.241335
  validation accuracy:		17.93 %
Epoch 863 of 2000 took 0.107s
  training loss:		2.294047
  validation loss:		2.244250
  validation accuracy:		12.93 %
Epoch 864 of 2000 took 0.107s
  training loss:		2.294822
  validation loss:		2.242932
  validation accuracy:		12.93 %
Epoch 865 of 2000 took 0.107s
  training loss:		2.293953
  validation loss:		2.244317
  validation accuracy:		13.48 %
Epoch 866 of 2000 took 0.107s
  training loss:		2.294497
  validation loss:		2.243655
  validation accuracy:		12.93 %
Epoch 867 of 2000 took 0.107s
  training loss:		2.294420
  validation loss:		2.245052
  validation accuracy:		12.83 %
Epoch 868 of 2000 took 0.107s
  training loss:		2.294035
  validation loss:		2.242528
  validation accuracy:		13.26 %
Epoch 869 of 2000 took 0.107s
  training loss:		2.294919
  validation loss:		2.242981
  validation accuracy:		13.04 %
Epoch 870 of 2000 took 0.107s
  training loss:		2.294943
  validation loss:		2.246424
  validation accuracy:		13.15 %
Epoch 871 of 2000 took 0.107s
  training loss:		2.294222
  validation loss:		2.247475
  validation accuracy:		12.83 %
Epoch 872 of 2000 took 0.107s
  training loss:		2.295195
  validation loss:		2.243211
  validation accuracy:		12.93 %
Epoch 873 of 2000 took 0.107s
  training loss:		2.294738
  validation loss:		2.243974
  validation accuracy:		11.85 %
Epoch 874 of 2000 took 0.107s
  training loss:		2.294809
  validation loss:		2.245366
  validation accuracy:		13.15 %
Epoch 875 of 2000 took 0.109s
  training loss:		2.294397
  validation loss:		2.242921
  validation accuracy:		12.93 %
Epoch 876 of 2000 took 0.111s
  training loss:		2.294636
  validation loss:		2.243752
  validation accuracy:		12.83 %
Epoch 877 of 2000 took 0.111s
  training loss:		2.294684
  validation loss:		2.243218
  validation accuracy:		12.83 %
Epoch 878 of 2000 took 0.111s
  training loss:		2.293659
  validation loss:		2.245036
  validation accuracy:		13.04 %
Epoch 879 of 2000 took 0.111s
  training loss:		2.294423
  validation loss:		2.242459
  validation accuracy:		13.26 %
Epoch 880 of 2000 took 0.110s
  training loss:		2.294426
  validation loss:		2.243864
  validation accuracy:		13.15 %
Epoch 881 of 2000 took 0.110s
  training loss:		2.294009
  validation loss:		2.241718
  validation accuracy:		13.04 %
Epoch 882 of 2000 took 0.111s
  training loss:		2.294003
  validation loss:		2.240009
  validation accuracy:		13.80 %
Epoch 883 of 2000 took 0.110s
  training loss:		2.294189
  validation loss:		2.244002
  validation accuracy:		12.83 %
Epoch 884 of 2000 took 0.110s
  training loss:		2.294101
  validation loss:		2.245238
  validation accuracy:		13.04 %
Epoch 885 of 2000 took 0.111s
  training loss:		2.293986
  validation loss:		2.242682
  validation accuracy:		13.15 %
Epoch 886 of 2000 took 0.110s
  training loss:		2.294193
  validation loss:		2.243671
  validation accuracy:		12.83 %
Epoch 887 of 2000 took 0.110s
  training loss:		2.294479
  validation loss:		2.246328
  validation accuracy:		13.04 %
Epoch 888 of 2000 took 0.110s
  training loss:		2.294114
  validation loss:		2.243295
  validation accuracy:		13.04 %
Epoch 889 of 2000 took 0.111s
  training loss:		2.294745
  validation loss:		2.246133
  validation accuracy:		13.04 %
Epoch 890 of 2000 took 0.110s
  training loss:		2.294548
  validation loss:		2.246415
  validation accuracy:		13.15 %
Epoch 891 of 2000 took 0.111s
  training loss:		2.294128
  validation loss:		2.247075
  validation accuracy:		13.04 %
Epoch 892 of 2000 took 0.111s
  training loss:		2.293443
  validation loss:		2.239796
  validation accuracy:		12.83 %
Epoch 893 of 2000 took 0.110s
  training loss:		2.294314
  validation loss:		2.241205
  validation accuracy:		13.15 %
Epoch 894 of 2000 took 0.110s
  training loss:		2.294001
  validation loss:		2.243668
  validation accuracy:		13.04 %
Epoch 895 of 2000 took 0.111s
  training loss:		2.293014
  validation loss:		2.242088
  validation accuracy:		13.70 %
Epoch 896 of 2000 took 0.111s
  training loss:		2.293911
  validation loss:		2.241428
  validation accuracy:		12.93 %
Epoch 897 of 2000 took 0.110s
  training loss:		2.293597
  validation loss:		2.244688
  validation accuracy:		12.93 %
Epoch 898 of 2000 took 0.110s
  training loss:		2.294703
  validation loss:		2.244732
  validation accuracy:		13.04 %
Epoch 899 of 2000 took 0.110s
  training loss:		2.294523
  validation loss:		2.241269
  validation accuracy:		12.83 %
Epoch 900 of 2000 took 0.110s
  training loss:		2.294023
  validation loss:		2.239585
  validation accuracy:		16.85 %
Epoch 901 of 2000 took 0.110s
  training loss:		2.294953
  validation loss:		2.246463
  validation accuracy:		12.61 %
Epoch 902 of 2000 took 0.111s
  training loss:		2.294809
  validation loss:		2.248971
  validation accuracy:		12.83 %
Epoch 903 of 2000 took 0.111s
  training loss:		2.294458
  validation loss:		2.247512
  validation accuracy:		12.93 %
Epoch 904 of 2000 took 0.111s
  training loss:		2.294486
  validation loss:		2.243773
  validation accuracy:		17.83 %
Epoch 905 of 2000 took 0.111s
  training loss:		2.293424
  validation loss:		2.242011
  validation accuracy:		12.93 %
Epoch 906 of 2000 took 0.110s
  training loss:		2.294522
  validation loss:		2.242753
  validation accuracy:		16.63 %
Epoch 907 of 2000 took 0.111s
  training loss:		2.293862
  validation loss:		2.243511
  validation accuracy:		12.93 %
Epoch 908 of 2000 took 0.111s
  training loss:		2.293977
  validation loss:		2.244505
  validation accuracy:		14.67 %
Epoch 909 of 2000 took 0.112s
  training loss:		2.293926
  validation loss:		2.245206
  validation accuracy:		13.04 %
Epoch 910 of 2000 took 0.114s
  training loss:		2.294398
  validation loss:		2.241069
  validation accuracy:		12.93 %
Epoch 911 of 2000 took 0.109s
  training loss:		2.294839
  validation loss:		2.244216
  validation accuracy:		12.83 %
Epoch 912 of 2000 took 0.107s
  training loss:		2.293559
  validation loss:		2.247601
  validation accuracy:		12.93 %
Epoch 913 of 2000 took 0.107s
  training loss:		2.295079
  validation loss:		2.246105
  validation accuracy:		15.22 %
Epoch 914 of 2000 took 0.107s
  training loss:		2.294181
  validation loss:		2.242838
  validation accuracy:		12.93 %
Epoch 915 of 2000 took 0.107s
  training loss:		2.294222
  validation loss:		2.240866
  validation accuracy:		12.83 %
Epoch 916 of 2000 took 0.107s
  training loss:		2.293902
  validation loss:		2.244506
  validation accuracy:		12.83 %
Epoch 917 of 2000 took 0.107s
  training loss:		2.294153
  validation loss:		2.246273
  validation accuracy:		12.93 %
Epoch 918 of 2000 took 0.107s
  training loss:		2.293453
  validation loss:		2.242024
  validation accuracy:		12.93 %
Epoch 919 of 2000 took 0.107s
  training loss:		2.294223
  validation loss:		2.241194
  validation accuracy:		13.04 %
Epoch 920 of 2000 took 0.107s
  training loss:		2.293911
  validation loss:		2.244655
  validation accuracy:		17.93 %
Epoch 921 of 2000 took 0.107s
  training loss:		2.293891
  validation loss:		2.242547
  validation accuracy:		16.74 %
Epoch 922 of 2000 took 0.108s
  training loss:		2.294356
  validation loss:		2.242574
  validation accuracy:		17.61 %
Epoch 923 of 2000 took 0.107s
  training loss:		2.293223
  validation loss:		2.243951
  validation accuracy:		13.48 %
Epoch 924 of 2000 took 0.107s
  training loss:		2.293705
  validation loss:		2.243645
  validation accuracy:		13.04 %
Epoch 925 of 2000 took 0.107s
  training loss:		2.294518
  validation loss:		2.244370
  validation accuracy:		12.93 %
Epoch 926 of 2000 took 0.107s
  training loss:		2.293594
  validation loss:		2.243772
  validation accuracy:		12.93 %
Epoch 927 of 2000 took 0.107s
  training loss:		2.293837
  validation loss:		2.241206
  validation accuracy:		13.37 %
Epoch 928 of 2000 took 0.107s
  training loss:		2.294697
  validation loss:		2.243400
  validation accuracy:		15.98 %
Epoch 929 of 2000 took 0.107s
  training loss:		2.294542
  validation loss:		2.244370
  validation accuracy:		14.35 %
Epoch 930 of 2000 took 0.107s
  training loss:		2.293666
  validation loss:		2.242565
  validation accuracy:		13.04 %
Epoch 931 of 2000 took 0.107s
  training loss:		2.294817
  validation loss:		2.245126
  validation accuracy:		13.80 %
Epoch 932 of 2000 took 0.107s
  training loss:		2.293938
  validation loss:		2.245410
  validation accuracy:		12.83 %
Epoch 933 of 2000 took 0.107s
  training loss:		2.294869
  validation loss:		2.241903
  validation accuracy:		14.89 %
Epoch 934 of 2000 took 0.107s
  training loss:		2.293672
  validation loss:		2.245064
  validation accuracy:		13.04 %
Epoch 935 of 2000 took 0.108s
  training loss:		2.295109
  validation loss:		2.248268
  validation accuracy:		13.04 %
Epoch 936 of 2000 took 0.107s
  training loss:		2.293397
  validation loss:		2.241167
  validation accuracy:		12.83 %
Epoch 937 of 2000 took 0.107s
  training loss:		2.293865
  validation loss:		2.240400
  validation accuracy:		15.33 %
Epoch 938 of 2000 took 0.107s
  training loss:		2.293953
  validation loss:		2.241904
  validation accuracy:		18.59 %
Epoch 939 of 2000 took 0.107s
  training loss:		2.294624
  validation loss:		2.244935
  validation accuracy:		19.13 %
Epoch 940 of 2000 took 0.107s
  training loss:		2.292643
  validation loss:		2.242911
  validation accuracy:		12.83 %
Epoch 941 of 2000 took 0.107s
  training loss:		2.294066
  validation loss:		2.242427
  validation accuracy:		12.93 %
Epoch 942 of 2000 took 0.107s
  training loss:		2.294154
  validation loss:		2.241943
  validation accuracy:		12.93 %
Epoch 943 of 2000 took 0.107s
  training loss:		2.293558
  validation loss:		2.242759
  validation accuracy:		13.04 %
Epoch 944 of 2000 took 0.107s
  training loss:		2.294842
  validation loss:		2.243216
  validation accuracy:		12.93 %
Epoch 945 of 2000 took 0.107s
  training loss:		2.294104
  validation loss:		2.245523
  validation accuracy:		18.26 %
Epoch 946 of 2000 took 0.107s
  training loss:		2.294702
  validation loss:		2.247898
  validation accuracy:		12.93 %
Epoch 947 of 2000 took 0.106s
  training loss:		2.294864
  validation loss:		2.248582
  validation accuracy:		12.93 %
Epoch 948 of 2000 took 0.104s
  training loss:		2.293487
  validation loss:		2.242452
  validation accuracy:		13.15 %
Epoch 949 of 2000 took 0.104s
  training loss:		2.294280
  validation loss:		2.239955
  validation accuracy:		13.70 %
Epoch 950 of 2000 took 0.104s
  training loss:		2.292883
  validation loss:		2.241169
  validation accuracy:		12.93 %
Epoch 951 of 2000 took 0.104s
  training loss:		2.293577
  validation loss:		2.241478
  validation accuracy:		12.93 %
Epoch 952 of 2000 took 0.104s
  training loss:		2.293506
  validation loss:		2.241374
  validation accuracy:		12.83 %
Epoch 953 of 2000 took 0.104s
  training loss:		2.294502
  validation loss:		2.242609
  validation accuracy:		14.78 %
Epoch 954 of 2000 took 0.104s
  training loss:		2.294337
  validation loss:		2.244460
  validation accuracy:		13.04 %
Epoch 955 of 2000 took 0.104s
  training loss:		2.292293
  validation loss:		2.241811
  validation accuracy:		12.93 %
Epoch 956 of 2000 took 0.104s
  training loss:		2.293983
  validation loss:		2.243282
  validation accuracy:		12.83 %
Epoch 957 of 2000 took 0.104s
  training loss:		2.293874
  validation loss:		2.241234
  validation accuracy:		13.48 %
Epoch 958 of 2000 took 0.104s
  training loss:		2.293904
  validation loss:		2.240387
  validation accuracy:		19.57 %
Epoch 959 of 2000 took 0.103s
  training loss:		2.293303
  validation loss:		2.242427
  validation accuracy:		13.04 %
Epoch 960 of 2000 took 0.104s
  training loss:		2.294750
  validation loss:		2.245664
  validation accuracy:		13.04 %
Epoch 961 of 2000 took 0.104s
  training loss:		2.293912
  validation loss:		2.244609
  validation accuracy:		17.28 %
Epoch 962 of 2000 took 0.104s
  training loss:		2.294180
  validation loss:		2.240637
  validation accuracy:		12.83 %
Epoch 963 of 2000 took 0.105s
  training loss:		2.293422
  validation loss:		2.241934
  validation accuracy:		12.93 %
Epoch 964 of 2000 took 0.104s
  training loss:		2.294448
  validation loss:		2.241230
  validation accuracy:		12.93 %
Epoch 965 of 2000 took 0.104s
  training loss:		2.294070
  validation loss:		2.244410
  validation accuracy:		14.24 %
Epoch 966 of 2000 took 0.104s
  training loss:		2.293732
  validation loss:		2.244940
  validation accuracy:		14.24 %
Epoch 967 of 2000 took 0.104s
  training loss:		2.294737
  validation loss:		2.247248
  validation accuracy:		13.04 %
Epoch 968 of 2000 took 0.104s
  training loss:		2.293953
  validation loss:		2.242540
  validation accuracy:		13.15 %
Epoch 969 of 2000 took 0.104s
  training loss:		2.293821
  validation loss:		2.239076
  validation accuracy:		12.93 %
Epoch 970 of 2000 took 0.104s
  training loss:		2.293734
  validation loss:		2.241803
  validation accuracy:		14.24 %
Epoch 971 of 2000 took 0.104s
  training loss:		2.293560
  validation loss:		2.246089
  validation accuracy:		14.67 %
Epoch 972 of 2000 took 0.104s
  training loss:		2.293508
  validation loss:		2.243225
  validation accuracy:		13.04 %
Epoch 973 of 2000 took 0.104s
  training loss:		2.293810
  validation loss:		2.244235
  validation accuracy:		12.93 %
Epoch 974 of 2000 took 0.104s
  training loss:		2.294947
  validation loss:		2.246661
  validation accuracy:		17.28 %
Epoch 975 of 2000 took 0.104s
  training loss:		2.293352
  validation loss:		2.243290
  validation accuracy:		17.07 %
Epoch 976 of 2000 took 0.104s
  training loss:		2.294174
  validation loss:		2.242208
  validation accuracy:		13.48 %
Epoch 977 of 2000 took 0.104s
  training loss:		2.293475
  validation loss:		2.242431
  validation accuracy:		13.26 %
Epoch 978 of 2000 took 0.103s
  training loss:		2.293767
  validation loss:		2.243700
  validation accuracy:		14.35 %
Epoch 979 of 2000 took 0.104s
  training loss:		2.293951
  validation loss:		2.241139
  validation accuracy:		12.93 %
Epoch 980 of 2000 took 0.104s
  training loss:		2.293988
  validation loss:		2.243830
  validation accuracy:		12.83 %
Epoch 981 of 2000 took 0.104s
  training loss:		2.294516
  validation loss:		2.244138
  validation accuracy:		13.48 %
Epoch 982 of 2000 took 0.104s
  training loss:		2.292995
  validation loss:		2.243888
  validation accuracy:		13.15 %
Epoch 983 of 2000 took 0.104s
  training loss:		2.294938
  validation loss:		2.242077
  validation accuracy:		14.46 %
Epoch 984 of 2000 took 0.104s
  training loss:		2.293884
  validation loss:		2.245342
  validation accuracy:		13.48 %
Epoch 985 of 2000 took 0.104s
  training loss:		2.293552
  validation loss:		2.244048
  validation accuracy:		14.24 %
Epoch 986 of 2000 took 0.104s
  training loss:		2.294285
  validation loss:		2.244971
  validation accuracy:		13.48 %
Epoch 987 of 2000 took 0.104s
  training loss:		2.293338
  validation loss:		2.244644
  validation accuracy:		13.37 %
Epoch 988 of 2000 took 0.104s
  training loss:		2.294110
  validation loss:		2.241521
  validation accuracy:		13.70 %
Epoch 989 of 2000 took 0.104s
  training loss:		2.293303
  validation loss:		2.242746
  validation accuracy:		13.04 %
Epoch 990 of 2000 took 0.104s
  training loss:		2.294014
  validation loss:		2.244309
  validation accuracy:		13.04 %
Epoch 991 of 2000 took 0.104s
  training loss:		2.293794
  validation loss:		2.241908
  validation accuracy:		12.93 %
Epoch 992 of 2000 took 0.104s
  training loss:		2.292900
  validation loss:		2.243793
  validation accuracy:		13.04 %
Epoch 993 of 2000 took 0.104s
  training loss:		2.292757
  validation loss:		2.242974
  validation accuracy:		13.37 %
Epoch 994 of 2000 took 0.104s
  training loss:		2.293790
  validation loss:		2.242579
  validation accuracy:		15.11 %
Epoch 995 of 2000 took 0.104s
  training loss:		2.293434
  validation loss:		2.243155
  validation accuracy:		12.93 %
Epoch 996 of 2000 took 0.109s
  training loss:		2.293726
  validation loss:		2.240254
  validation accuracy:		12.93 %
Epoch 997 of 2000 took 0.104s
  training loss:		2.292483
  validation loss:		2.239557
  validation accuracy:		13.04 %
Epoch 998 of 2000 took 0.104s
  training loss:		2.292628
  validation loss:		2.241263
  validation accuracy:		12.93 %
Epoch 999 of 2000 took 0.104s
  training loss:		2.293804
  validation loss:		2.240146
  validation accuracy:		18.80 %
Epoch 1000 of 2000 took 0.104s
  training loss:		2.293476
  validation loss:		2.241699
  validation accuracy:		13.04 %
Epoch 1001 of 2000 took 0.104s
  training loss:		2.293556
  validation loss:		2.243644
  validation accuracy:		18.26 %
Epoch 1002 of 2000 took 0.104s
  training loss:		2.294648
  validation loss:		2.245592
  validation accuracy:		12.83 %
Epoch 1003 of 2000 took 0.104s
  training loss:		2.294248
  validation loss:		2.244606
  validation accuracy:		12.83 %
Epoch 1004 of 2000 took 0.104s
  training loss:		2.293127
  validation loss:		2.243692
  validation accuracy:		13.04 %
Epoch 1005 of 2000 took 0.104s
  training loss:		2.293902
  validation loss:		2.243462
  validation accuracy:		13.04 %
Epoch 1006 of 2000 took 0.104s
  training loss:		2.293241
  validation loss:		2.240177
  validation accuracy:		13.70 %
Epoch 1007 of 2000 took 0.104s
  training loss:		2.293906
  validation loss:		2.242850
  validation accuracy:		13.48 %
Epoch 1008 of 2000 took 0.104s
  training loss:		2.293683
  validation loss:		2.247176
  validation accuracy:		14.24 %
Epoch 1009 of 2000 took 0.104s
  training loss:		2.292549
  validation loss:		2.242418
  validation accuracy:		12.83 %
Epoch 1010 of 2000 took 0.104s
  training loss:		2.293441
  validation loss:		2.241228
  validation accuracy:		13.04 %
Epoch 1011 of 2000 took 0.104s
  training loss:		2.292605
  validation loss:		2.242387
  validation accuracy:		13.70 %
Epoch 1012 of 2000 took 0.104s
  training loss:		2.293920
  validation loss:		2.241924
  validation accuracy:		13.04 %
Epoch 1013 of 2000 took 0.104s
  training loss:		2.292427
  validation loss:		2.241695
  validation accuracy:		12.83 %
Epoch 1014 of 2000 took 0.104s
  training loss:		2.292858
  validation loss:		2.242038
  validation accuracy:		13.04 %
Epoch 1015 of 2000 took 0.104s
  training loss:		2.293383
  validation loss:		2.238615
  validation accuracy:		12.93 %
Epoch 1016 of 2000 took 0.104s
  training loss:		2.292673
  validation loss:		2.241570
  validation accuracy:		13.37 %
Epoch 1017 of 2000 took 0.104s
  training loss:		2.293211
  validation loss:		2.242307
  validation accuracy:		18.37 %
Epoch 1018 of 2000 took 0.104s
  training loss:		2.292919
  validation loss:		2.242463
  validation accuracy:		13.04 %
Epoch 1019 of 2000 took 0.104s
  training loss:		2.293383
  validation loss:		2.241733
  validation accuracy:		13.15 %
Epoch 1020 of 2000 took 0.104s
  training loss:		2.292351
  validation loss:		2.240458
  validation accuracy:		13.37 %
Epoch 1021 of 2000 took 0.105s
  training loss:		2.292842
  validation loss:		2.239211
  validation accuracy:		16.52 %
Epoch 1022 of 2000 took 0.104s
  training loss:		2.293621
  validation loss:		2.243326
  validation accuracy:		15.54 %
Epoch 1023 of 2000 took 0.104s
  training loss:		2.293089
  validation loss:		2.242888
  validation accuracy:		16.52 %
Epoch 1024 of 2000 took 0.104s
  training loss:		2.293270
  validation loss:		2.243473
  validation accuracy:		13.37 %
Epoch 1025 of 2000 took 0.104s
  training loss:		2.293451
  validation loss:		2.243785
  validation accuracy:		12.93 %
Epoch 1026 of 2000 took 0.104s
  training loss:		2.293241
  validation loss:		2.242000
  validation accuracy:		14.57 %
Epoch 1027 of 2000 took 0.104s
  training loss:		2.293231
  validation loss:		2.242428
  validation accuracy:		13.26 %
Epoch 1028 of 2000 took 0.104s
  training loss:		2.293630
  validation loss:		2.244585
  validation accuracy:		14.24 %
Epoch 1029 of 2000 took 0.104s
  training loss:		2.292875
  validation loss:		2.244268
  validation accuracy:		19.02 %
Epoch 1030 of 2000 took 0.103s
  training loss:		2.292793
  validation loss:		2.243514
  validation accuracy:		12.93 %
Epoch 1031 of 2000 took 0.104s
  training loss:		2.292606
  validation loss:		2.240497
  validation accuracy:		12.83 %
Epoch 1032 of 2000 took 0.104s
  training loss:		2.292965
  validation loss:		2.238730
  validation accuracy:		20.00 %
Epoch 1033 of 2000 took 0.104s
  training loss:		2.293138
  validation loss:		2.242622
  validation accuracy:		13.15 %
Epoch 1034 of 2000 took 0.104s
  training loss:		2.293110
  validation loss:		2.243656
  validation accuracy:		14.57 %
Epoch 1035 of 2000 took 0.104s
  training loss:		2.291924
  validation loss:		2.241591
  validation accuracy:		12.83 %
Epoch 1036 of 2000 took 0.104s
  training loss:		2.293152
  validation loss:		2.238167
  validation accuracy:		17.39 %
Epoch 1037 of 2000 took 0.104s
  training loss:		2.293404
  validation loss:		2.242859
  validation accuracy:		12.93 %
Epoch 1038 of 2000 took 0.104s
  training loss:		2.292191
  validation loss:		2.241858
  validation accuracy:		13.26 %
Epoch 1039 of 2000 took 0.104s
  training loss:		2.293010
  validation loss:		2.242980
  validation accuracy:		14.24 %
Epoch 1040 of 2000 took 0.104s
  training loss:		2.292452
  validation loss:		2.243933
  validation accuracy:		15.33 %
Epoch 1041 of 2000 took 0.104s
  training loss:		2.292744
  validation loss:		2.241462
  validation accuracy:		15.87 %
Epoch 1042 of 2000 took 0.104s
  training loss:		2.293226
  validation loss:		2.240936
  validation accuracy:		13.04 %
Epoch 1043 of 2000 took 0.104s
  training loss:		2.292473
  validation loss:		2.242044
  validation accuracy:		18.26 %
Epoch 1044 of 2000 took 0.104s
  training loss:		2.291379
  validation loss:		2.241030
  validation accuracy:		14.02 %
Epoch 1045 of 2000 took 0.104s
  training loss:		2.292080
  validation loss:		2.237696
  validation accuracy:		13.80 %
Epoch 1046 of 2000 took 0.104s
  training loss:		2.292085
  validation loss:		2.238471
  validation accuracy:		20.22 %
Epoch 1047 of 2000 took 0.104s
  training loss:		2.292900
  validation loss:		2.243426
  validation accuracy:		12.93 %
Epoch 1048 of 2000 took 0.104s
  training loss:		2.292766
  validation loss:		2.244651
  validation accuracy:		12.93 %
Epoch 1049 of 2000 took 0.104s
  training loss:		2.292798
  validation loss:		2.241561
  validation accuracy:		20.65 %
Epoch 1050 of 2000 took 0.105s
  training loss:		2.292011
  validation loss:		2.240747
  validation accuracy:		18.04 %
Epoch 1051 of 2000 took 0.104s
  training loss:		2.293348
  validation loss:		2.243637
  validation accuracy:		20.22 %
Epoch 1052 of 2000 took 0.104s
  training loss:		2.292424
  validation loss:		2.243298
  validation accuracy:		15.33 %
Epoch 1053 of 2000 took 0.104s
  training loss:		2.291967
  validation loss:		2.242308
  validation accuracy:		18.91 %
Epoch 1054 of 2000 took 0.104s
  training loss:		2.291846
  validation loss:		2.241213
  validation accuracy:		18.59 %
Epoch 1055 of 2000 took 0.103s
  training loss:		2.291435
  validation loss:		2.239089
  validation accuracy:		18.26 %
Epoch 1056 of 2000 took 0.104s
  training loss:		2.292424
  validation loss:		2.238204
  validation accuracy:		14.57 %
Epoch 1057 of 2000 took 0.104s
  training loss:		2.291798
  validation loss:		2.239429
  validation accuracy:		14.89 %
Epoch 1058 of 2000 took 0.104s
  training loss:		2.291491
  validation loss:		2.240627
  validation accuracy:		20.33 %
Epoch 1059 of 2000 took 0.103s
  training loss:		2.292812
  validation loss:		2.238320
  validation accuracy:		16.20 %
Epoch 1060 of 2000 took 0.104s
  training loss:		2.293201
  validation loss:		2.243445
  validation accuracy:		18.37 %
Epoch 1061 of 2000 took 0.104s
  training loss:		2.291741
  validation loss:		2.245350
  validation accuracy:		19.57 %
Epoch 1062 of 2000 took 0.104s
  training loss:		2.292128
  validation loss:		2.241154
  validation accuracy:		13.59 %
Epoch 1063 of 2000 took 0.104s
  training loss:		2.291581
  validation loss:		2.239913
  validation accuracy:		19.67 %
Epoch 1064 of 2000 took 0.104s
  training loss:		2.292698
  validation loss:		2.241809
  validation accuracy:		12.93 %
Epoch 1065 of 2000 took 0.104s
  training loss:		2.292541
  validation loss:		2.246645
  validation accuracy:		17.28 %
Epoch 1066 of 2000 took 0.104s
  training loss:		2.291189
  validation loss:		2.241178
  validation accuracy:		12.93 %
Epoch 1067 of 2000 took 0.104s
  training loss:		2.292851
  validation loss:		2.238483
  validation accuracy:		14.78 %
Epoch 1068 of 2000 took 0.104s
  training loss:		2.292875
  validation loss:		2.238521
  validation accuracy:		13.48 %
Epoch 1069 of 2000 took 0.104s
  training loss:		2.291928
  validation loss:		2.244014
  validation accuracy:		16.09 %
Epoch 1070 of 2000 took 0.104s
  training loss:		2.291984
  validation loss:		2.242163
  validation accuracy:		15.43 %
Epoch 1071 of 2000 took 0.104s
  training loss:		2.292360
  validation loss:		2.242727
  validation accuracy:		17.61 %
Epoch 1072 of 2000 took 0.104s
  training loss:		2.291376
  validation loss:		2.238689
  validation accuracy:		14.02 %
Epoch 1073 of 2000 took 0.103s
  training loss:		2.292446
  validation loss:		2.241208
  validation accuracy:		13.04 %
Epoch 1074 of 2000 took 0.104s
  training loss:		2.291906
  validation loss:		2.243974
  validation accuracy:		18.80 %
Epoch 1075 of 2000 took 0.104s
  training loss:		2.292488
  validation loss:		2.241117
  validation accuracy:		17.07 %
Epoch 1076 of 2000 took 0.104s
  training loss:		2.292224
  validation loss:		2.239036
  validation accuracy:		15.76 %
Epoch 1077 of 2000 took 0.104s
  training loss:		2.291608
  validation loss:		2.238482
  validation accuracy:		16.63 %
Epoch 1078 of 2000 took 0.104s
  training loss:		2.291857
  validation loss:		2.241742
  validation accuracy:		16.20 %
Epoch 1079 of 2000 took 0.105s
  training loss:		2.292483
  validation loss:		2.241266
  validation accuracy:		13.26 %
Epoch 1080 of 2000 took 0.104s
  training loss:		2.292329
  validation loss:		2.246197
  validation accuracy:		14.78 %
Epoch 1081 of 2000 took 0.104s
  training loss:		2.291279
  validation loss:		2.238941
  validation accuracy:		15.76 %
Epoch 1082 of 2000 took 0.103s
  training loss:		2.290949
  validation loss:		2.239101
  validation accuracy:		20.00 %
Epoch 1083 of 2000 took 0.104s
  training loss:		2.291453
  validation loss:		2.237641
  validation accuracy:		17.17 %
Epoch 1084 of 2000 took 0.103s
  training loss:		2.291676
  validation loss:		2.241713
  validation accuracy:		16.20 %
Epoch 1085 of 2000 took 0.104s
  training loss:		2.290953
  validation loss:		2.242635
  validation accuracy:		12.93 %
Epoch 1086 of 2000 took 0.104s
  training loss:		2.290708
  validation loss:		2.239787
  validation accuracy:		17.17 %
Epoch 1087 of 2000 took 0.104s
  training loss:		2.291945
  validation loss:		2.238215
  validation accuracy:		13.26 %
Epoch 1088 of 2000 took 0.104s
  training loss:		2.291376
  validation loss:		2.238593
  validation accuracy:		19.24 %
Epoch 1089 of 2000 took 0.104s
  training loss:		2.290986
  validation loss:		2.238848
  validation accuracy:		17.17 %
Epoch 1090 of 2000 took 0.104s
  training loss:		2.291222
  validation loss:		2.240610
  validation accuracy:		19.02 %
Epoch 1091 of 2000 took 0.104s
  training loss:		2.291685
  validation loss:		2.240416
  validation accuracy:		19.13 %
Epoch 1092 of 2000 took 0.104s
  training loss:		2.291527
  validation loss:		2.244641
  validation accuracy:		16.30 %
Epoch 1093 of 2000 took 0.109s
  training loss:		2.291062
  validation loss:		2.240503
  validation accuracy:		15.22 %
Epoch 1094 of 2000 took 0.104s
  training loss:		2.290955
  validation loss:		2.239449
  validation accuracy:		17.93 %
Epoch 1095 of 2000 took 0.104s
  training loss:		2.290724
  validation loss:		2.237246
  validation accuracy:		17.50 %
Epoch 1096 of 2000 took 0.104s
  training loss:		2.291246
  validation loss:		2.241845
  validation accuracy:		13.70 %
Epoch 1097 of 2000 took 0.104s
  training loss:		2.290643
  validation loss:		2.238947
  validation accuracy:		18.48 %
Epoch 1098 of 2000 took 0.104s
  training loss:		2.290207
  validation loss:		2.236816
  validation accuracy:		14.67 %
Epoch 1099 of 2000 took 0.104s
  training loss:		2.290561
  validation loss:		2.238219
  validation accuracy:		15.11 %
Epoch 1100 of 2000 took 0.104s
  training loss:		2.290496
  validation loss:		2.240658
  validation accuracy:		21.09 %
Epoch 1101 of 2000 took 0.104s
  training loss:		2.290934
  validation loss:		2.243314
  validation accuracy:		18.70 %
Epoch 1102 of 2000 took 0.104s
  training loss:		2.290759
  validation loss:		2.239354
  validation accuracy:		17.93 %
Epoch 1103 of 2000 took 0.104s
  training loss:		2.289787
  validation loss:		2.239438
  validation accuracy:		21.30 %
Epoch 1104 of 2000 took 0.104s
  training loss:		2.290686
  validation loss:		2.237189
  validation accuracy:		18.26 %
Epoch 1105 of 2000 took 0.103s
  training loss:		2.290354
  validation loss:		2.240636
  validation accuracy:		22.17 %
Epoch 1106 of 2000 took 0.104s
  training loss:		2.289851
  validation loss:		2.239682
  validation accuracy:		18.70 %
Epoch 1107 of 2000 took 0.103s
  training loss:		2.289409
  validation loss:		2.236686
  validation accuracy:		16.74 %
Epoch 1108 of 2000 took 0.104s
  training loss:		2.289292
  validation loss:		2.237548
  validation accuracy:		19.02 %
Epoch 1109 of 2000 took 0.104s
  training loss:		2.289455
  validation loss:		2.236457
  validation accuracy:		21.30 %
Epoch 1110 of 2000 took 0.104s
  training loss:		2.288941
  validation loss:		2.236220
  validation accuracy:		17.93 %
Epoch 1111 of 2000 took 0.104s
  training loss:		2.289494
  validation loss:		2.240079
  validation accuracy:		18.70 %
Epoch 1112 of 2000 took 0.104s
  training loss:		2.289500
  validation loss:		2.240770
  validation accuracy:		19.46 %
Epoch 1113 of 2000 took 0.104s
  training loss:		2.288144
  validation loss:		2.236475
  validation accuracy:		20.22 %
Epoch 1114 of 2000 took 0.104s
  training loss:		2.288632
  validation loss:		2.234470
  validation accuracy:		20.87 %
Epoch 1115 of 2000 took 0.104s
  training loss:		2.289516
  validation loss:		2.235398
  validation accuracy:		16.52 %
Epoch 1116 of 2000 took 0.104s
  training loss:		2.288467
  validation loss:		2.238337
  validation accuracy:		21.52 %
Epoch 1117 of 2000 took 0.104s
  training loss:		2.288682
  validation loss:		2.238209
  validation accuracy:		14.35 %
Epoch 1118 of 2000 took 0.104s
  training loss:		2.288387
  validation loss:		2.237463
  validation accuracy:		13.59 %
Epoch 1119 of 2000 took 0.104s
  training loss:		2.288772
  validation loss:		2.237646
  validation accuracy:		15.11 %
Epoch 1120 of 2000 took 0.104s
  training loss:		2.287626
  validation loss:		2.233618
  validation accuracy:		19.89 %
Epoch 1121 of 2000 took 0.103s
  training loss:		2.287833
  validation loss:		2.234436
  validation accuracy:		20.43 %
Epoch 1122 of 2000 took 0.104s
  training loss:		2.288199
  validation loss:		2.235026
  validation accuracy:		15.87 %
Epoch 1123 of 2000 took 0.103s
  training loss:		2.288518
  validation loss:		2.236055
  validation accuracy:		15.76 %
Epoch 1124 of 2000 took 0.104s
  training loss:		2.288057
  validation loss:		2.235472
  validation accuracy:		20.87 %
Epoch 1125 of 2000 took 0.104s
  training loss:		2.288459
  validation loss:		2.237656
  validation accuracy:		21.74 %
Epoch 1126 of 2000 took 0.104s
  training loss:		2.287563
  validation loss:		2.240292
  validation accuracy:		18.04 %
Epoch 1127 of 2000 took 0.104s
  training loss:		2.288011
  validation loss:		2.234119
  validation accuracy:		22.28 %
Epoch 1128 of 2000 took 0.104s
  training loss:		2.287020
  validation loss:		2.232396
  validation accuracy:		16.30 %
Epoch 1129 of 2000 took 0.104s
  training loss:		2.288252
  validation loss:		2.235315
  validation accuracy:		16.41 %
Epoch 1130 of 2000 took 0.103s
  training loss:		2.287434
  validation loss:		2.239881
  validation accuracy:		23.04 %
Epoch 1131 of 2000 took 0.104s
  training loss:		2.287358
  validation loss:		2.236274
  validation accuracy:		20.43 %
Epoch 1132 of 2000 took 0.104s
  training loss:		2.287328
  validation loss:		2.236878
  validation accuracy:		20.11 %
Epoch 1133 of 2000 took 0.104s
  training loss:		2.286378
  validation loss:		2.232760
  validation accuracy:		16.52 %
Epoch 1134 of 2000 took 0.104s
  training loss:		2.285813
  validation loss:		2.236027
  validation accuracy:		16.20 %
Epoch 1135 of 2000 took 0.104s
  training loss:		2.286484
  validation loss:		2.231315
  validation accuracy:		19.35 %
Epoch 1136 of 2000 took 0.104s
  training loss:		2.285932
  validation loss:		2.234137
  validation accuracy:		21.41 %
Epoch 1137 of 2000 took 0.105s
  training loss:		2.285427
  validation loss:		2.235057
  validation accuracy:		16.85 %
Epoch 1138 of 2000 took 0.104s
  training loss:		2.286098
  validation loss:		2.235223
  validation accuracy:		22.39 %
Epoch 1139 of 2000 took 0.104s
  training loss:		2.285714
  validation loss:		2.232417
  validation accuracy:		22.07 %
Epoch 1140 of 2000 took 0.104s
  training loss:		2.285817
  validation loss:		2.231373
  validation accuracy:		18.26 %
Epoch 1141 of 2000 took 0.104s
  training loss:		2.285316
  validation loss:		2.233021
  validation accuracy:		22.93 %
Epoch 1142 of 2000 took 0.104s
  training loss:		2.284277
  validation loss:		2.234663
  validation accuracy:		21.74 %
Epoch 1143 of 2000 took 0.104s
  training loss:		2.284764
  validation loss:		2.233204
  validation accuracy:		21.30 %
Epoch 1144 of 2000 took 0.104s
  training loss:		2.283267
  validation loss:		2.229084
  validation accuracy:		19.46 %
Epoch 1145 of 2000 took 0.104s
  training loss:		2.284538
  validation loss:		2.230791
  validation accuracy:		21.09 %
Epoch 1146 of 2000 took 0.104s
  training loss:		2.284016
  validation loss:		2.234514
  validation accuracy:		21.63 %
Epoch 1147 of 2000 took 0.104s
  training loss:		2.283230
  validation loss:		2.231098
  validation accuracy:		23.37 %
Epoch 1148 of 2000 took 0.104s
  training loss:		2.283673
  validation loss:		2.230953
  validation accuracy:		21.52 %
Epoch 1149 of 2000 took 0.104s
  training loss:		2.282904
  validation loss:		2.231793
  validation accuracy:		21.74 %
Epoch 1150 of 2000 took 0.104s
  training loss:		2.281505
  validation loss:		2.230539
  validation accuracy:		18.15 %
Epoch 1151 of 2000 took 0.104s
  training loss:		2.281616
  validation loss:		2.226191
  validation accuracy:		22.17 %
Epoch 1152 of 2000 took 0.104s
  training loss:		2.280932
  validation loss:		2.225208
  validation accuracy:		22.50 %
Epoch 1153 of 2000 took 0.104s
  training loss:		2.279939
  validation loss:		2.226231
  validation accuracy:		22.39 %
Epoch 1154 of 2000 took 0.104s
  training loss:		2.280048
  validation loss:		2.226658
  validation accuracy:		22.83 %
Epoch 1155 of 2000 took 0.104s
  training loss:		2.279636
  validation loss:		2.223293
  validation accuracy:		23.70 %
Epoch 1156 of 2000 took 0.104s
  training loss:		2.280107
  validation loss:		2.226832
  validation accuracy:		20.98 %
Epoch 1157 of 2000 took 0.104s
  training loss:		2.280143
  validation loss:		2.227327
  validation accuracy:		24.67 %
Epoch 1158 of 2000 took 0.104s
  training loss:		2.279207
  validation loss:		2.226732
  validation accuracy:		23.37 %
Epoch 1159 of 2000 took 0.104s
  training loss:		2.278572
  validation loss:		2.225043
  validation accuracy:		19.02 %
Epoch 1160 of 2000 took 0.104s
  training loss:		2.278716
  validation loss:		2.231374
  validation accuracy:		24.35 %
Epoch 1161 of 2000 took 0.104s
  training loss:		2.276370
  validation loss:		2.224750
  validation accuracy:		20.65 %
Epoch 1162 of 2000 took 0.104s
  training loss:		2.276755
  validation loss:		2.221963
  validation accuracy:		22.07 %
Epoch 1163 of 2000 took 0.104s
  training loss:		2.275640
  validation loss:		2.222452
  validation accuracy:		19.78 %
Epoch 1164 of 2000 took 0.104s
  training loss:		2.276041
  validation loss:		2.224369
  validation accuracy:		23.59 %
Epoch 1165 of 2000 took 0.104s
  training loss:		2.274692
  validation loss:		2.224421
  validation accuracy:		22.61 %
Epoch 1166 of 2000 took 0.105s
  training loss:		2.273267
  validation loss:		2.221784
  validation accuracy:		24.46 %
Epoch 1167 of 2000 took 0.104s
  training loss:		2.272702
  validation loss:		2.216861
  validation accuracy:		22.93 %
Epoch 1168 of 2000 took 0.104s
  training loss:		2.272313
  validation loss:		2.220214
  validation accuracy:		22.28 %
Epoch 1169 of 2000 took 0.104s
  training loss:		2.271262
  validation loss:		2.217719
  validation accuracy:		22.39 %
Epoch 1170 of 2000 took 0.104s
  training loss:		2.269422
  validation loss:		2.217568
  validation accuracy:		23.15 %
Epoch 1171 of 2000 took 0.104s
  training loss:		2.269381
  validation loss:		2.213078
  validation accuracy:		22.17 %
Epoch 1172 of 2000 took 0.104s
  training loss:		2.267194
  validation loss:		2.208676
  validation accuracy:		22.83 %
Epoch 1173 of 2000 took 0.104s
  training loss:		2.266570
  validation loss:		2.209898
  validation accuracy:		23.48 %
Epoch 1174 of 2000 took 0.104s
  training loss:		2.264808
  validation loss:		2.210753
  validation accuracy:		22.72 %
Epoch 1175 of 2000 took 0.104s
  training loss:		2.264771
  validation loss:		2.210430
  validation accuracy:		22.72 %
Epoch 1176 of 2000 took 0.104s
  training loss:		2.262069
  validation loss:		2.207544
  validation accuracy:		23.37 %
Epoch 1177 of 2000 took 0.104s
  training loss:		2.260212
  validation loss:		2.204637
  validation accuracy:		23.04 %
Epoch 1178 of 2000 took 0.104s
  training loss:		2.258952
  validation loss:		2.199885
  validation accuracy:		24.02 %
Epoch 1179 of 2000 took 0.104s
  training loss:		2.257225
  validation loss:		2.200223
  validation accuracy:		25.43 %
Epoch 1180 of 2000 took 0.107s
  training loss:		2.254887
  validation loss:		2.197060
  validation accuracy:		24.78 %
Epoch 1181 of 2000 took 0.107s
  training loss:		2.252364
  validation loss:		2.195690
  validation accuracy:		23.15 %
Epoch 1182 of 2000 took 0.107s
  training loss:		2.251163
  validation loss:		2.192198
  validation accuracy:		24.24 %
Epoch 1183 of 2000 took 0.107s
  training loss:		2.247699
  validation loss:		2.190979
  validation accuracy:		25.22 %
Epoch 1184 of 2000 took 0.107s
  training loss:		2.245747
  validation loss:		2.188339
  validation accuracy:		24.89 %
Epoch 1185 of 2000 took 0.107s
  training loss:		2.242613
  validation loss:		2.186745
  validation accuracy:		23.04 %
Epoch 1186 of 2000 took 0.107s
  training loss:		2.239471
  validation loss:		2.180085
  validation accuracy:		23.70 %
Epoch 1187 of 2000 took 0.107s
  training loss:		2.235686
  validation loss:		2.176065
  validation accuracy:		23.80 %
Epoch 1188 of 2000 took 0.107s
  training loss:		2.230500
  validation loss:		2.169336
  validation accuracy:		24.35 %
Epoch 1189 of 2000 took 0.107s
  training loss:		2.226830
  validation loss:		2.160223
  validation accuracy:		23.80 %
Epoch 1190 of 2000 took 0.107s
  training loss:		2.220893
  validation loss:		2.158675
  validation accuracy:		23.15 %
Epoch 1191 of 2000 took 0.107s
  training loss:		2.217032
  validation loss:		2.151395
  validation accuracy:		24.67 %
Epoch 1192 of 2000 took 0.107s
  training loss:		2.210231
  validation loss:		2.147045
  validation accuracy:		24.35 %
Epoch 1193 of 2000 took 0.107s
  training loss:		2.202136
  validation loss:		2.136595
  validation accuracy:		23.26 %
Epoch 1194 of 2000 took 0.107s
  training loss:		2.194675
  validation loss:		2.127766
  validation accuracy:		26.52 %
Epoch 1195 of 2000 took 0.108s
  training loss:		2.186947
  validation loss:		2.118477
  validation accuracy:		25.43 %
Epoch 1196 of 2000 took 0.107s
  training loss:		2.177432
  validation loss:		2.108744
  validation accuracy:		24.35 %
Epoch 1197 of 2000 took 0.107s
  training loss:		2.165124
  validation loss:		2.093035
  validation accuracy:		24.02 %
Epoch 1198 of 2000 took 0.111s
  training loss:		2.154125
  validation loss:		2.081290
  validation accuracy:		24.24 %
Epoch 1199 of 2000 took 0.108s
  training loss:		2.139818
  validation loss:		2.066624
  validation accuracy:		27.83 %
Epoch 1200 of 2000 took 0.107s
  training loss:		2.124380
  validation loss:		2.045331
  validation accuracy:		27.61 %
Epoch 1201 of 2000 took 0.107s
  training loss:		2.110749
  validation loss:		2.026173
  validation accuracy:		25.22 %
Epoch 1202 of 2000 took 0.107s
  training loss:		2.090562
  validation loss:		2.005080
  validation accuracy:		26.41 %
Epoch 1203 of 2000 took 0.107s
  training loss:		2.071999
  validation loss:		1.985972
  validation accuracy:		25.43 %
Epoch 1204 of 2000 took 0.107s
  training loss:		2.048203
  validation loss:		1.962445
  validation accuracy:		27.28 %
Epoch 1205 of 2000 took 0.107s
  training loss:		2.029554
  validation loss:		1.940798
  validation accuracy:		26.85 %
Epoch 1206 of 2000 took 0.107s
  training loss:		2.003211
  validation loss:		1.911368
  validation accuracy:		27.83 %
Epoch 1207 of 2000 took 0.107s
  training loss:		1.980731
  validation loss:		1.888608
  validation accuracy:		28.15 %
Epoch 1208 of 2000 took 0.107s
  training loss:		1.960636
  validation loss:		1.865240
  validation accuracy:		29.13 %
Epoch 1209 of 2000 took 0.107s
  training loss:		1.938482
  validation loss:		1.845323
  validation accuracy:		29.24 %
Epoch 1210 of 2000 took 0.107s
  training loss:		1.910023
  validation loss:		1.821705
  validation accuracy:		30.33 %
Epoch 1211 of 2000 took 0.107s
  training loss:		1.892182
  validation loss:		1.797554
  validation accuracy:		30.76 %
Epoch 1212 of 2000 took 0.107s
  training loss:		1.868620
  validation loss:		1.777277
  validation accuracy:		31.85 %
Epoch 1213 of 2000 took 0.107s
  training loss:		1.848581
  validation loss:		1.757896
  validation accuracy:		32.93 %
Epoch 1214 of 2000 took 0.107s
  training loss:		1.825994
  validation loss:		1.740432
  validation accuracy:		32.72 %
Epoch 1215 of 2000 took 0.107s
  training loss:		1.802814
  validation loss:		1.726973
  validation accuracy:		31.96 %
Epoch 1216 of 2000 took 0.107s
  training loss:		1.789796
  validation loss:		1.708314
  validation accuracy:		33.37 %
Epoch 1217 of 2000 took 0.105s
  training loss:		1.772824
  validation loss:		1.698175
  validation accuracy:		33.91 %
Epoch 1218 of 2000 took 0.104s
  training loss:		1.753628
  validation loss:		1.671006
  validation accuracy:		34.46 %
Epoch 1219 of 2000 took 0.104s
  training loss:		1.746197
  validation loss:		1.657718
  validation accuracy:		34.78 %
Epoch 1220 of 2000 took 0.104s
  training loss:		1.722379
  validation loss:		1.645981
  validation accuracy:		34.35 %
Epoch 1221 of 2000 took 0.104s
  training loss:		1.707289
  validation loss:		1.636109
  validation accuracy:		35.33 %
Epoch 1222 of 2000 took 0.104s
  training loss:		1.694046
  validation loss:		1.621043
  validation accuracy:		34.67 %
Epoch 1223 of 2000 took 0.105s
  training loss:		1.678629
  validation loss:		1.607265
  validation accuracy:		35.11 %
Epoch 1224 of 2000 took 0.104s
  training loss:		1.658716
  validation loss:		1.593320
  validation accuracy:		36.09 %
Epoch 1225 of 2000 took 0.104s
  training loss:		1.648380
  validation loss:		1.582564
  validation accuracy:		37.07 %
Epoch 1226 of 2000 took 0.104s
  training loss:		1.643059
  validation loss:		1.572444
  validation accuracy:		35.76 %
Epoch 1227 of 2000 took 0.104s
  training loss:		1.619788
  validation loss:		1.561364
  validation accuracy:		37.39 %
Epoch 1228 of 2000 took 0.104s
  training loss:		1.609913
  validation loss:		1.552845
  validation accuracy:		38.70 %
Epoch 1229 of 2000 took 0.104s
  training loss:		1.598315
  validation loss:		1.541314
  validation accuracy:		38.04 %
Epoch 1230 of 2000 took 0.104s
  training loss:		1.589123
  validation loss:		1.532285
  validation accuracy:		38.37 %
Epoch 1231 of 2000 took 0.104s
  training loss:		1.576796
  validation loss:		1.520518
  validation accuracy:		39.02 %
Epoch 1232 of 2000 took 0.104s
  training loss:		1.570238
  validation loss:		1.514013
  validation accuracy:		39.57 %
Epoch 1233 of 2000 took 0.104s
  training loss:		1.547729
  validation loss:		1.503844
  validation accuracy:		40.00 %
Epoch 1234 of 2000 took 0.104s
  training loss:		1.543183
  validation loss:		1.495265
  validation accuracy:		40.98 %
Epoch 1235 of 2000 took 0.104s
  training loss:		1.539843
  validation loss:		1.490324
  validation accuracy:		41.09 %
Epoch 1236 of 2000 took 0.104s
  training loss:		1.524640
  validation loss:		1.481476
  validation accuracy:		41.85 %
Epoch 1237 of 2000 took 0.104s
  training loss:		1.522237
  validation loss:		1.482252
  validation accuracy:		41.85 %
Epoch 1238 of 2000 took 0.104s
  training loss:		1.514003
  validation loss:		1.476131
  validation accuracy:		41.74 %
Epoch 1239 of 2000 took 0.104s
  training loss:		1.507965
  validation loss:		1.472002
  validation accuracy:		41.96 %
Epoch 1240 of 2000 took 0.104s
  training loss:		1.495638
  validation loss:		1.459284
  validation accuracy:		42.39 %
Epoch 1241 of 2000 took 0.104s
  training loss:		1.495205
  validation loss:		1.455931
  validation accuracy:		42.50 %
Epoch 1242 of 2000 took 0.104s
  training loss:		1.485168
  validation loss:		1.471791
  validation accuracy:		42.72 %
Epoch 1243 of 2000 took 0.104s
  training loss:		1.481259
  validation loss:		1.446909
  validation accuracy:		42.93 %
Epoch 1244 of 2000 took 0.104s
  training loss:		1.476820
  validation loss:		1.442654
  validation accuracy:		43.70 %
Epoch 1245 of 2000 took 0.104s
  training loss:		1.466244
  validation loss:		1.437288
  validation accuracy:		43.80 %
Epoch 1246 of 2000 took 0.104s
  training loss:		1.473211
  validation loss:		1.434822
  validation accuracy:		43.70 %
Epoch 1247 of 2000 took 0.104s
  training loss:		1.457648
  validation loss:		1.435559
  validation accuracy:		44.78 %
Epoch 1248 of 2000 took 0.104s
  training loss:		1.451471
  validation loss:		1.432731
  validation accuracy:		44.78 %
Epoch 1249 of 2000 took 0.104s
  training loss:		1.452945
  validation loss:		1.433164
  validation accuracy:		44.78 %
Epoch 1250 of 2000 took 0.104s
  training loss:		1.447612
  validation loss:		1.433110
  validation accuracy:		44.13 %
Epoch 1251 of 2000 took 0.104s
  training loss:		1.451510
  validation loss:		1.419061
  validation accuracy:		45.43 %
Epoch 1252 of 2000 took 0.105s
  training loss:		1.440085
  validation loss:		1.425968
  validation accuracy:		45.22 %
Epoch 1253 of 2000 took 0.104s
  training loss:		1.436883
  validation loss:		1.412009
  validation accuracy:		45.54 %
Epoch 1254 of 2000 took 0.104s
  training loss:		1.581386
  validation loss:		1.499210
  validation accuracy:		41.74 %
Epoch 1255 of 2000 took 0.104s
  training loss:		1.453023
  validation loss:		1.427342
  validation accuracy:		46.52 %
Epoch 1256 of 2000 took 0.104s
  training loss:		1.428955
  validation loss:		1.412302
  validation accuracy:		46.41 %
Epoch 1257 of 2000 took 0.104s
  training loss:		1.443836
  validation loss:		1.576601
  validation accuracy:		39.24 %
Epoch 1258 of 2000 took 0.104s
  training loss:		1.609738
  validation loss:		1.420902
  validation accuracy:		47.72 %
Epoch 1259 of 2000 took 0.104s
  training loss:		1.423544
  validation loss:		1.438765
  validation accuracy:		47.50 %
Epoch 1260 of 2000 took 0.104s
  training loss:		1.424724
  validation loss:		1.410056
  validation accuracy:		46.41 %
Epoch 1261 of 2000 took 0.104s
  training loss:		1.433728
  validation loss:		1.397041
  validation accuracy:		46.63 %
Epoch 1262 of 2000 took 0.104s
  training loss:		1.409754
  validation loss:		1.433142
  validation accuracy:		45.87 %
Epoch 1263 of 2000 took 0.104s
  training loss:		1.503636
  validation loss:		1.505181
  validation accuracy:		43.37 %
Epoch 1264 of 2000 took 0.104s
  training loss:		1.440796
  validation loss:		1.407737
  validation accuracy:		47.93 %
Epoch 1265 of 2000 took 0.104s
  training loss:		1.424178
  validation loss:		1.469186
  validation accuracy:		44.35 %
Epoch 1266 of 2000 took 0.104s
  training loss:		1.418940
  validation loss:		1.394152
  validation accuracy:		48.37 %
Epoch 1267 of 2000 took 0.104s
  training loss:		1.412877
  validation loss:		1.473215
  validation accuracy:		44.13 %
Epoch 1268 of 2000 took 0.104s
  training loss:		1.443419
  validation loss:		1.403662
  validation accuracy:		49.13 %
Epoch 1269 of 2000 took 0.104s
  training loss:		1.440739
  validation loss:		1.545589
  validation accuracy:		41.09 %
Epoch 1270 of 2000 took 0.104s
  training loss:		1.438521
  validation loss:		1.389042
  validation accuracy:		49.02 %
Epoch 1271 of 2000 took 0.104s
  training loss:		1.389783
  validation loss:		1.384924
  validation accuracy:		48.37 %
Epoch 1272 of 2000 took 0.104s
  training loss:		1.549473
  validation loss:		1.710876
  validation accuracy:		37.07 %
Epoch 1273 of 2000 took 0.104s
  training loss:		1.450003
  validation loss:		1.406732
  validation accuracy:		49.24 %
Epoch 1274 of 2000 took 0.104s
  training loss:		1.385788
  validation loss:		1.391452
  validation accuracy:		49.46 %
Epoch 1275 of 2000 took 0.104s
  training loss:		1.387158
  validation loss:		1.391953
  validation accuracy:		46.30 %
Epoch 1276 of 2000 took 0.104s
  training loss:		1.396135
  validation loss:		1.380581
  validation accuracy:		49.13 %
Epoch 1277 of 2000 took 0.104s
  training loss:		1.392407
  validation loss:		1.391062
  validation accuracy:		48.59 %
Epoch 1278 of 2000 took 0.104s
  training loss:		1.486189
  validation loss:		1.604460
  validation accuracy:		38.91 %
Epoch 1279 of 2000 took 0.104s
  training loss:		1.528052
  validation loss:		1.425095
  validation accuracy:		49.02 %
Epoch 1280 of 2000 took 0.104s
  training loss:		1.394717
  validation loss:		1.398961
  validation accuracy:		49.78 %
Epoch 1281 of 2000 took 0.105s
  training loss:		1.414569
  validation loss:		1.398585
  validation accuracy:		47.39 %
Epoch 1282 of 2000 took 0.104s
  training loss:		1.380547
  validation loss:		1.400865
  validation accuracy:		47.50 %
Epoch 1283 of 2000 took 0.104s
  training loss:		1.411650
  validation loss:		1.422930
  validation accuracy:		48.26 %
Epoch 1284 of 2000 took 0.104s
  training loss:		1.404963
  validation loss:		1.383292
  validation accuracy:		49.78 %
Epoch 1285 of 2000 took 0.104s
  training loss:		1.375845
  validation loss:		1.418917
  validation accuracy:		45.33 %
Epoch 1286 of 2000 took 0.104s
  training loss:		1.392857
  validation loss:		1.400927
  validation accuracy:		49.35 %
Epoch 1287 of 2000 took 0.104s
  training loss:		1.452837
  validation loss:		1.382915
  validation accuracy:		50.00 %
Epoch 1288 of 2000 took 0.104s
  training loss:		1.368222
  validation loss:		1.387105
  validation accuracy:		49.57 %
Epoch 1289 of 2000 took 0.104s
  training loss:		1.368704
  validation loss:		1.391377
  validation accuracy:		47.17 %
Epoch 1290 of 2000 took 0.104s
  training loss:		1.373405
  validation loss:		1.383061
  validation accuracy:		48.26 %
Epoch 1291 of 2000 took 0.104s
  training loss:		1.386545
  validation loss:		1.409252
  validation accuracy:		46.85 %
Epoch 1292 of 2000 took 0.104s
  training loss:		1.375847
  validation loss:		1.400649
  validation accuracy:		49.13 %
Epoch 1293 of 2000 took 0.104s
  training loss:		1.550385
  validation loss:		1.546309
  validation accuracy:		41.74 %
Epoch 1294 of 2000 took 0.104s
  training loss:		1.409237
  validation loss:		1.391652
  validation accuracy:		51.09 %
Epoch 1295 of 2000 took 0.104s
  training loss:		1.367157
  validation loss:		1.429295
  validation accuracy:		45.87 %
Epoch 1296 of 2000 took 0.104s
  training loss:		1.378665
  validation loss:		1.368140
  validation accuracy:		49.57 %
Epoch 1297 of 2000 took 0.104s
  training loss:		1.355634
  validation loss:		1.364894
  validation accuracy:		49.02 %
Epoch 1298 of 2000 took 0.104s
  training loss:		1.364034
  validation loss:		1.384441
  validation accuracy:		50.43 %
Epoch 1299 of 2000 took 0.104s
  training loss:		1.362890
  validation loss:		1.363805
  validation accuracy:		50.11 %
Epoch 1300 of 2000 took 0.104s
  training loss:		1.441671
  validation loss:		1.532793
  validation accuracy:		42.83 %
Epoch 1301 of 2000 took 0.104s
  training loss:		1.513775
  validation loss:		1.400430
  validation accuracy:		49.78 %
Epoch 1302 of 2000 took 0.104s
  training loss:		1.367431
  validation loss:		1.388803
  validation accuracy:		49.57 %
Epoch 1303 of 2000 took 0.104s
  training loss:		1.402385
  validation loss:		1.405435
  validation accuracy:		48.59 %
Epoch 1304 of 2000 took 0.104s
  training loss:		1.452904
  validation loss:		1.408661
  validation accuracy:		47.39 %
Epoch 1305 of 2000 took 0.104s
  training loss:		1.367050
  validation loss:		1.374641
  validation accuracy:		48.80 %
Epoch 1306 of 2000 took 0.104s
  training loss:		1.361977
  validation loss:		1.361353
  validation accuracy:		49.46 %
Epoch 1307 of 2000 took 0.104s
  training loss:		1.356431
  validation loss:		1.378399
  validation accuracy:		49.78 %
Epoch 1308 of 2000 took 0.104s
  training loss:		1.392082
  validation loss:		1.411594
  validation accuracy:		46.85 %
Epoch 1309 of 2000 took 0.104s
  training loss:		1.366459
  validation loss:		1.394691
  validation accuracy:		47.83 %
Epoch 1310 of 2000 took 0.105s
  training loss:		1.387045
  validation loss:		1.367023
  validation accuracy:		48.91 %
Epoch 1311 of 2000 took 0.104s
  training loss:		1.348223
  validation loss:		1.365276
  validation accuracy:		49.24 %
Epoch 1312 of 2000 took 0.104s
  training loss:		1.376029
  validation loss:		1.372849
  validation accuracy:		49.24 %
Epoch 1313 of 2000 took 0.104s
  training loss:		1.394770
  validation loss:		1.408380
  validation accuracy:		46.30 %
Epoch 1314 of 2000 took 0.107s
  training loss:		1.382967
  validation loss:		1.381578
  validation accuracy:		49.35 %
Epoch 1315 of 2000 took 0.106s
  training loss:		1.459634
  validation loss:		1.470462
  validation accuracy:		45.98 %
Epoch 1316 of 2000 took 0.104s
  training loss:		1.389619
  validation loss:		1.393347
  validation accuracy:		48.04 %
Epoch 1317 of 2000 took 0.104s
  training loss:		1.388672
  validation loss:		1.384547
  validation accuracy:		48.59 %
Epoch 1318 of 2000 took 0.104s
  training loss:		1.338223
  validation loss:		1.389277
  validation accuracy:		47.17 %
Epoch 1319 of 2000 took 0.104s
  training loss:		1.355553
  validation loss:		1.359518
  validation accuracy:		49.46 %
Epoch 1320 of 2000 took 0.104s
  training loss:		1.342933
  validation loss:		1.357903
  validation accuracy:		49.57 %
Epoch 1321 of 2000 took 0.104s
  training loss:		1.395102
  validation loss:		1.547897
  validation accuracy:		41.41 %
Epoch 1322 of 2000 took 0.104s
  training loss:		1.427115
  validation loss:		1.442605
  validation accuracy:		42.72 %
Epoch 1323 of 2000 took 0.104s
  training loss:		1.382395
  validation loss:		1.461335
  validation accuracy:		42.07 %
Epoch 1324 of 2000 took 0.104s
  training loss:		1.464281
  validation loss:		1.369037
  validation accuracy:		50.11 %
Epoch 1325 of 2000 took 0.104s
  training loss:		1.354684
  validation loss:		1.379347
  validation accuracy:		48.04 %
Epoch 1326 of 2000 took 0.104s
  training loss:		1.352260
  validation loss:		1.390655
  validation accuracy:		47.39 %
Epoch 1327 of 2000 took 0.104s
  training loss:		1.384760
  validation loss:		1.379886
  validation accuracy:		47.07 %
Epoch 1328 of 2000 took 0.104s
  training loss:		1.374656
  validation loss:		1.390860
  validation accuracy:		47.83 %
Epoch 1329 of 2000 took 0.104s
  training loss:		1.367486
  validation loss:		1.376640
  validation accuracy:		49.02 %
Epoch 1330 of 2000 took 0.104s
  training loss:		1.388004
  validation loss:		1.371639
  validation accuracy:		48.15 %
Epoch 1331 of 2000 took 0.104s
  training loss:		1.338902
  validation loss:		1.368471
  validation accuracy:		48.26 %
Epoch 1332 of 2000 took 0.104s
  training loss:		1.330189
  validation loss:		1.357049
  validation accuracy:		50.22 %
Epoch 1333 of 2000 took 0.104s
  training loss:		1.332218
  validation loss:		1.359493
  validation accuracy:		50.22 %
Epoch 1334 of 2000 took 0.104s
  training loss:		1.352117
  validation loss:		1.354286
  validation accuracy:		50.54 %
Epoch 1335 of 2000 took 0.104s
  training loss:		1.336967
  validation loss:		1.352577
  validation accuracy:		49.57 %
Epoch 1336 of 2000 took 0.104s
  training loss:		1.345503
  validation loss:		1.378048
  validation accuracy:		49.89 %
Epoch 1337 of 2000 took 0.104s
  training loss:		1.353784
  validation loss:		1.364786
  validation accuracy:		48.48 %
Epoch 1338 of 2000 took 0.104s
  training loss:		1.375159
  validation loss:		1.389718
  validation accuracy:		48.15 %
Epoch 1339 of 2000 took 0.105s
  training loss:		1.332645
  validation loss:		1.377628
  validation accuracy:		50.11 %
Epoch 1340 of 2000 took 0.104s
  training loss:		1.362339
  validation loss:		1.419384
  validation accuracy:		46.41 %
Epoch 1341 of 2000 took 0.104s
  training loss:		1.351462
  validation loss:		1.350354
  validation accuracy:		50.11 %
Epoch 1342 of 2000 took 0.104s
  training loss:		1.363405
  validation loss:		1.403826
  validation accuracy:		47.93 %
Epoch 1343 of 2000 took 0.104s
  training loss:		1.444402
  validation loss:		1.382222
  validation accuracy:		48.37 %
Epoch 1344 of 2000 took 0.104s
  training loss:		1.343212
  validation loss:		1.361850
  validation accuracy:		48.91 %
Epoch 1345 of 2000 took 0.104s
  training loss:		1.349828
  validation loss:		1.375842
  validation accuracy:		48.80 %
Epoch 1346 of 2000 took 0.104s
  training loss:		1.323331
  validation loss:		1.350925
  validation accuracy:		49.35 %
Epoch 1347 of 2000 took 0.104s
  training loss:		1.334457
  validation loss:		1.378674
  validation accuracy:		48.91 %
Epoch 1348 of 2000 took 0.104s
  training loss:		1.421949
  validation loss:		1.508345
  validation accuracy:		41.63 %
Epoch 1349 of 2000 took 0.104s
  training loss:		1.360572
  validation loss:		1.359303
  validation accuracy:		51.30 %
Epoch 1350 of 2000 took 0.104s
  training loss:		1.352122
  validation loss:		1.351947
  validation accuracy:		50.33 %
Epoch 1351 of 2000 took 0.104s
  training loss:		1.356432
  validation loss:		1.352428
  validation accuracy:		49.24 %
Epoch 1352 of 2000 took 0.104s
  training loss:		1.342670
  validation loss:		1.409440
  validation accuracy:		44.78 %
Epoch 1353 of 2000 took 0.104s
  training loss:		1.341944
  validation loss:		1.396186
  validation accuracy:		46.96 %
Epoch 1354 of 2000 took 0.104s
  training loss:		1.372607
  validation loss:		1.472901
  validation accuracy:		43.15 %
Epoch 1355 of 2000 took 0.104s
  training loss:		1.387052
  validation loss:		1.419098
  validation accuracy:		45.33 %
Epoch 1356 of 2000 took 0.104s
  training loss:		1.349167
  validation loss:		1.348127
  validation accuracy:		50.22 %
Epoch 1357 of 2000 took 0.104s
  training loss:		1.346953
  validation loss:		1.502778
  validation accuracy:		41.52 %
Epoch 1358 of 2000 took 0.104s
  training loss:		1.377871
  validation loss:		1.346727
  validation accuracy:		50.22 %
Epoch 1359 of 2000 took 0.104s
  training loss:		1.371082
  validation loss:		1.350168
  validation accuracy:		49.89 %
Epoch 1360 of 2000 took 0.104s
  training loss:		1.337410
  validation loss:		1.351511
  validation accuracy:		49.02 %
Epoch 1361 of 2000 took 0.104s
  training loss:		1.326569
  validation loss:		1.370131
  validation accuracy:		48.04 %
Epoch 1362 of 2000 took 0.104s
  training loss:		1.330636
  validation loss:		1.336882
  validation accuracy:		49.89 %
Epoch 1363 of 2000 took 0.104s
  training loss:		1.322999
  validation loss:		1.354309
  validation accuracy:		48.37 %
Epoch 1364 of 2000 took 0.104s
  training loss:		1.374107
  validation loss:		1.351825
  validation accuracy:		49.02 %
Epoch 1365 of 2000 took 0.104s
  training loss:		1.370347
  validation loss:		1.425053
  validation accuracy:		44.57 %
Epoch 1366 of 2000 took 0.104s
  training loss:		1.338969
  validation loss:		1.364240
  validation accuracy:		48.26 %
Epoch 1367 of 2000 took 0.104s
  training loss:		1.326904
  validation loss:		1.342494
  validation accuracy:		49.67 %
Epoch 1368 of 2000 took 0.105s
  training loss:		1.327547
  validation loss:		1.370970
  validation accuracy:		48.48 %
Epoch 1369 of 2000 took 0.104s
  training loss:		1.345634
  validation loss:		1.346777
  validation accuracy:		50.22 %
Epoch 1370 of 2000 took 0.104s
  training loss:		1.329118
  validation loss:		1.363306
  validation accuracy:		48.70 %
Epoch 1371 of 2000 took 0.104s
  training loss:		1.348699
  validation loss:		1.429085
  validation accuracy:		44.24 %
Epoch 1372 of 2000 took 0.104s
  training loss:		1.327159
  validation loss:		1.351872
  validation accuracy:		49.24 %
Epoch 1373 of 2000 took 0.104s
  training loss:		1.341671
  validation loss:		1.360409
  validation accuracy:		48.91 %
Epoch 1374 of 2000 took 0.104s
  training loss:		1.318198
  validation loss:		1.353560
  validation accuracy:		49.24 %
Epoch 1375 of 2000 took 0.104s
  training loss:		1.339994
  validation loss:		1.349565
  validation accuracy:		49.67 %
Epoch 1376 of 2000 took 0.104s
  training loss:		1.320784
  validation loss:		1.343947
  validation accuracy:		49.46 %
Epoch 1377 of 2000 took 0.104s
  training loss:		1.330608
  validation loss:		1.373613
  validation accuracy:		47.39 %
Epoch 1378 of 2000 took 0.104s
  training loss:		1.360999
  validation loss:		1.357511
  validation accuracy:		49.02 %
Epoch 1379 of 2000 took 0.104s
  training loss:		1.397019
  validation loss:		1.465352
  validation accuracy:		42.50 %
Epoch 1380 of 2000 took 0.104s
  training loss:		1.329818
  validation loss:		1.361853
  validation accuracy:		48.26 %
Epoch 1381 of 2000 took 0.104s
  training loss:		1.327369
  validation loss:		1.343659
  validation accuracy:		50.33 %
Epoch 1382 of 2000 took 0.104s
  training loss:		1.318151
  validation loss:		1.345641
  validation accuracy:		49.35 %
Epoch 1383 of 2000 took 0.104s
  training loss:		1.340877
  validation loss:		1.346329
  validation accuracy:		49.78 %
Epoch 1384 of 2000 took 0.104s
  training loss:		1.331527
  validation loss:		1.357133
  validation accuracy:		48.48 %
Epoch 1385 of 2000 took 0.104s
  training loss:		1.327993
  validation loss:		1.349131
  validation accuracy:		48.91 %
Epoch 1386 of 2000 took 0.107s
  training loss:		1.319652
  validation loss:		1.348047
  validation accuracy:		49.78 %
Epoch 1387 of 2000 took 0.107s
  training loss:		1.320818
  validation loss:		1.355914
  validation accuracy:		48.59 %
Epoch 1388 of 2000 took 0.107s
  training loss:		1.338202
  validation loss:		1.348028
  validation accuracy:		49.35 %
Epoch 1389 of 2000 took 0.107s
  training loss:		1.330270
  validation loss:		1.347346
  validation accuracy:		50.00 %
Epoch 1390 of 2000 took 0.105s
  training loss:		1.325511
  validation loss:		1.399260
  validation accuracy:		45.54 %
Epoch 1391 of 2000 took 0.104s
  training loss:		1.514543
  validation loss:		1.444169
  validation accuracy:		43.80 %
Epoch 1392 of 2000 took 0.104s
  training loss:		1.332892
  validation loss:		1.354076
  validation accuracy:		49.67 %
Epoch 1393 of 2000 took 0.104s
  training loss:		1.328479
  validation loss:		1.349631
  validation accuracy:		48.70 %
Epoch 1394 of 2000 took 0.104s
  training loss:		1.321631
  validation loss:		1.347155
  validation accuracy:		48.48 %
Epoch 1395 of 2000 took 0.104s
  training loss:		1.324758
  validation loss:		1.352465
  validation accuracy:		47.93 %
Epoch 1396 of 2000 took 0.104s
  training loss:		1.327553
  validation loss:		1.340467
  validation accuracy:		49.67 %
Epoch 1397 of 2000 took 0.105s
  training loss:		1.326483
  validation loss:		1.344614
  validation accuracy:		49.24 %
Epoch 1398 of 2000 took 0.104s
  training loss:		1.324521
  validation loss:		1.338631
  validation accuracy:		49.57 %
Epoch 1399 of 2000 took 0.104s
  training loss:		1.377197
  validation loss:		1.363951
  validation accuracy:		48.48 %
Epoch 1400 of 2000 took 0.104s
  training loss:		1.333492
  validation loss:		1.426198
  validation accuracy:		44.46 %
Epoch 1401 of 2000 took 0.104s
  training loss:		1.342780
  validation loss:		1.349215
  validation accuracy:		48.70 %
Epoch 1402 of 2000 took 0.104s
  training loss:		1.336867
  validation loss:		1.430218
  validation accuracy:		44.13 %
Epoch 1403 of 2000 took 0.104s
  training loss:		1.359648
  validation loss:		1.357661
  validation accuracy:		47.83 %
Epoch 1404 of 2000 took 0.104s
  training loss:		1.329060
  validation loss:		1.345345
  validation accuracy:		48.80 %
Epoch 1405 of 2000 took 0.104s
  training loss:		1.325310
  validation loss:		1.339287
  validation accuracy:		49.02 %
Epoch 1406 of 2000 took 0.104s
  training loss:		1.333904
  validation loss:		1.388403
  validation accuracy:		44.78 %
Epoch 1407 of 2000 took 0.104s
  training loss:		1.320863
  validation loss:		1.344467
  validation accuracy:		48.59 %
Epoch 1408 of 2000 took 0.104s
  training loss:		1.335087
  validation loss:		1.349733
  validation accuracy:		49.67 %
Epoch 1409 of 2000 took 0.104s
  training loss:		1.318313
  validation loss:		1.345481
  validation accuracy:		48.70 %
Epoch 1410 of 2000 took 0.104s
  training loss:		1.323313
  validation loss:		1.358184
  validation accuracy:		48.26 %
Epoch 1411 of 2000 took 0.104s
  training loss:		1.321967
  validation loss:		1.344359
  validation accuracy:		48.59 %
Epoch 1412 of 2000 took 0.104s
  training loss:		1.311222
  validation loss:		1.345023
  validation accuracy:		48.70 %
Epoch 1413 of 2000 took 0.104s
  training loss:		1.316266
  validation loss:		1.357353
  validation accuracy:		47.83 %
Epoch 1414 of 2000 took 0.104s
  training loss:		1.336662
  validation loss:		1.347946
  validation accuracy:		47.72 %
Epoch 1415 of 2000 took 0.104s
  training loss:		1.325849
  validation loss:		1.374589
  validation accuracy:		47.07 %
Epoch 1416 of 2000 took 0.104s
  training loss:		1.338962
  validation loss:		1.387370
  validation accuracy:		45.87 %
Epoch 1417 of 2000 took 0.104s
  training loss:		1.319935
  validation loss:		1.341536
  validation accuracy:		49.02 %
Epoch 1418 of 2000 took 0.104s
  training loss:		1.315321
  validation loss:		1.352832
  validation accuracy:		48.26 %
Epoch 1419 of 2000 took 0.104s
  training loss:		1.322322
  validation loss:		1.352088
  validation accuracy:		47.93 %
Epoch 1420 of 2000 took 0.104s
  training loss:		1.343966
  validation loss:		1.436017
  validation accuracy:		42.50 %
Epoch 1421 of 2000 took 0.104s
  training loss:		1.361745
  validation loss:		1.359099
  validation accuracy:		48.48 %
Epoch 1422 of 2000 took 0.104s
  training loss:		1.338501
  validation loss:		1.344384
  validation accuracy:		48.70 %
Epoch 1423 of 2000 took 0.104s
  training loss:		1.337856
  validation loss:		1.353916
  validation accuracy:		46.09 %
Epoch 1424 of 2000 took 0.104s
  training loss:		1.318769
  validation loss:		1.371094
  validation accuracy:		45.76 %
Epoch 1425 of 2000 took 0.104s
  training loss:		1.338927
  validation loss:		1.392039
  validation accuracy:		45.76 %
Epoch 1426 of 2000 took 0.105s
  training loss:		1.349901
  validation loss:		1.352545
  validation accuracy:		47.39 %
Epoch 1427 of 2000 took 0.104s
  training loss:		1.320590
  validation loss:		1.358587
  validation accuracy:		47.83 %
Epoch 1428 of 2000 took 0.104s
  training loss:		1.327268
  validation loss:		1.350878
  validation accuracy:		48.91 %
Epoch 1429 of 2000 took 0.104s
  training loss:		1.320428
  validation loss:		1.339413
  validation accuracy:		48.80 %
Epoch 1430 of 2000 took 0.104s
  training loss:		1.335427
  validation loss:		1.404299
  validation accuracy:		44.13 %
Epoch 1431 of 2000 took 0.104s
  training loss:		1.338082
  validation loss:		1.345313
  validation accuracy:		47.50 %
Epoch 1432 of 2000 took 0.104s
  training loss:		1.312083
  validation loss:		1.347538
  validation accuracy:		48.04 %
Epoch 1433 of 2000 took 0.104s
  training loss:		1.321715
  validation loss:		1.344470
  validation accuracy:		49.24 %
Epoch 1434 of 2000 took 0.104s
  training loss:		1.349053
  validation loss:		1.356500
  validation accuracy:		47.17 %
Epoch 1435 of 2000 took 0.104s
  training loss:		1.327643
  validation loss:		1.344177
  validation accuracy:		48.04 %
Epoch 1436 of 2000 took 0.104s
  training loss:		1.321595
  validation loss:		1.378201
  validation accuracy:		46.74 %
Epoch 1437 of 2000 took 0.104s
  training loss:		1.345674
  validation loss:		1.351260
  validation accuracy:		48.04 %
Epoch 1438 of 2000 took 0.104s
  training loss:		1.326467
  validation loss:		1.345464
  validation accuracy:		48.37 %
Epoch 1439 of 2000 took 0.104s
  training loss:		1.319631
  validation loss:		1.360872
  validation accuracy:		47.50 %
Epoch 1440 of 2000 took 0.104s
  training loss:		1.348383
  validation loss:		1.347302
  validation accuracy:		47.61 %
Epoch 1441 of 2000 took 0.104s
  training loss:		1.322967
  validation loss:		1.354998
  validation accuracy:		46.74 %
Epoch 1442 of 2000 took 0.104s
  training loss:		1.377912
  validation loss:		1.363489
  validation accuracy:		47.72 %
Epoch 1443 of 2000 took 0.108s
  training loss:		1.323276
  validation loss:		1.357726
  validation accuracy:		47.07 %
Epoch 1444 of 2000 took 0.104s
  training loss:		1.322888
  validation loss:		1.364014
  validation accuracy:		46.20 %
Epoch 1445 of 2000 took 0.104s
  training loss:		1.327504
  validation loss:		1.343583
  validation accuracy:		47.83 %
Epoch 1446 of 2000 took 0.104s
  training loss:		1.312948
  validation loss:		1.342198
  validation accuracy:		47.93 %
Epoch 1447 of 2000 took 0.104s
  training loss:		1.315122
  validation loss:		1.341686
  validation accuracy:		48.26 %
Epoch 1448 of 2000 took 0.104s
  training loss:		1.321751
  validation loss:		1.341386
  validation accuracy:		47.61 %
Epoch 1449 of 2000 took 0.104s
  training loss:		1.323637
  validation loss:		1.361924
  validation accuracy:		47.50 %
Epoch 1450 of 2000 took 0.104s
  training loss:		1.316296
  validation loss:		1.341913
  validation accuracy:		47.28 %
Epoch 1451 of 2000 took 0.104s
  training loss:		1.317931
  validation loss:		1.345148
  validation accuracy:		47.83 %
Epoch 1452 of 2000 took 0.104s
  training loss:		1.327707
  validation loss:		1.342008
  validation accuracy:		47.50 %
Epoch 1453 of 2000 took 0.104s
  training loss:		1.310137
  validation loss:		1.355495
  validation accuracy:		47.17 %
Epoch 1454 of 2000 took 0.104s
  training loss:		1.335031
  validation loss:		1.346266
  validation accuracy:		47.61 %
Epoch 1455 of 2000 took 0.105s
  training loss:		1.334602
  validation loss:		1.341227
  validation accuracy:		47.39 %
Epoch 1456 of 2000 took 0.104s
  training loss:		1.316221
  validation loss:		1.339868
  validation accuracy:		47.50 %
Epoch 1457 of 2000 took 0.104s
  training loss:		1.303778
  validation loss:		1.349190
  validation accuracy:		47.61 %
Epoch 1458 of 2000 took 0.104s
  training loss:		1.313122
  validation loss:		1.340176
  validation accuracy:		46.52 %
Epoch 1459 of 2000 took 0.104s
  training loss:		1.321843
  validation loss:		1.340263
  validation accuracy:		47.50 %
Epoch 1460 of 2000 took 0.104s
  training loss:		1.326883
  validation loss:		1.341591
  validation accuracy:		47.50 %
Epoch 1461 of 2000 took 0.104s
  training loss:		1.313823
  validation loss:		1.343284
  validation accuracy:		46.96 %
Epoch 1462 of 2000 took 0.104s
  training loss:		1.325838
  validation loss:		1.343546
  validation accuracy:		47.17 %
Epoch 1463 of 2000 took 0.104s
  training loss:		1.342115
  validation loss:		1.344861
  validation accuracy:		46.74 %
Epoch 1464 of 2000 took 0.104s
  training loss:		1.320227
  validation loss:		1.350037
  validation accuracy:		45.98 %
Epoch 1465 of 2000 took 0.104s
  training loss:		1.316771
  validation loss:		1.339086
  validation accuracy:		47.07 %
Epoch 1466 of 2000 took 0.104s
  training loss:		1.315133
  validation loss:		1.361046
  validation accuracy:		45.65 %
Epoch 1467 of 2000 took 0.104s
  training loss:		1.324094
  validation loss:		1.340774
  validation accuracy:		46.30 %
Epoch 1468 of 2000 took 0.104s
  training loss:		1.314916
  validation loss:		1.356010
  validation accuracy:		45.98 %
Epoch 1469 of 2000 took 0.104s
  training loss:		1.316288
  validation loss:		1.340215
  validation accuracy:		46.96 %
Epoch 1470 of 2000 took 0.104s
  training loss:		1.315305
  validation loss:		1.343433
  validation accuracy:		46.20 %
Epoch 1471 of 2000 took 0.104s
  training loss:		1.329959
  validation loss:		1.352641
  validation accuracy:		45.87 %
Epoch 1472 of 2000 took 0.104s
  training loss:		1.320199
  validation loss:		1.349512
  validation accuracy:		45.33 %
Epoch 1473 of 2000 took 0.104s
  training loss:		1.327855
  validation loss:		1.336816
  validation accuracy:		47.39 %
Epoch 1474 of 2000 took 0.104s
  training loss:		1.313717
  validation loss:		1.348548
  validation accuracy:		46.74 %
Epoch 1475 of 2000 took 0.104s
  training loss:		1.313481
  validation loss:		1.344359
  validation accuracy:		45.76 %
Epoch 1476 of 2000 took 0.104s
  training loss:		1.310776
  validation loss:		1.343156
  validation accuracy:		46.63 %
Epoch 1477 of 2000 took 0.104s
  training loss:		1.312345
  validation loss:		1.335071
  validation accuracy:		46.52 %
Epoch 1478 of 2000 took 0.104s
  training loss:		1.323601
  validation loss:		1.365132
  validation accuracy:		45.65 %
Epoch 1479 of 2000 took 0.104s
  training loss:		1.348349
  validation loss:		1.401966
  validation accuracy:		44.57 %
Epoch 1480 of 2000 took 0.104s
  training loss:		1.368244
  validation loss:		1.344498
  validation accuracy:		45.22 %
Epoch 1481 of 2000 took 0.104s
  training loss:		1.332546
  validation loss:		1.349782
  validation accuracy:		46.52 %
Epoch 1482 of 2000 took 0.104s
  training loss:		1.321812
  validation loss:		1.341222
  validation accuracy:		46.52 %
Epoch 1483 of 2000 took 0.104s
  training loss:		1.309347
  validation loss:		1.338083
  validation accuracy:		45.54 %
Epoch 1484 of 2000 took 0.105s
  training loss:		1.327489
  validation loss:		1.346935
  validation accuracy:		46.41 %
Epoch 1485 of 2000 took 0.104s
  training loss:		1.316929
  validation loss:		1.356237
  validation accuracy:		45.87 %
Epoch 1486 of 2000 took 0.104s
  training loss:		1.316071
  validation loss:		1.347973
  validation accuracy:		45.54 %
Epoch 1487 of 2000 took 0.104s
  training loss:		1.314970
  validation loss:		1.340531
  validation accuracy:		45.33 %
Epoch 1488 of 2000 took 0.104s
  training loss:		1.317231
  validation loss:		1.337833
  validation accuracy:		46.09 %
Epoch 1489 of 2000 took 0.104s
  training loss:		1.334530
  validation loss:		1.340516
  validation accuracy:		45.87 %
Epoch 1490 of 2000 took 0.104s
  training loss:		1.314923
  validation loss:		1.345172
  validation accuracy:		45.76 %
Epoch 1491 of 2000 took 0.104s
  training loss:		1.340206
  validation loss:		1.406216
  validation accuracy:		43.91 %
Epoch 1492 of 2000 took 0.104s
  training loss:		1.334311
  validation loss:		1.361196
  validation accuracy:		46.09 %
Epoch 1493 of 2000 took 0.104s
  training loss:		1.323051
  validation loss:		1.357117
  validation accuracy:		46.09 %
Epoch 1494 of 2000 took 0.104s
  training loss:		1.322576
  validation loss:		1.367245
  validation accuracy:		45.33 %
Epoch 1495 of 2000 took 0.104s
  training loss:		1.320819
  validation loss:		1.338725
  validation accuracy:		45.98 %
Epoch 1496 of 2000 took 0.104s
  training loss:		1.310127
  validation loss:		1.333796
  validation accuracy:		45.87 %
Epoch 1497 of 2000 took 0.104s
  training loss:		1.323196
  validation loss:		1.359367
  validation accuracy:		45.76 %
Epoch 1498 of 2000 took 0.104s
  training loss:		1.323101
  validation loss:		1.339748
  validation accuracy:		45.87 %
Epoch 1499 of 2000 took 0.104s
  training loss:		1.348562
  validation loss:		1.356971
  validation accuracy:		45.87 %
Epoch 1500 of 2000 took 0.104s
  training loss:		1.315784
  validation loss:		1.343086
  validation accuracy:		45.98 %
Epoch 1501 of 2000 took 0.104s
  training loss:		1.320935
  validation loss:		1.367397
  validation accuracy:		45.11 %
Epoch 1502 of 2000 took 0.104s
  training loss:		1.331722
  validation loss:		1.377769
  validation accuracy:		44.67 %
Epoch 1503 of 2000 took 0.104s
  training loss:		1.346584
  validation loss:		1.345808
  validation accuracy:		45.65 %
Epoch 1504 of 2000 took 0.104s
  training loss:		1.322038
  validation loss:		1.340913
  validation accuracy:		46.09 %
Epoch 1505 of 2000 took 0.104s
  training loss:		1.315591
  validation loss:		1.349478
  validation accuracy:		46.09 %
Epoch 1506 of 2000 took 0.104s
  training loss:		1.333966
  validation loss:		1.338514
  validation accuracy:		45.54 %
Epoch 1507 of 2000 took 0.104s
  training loss:		1.324504
  validation loss:		1.335721
  validation accuracy:		46.85 %
Epoch 1508 of 2000 took 0.104s
  training loss:		1.308639
  validation loss:		1.337103
  validation accuracy:		46.30 %
Epoch 1509 of 2000 took 0.104s
  training loss:		1.304668
  validation loss:		1.343368
  validation accuracy:		45.54 %
Epoch 1510 of 2000 took 0.104s
  training loss:		1.314436
  validation loss:		1.369872
  validation accuracy:		46.30 %
Epoch 1511 of 2000 took 0.104s
  training loss:		1.316310
  validation loss:		1.342276
  validation accuracy:		45.76 %
Epoch 1512 of 2000 took 0.104s
  training loss:		1.322190
  validation loss:		1.338451
  validation accuracy:		46.30 %
Epoch 1513 of 2000 took 0.105s
  training loss:		1.320854
  validation loss:		1.349427
  validation accuracy:		46.52 %
Epoch 1514 of 2000 took 0.104s
  training loss:		1.337255
  validation loss:		1.351624
  validation accuracy:		45.65 %
Epoch 1515 of 2000 took 0.104s
  training loss:		1.321495
  validation loss:		1.337366
  validation accuracy:		46.41 %
Epoch 1516 of 2000 took 0.104s
  training loss:		1.310019
  validation loss:		1.349828
  validation accuracy:		46.09 %
Epoch 1517 of 2000 took 0.104s
  training loss:		1.319957
  validation loss:		1.367277
  validation accuracy:		45.87 %
Epoch 1518 of 2000 took 0.104s
  training loss:		1.313807
  validation loss:		1.352650
  validation accuracy:		45.00 %
Epoch 1519 of 2000 took 0.104s
  training loss:		1.313553
  validation loss:		1.339371
  validation accuracy:		45.33 %
Epoch 1520 of 2000 took 0.104s
  training loss:		1.323368
  validation loss:		1.353522
  validation accuracy:		46.74 %
Epoch 1521 of 2000 took 0.104s
  training loss:		1.321989
  validation loss:		1.345953
  validation accuracy:		47.50 %
Epoch 1522 of 2000 took 0.104s
  training loss:		1.319963
  validation loss:		1.339559
  validation accuracy:		47.61 %
Epoch 1523 of 2000 took 0.104s
  training loss:		1.316454
  validation loss:		1.337375
  validation accuracy:		46.74 %
Epoch 1524 of 2000 took 0.104s
  training loss:		1.318561
  validation loss:		1.334631
  validation accuracy:		47.17 %
Epoch 1525 of 2000 took 0.104s
  training loss:		1.315300
  validation loss:		1.362638
  validation accuracy:		46.63 %
Epoch 1526 of 2000 took 0.104s
  training loss:		1.330831
  validation loss:		1.337028
  validation accuracy:		46.96 %
Epoch 1527 of 2000 took 0.104s
  training loss:		1.313485
  validation loss:		1.332691
  validation accuracy:		47.28 %
Epoch 1528 of 2000 took 0.104s
  training loss:		1.316145
  validation loss:		1.337294
  validation accuracy:		47.39 %
Epoch 1529 of 2000 took 0.104s
  training loss:		1.326213
  validation loss:		1.372780
  validation accuracy:		46.85 %
Epoch 1530 of 2000 took 0.104s
  training loss:		1.317365
  validation loss:		1.342303
  validation accuracy:		46.96 %
Epoch 1531 of 2000 took 0.104s
  training loss:		1.307412
  validation loss:		1.334827
  validation accuracy:		47.83 %
Epoch 1532 of 2000 took 0.104s
  training loss:		1.327317
  validation loss:		1.356699
  validation accuracy:		46.74 %
Epoch 1533 of 2000 took 0.104s
  training loss:		1.315756
  validation loss:		1.335497
  validation accuracy:		48.26 %
Epoch 1534 of 2000 took 0.104s
  training loss:		1.314243
  validation loss:		1.380324
  validation accuracy:		47.50 %
Epoch 1535 of 2000 took 0.104s
  training loss:		1.313502
  validation loss:		1.330205
  validation accuracy:		46.52 %
Epoch 1536 of 2000 took 0.104s
  training loss:		1.318074
  validation loss:		1.338906
  validation accuracy:		46.74 %
Epoch 1537 of 2000 took 0.104s
  training loss:		1.310923
  validation loss:		1.337632
  validation accuracy:		45.98 %
Epoch 1538 of 2000 took 0.104s
  training loss:		1.312444
  validation loss:		1.329616
  validation accuracy:		48.04 %
Epoch 1539 of 2000 took 0.104s
  training loss:		1.305831
  validation loss:		1.329165
  validation accuracy:		48.15 %
Epoch 1540 of 2000 took 0.104s
  training loss:		1.308049
  validation loss:		1.349356
  validation accuracy:		46.96 %
Epoch 1541 of 2000 took 0.104s
  training loss:		1.315056
  validation loss:		1.339264
  validation accuracy:		47.39 %
Epoch 1542 of 2000 took 0.105s
  training loss:		1.339525
  validation loss:		1.334901
  validation accuracy:		47.39 %
Epoch 1543 of 2000 took 0.104s
  training loss:		1.305376
  validation loss:		1.374547
  validation accuracy:		46.96 %
Epoch 1544 of 2000 took 0.104s
  training loss:		1.311109
  validation loss:		1.333997
  validation accuracy:		47.72 %
Epoch 1545 of 2000 took 0.104s
  training loss:		1.304095
  validation loss:		1.346106
  validation accuracy:		46.52 %
Epoch 1546 of 2000 took 0.104s
  training loss:		1.312141
  validation loss:		1.341994
  validation accuracy:		48.26 %
Epoch 1547 of 2000 took 0.104s
  training loss:		1.311945
  validation loss:		1.331575
  validation accuracy:		49.35 %
Epoch 1548 of 2000 took 0.104s
  training loss:		1.305837
  validation loss:		1.337170
  validation accuracy:		48.15 %
Epoch 1549 of 2000 took 0.104s
  training loss:		1.348725
  validation loss:		1.353236
  validation accuracy:		47.93 %
Epoch 1550 of 2000 took 0.104s
  training loss:		1.315274
  validation loss:		1.339594
  validation accuracy:		49.24 %
Epoch 1551 of 2000 took 0.104s
  training loss:		1.303440
  validation loss:		1.335092
  validation accuracy:		50.00 %
Epoch 1552 of 2000 took 0.104s
  training loss:		1.307311
  validation loss:		1.331401
  validation accuracy:		47.93 %
Epoch 1553 of 2000 took 0.104s
  training loss:		1.311134
  validation loss:		1.336066
  validation accuracy:		49.02 %
Epoch 1554 of 2000 took 0.104s
  training loss:		1.309150
  validation loss:		1.337628
  validation accuracy:		48.91 %
Epoch 1555 of 2000 took 0.104s
  training loss:		1.306181
  validation loss:		1.345803
  validation accuracy:		47.93 %
Epoch 1556 of 2000 took 0.104s
  training loss:		1.319550
  validation loss:		1.337352
  validation accuracy:		48.91 %
Epoch 1557 of 2000 took 0.104s
  training loss:		1.305733
  validation loss:		1.329236
  validation accuracy:		50.22 %
Epoch 1558 of 2000 took 0.104s
  training loss:		1.309572
  validation loss:		1.333707
  validation accuracy:		49.35 %
Epoch 1559 of 2000 took 0.104s
  training loss:		1.315105
  validation loss:		1.333496
  validation accuracy:		48.80 %
Epoch 1560 of 2000 took 0.104s
  training loss:		1.323682
  validation loss:		1.325983
  validation accuracy:		50.43 %
Epoch 1561 of 2000 took 0.104s
  training loss:		1.301302
  validation loss:		1.324834
  validation accuracy:		50.43 %
Epoch 1562 of 2000 took 0.104s
  training loss:		1.313192
  validation loss:		1.335546
  validation accuracy:		49.57 %
Epoch 1563 of 2000 took 0.104s
  training loss:		1.310181
  validation loss:		1.335201
  validation accuracy:		50.11 %
Epoch 1564 of 2000 took 0.104s
  training loss:		1.320211
  validation loss:		1.332843
  validation accuracy:		49.78 %
Epoch 1565 of 2000 took 0.104s
  training loss:		1.327477
  validation loss:		1.352622
  validation accuracy:		48.80 %
Epoch 1566 of 2000 took 0.104s
  training loss:		1.309460
  validation loss:		1.318722
  validation accuracy:		51.63 %
Epoch 1567 of 2000 took 0.104s
  training loss:		1.294666
  validation loss:		1.316632
  validation accuracy:		51.74 %
Epoch 1568 of 2000 took 0.104s
  training loss:		1.297756
  validation loss:		1.322650
  validation accuracy:		50.00 %
Epoch 1569 of 2000 took 0.104s
  training loss:		1.313454
  validation loss:		1.326071
  validation accuracy:		50.11 %
Epoch 1570 of 2000 took 0.104s
  training loss:		1.303804
  validation loss:		1.323935
  validation accuracy:		51.74 %
Epoch 1571 of 2000 took 0.105s
  training loss:		1.294032
  validation loss:		1.336117
  validation accuracy:		50.87 %
Epoch 1572 of 2000 took 0.104s
  training loss:		1.301442
  validation loss:		1.337709
  validation accuracy:		50.00 %
Epoch 1573 of 2000 took 0.104s
  training loss:		1.320304
  validation loss:		1.339631
  validation accuracy:		49.67 %
Epoch 1574 of 2000 took 0.104s
  training loss:		1.311783
  validation loss:		1.336402
  validation accuracy:		49.57 %
Epoch 1575 of 2000 took 0.104s
  training loss:		1.298864
  validation loss:		1.337548
  validation accuracy:		49.67 %
Epoch 1576 of 2000 took 0.104s
  training loss:		1.299909
  validation loss:		1.315016
  validation accuracy:		50.87 %
Epoch 1577 of 2000 took 0.104s
  training loss:		1.294300
  validation loss:		1.313778
  validation accuracy:		52.07 %
Epoch 1578 of 2000 took 0.104s
  training loss:		1.312908
  validation loss:		1.318575
  validation accuracy:		51.20 %
Epoch 1579 of 2000 took 0.107s
  training loss:		1.303066
  validation loss:		1.318113
  validation accuracy:		51.30 %
Epoch 1580 of 2000 took 0.106s
  training loss:		1.299683
  validation loss:		1.313754
  validation accuracy:		50.87 %
Epoch 1581 of 2000 took 0.104s
  training loss:		1.294136
  validation loss:		1.311730
  validation accuracy:		52.50 %
Epoch 1582 of 2000 took 0.104s
  training loss:		1.293354
  validation loss:		1.316767
  validation accuracy:		51.41 %
Epoch 1583 of 2000 took 0.104s
  training loss:		1.291994
  validation loss:		1.311545
  validation accuracy:		51.74 %
Epoch 1584 of 2000 took 0.109s
  training loss:		1.292907
  validation loss:		1.313000
  validation accuracy:		51.96 %
Epoch 1585 of 2000 took 0.104s
  training loss:		1.291157
  validation loss:		1.337258
  validation accuracy:		49.78 %
Epoch 1586 of 2000 took 0.104s
  training loss:		1.299321
  validation loss:		1.318897
  validation accuracy:		50.54 %
Epoch 1587 of 2000 took 0.104s
  training loss:		1.293277
  validation loss:		1.307662
  validation accuracy:		52.07 %
Epoch 1588 of 2000 took 0.104s
  training loss:		1.302605
  validation loss:		1.311962
  validation accuracy:		50.98 %
Epoch 1589 of 2000 took 0.104s
  training loss:		1.293874
  validation loss:		1.314346
  validation accuracy:		52.61 %
Epoch 1590 of 2000 took 0.104s
  training loss:		1.289443
  validation loss:		1.309334
  validation accuracy:		51.20 %
Epoch 1591 of 2000 took 0.104s
  training loss:		1.297486
  validation loss:		1.315065
  validation accuracy:		50.98 %
Epoch 1592 of 2000 took 0.104s
  training loss:		1.289147
  validation loss:		1.304750
  validation accuracy:		53.37 %
Epoch 1593 of 2000 took 0.104s
  training loss:		1.293877
  validation loss:		1.326065
  validation accuracy:		50.43 %
Epoch 1594 of 2000 took 0.104s
  training loss:		1.297726
  validation loss:		1.343598
  validation accuracy:		49.78 %
Epoch 1595 of 2000 took 0.104s
  training loss:		1.298030
  validation loss:		1.306265
  validation accuracy:		51.52 %
Epoch 1596 of 2000 took 0.104s
  training loss:		1.288783
  validation loss:		1.308669
  validation accuracy:		52.72 %
Epoch 1597 of 2000 took 0.104s
  training loss:		1.318429
  validation loss:		1.308867
  validation accuracy:		53.04 %
Epoch 1598 of 2000 took 0.104s
  training loss:		1.292501
  validation loss:		1.300406
  validation accuracy:		53.37 %
Epoch 1599 of 2000 took 0.104s
  training loss:		1.291101
  validation loss:		1.316639
  validation accuracy:		50.65 %
Epoch 1600 of 2000 took 0.105s
  training loss:		1.308511
  validation loss:		1.301684
  validation accuracy:		54.02 %
Epoch 1601 of 2000 took 0.104s
  training loss:		1.298436
  validation loss:		1.296017
  validation accuracy:		54.46 %
Epoch 1602 of 2000 took 0.104s
  training loss:		1.279661
  validation loss:		1.315136
  validation accuracy:		51.41 %
Epoch 1603 of 2000 took 0.104s
  training loss:		1.306583
  validation loss:		1.298582
  validation accuracy:		53.37 %
Epoch 1604 of 2000 took 0.104s
  training loss:		1.280705
  validation loss:		1.314688
  validation accuracy:		51.52 %
Epoch 1605 of 2000 took 0.104s
  training loss:		1.284193
  validation loss:		1.314481
  validation accuracy:		50.33 %
Epoch 1606 of 2000 took 0.104s
  training loss:		1.287941
  validation loss:		1.298788
  validation accuracy:		52.39 %
Epoch 1607 of 2000 took 0.104s
  training loss:		1.284516
  validation loss:		1.304938
  validation accuracy:		53.48 %
Epoch 1608 of 2000 took 0.104s
  training loss:		1.290359
  validation loss:		1.296212
  validation accuracy:		51.85 %
Epoch 1609 of 2000 took 0.104s
  training loss:		1.284858
  validation loss:		1.297210
  validation accuracy:		52.50 %
Epoch 1610 of 2000 took 0.104s
  training loss:		1.289857
  validation loss:		1.299741
  validation accuracy:		52.50 %
Epoch 1611 of 2000 took 0.104s
  training loss:		1.290675
  validation loss:		1.293206
  validation accuracy:		52.17 %
Epoch 1612 of 2000 took 0.104s
  training loss:		1.287065
  validation loss:		1.303016
  validation accuracy:		51.20 %
Epoch 1613 of 2000 took 0.104s
  training loss:		1.302029
  validation loss:		1.375505
  validation accuracy:		46.52 %
Epoch 1614 of 2000 took 0.104s
  training loss:		1.308594
  validation loss:		1.304451
  validation accuracy:		52.50 %
Epoch 1615 of 2000 took 0.104s
  training loss:		1.286792
  validation loss:		1.295717
  validation accuracy:		52.28 %
Epoch 1616 of 2000 took 0.104s
  training loss:		1.287801
  validation loss:		1.296499
  validation accuracy:		52.61 %
Epoch 1617 of 2000 took 0.104s
  training loss:		1.292980
  validation loss:		1.301241
  validation accuracy:		53.26 %
Epoch 1618 of 2000 took 0.104s
  training loss:		1.277702
  validation loss:		1.301706
  validation accuracy:		50.87 %
Epoch 1619 of 2000 took 0.104s
  training loss:		1.290163
  validation loss:		1.296378
  validation accuracy:		51.96 %
Epoch 1620 of 2000 took 0.104s
  training loss:		1.284015
  validation loss:		1.300353
  validation accuracy:		52.93 %
Epoch 1621 of 2000 took 0.104s
  training loss:		1.286355
  validation loss:		1.307999
  validation accuracy:		50.00 %
Epoch 1622 of 2000 took 0.104s
  training loss:		1.295310
  validation loss:		1.303124
  validation accuracy:		53.37 %
Epoch 1623 of 2000 took 0.104s
  training loss:		1.289492
  validation loss:		1.293082
  validation accuracy:		54.02 %
Epoch 1624 of 2000 took 0.104s
  training loss:		1.289546
  validation loss:		1.291429
  validation accuracy:		53.59 %
Epoch 1625 of 2000 took 0.104s
  training loss:		1.288571
  validation loss:		1.291687
  validation accuracy:		52.83 %
Epoch 1626 of 2000 took 0.104s
  training loss:		1.291783
  validation loss:		1.302470
  validation accuracy:		50.87 %
Epoch 1627 of 2000 took 0.104s
  training loss:		1.285701
  validation loss:		1.314920
  validation accuracy:		50.11 %
Epoch 1628 of 2000 took 0.104s
  training loss:		1.284569
  validation loss:		1.295464
  validation accuracy:		53.91 %
Epoch 1629 of 2000 took 0.105s
  training loss:		1.280898
  validation loss:		1.296943
  validation accuracy:		51.74 %
Epoch 1630 of 2000 took 0.104s
  training loss:		1.292956
  validation loss:		1.304605
  validation accuracy:		50.00 %
Epoch 1631 of 2000 took 0.104s
  training loss:		1.283552
  validation loss:		1.291733
  validation accuracy:		52.93 %
Epoch 1632 of 2000 took 0.104s
  training loss:		1.280544
  validation loss:		1.297397
  validation accuracy:		52.72 %
Epoch 1633 of 2000 took 0.104s
  training loss:		1.287251
  validation loss:		1.289234
  validation accuracy:		53.70 %
Epoch 1634 of 2000 took 0.104s
  training loss:		1.279692
  validation loss:		1.310196
  validation accuracy:		50.00 %
Epoch 1635 of 2000 took 0.104s
  training loss:		1.291529
  validation loss:		1.293328
  validation accuracy:		52.07 %
Epoch 1636 of 2000 took 0.104s
  training loss:		1.279042
  validation loss:		1.294315
  validation accuracy:		53.91 %
Epoch 1637 of 2000 took 0.104s
  training loss:		1.276208
  validation loss:		1.294925
  validation accuracy:		51.30 %
Epoch 1638 of 2000 took 0.104s
  training loss:		1.299239
  validation loss:		1.329894
  validation accuracy:		49.13 %
Epoch 1639 of 2000 took 0.104s
  training loss:		1.283410
  validation loss:		1.297652
  validation accuracy:		51.96 %
Epoch 1640 of 2000 took 0.104s
  training loss:		1.286055
  validation loss:		1.298325
  validation accuracy:		51.85 %
Epoch 1641 of 2000 took 0.104s
  training loss:		1.280646
  validation loss:		1.310160
  validation accuracy:		52.50 %
Epoch 1642 of 2000 took 0.104s
  training loss:		1.297859
  validation loss:		1.288864
  validation accuracy:		52.28 %
Epoch 1643 of 2000 took 0.104s
  training loss:		1.285397
  validation loss:		1.297817
  validation accuracy:		53.04 %
Epoch 1644 of 2000 took 0.104s
  training loss:		1.296219
  validation loss:		1.298899
  validation accuracy:		51.63 %
Epoch 1645 of 2000 took 0.104s
  training loss:		1.283453
  validation loss:		1.294650
  validation accuracy:		51.85 %
Epoch 1646 of 2000 took 0.104s
  training loss:		1.295889
  validation loss:		1.297207
  validation accuracy:		51.85 %
Epoch 1647 of 2000 took 0.104s
  training loss:		1.279676
  validation loss:		1.324174
  validation accuracy:		51.74 %
Epoch 1648 of 2000 took 0.104s
  training loss:		1.298239
  validation loss:		1.308796
  validation accuracy:		52.28 %
Epoch 1649 of 2000 took 0.104s
  training loss:		1.285021
  validation loss:		1.298377
  validation accuracy:		51.41 %
Epoch 1650 of 2000 took 0.104s
  training loss:		1.286001
  validation loss:		1.292815
  validation accuracy:		52.39 %
Epoch 1651 of 2000 took 0.104s
  training loss:		1.289139
  validation loss:		1.302275
  validation accuracy:		53.37 %
Epoch 1652 of 2000 took 0.107s
  training loss:		1.287261
  validation loss:		1.291065
  validation accuracy:		51.52 %
Epoch 1653 of 2000 took 0.107s
  training loss:		1.277964
  validation loss:		1.290152
  validation accuracy:		52.83 %
Epoch 1654 of 2000 took 0.107s
  training loss:		1.283262
  validation loss:		1.298211
  validation accuracy:		53.04 %
Epoch 1655 of 2000 took 0.107s
  training loss:		1.288645
  validation loss:		1.308443
  validation accuracy:		51.52 %
Epoch 1656 of 2000 took 0.107s
  training loss:		1.279919
  validation loss:		1.284838
  validation accuracy:		53.15 %
Epoch 1657 of 2000 took 0.107s
  training loss:		1.280024
  validation loss:		1.288716
  validation accuracy:		52.61 %
Epoch 1658 of 2000 took 0.104s
  training loss:		1.285608
  validation loss:		1.310820
  validation accuracy:		50.22 %
Epoch 1659 of 2000 took 0.104s
  training loss:		1.280633
  validation loss:		1.285286
  validation accuracy:		52.93 %
Epoch 1660 of 2000 took 0.104s
  training loss:		1.282868
  validation loss:		1.298496
  validation accuracy:		52.83 %
Epoch 1661 of 2000 took 0.105s
  training loss:		1.281402
  validation loss:		1.306064
  validation accuracy:		51.30 %
Epoch 1662 of 2000 took 0.104s
  training loss:		1.280892
  validation loss:		1.317083
  validation accuracy:		52.50 %
Epoch 1663 of 2000 took 0.104s
  training loss:		1.289103
  validation loss:		1.296149
  validation accuracy:		51.41 %
Epoch 1664 of 2000 took 0.104s
  training loss:		1.308722
  validation loss:		1.303457
  validation accuracy:		51.30 %
Epoch 1665 of 2000 took 0.104s
  training loss:		1.278993
  validation loss:		1.296606
  validation accuracy:		52.72 %
Epoch 1666 of 2000 took 0.104s
  training loss:		1.284697
  validation loss:		1.295410
  validation accuracy:		51.63 %
Epoch 1667 of 2000 took 0.104s
  training loss:		1.287205
  validation loss:		1.286125
  validation accuracy:		53.37 %
Epoch 1668 of 2000 took 0.104s
  training loss:		1.298169
  validation loss:		1.286644
  validation accuracy:		53.15 %
Epoch 1669 of 2000 took 0.104s
  training loss:		1.274403
  validation loss:		1.290015
  validation accuracy:		52.07 %
Epoch 1670 of 2000 took 0.104s
  training loss:		1.287680
  validation loss:		1.301481
  validation accuracy:		50.98 %
Epoch 1671 of 2000 took 0.104s
  training loss:		1.287871
  validation loss:		1.286945
  validation accuracy:		53.59 %
Epoch 1672 of 2000 took 0.104s
  training loss:		1.280478
  validation loss:		1.288431
  validation accuracy:		53.48 %
Epoch 1673 of 2000 took 0.104s
  training loss:		1.276853
  validation loss:		1.284708
  validation accuracy:		54.13 %
Epoch 1674 of 2000 took 0.104s
  training loss:		1.277255
  validation loss:		1.287124
  validation accuracy:		52.83 %
Epoch 1675 of 2000 took 0.104s
  training loss:		1.290947
  validation loss:		1.289730
  validation accuracy:		53.04 %
Epoch 1676 of 2000 took 0.104s
  training loss:		1.287076
  validation loss:		1.286882
  validation accuracy:		53.70 %
Epoch 1677 of 2000 took 0.104s
  training loss:		1.295182
  validation loss:		1.287164
  validation accuracy:		53.48 %
Epoch 1678 of 2000 took 0.104s
  training loss:		1.298858
  validation loss:		1.293130
  validation accuracy:		53.04 %
Epoch 1679 of 2000 took 0.104s
  training loss:		1.285330
  validation loss:		1.285361
  validation accuracy:		53.04 %
Epoch 1680 of 2000 took 0.104s
  training loss:		1.291653
  validation loss:		1.289773
  validation accuracy:		52.50 %
Epoch 1681 of 2000 took 0.104s
  training loss:		1.285868
  validation loss:		1.309419
  validation accuracy:		53.04 %
Epoch 1682 of 2000 took 0.104s
  training loss:		1.282515
  validation loss:		1.289613
  validation accuracy:		53.59 %
Epoch 1683 of 2000 took 0.104s
  training loss:		1.279011
  validation loss:		1.287983
  validation accuracy:		53.15 %
Epoch 1684 of 2000 took 0.104s
  training loss:		1.280097
  validation loss:		1.282746
  validation accuracy:		53.48 %
Epoch 1685 of 2000 took 0.104s
  training loss:		1.272575
  validation loss:		1.288335
  validation accuracy:		52.17 %
Epoch 1686 of 2000 took 0.104s
  training loss:		1.283000
  validation loss:		1.309029
  validation accuracy:		50.33 %
Epoch 1687 of 2000 took 0.104s
  training loss:		1.293275
  validation loss:		1.324048
  validation accuracy:		50.00 %
Epoch 1688 of 2000 took 0.104s
  training loss:		1.292757
  validation loss:		1.290750
  validation accuracy:		52.50 %
Epoch 1689 of 2000 took 0.104s
  training loss:		1.287399
  validation loss:		1.297911
  validation accuracy:		53.15 %
Epoch 1690 of 2000 took 0.104s
  training loss:		1.288100
  validation loss:		1.291493
  validation accuracy:		52.07 %
Epoch 1691 of 2000 took 0.104s
  training loss:		1.286270
  validation loss:		1.285539
  validation accuracy:		53.70 %
Epoch 1692 of 2000 took 0.104s
  training loss:		1.284003
  validation loss:		1.289782
  validation accuracy:		53.26 %
Epoch 1693 of 2000 took 0.105s
  training loss:		1.281522
  validation loss:		1.294326
  validation accuracy:		53.15 %
Epoch 1694 of 2000 took 0.104s
  training loss:		1.287015
  validation loss:		1.310294
  validation accuracy:		51.20 %
Epoch 1695 of 2000 took 0.104s
  training loss:		1.299344
  validation loss:		1.285840
  validation accuracy:		52.83 %
Epoch 1696 of 2000 took 0.104s
  training loss:		1.287991
  validation loss:		1.291372
  validation accuracy:		52.72 %
Epoch 1697 of 2000 took 0.104s
  training loss:		1.288841
  validation loss:		1.289122
  validation accuracy:		53.15 %
Epoch 1698 of 2000 took 0.104s
  training loss:		1.280868
  validation loss:		1.298131
  validation accuracy:		51.96 %
Epoch 1699 of 2000 took 0.104s
  training loss:		1.281649
  validation loss:		1.287470
  validation accuracy:		53.91 %
Epoch 1700 of 2000 took 0.104s
  training loss:		1.269386
  validation loss:		1.296914
  validation accuracy:		53.70 %
Epoch 1701 of 2000 took 0.104s
  training loss:		1.302756
  validation loss:		1.341306
  validation accuracy:		51.30 %
Epoch 1702 of 2000 took 0.104s
  training loss:		1.294002
  validation loss:		1.299084
  validation accuracy:		52.72 %
Epoch 1703 of 2000 took 0.104s
  training loss:		1.280442
  validation loss:		1.284833
  validation accuracy:		53.59 %
Epoch 1704 of 2000 took 0.104s
  training loss:		1.282551
  validation loss:		1.289809
  validation accuracy:		51.30 %
Epoch 1705 of 2000 took 0.104s
  training loss:		1.278321
  validation loss:		1.350167
  validation accuracy:		48.26 %
Epoch 1706 of 2000 took 0.104s
  training loss:		1.293030
  validation loss:		1.291142
  validation accuracy:		51.96 %
Epoch 1707 of 2000 took 0.104s
  training loss:		1.279668
  validation loss:		1.284384
  validation accuracy:		53.70 %
Epoch 1708 of 2000 took 0.104s
  training loss:		1.287109
  validation loss:		1.296408
  validation accuracy:		52.61 %
Epoch 1709 of 2000 took 0.104s
  training loss:		1.282898
  validation loss:		1.289635
  validation accuracy:		52.93 %
Epoch 1710 of 2000 took 0.104s
  training loss:		1.311271
  validation loss:		1.290047
  validation accuracy:		53.15 %
Epoch 1711 of 2000 took 0.104s
  training loss:		1.274411
  validation loss:		1.290293
  validation accuracy:		52.83 %
Epoch 1712 of 2000 took 0.104s
  training loss:		1.287658
  validation loss:		1.290654
  validation accuracy:		51.30 %
Epoch 1713 of 2000 took 0.104s
  training loss:		1.288104
  validation loss:		1.285145
  validation accuracy:		53.70 %
Epoch 1714 of 2000 took 0.104s
  training loss:		1.285531
  validation loss:		1.288683
  validation accuracy:		53.48 %
Epoch 1715 of 2000 took 0.105s
  training loss:		1.286481
  validation loss:		1.321618
  validation accuracy:		52.93 %
Epoch 1716 of 2000 took 0.104s
  training loss:		1.287040
  validation loss:		1.291327
  validation accuracy:		52.72 %
Epoch 1717 of 2000 took 0.104s
  training loss:		1.280591
  validation loss:		1.292732
  validation accuracy:		52.07 %
Epoch 1718 of 2000 took 0.104s
  training loss:		1.296000
  validation loss:		1.288359
  validation accuracy:		53.37 %
Epoch 1719 of 2000 took 0.104s
  training loss:		1.273739
  validation loss:		1.293305
  validation accuracy:		53.26 %
Epoch 1720 of 2000 took 0.104s
  training loss:		1.297582
  validation loss:		1.293147
  validation accuracy:		53.59 %
Epoch 1721 of 2000 took 0.104s
  training loss:		1.278853
  validation loss:		1.288975
  validation accuracy:		52.50 %
Epoch 1722 of 2000 took 0.104s
  training loss:		1.283131
  validation loss:		1.289445
  validation accuracy:		52.72 %
Epoch 1723 of 2000 took 0.104s
  training loss:		1.279225
  validation loss:		1.292908
  validation accuracy:		52.07 %
Epoch 1724 of 2000 took 0.104s
  training loss:		1.284429
  validation loss:		1.334961
  validation accuracy:		48.48 %
Epoch 1725 of 2000 took 0.104s
  training loss:		1.285215
  validation loss:		1.283868
  validation accuracy:		53.48 %
Epoch 1726 of 2000 took 0.104s
  training loss:		1.289371
  validation loss:		1.288207
  validation accuracy:		53.26 %
Epoch 1727 of 2000 took 0.104s
  training loss:		1.283904
  validation loss:		1.291516
  validation accuracy:		53.04 %
Epoch 1728 of 2000 took 0.104s
  training loss:		1.282436
  validation loss:		1.288900
  validation accuracy:		53.15 %
Epoch 1729 of 2000 took 0.107s
  training loss:		1.287409
  validation loss:		1.309514
  validation accuracy:		51.20 %
Epoch 1730 of 2000 took 0.106s
  training loss:		1.290276
  validation loss:		1.288698
  validation accuracy:		51.85 %
Epoch 1731 of 2000 took 0.104s
  training loss:		1.276507
  validation loss:		1.325942
  validation accuracy:		49.57 %
Epoch 1732 of 2000 took 0.104s
  training loss:		1.284261
  validation loss:		1.286038
  validation accuracy:		53.48 %
Epoch 1733 of 2000 took 0.104s
  training loss:		1.280993
  validation loss:		1.299027
  validation accuracy:		51.74 %
Epoch 1734 of 2000 took 0.104s
  training loss:		1.290175
  validation loss:		1.288685
  validation accuracy:		53.15 %
Epoch 1735 of 2000 took 0.104s
  training loss:		1.292677
  validation loss:		1.311999
  validation accuracy:		50.65 %
Epoch 1736 of 2000 took 0.104s
  training loss:		1.299335
  validation loss:		1.301336
  validation accuracy:		50.98 %
Epoch 1737 of 2000 took 0.104s
  training loss:		1.286464
  validation loss:		1.300960
  validation accuracy:		52.61 %
Epoch 1738 of 2000 took 0.104s
  training loss:		1.280643
  validation loss:		1.285340
  validation accuracy:		53.15 %
Epoch 1739 of 2000 took 0.108s
  training loss:		1.284793
  validation loss:		1.286275
  validation accuracy:		53.48 %
Epoch 1740 of 2000 took 0.104s
  training loss:		1.287344
  validation loss:		1.312585
  validation accuracy:		53.15 %
Epoch 1741 of 2000 took 0.102s
  training loss:		1.294499
  validation loss:		1.301810
  validation accuracy:		52.93 %
Epoch 1742 of 2000 took 0.097s
  training loss:		1.290028
  validation loss:		1.293907
  validation accuracy:		52.72 %
Epoch 1743 of 2000 took 0.097s
  training loss:		1.287940
  validation loss:		1.293558
  validation accuracy:		52.72 %
Epoch 1744 of 2000 took 0.097s
  training loss:		1.289845
  validation loss:		1.291856
  validation accuracy:		53.48 %
Epoch 1745 of 2000 took 0.098s
  training loss:		1.284334
  validation loss:		1.299396
  validation accuracy:		53.37 %
Epoch 1746 of 2000 took 0.097s
  training loss:		1.289472
  validation loss:		1.288754
  validation accuracy:		53.37 %
Epoch 1747 of 2000 took 0.097s
  training loss:		1.275826
  validation loss:		1.290746
  validation accuracy:		52.17 %
Epoch 1748 of 2000 took 0.097s
  training loss:		1.282376
  validation loss:		1.286254
  validation accuracy:		52.93 %
Epoch 1749 of 2000 took 0.097s
  training loss:		1.280724
  validation loss:		1.290622
  validation accuracy:		52.61 %
Epoch 1750 of 2000 took 0.097s
  training loss:		1.275127
  validation loss:		1.290855
  validation accuracy:		53.15 %
Epoch 1751 of 2000 took 0.097s
  training loss:		1.277445
  validation loss:		1.294464
  validation accuracy:		52.39 %
Epoch 1752 of 2000 took 0.097s
  training loss:		1.290579
  validation loss:		1.290976
  validation accuracy:		52.72 %
Epoch 1753 of 2000 took 0.097s
  training loss:		1.279112
  validation loss:		1.285868
  validation accuracy:		53.70 %
Epoch 1754 of 2000 took 0.097s
  training loss:		1.283667
  validation loss:		1.287466
  validation accuracy:		52.72 %
Epoch 1755 of 2000 took 0.097s
  training loss:		1.281322
  validation loss:		1.290704
  validation accuracy:		52.61 %
Epoch 1756 of 2000 took 0.097s
  training loss:		1.287922
  validation loss:		1.290870
  validation accuracy:		52.83 %
Epoch 1757 of 2000 took 0.097s
  training loss:		1.287915
  validation loss:		1.288917
  validation accuracy:		53.37 %
Epoch 1758 of 2000 took 0.097s
  training loss:		1.276603
  validation loss:		1.319862
  validation accuracy:		52.39 %
Epoch 1759 of 2000 took 0.097s
  training loss:		1.302579
  validation loss:		1.290388
  validation accuracy:		52.93 %
Epoch 1760 of 2000 took 0.097s
  training loss:		1.286921
  validation loss:		1.310799
  validation accuracy:		50.87 %
Epoch 1761 of 2000 took 0.097s
  training loss:		1.286994
  validation loss:		1.307259
  validation accuracy:		52.39 %
Epoch 1762 of 2000 took 0.097s
  training loss:		1.285355
  validation loss:		1.285395
  validation accuracy:		53.26 %
Epoch 1763 of 2000 took 0.097s
  training loss:		1.278509
  validation loss:		1.306478
  validation accuracy:		52.93 %
Epoch 1764 of 2000 took 0.097s
  training loss:		1.279937
  validation loss:		1.284466
  validation accuracy:		53.15 %
Epoch 1765 of 2000 took 0.097s
  training loss:		1.284161
  validation loss:		1.286771
  validation accuracy:		53.04 %
Epoch 1766 of 2000 took 0.097s
  training loss:		1.277244
  validation loss:		1.285662
  validation accuracy:		52.39 %
Epoch 1767 of 2000 took 0.097s
  training loss:		1.280582
  validation loss:		1.289584
  validation accuracy:		52.50 %
Epoch 1768 of 2000 took 0.097s
  training loss:		1.290163
  validation loss:		1.296824
  validation accuracy:		51.96 %
Epoch 1769 of 2000 took 0.097s
  training loss:		1.278406
  validation loss:		1.293401
  validation accuracy:		53.26 %
Epoch 1770 of 2000 took 0.097s
  training loss:		1.288994
  validation loss:		1.295859
  validation accuracy:		51.63 %
Epoch 1771 of 2000 took 0.097s
  training loss:		1.277971
  validation loss:		1.290701
  validation accuracy:		52.50 %
Epoch 1772 of 2000 took 0.097s
  training loss:		1.271288
  validation loss:		1.303575
  validation accuracy:		51.30 %
Epoch 1773 of 2000 took 0.097s
  training loss:		1.287650
  validation loss:		1.285962
  validation accuracy:		52.61 %
Epoch 1774 of 2000 took 0.097s
  training loss:		1.283567
  validation loss:		1.297909
  validation accuracy:		53.15 %
Epoch 1775 of 2000 took 0.097s
  training loss:		1.300168
  validation loss:		1.293897
  validation accuracy:		53.70 %
Epoch 1776 of 2000 took 0.098s
  training loss:		1.283971
  validation loss:		1.287041
  validation accuracy:		53.37 %
Epoch 1777 of 2000 took 0.097s
  training loss:		1.287998
  validation loss:		1.287890
  validation accuracy:		52.93 %
Epoch 1778 of 2000 took 0.097s
  training loss:		1.282133
  validation loss:		1.282931
  validation accuracy:		53.48 %
Epoch 1779 of 2000 took 0.097s
  training loss:		1.281955
  validation loss:		1.288707
  validation accuracy:		53.59 %
Epoch 1780 of 2000 took 0.097s
  training loss:		1.279368
  validation loss:		1.285396
  validation accuracy:		52.72 %
Epoch 1781 of 2000 took 0.097s
  training loss:		1.272136
  validation loss:		1.282965
  validation accuracy:		53.15 %
Epoch 1782 of 2000 took 0.097s
  training loss:		1.266497
  validation loss:		1.288237
  validation accuracy:		52.50 %
Epoch 1783 of 2000 took 0.097s
  training loss:		1.289010
  validation loss:		1.292407
  validation accuracy:		51.85 %
Epoch 1784 of 2000 took 0.097s
  training loss:		1.283864
  validation loss:		1.291397
  validation accuracy:		52.83 %
Epoch 1785 of 2000 took 0.097s
  training loss:		1.289376
  validation loss:		1.299221
  validation accuracy:		52.07 %
Epoch 1786 of 2000 took 0.097s
  training loss:		1.289340
  validation loss:		1.291628
  validation accuracy:		52.61 %
Epoch 1787 of 2000 took 0.097s
  training loss:		1.288538
  validation loss:		1.288847
  validation accuracy:		53.26 %
Epoch 1788 of 2000 took 0.097s
  training loss:		1.281629
  validation loss:		1.286731
  validation accuracy:		52.93 %
Epoch 1789 of 2000 took 0.097s
  training loss:		1.300740
  validation loss:		1.287701
  validation accuracy:		53.48 %
Epoch 1790 of 2000 took 0.097s
  training loss:		1.277891
  validation loss:		1.290848
  validation accuracy:		52.83 %
Epoch 1791 of 2000 took 0.097s
  training loss:		1.279341
  validation loss:		1.288634
  validation accuracy:		53.26 %
Epoch 1792 of 2000 took 0.097s
  training loss:		1.295836
  validation loss:		1.286614
  validation accuracy:		53.48 %
Epoch 1793 of 2000 took 0.097s
  training loss:		1.285619
  validation loss:		1.299960
  validation accuracy:		51.85 %
Epoch 1794 of 2000 took 0.097s
  training loss:		1.283194
  validation loss:		1.294104
  validation accuracy:		52.83 %
Epoch 1795 of 2000 took 0.097s
  training loss:		1.277809
  validation loss:		1.290461
  validation accuracy:		52.50 %
Epoch 1796 of 2000 took 0.097s
  training loss:		1.283015
  validation loss:		1.285398
  validation accuracy:		53.48 %
Epoch 1797 of 2000 took 0.097s
  training loss:		1.288764
  validation loss:		1.290058
  validation accuracy:		52.17 %
Epoch 1798 of 2000 took 0.097s
  training loss:		1.287297
  validation loss:		1.305922
  validation accuracy:		52.07 %
Epoch 1799 of 2000 took 0.097s
  training loss:		1.286791
  validation loss:		1.291390
  validation accuracy:		52.39 %
Epoch 1800 of 2000 took 0.097s
  training loss:		1.283838
  validation loss:		1.286372
  validation accuracy:		53.48 %
Epoch 1801 of 2000 took 0.097s
  training loss:		1.286015
  validation loss:		1.292670
  validation accuracy:		52.07 %
Epoch 1802 of 2000 took 0.097s
  training loss:		1.278126
  validation loss:		1.288890
  validation accuracy:		52.50 %
Epoch 1803 of 2000 took 0.097s
  training loss:		1.281846
  validation loss:		1.283707
  validation accuracy:		53.26 %
Epoch 1804 of 2000 took 0.097s
  training loss:		1.291111
  validation loss:		1.286807
  validation accuracy:		53.26 %
Epoch 1805 of 2000 took 0.097s
  training loss:		1.272004
  validation loss:		1.288821
  validation accuracy:		52.39 %
Epoch 1806 of 2000 took 0.097s
  training loss:		1.285537
  validation loss:		1.286148
  validation accuracy:		53.26 %
Epoch 1807 of 2000 took 0.098s
  training loss:		1.277562
  validation loss:		1.293742
  validation accuracy:		52.72 %
Epoch 1808 of 2000 took 0.097s
  training loss:		1.281524
  validation loss:		1.310865
  validation accuracy:		53.26 %
Epoch 1809 of 2000 took 0.097s
  training loss:		1.292245
  validation loss:		1.292877
  validation accuracy:		53.91 %
Epoch 1810 of 2000 took 0.097s
  training loss:		1.273358
  validation loss:		1.302430
  validation accuracy:		51.74 %
Epoch 1811 of 2000 took 0.097s
  training loss:		1.285536
  validation loss:		1.287864
  validation accuracy:		53.04 %
Epoch 1812 of 2000 took 0.097s
  training loss:		1.274739
  validation loss:		1.293370
  validation accuracy:		52.93 %
Epoch 1813 of 2000 took 0.097s
  training loss:		1.281566
  validation loss:		1.296161
  validation accuracy:		52.28 %
Epoch 1814 of 2000 took 0.097s
  training loss:		1.302768
  validation loss:		1.292754
  validation accuracy:		52.39 %
Epoch 1815 of 2000 took 0.097s
  training loss:		1.284709
  validation loss:		1.297077
  validation accuracy:		52.93 %
Epoch 1816 of 2000 took 0.097s
  training loss:		1.286893
  validation loss:		1.287075
  validation accuracy:		53.80 %
Epoch 1817 of 2000 took 0.097s
  training loss:		1.288261
  validation loss:		1.303123
  validation accuracy:		52.83 %
Epoch 1818 of 2000 took 0.097s
  training loss:		1.281343
  validation loss:		1.288713
  validation accuracy:		54.02 %
Epoch 1819 of 2000 took 0.097s
  training loss:		1.279212
  validation loss:		1.284879
  validation accuracy:		52.83 %
Epoch 1820 of 2000 took 0.097s
  training loss:		1.282043
  validation loss:		1.289604
  validation accuracy:		52.07 %
Epoch 1821 of 2000 took 0.097s
  training loss:		1.279384
  validation loss:		1.302167
  validation accuracy:		52.17 %
Epoch 1822 of 2000 took 0.097s
  training loss:		1.279932
  validation loss:		1.291795
  validation accuracy:		52.93 %
Epoch 1823 of 2000 took 0.097s
  training loss:		1.290174
  validation loss:		1.290504
  validation accuracy:		52.17 %
Epoch 1824 of 2000 took 0.097s
  training loss:		1.278031
  validation loss:		1.287873
  validation accuracy:		52.50 %
Epoch 1825 of 2000 took 0.097s
  training loss:		1.279593
  validation loss:		1.283814
  validation accuracy:		53.04 %
Epoch 1826 of 2000 took 0.097s
  training loss:		1.282517
  validation loss:		1.285875
  validation accuracy:		53.04 %
Epoch 1827 of 2000 took 0.097s
  training loss:		1.280150
  validation loss:		1.288099
  validation accuracy:		53.48 %
Epoch 1828 of 2000 took 0.097s
  training loss:		1.294541
  validation loss:		1.290165
  validation accuracy:		52.07 %
Epoch 1829 of 2000 took 0.097s
  training loss:		1.277408
  validation loss:		1.289664
  validation accuracy:		52.61 %
Epoch 1830 of 2000 took 0.097s
  training loss:		1.282347
  validation loss:		1.297589
  validation accuracy:		52.39 %
Epoch 1831 of 2000 took 0.097s
  training loss:		1.278609
  validation loss:		1.290065
  validation accuracy:		52.93 %
Epoch 1832 of 2000 took 0.097s
  training loss:		1.281208
  validation loss:		1.314764
  validation accuracy:		50.33 %
Epoch 1833 of 2000 took 0.097s
  training loss:		1.281225
  validation loss:		1.304601
  validation accuracy:		51.74 %
Epoch 1834 of 2000 took 0.097s
  training loss:		1.280010
  validation loss:		1.289012
  validation accuracy:		53.04 %
Epoch 1835 of 2000 took 0.097s
  training loss:		1.278713
  validation loss:		1.298082
  validation accuracy:		53.37 %
Epoch 1836 of 2000 took 0.097s
  training loss:		1.287429
  validation loss:		1.292640
  validation accuracy:		53.48 %
Epoch 1837 of 2000 took 0.097s
  training loss:		1.293705
  validation loss:		1.291883
  validation accuracy:		52.72 %
Epoch 1838 of 2000 took 0.098s
  training loss:		1.272802
  validation loss:		1.290580
  validation accuracy:		52.28 %
Epoch 1839 of 2000 took 0.097s
  training loss:		1.289224
  validation loss:		1.281622
  validation accuracy:		52.72 %
Epoch 1840 of 2000 took 0.097s
  training loss:		1.286237
  validation loss:		1.316654
  validation accuracy:		51.85 %
Epoch 1841 of 2000 took 0.097s
  training loss:		1.286311
  validation loss:		1.290842
  validation accuracy:		52.72 %
Epoch 1842 of 2000 took 0.097s
  training loss:		1.279408
  validation loss:		1.288936
  validation accuracy:		53.59 %
Epoch 1843 of 2000 took 0.097s
  training loss:		1.288754
  validation loss:		1.320893
  validation accuracy:		52.83 %
Epoch 1844 of 2000 took 0.097s
  training loss:		1.296666
  validation loss:		1.290644
  validation accuracy:		53.04 %
Epoch 1845 of 2000 took 0.097s
  training loss:		1.269759
  validation loss:		1.286823
  validation accuracy:		53.26 %
Epoch 1846 of 2000 took 0.097s
  training loss:		1.285468
  validation loss:		1.293197
  validation accuracy:		52.50 %
Epoch 1847 of 2000 took 0.097s
  training loss:		1.294173
  validation loss:		1.291843
  validation accuracy:		52.50 %
Epoch 1848 of 2000 took 0.097s
  training loss:		1.284018
  validation loss:		1.292311
  validation accuracy:		52.83 %
Epoch 1849 of 2000 took 0.097s
  training loss:		1.277867
  validation loss:		1.288482
  validation accuracy:		53.48 %
Epoch 1850 of 2000 took 0.097s
  training loss:		1.294087
  validation loss:		1.287959
  validation accuracy:		52.83 %
Epoch 1851 of 2000 took 0.097s
  training loss:		1.283046
  validation loss:		1.308550
  validation accuracy:		53.04 %
Epoch 1852 of 2000 took 0.097s
  training loss:		1.286833
  validation loss:		1.286662
  validation accuracy:		53.15 %
Epoch 1853 of 2000 took 0.097s
  training loss:		1.287040
  validation loss:		1.350482
  validation accuracy:		51.41 %
Epoch 1854 of 2000 took 0.097s
  training loss:		1.288269
  validation loss:		1.297036
  validation accuracy:		52.17 %
Epoch 1855 of 2000 took 0.097s
  training loss:		1.281945
  validation loss:		1.291776
  validation accuracy:		52.72 %
Epoch 1856 of 2000 took 0.097s
  training loss:		1.295286
  validation loss:		1.293116
  validation accuracy:		52.93 %
Epoch 1857 of 2000 took 0.097s
  training loss:		1.268845
  validation loss:		1.291168
  validation accuracy:		52.72 %
Epoch 1858 of 2000 took 0.097s
  training loss:		1.292630
  validation loss:		1.286119
  validation accuracy:		53.48 %
Epoch 1859 of 2000 took 0.097s
  training loss:		1.287789
  validation loss:		1.284869
  validation accuracy:		53.80 %
Epoch 1860 of 2000 took 0.097s
  training loss:		1.296322
  validation loss:		1.291805
  validation accuracy:		52.83 %
Epoch 1861 of 2000 took 0.097s
  training loss:		1.293690
  validation loss:		1.296152
  validation accuracy:		52.72 %
Epoch 1862 of 2000 took 0.097s
  training loss:		1.285793
  validation loss:		1.314705
  validation accuracy:		50.33 %
Epoch 1863 of 2000 took 0.097s
  training loss:		1.284985
  validation loss:		1.322158
  validation accuracy:		49.78 %
Epoch 1864 of 2000 took 0.097s
  training loss:		1.272487
  validation loss:		1.286921
  validation accuracy:		53.15 %
Epoch 1865 of 2000 took 0.097s
  training loss:		1.295577
  validation loss:		1.286178
  validation accuracy:		52.83 %
Epoch 1866 of 2000 took 0.097s
  training loss:		1.279706
  validation loss:		1.286046
  validation accuracy:		53.26 %
Epoch 1867 of 2000 took 0.097s
  training loss:		1.282676
  validation loss:		1.288050
  validation accuracy:		53.26 %
Epoch 1868 of 2000 took 0.097s
  training loss:		1.289516
  validation loss:		1.286484
  validation accuracy:		52.93 %
Epoch 1869 of 2000 took 0.098s
  training loss:		1.278491
  validation loss:		1.294221
  validation accuracy:		52.83 %
Epoch 1870 of 2000 took 0.097s
  training loss:		1.285791
  validation loss:		1.294660
  validation accuracy:		52.61 %
Epoch 1871 of 2000 took 0.097s
  training loss:		1.295222
  validation loss:		1.298761
  validation accuracy:		52.61 %
Epoch 1872 of 2000 took 0.097s
  training loss:		1.280470
  validation loss:		1.289599
  validation accuracy:		53.80 %
Epoch 1873 of 2000 took 0.097s
  training loss:		1.286181
  validation loss:		1.284318
  validation accuracy:		52.50 %
Epoch 1874 of 2000 took 0.097s
  training loss:		1.281280
  validation loss:		1.291361
  validation accuracy:		52.61 %
Epoch 1875 of 2000 took 0.097s
  training loss:		1.292917
  validation loss:		1.325586
  validation accuracy:		51.96 %
Epoch 1876 of 2000 took 0.097s
  training loss:		1.290597
  validation loss:		1.287755
  validation accuracy:		52.93 %
Epoch 1877 of 2000 took 0.097s
  training loss:		1.289599
  validation loss:		1.286073
  validation accuracy:		53.80 %
Epoch 1878 of 2000 took 0.097s
  training loss:		1.283144
  validation loss:		1.289328
  validation accuracy:		52.17 %
Epoch 1879 of 2000 took 0.097s
  training loss:		1.282552
  validation loss:		1.288879
  validation accuracy:		52.61 %
Epoch 1880 of 2000 took 0.097s
  training loss:		1.283591
  validation loss:		1.301694
  validation accuracy:		52.93 %
Epoch 1881 of 2000 took 0.097s
  training loss:		1.281711
  validation loss:		1.285323
  validation accuracy:		52.50 %
Epoch 1882 of 2000 took 0.097s
  training loss:		1.275471
  validation loss:		1.291449
  validation accuracy:		53.04 %
Epoch 1883 of 2000 took 0.097s
  training loss:		1.281450
  validation loss:		1.292712
  validation accuracy:		53.37 %
Epoch 1884 of 2000 took 0.097s
  training loss:		1.283881
  validation loss:		1.287676
  validation accuracy:		53.37 %
Epoch 1885 of 2000 took 0.097s
  training loss:		1.280451
  validation loss:		1.288089
  validation accuracy:		52.72 %
Epoch 1886 of 2000 took 0.097s
  training loss:		1.278104
  validation loss:		1.287819
  validation accuracy:		53.59 %
Epoch 1887 of 2000 took 0.097s
  training loss:		1.274691
  validation loss:		1.289883
  validation accuracy:		52.61 %
Epoch 1888 of 2000 took 0.097s
  training loss:		1.278487
  validation loss:		1.289893
  validation accuracy:		53.37 %
Epoch 1889 of 2000 took 0.097s
  training loss:		1.291551
  validation loss:		1.287512
  validation accuracy:		53.59 %
Epoch 1890 of 2000 took 0.097s
  training loss:		1.282089
  validation loss:		1.293620
  validation accuracy:		52.28 %
Epoch 1891 of 2000 took 0.097s
  training loss:		1.283817
  validation loss:		1.294511
  validation accuracy:		51.85 %
Epoch 1892 of 2000 took 0.097s
  training loss:		1.280960
  validation loss:		1.290124
  validation accuracy:		52.50 %
Epoch 1893 of 2000 took 0.097s
  training loss:		1.284611
  validation loss:		1.292945
  validation accuracy:		51.96 %
Epoch 1894 of 2000 took 0.097s
  training loss:		1.295233
  validation loss:		1.287805
  validation accuracy:		53.91 %
Epoch 1895 of 2000 took 0.097s
  training loss:		1.286878
  validation loss:		1.292750
  validation accuracy:		53.37 %
Epoch 1896 of 2000 took 0.097s
  training loss:		1.286434
  validation loss:		1.294069
  validation accuracy:		52.17 %
Epoch 1897 of 2000 took 0.097s
  training loss:		1.282030
  validation loss:		1.291341
  validation accuracy:		52.93 %
Epoch 1898 of 2000 took 0.097s
  training loss:		1.283980
  validation loss:		1.292874
  validation accuracy:		53.48 %
Epoch 1899 of 2000 took 0.097s
  training loss:		1.280903
  validation loss:		1.294903
  validation accuracy:		53.04 %
Epoch 1900 of 2000 took 0.098s
  training loss:		1.287528
  validation loss:		1.294416
  validation accuracy:		52.61 %
Epoch 1901 of 2000 took 0.097s
  training loss:		1.288061
  validation loss:		1.286129
  validation accuracy:		53.15 %
Epoch 1902 of 2000 took 0.097s
  training loss:		1.286314
  validation loss:		1.286832
  validation accuracy:		53.59 %
Epoch 1903 of 2000 took 0.097s
  training loss:		1.281171
  validation loss:		1.292193
  validation accuracy:		52.93 %
Epoch 1904 of 2000 took 0.097s
  training loss:		1.281548
  validation loss:		1.300311
  validation accuracy:		53.15 %
Epoch 1905 of 2000 took 0.097s
  training loss:		1.271858
  validation loss:		1.292077
  validation accuracy:		53.04 %
Epoch 1906 of 2000 took 0.097s
  training loss:		1.276793
  validation loss:		1.282320
  validation accuracy:		53.59 %
Epoch 1907 of 2000 took 0.097s
  training loss:		1.271451
  validation loss:		1.286981
  validation accuracy:		53.37 %
Epoch 1908 of 2000 took 0.097s
  training loss:		1.278213
  validation loss:		1.289861
  validation accuracy:		53.15 %
Epoch 1909 of 2000 took 0.097s
  training loss:		1.278135
  validation loss:		1.298530
  validation accuracy:		53.26 %
Epoch 1910 of 2000 took 0.097s
  training loss:		1.272980
  validation loss:		1.307232
  validation accuracy:		52.39 %
Epoch 1911 of 2000 took 0.097s
  training loss:		1.284068
  validation loss:		1.289706
  validation accuracy:		52.61 %
Epoch 1912 of 2000 took 0.097s
  training loss:		1.285973
  validation loss:		1.285474
  validation accuracy:		53.48 %
Epoch 1913 of 2000 took 0.097s
  training loss:		1.285582
  validation loss:		1.307133
  validation accuracy:		51.30 %
Epoch 1914 of 2000 took 0.097s
  training loss:		1.275098
  validation loss:		1.289769
  validation accuracy:		52.93 %
Epoch 1915 of 2000 took 0.097s
  training loss:		1.280200
  validation loss:		1.294421
  validation accuracy:		52.72 %
Epoch 1916 of 2000 took 0.097s
  training loss:		1.282359
  validation loss:		1.285633
  validation accuracy:		53.48 %
Epoch 1917 of 2000 took 0.097s
  training loss:		1.290840
  validation loss:		1.298311
  validation accuracy:		53.04 %
Epoch 1918 of 2000 took 0.097s
  training loss:		1.276172
  validation loss:		1.302749
  validation accuracy:		52.61 %
Epoch 1919 of 2000 took 0.097s
  training loss:		1.282797
  validation loss:		1.289981
  validation accuracy:		52.93 %
Epoch 1920 of 2000 took 0.097s
  training loss:		1.280752
  validation loss:		1.285032
  validation accuracy:		53.04 %
Epoch 1921 of 2000 took 0.097s
  training loss:		1.283254
  validation loss:		1.282182
  validation accuracy:		54.24 %
Epoch 1922 of 2000 took 0.103s
  training loss:		1.285032
  validation loss:		1.290549
  validation accuracy:		53.26 %
Epoch 1923 of 2000 took 0.098s
  training loss:		1.280129
  validation loss:		1.293099
  validation accuracy:		52.83 %
Epoch 1924 of 2000 took 0.097s
  training loss:		1.283315
  validation loss:		1.288257
  validation accuracy:		53.15 %
Epoch 1925 of 2000 took 0.097s
  training loss:		1.279899
  validation loss:		1.287016
  validation accuracy:		54.24 %
Epoch 1926 of 2000 took 0.097s
  training loss:		1.281444
  validation loss:		1.289934
  validation accuracy:		52.93 %
Epoch 1927 of 2000 took 0.096s
  training loss:		1.273342
  validation loss:		1.284116
  validation accuracy:		53.04 %
Epoch 1928 of 2000 took 0.097s
  training loss:		1.281604
  validation loss:		1.288196
  validation accuracy:		53.04 %
Epoch 1929 of 2000 took 0.097s
  training loss:		1.286212
  validation loss:		1.287297
  validation accuracy:		53.26 %
Epoch 1930 of 2000 took 0.097s
  training loss:		1.292991
  validation loss:		1.299780
  validation accuracy:		53.37 %
Epoch 1931 of 2000 took 0.097s
  training loss:		1.286697
  validation loss:		1.290693
  validation accuracy:		53.04 %
Epoch 1932 of 2000 took 0.097s
  training loss:		1.275750
  validation loss:		1.286910
  validation accuracy:		53.59 %
Epoch 1933 of 2000 took 0.096s
  training loss:		1.292823
  validation loss:		1.286704
  validation accuracy:		53.37 %
Epoch 1934 of 2000 took 0.097s
  training loss:		1.278016
  validation loss:		1.288526
  validation accuracy:		52.72 %
Epoch 1935 of 2000 took 0.097s
  training loss:		1.278827
  validation loss:		1.284730
  validation accuracy:		53.37 %
Epoch 1936 of 2000 took 0.097s
  training loss:		1.280687
  validation loss:		1.307438
  validation accuracy:		51.85 %
Epoch 1937 of 2000 took 0.097s
  training loss:		1.283934
  validation loss:		1.288572
  validation accuracy:		52.83 %
Epoch 1938 of 2000 took 0.097s
  training loss:		1.284942
  validation loss:		1.286970
  validation accuracy:		53.04 %
Epoch 1939 of 2000 took 0.097s
  training loss:		1.279365
  validation loss:		1.289050
  validation accuracy:		53.15 %
Epoch 1940 of 2000 took 0.097s
  training loss:		1.281132
  validation loss:		1.287380
  validation accuracy:		53.04 %
Epoch 1941 of 2000 took 0.096s
  training loss:		1.271395
  validation loss:		1.285551
  validation accuracy:		53.37 %
Epoch 1942 of 2000 took 0.097s
  training loss:		1.288094
  validation loss:		1.304897
  validation accuracy:		51.63 %
Epoch 1943 of 2000 took 0.097s
  training loss:		1.285792
  validation loss:		1.302204
  validation accuracy:		52.50 %
Epoch 1944 of 2000 took 0.097s
  training loss:		1.296329
  validation loss:		1.294386
  validation accuracy:		53.26 %
Epoch 1945 of 2000 took 0.097s
  training loss:		1.280558
  validation loss:		1.292509
  validation accuracy:		53.04 %
Epoch 1946 of 2000 took 0.097s
  training loss:		1.271672
  validation loss:		1.294495
  validation accuracy:		53.59 %
Epoch 1947 of 2000 took 0.096s
  training loss:		1.281244
  validation loss:		1.288281
  validation accuracy:		52.61 %
Epoch 1948 of 2000 took 0.097s
  training loss:		1.284409
  validation loss:		1.287671
  validation accuracy:		53.15 %
Epoch 1949 of 2000 took 0.096s
  training loss:		1.278674
  validation loss:		1.283149
  validation accuracy:		53.80 %
Epoch 1950 of 2000 took 0.097s
  training loss:		1.277994
  validation loss:		1.284896
  validation accuracy:		53.26 %
Epoch 1951 of 2000 took 0.097s
  training loss:		1.283610
  validation loss:		1.289947
  validation accuracy:		53.37 %
Epoch 1952 of 2000 took 0.096s
  training loss:		1.275442
  validation loss:		1.289659
  validation accuracy:		53.04 %
Epoch 1953 of 2000 took 0.097s
  training loss:		1.283930
  validation loss:		1.286961
  validation accuracy:		53.80 %
Epoch 1954 of 2000 took 0.096s
  training loss:		1.278102
  validation loss:		1.285913
  validation accuracy:		53.80 %
Epoch 1955 of 2000 took 0.097s
  training loss:		1.283469
  validation loss:		1.289297
  validation accuracy:		52.39 %
Epoch 1956 of 2000 took 0.097s
  training loss:		1.296135
  validation loss:		1.289851
  validation accuracy:		53.37 %
Epoch 1957 of 2000 took 0.097s
  training loss:		1.268865
  validation loss:		1.295142
  validation accuracy:		52.72 %
Epoch 1958 of 2000 took 0.097s
  training loss:		1.283912
  validation loss:		1.291971
  validation accuracy:		53.70 %
Epoch 1959 of 2000 took 0.097s
  training loss:		1.287893
  validation loss:		1.349957
  validation accuracy:		47.83 %
Epoch 1960 of 2000 took 0.097s
  training loss:		1.306047
  validation loss:		1.292155
  validation accuracy:		52.17 %
Epoch 1961 of 2000 took 0.097s
  training loss:		1.282404
  validation loss:		1.289324
  validation accuracy:		53.59 %
Epoch 1962 of 2000 took 0.097s
  training loss:		1.275101
  validation loss:		1.301740
  validation accuracy:		53.04 %
Epoch 1963 of 2000 took 0.097s
  training loss:		1.285758
  validation loss:		1.295315
  validation accuracy:		52.50 %
Epoch 1964 of 2000 took 0.097s
  training loss:		1.277712
  validation loss:		1.302539
  validation accuracy:		51.63 %
Epoch 1965 of 2000 took 0.097s
  training loss:		1.280943
  validation loss:		1.288527
  validation accuracy:		52.93 %
Epoch 1966 of 2000 took 0.097s
  training loss:		1.283763
  validation loss:		1.299035
  validation accuracy:		53.04 %
Epoch 1967 of 2000 took 0.097s
  training loss:		1.289652
  validation loss:		1.293963
  validation accuracy:		53.70 %
Epoch 1968 of 2000 took 0.097s
  training loss:		1.276006
  validation loss:		1.288851
  validation accuracy:		53.70 %
Epoch 1969 of 2000 took 0.097s
  training loss:		1.281524
  validation loss:		1.284255
  validation accuracy:		53.48 %
Epoch 1970 of 2000 took 0.096s
  training loss:		1.281122
  validation loss:		1.296546
  validation accuracy:		53.15 %
Epoch 1971 of 2000 took 0.097s
  training loss:		1.285913
  validation loss:		1.289171
  validation accuracy:		53.15 %
Epoch 1972 of 2000 took 0.096s
  training loss:		1.278250
  validation loss:		1.290637
  validation accuracy:		53.48 %
Epoch 1973 of 2000 took 0.097s
  training loss:		1.283314
  validation loss:		1.286925
  validation accuracy:		53.15 %
Epoch 1974 of 2000 took 0.097s
  training loss:		1.286502
  validation loss:		1.289661
  validation accuracy:		53.48 %
Epoch 1975 of 2000 took 0.097s
  training loss:		1.285267
  validation loss:		1.288567
  validation accuracy:		53.37 %
Epoch 1976 of 2000 took 0.097s
  training loss:		1.269481
  validation loss:		1.304651
  validation accuracy:		51.52 %
Epoch 1977 of 2000 took 0.097s
  training loss:		1.284508
  validation loss:		1.288384
  validation accuracy:		52.17 %
Epoch 1978 of 2000 took 0.096s
  training loss:		1.283009
  validation loss:		1.284579
  validation accuracy:		53.70 %
Epoch 1979 of 2000 took 0.097s
  training loss:		1.279502
  validation loss:		1.292475
  validation accuracy:		53.70 %
Epoch 1980 of 2000 took 0.096s
  training loss:		1.278227
  validation loss:		1.295164
  validation accuracy:		52.39 %
Epoch 1981 of 2000 took 0.097s
  training loss:		1.277950
  validation loss:		1.287787
  validation accuracy:		53.48 %
Epoch 1982 of 2000 took 0.097s
  training loss:		1.285583
  validation loss:		1.293007
  validation accuracy:		54.13 %
Epoch 1983 of 2000 took 0.096s
  training loss:		1.276309
  validation loss:		1.289262
  validation accuracy:		52.61 %
Epoch 1984 of 2000 took 0.097s
  training loss:		1.281795
  validation loss:		1.289474
  validation accuracy:		52.83 %
Epoch 1985 of 2000 took 0.097s
  training loss:		1.278612
  validation loss:		1.284687
  validation accuracy:		53.15 %
Epoch 1986 of 2000 took 0.097s
  training loss:		1.281241
  validation loss:		1.301714
  validation accuracy:		52.39 %
Epoch 1987 of 2000 took 0.097s
  training loss:		1.282539
  validation loss:		1.289320
  validation accuracy:		53.04 %
Epoch 1988 of 2000 took 0.097s
  training loss:		1.287043
  validation loss:		1.289372
  validation accuracy:		53.15 %
Epoch 1989 of 2000 took 0.096s
  training loss:		1.278663
  validation loss:		1.301539
  validation accuracy:		52.28 %
Epoch 1990 of 2000 took 0.097s
  training loss:		1.279674
  validation loss:		1.285020
  validation accuracy:		53.59 %
Epoch 1991 of 2000 took 0.097s
  training loss:		1.289614
  validation loss:		1.287431
  validation accuracy:		53.26 %
Epoch 1992 of 2000 took 0.097s
  training loss:		1.282530
  validation loss:		1.292612
  validation accuracy:		53.04 %
Epoch 1993 of 2000 took 0.097s
  training loss:		1.271515
  validation loss:		1.292514
  validation accuracy:		51.85 %
Epoch 1994 of 2000 took 0.097s
  training loss:		1.281752
  validation loss:		1.283549
  validation accuracy:		52.83 %
Epoch 1995 of 2000 took 0.096s
  training loss:		1.281175
  validation loss:		1.286421
  validation accuracy:		53.15 %
Epoch 1996 of 2000 took 0.097s
  training loss:		1.283340
  validation loss:		1.291905
  validation accuracy:		52.50 %
Epoch 1997 of 2000 took 0.096s
  training loss:		1.277607
  validation loss:		1.291279
  validation accuracy:		53.37 %
Epoch 1998 of 2000 took 0.097s
  training loss:		1.285007
  validation loss:		1.292496
  validation accuracy:		52.72 %
Epoch 1999 of 2000 took 0.097s
  training loss:		1.288533
  validation loss:		1.287303
  validation accuracy:		53.37 %
Epoch 2000 of 2000 took 0.097s
  training loss:		1.280274
  validation loss:		1.302127
  validation accuracy:		53.70 %
Final results:
  test loss:			1.476174
  test accuracy:		48.91 %
