Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (3, 50, 50)...
decomposing tensor B of shape (3, 50)...
Starting training...
Epoch 1 of 2000 took 0.100s
  training loss:		2.988359
  validation loss:		2.979770
  validation accuracy:		12.83 %
Epoch 2 of 2000 took 0.095s
  training loss:		2.969445
  validation loss:		2.956152
  validation accuracy:		13.04 %
Epoch 3 of 2000 took 0.096s
  training loss:		2.944509
  validation loss:		2.929523
  validation accuracy:		13.04 %
Epoch 4 of 2000 took 0.095s
  training loss:		2.918370
  validation loss:		2.902263
  validation accuracy:		13.04 %
Epoch 5 of 2000 took 0.095s
  training loss:		2.891387
  validation loss:		2.874944
  validation accuracy:		13.04 %
Epoch 6 of 2000 took 0.095s
  training loss:		2.864351
  validation loss:		2.847181
  validation accuracy:		13.04 %
Epoch 7 of 2000 took 0.095s
  training loss:		2.838778
  validation loss:		2.819168
  validation accuracy:		13.04 %
Epoch 8 of 2000 took 0.095s
  training loss:		2.813010
  validation loss:		2.790590
  validation accuracy:		13.04 %
Epoch 9 of 2000 took 0.095s
  training loss:		2.784909
  validation loss:		2.761121
  validation accuracy:		13.04 %
Epoch 10 of 2000 took 0.095s
  training loss:		2.756690
  validation loss:		2.731003
  validation accuracy:		13.04 %
Epoch 11 of 2000 took 0.095s
  training loss:		2.729278
  validation loss:		2.700163
  validation accuracy:		13.04 %
Epoch 12 of 2000 took 0.095s
  training loss:		2.700483
  validation loss:		2.668400
  validation accuracy:		13.04 %
Epoch 13 of 2000 took 0.095s
  training loss:		2.673234
  validation loss:		2.636149
  validation accuracy:		13.04 %
Epoch 14 of 2000 took 0.095s
  training loss:		2.645081
  validation loss:		2.603693
  validation accuracy:		13.04 %
Epoch 15 of 2000 took 0.095s
  training loss:		2.616320
  validation loss:		2.571034
  validation accuracy:		13.04 %
Epoch 16 of 2000 took 0.096s
  training loss:		2.587648
  validation loss:		2.539159
  validation accuracy:		13.04 %
Epoch 17 of 2000 took 0.096s
  training loss:		2.559124
  validation loss:		2.507929
  validation accuracy:		13.04 %
Epoch 18 of 2000 took 0.096s
  training loss:		2.531464
  validation loss:		2.477533
  validation accuracy:		13.04 %
Epoch 19 of 2000 took 0.096s
  training loss:		2.506260
  validation loss:		2.448517
  validation accuracy:		13.04 %
Epoch 20 of 2000 took 0.096s
  training loss:		2.484474
  validation loss:		2.421610
  validation accuracy:		13.70 %
Epoch 21 of 2000 took 0.096s
  training loss:		2.462326
  validation loss:		2.397137
  validation accuracy:		12.93 %
Epoch 22 of 2000 took 0.096s
  training loss:		2.440262
  validation loss:		2.374584
  validation accuracy:		12.93 %
Epoch 23 of 2000 took 0.096s
  training loss:		2.420544
  validation loss:		2.354899
  validation accuracy:		12.93 %
Epoch 24 of 2000 took 0.096s
  training loss:		2.406054
  validation loss:		2.338184
  validation accuracy:		13.70 %
Epoch 25 of 2000 took 0.096s
  training loss:		2.392046
  validation loss:		2.323890
  validation accuracy:		13.04 %
Epoch 26 of 2000 took 0.096s
  training loss:		2.377167
  validation loss:		2.311420
  validation accuracy:		15.33 %
Epoch 27 of 2000 took 0.096s
  training loss:		2.365877
  validation loss:		2.301460
  validation accuracy:		12.83 %
Epoch 28 of 2000 took 0.096s
  training loss:		2.356512
  validation loss:		2.293173
  validation accuracy:		13.04 %
Epoch 29 of 2000 took 0.096s
  training loss:		2.349827
  validation loss:		2.286227
  validation accuracy:		13.15 %
Epoch 30 of 2000 took 0.096s
  training loss:		2.343394
  validation loss:		2.282621
  validation accuracy:		13.04 %
Epoch 31 of 2000 took 0.096s
  training loss:		2.336644
  validation loss:		2.277580
  validation accuracy:		13.04 %
Epoch 32 of 2000 took 0.096s
  training loss:		2.333139
  validation loss:		2.274702
  validation accuracy:		13.04 %
Epoch 33 of 2000 took 0.096s
  training loss:		2.329320
  validation loss:		2.271872
  validation accuracy:		13.04 %
Epoch 34 of 2000 took 0.096s
  training loss:		2.325872
  validation loss:		2.270488
  validation accuracy:		13.04 %
Epoch 35 of 2000 took 0.097s
  training loss:		2.323293
  validation loss:		2.269936
  validation accuracy:		13.04 %
Epoch 36 of 2000 took 0.096s
  training loss:		2.318979
  validation loss:		2.267528
  validation accuracy:		13.04 %
Epoch 37 of 2000 took 0.096s
  training loss:		2.318280
  validation loss:		2.265716
  validation accuracy:		13.04 %
Epoch 38 of 2000 took 0.096s
  training loss:		2.315503
  validation loss:		2.263327
  validation accuracy:		12.83 %
Epoch 39 of 2000 took 0.096s
  training loss:		2.314184
  validation loss:		2.262139
  validation accuracy:		13.91 %
Epoch 40 of 2000 took 0.096s
  training loss:		2.313019
  validation loss:		2.260367
  validation accuracy:		17.72 %
Epoch 41 of 2000 took 0.099s
  training loss:		2.310672
  validation loss:		2.258353
  validation accuracy:		13.04 %
Epoch 42 of 2000 took 0.099s
  training loss:		2.309626
  validation loss:		2.257685
  validation accuracy:		13.04 %
Epoch 43 of 2000 took 0.102s
  training loss:		2.308778
  validation loss:		2.256102
  validation accuracy:		13.04 %
Epoch 44 of 2000 took 0.099s
  training loss:		2.308246
  validation loss:		2.255428
  validation accuracy:		12.93 %
Epoch 45 of 2000 took 0.099s
  training loss:		2.306853
  validation loss:		2.256307
  validation accuracy:		13.04 %
Epoch 46 of 2000 took 0.099s
  training loss:		2.305986
  validation loss:		2.254191
  validation accuracy:		13.04 %
Epoch 47 of 2000 took 0.099s
  training loss:		2.305532
  validation loss:		2.253809
  validation accuracy:		13.04 %
Epoch 48 of 2000 took 0.099s
  training loss:		2.304935
  validation loss:		2.252984
  validation accuracy:		13.04 %
Epoch 49 of 2000 took 0.099s
  training loss:		2.305105
  validation loss:		2.253869
  validation accuracy:		13.04 %
Epoch 50 of 2000 took 0.099s
  training loss:		2.303793
  validation loss:		2.252692
  validation accuracy:		13.04 %
Epoch 51 of 2000 took 0.099s
  training loss:		2.302783
  validation loss:		2.251659
  validation accuracy:		13.59 %
Epoch 52 of 2000 took 0.099s
  training loss:		2.303241
  validation loss:		2.249374
  validation accuracy:		13.04 %
Epoch 53 of 2000 took 0.099s
  training loss:		2.301840
  validation loss:		2.249375
  validation accuracy:		13.04 %
Epoch 54 of 2000 took 0.097s
  training loss:		2.301813
  validation loss:		2.250036
  validation accuracy:		12.83 %
Epoch 55 of 2000 took 0.096s
  training loss:		2.302210
  validation loss:		2.251767
  validation accuracy:		13.04 %
Epoch 56 of 2000 took 0.096s
  training loss:		2.301993
  validation loss:		2.250678
  validation accuracy:		12.93 %
Epoch 57 of 2000 took 0.096s
  training loss:		2.301139
  validation loss:		2.251387
  validation accuracy:		12.93 %
Epoch 58 of 2000 took 0.096s
  training loss:		2.301316
  validation loss:		2.249619
  validation accuracy:		13.04 %
Epoch 59 of 2000 took 0.096s
  training loss:		2.301425
  validation loss:		2.251363
  validation accuracy:		13.04 %
Epoch 60 of 2000 took 0.096s
  training loss:		2.300817
  validation loss:		2.250161
  validation accuracy:		13.04 %
Epoch 61 of 2000 took 0.096s
  training loss:		2.300056
  validation loss:		2.251671
  validation accuracy:		12.83 %
Epoch 62 of 2000 took 0.096s
  training loss:		2.300290
  validation loss:		2.250179
  validation accuracy:		12.83 %
Epoch 63 of 2000 took 0.096s
  training loss:		2.300531
  validation loss:		2.249366
  validation accuracy:		12.83 %
Epoch 64 of 2000 took 0.096s
  training loss:		2.300071
  validation loss:		2.250974
  validation accuracy:		15.43 %
Epoch 65 of 2000 took 0.096s
  training loss:		2.299925
  validation loss:		2.251835
  validation accuracy:		13.04 %
Epoch 66 of 2000 took 0.097s
  training loss:		2.299396
  validation loss:		2.249445
  validation accuracy:		13.48 %
Epoch 67 of 2000 took 0.096s
  training loss:		2.298366
  validation loss:		2.248628
  validation accuracy:		12.83 %
Epoch 68 of 2000 took 0.096s
  training loss:		2.299228
  validation loss:		2.247217
  validation accuracy:		13.04 %
Epoch 69 of 2000 took 0.096s
  training loss:		2.298619
  validation loss:		2.245965
  validation accuracy:		12.93 %
Epoch 70 of 2000 took 0.096s
  training loss:		2.298984
  validation loss:		2.245878
  validation accuracy:		12.93 %
Epoch 71 of 2000 took 0.097s
  training loss:		2.297807
  validation loss:		2.248411
  validation accuracy:		12.93 %
Epoch 72 of 2000 took 0.096s
  training loss:		2.298523
  validation loss:		2.250437
  validation accuracy:		12.93 %
Epoch 73 of 2000 took 0.096s
  training loss:		2.299160
  validation loss:		2.249491
  validation accuracy:		12.83 %
Epoch 74 of 2000 took 0.096s
  training loss:		2.297829
  validation loss:		2.247237
  validation accuracy:		12.93 %
Epoch 75 of 2000 took 0.100s
  training loss:		2.298278
  validation loss:		2.246693
  validation accuracy:		12.93 %
Epoch 76 of 2000 took 0.099s
  training loss:		2.297369
  validation loss:		2.246972
  validation accuracy:		12.93 %
Epoch 77 of 2000 took 0.099s
  training loss:		2.297109
  validation loss:		2.247201
  validation accuracy:		14.13 %
Epoch 78 of 2000 took 0.102s
  training loss:		2.297486
  validation loss:		2.246245
  validation accuracy:		12.83 %
Epoch 79 of 2000 took 0.102s
  training loss:		2.298278
  validation loss:		2.247842
  validation accuracy:		12.83 %
Epoch 80 of 2000 took 0.100s
  training loss:		2.297370
  validation loss:		2.246746
  validation accuracy:		13.15 %
Epoch 81 of 2000 took 0.096s
  training loss:		2.297184
  validation loss:		2.246870
  validation accuracy:		13.04 %
Epoch 82 of 2000 took 0.096s
  training loss:		2.297827
  validation loss:		2.247137
  validation accuracy:		13.04 %
Epoch 83 of 2000 took 0.096s
  training loss:		2.298026
  validation loss:		2.246591
  validation accuracy:		12.83 %
Epoch 84 of 2000 took 0.096s
  training loss:		2.297259
  validation loss:		2.247904
  validation accuracy:		12.83 %
Epoch 85 of 2000 took 0.096s
  training loss:		2.297574
  validation loss:		2.249069
  validation accuracy:		12.93 %
Epoch 86 of 2000 took 0.096s
  training loss:		2.297266
  validation loss:		2.247858
  validation accuracy:		12.93 %
Epoch 87 of 2000 took 0.096s
  training loss:		2.296844
  validation loss:		2.244670
  validation accuracy:		13.04 %
Epoch 88 of 2000 took 0.096s
  training loss:		2.296758
  validation loss:		2.244992
  validation accuracy:		16.96 %
Epoch 89 of 2000 took 0.096s
  training loss:		2.296457
  validation loss:		2.244792
  validation accuracy:		13.04 %
Epoch 90 of 2000 took 0.096s
  training loss:		2.296486
  validation loss:		2.243850
  validation accuracy:		12.83 %
Epoch 91 of 2000 took 0.096s
  training loss:		2.296610
  validation loss:		2.245821
  validation accuracy:		13.48 %
Epoch 92 of 2000 took 0.096s
  training loss:		2.295883
  validation loss:		2.244436
  validation accuracy:		13.04 %
Epoch 93 of 2000 took 0.096s
  training loss:		2.297375
  validation loss:		2.246186
  validation accuracy:		15.43 %
Epoch 94 of 2000 took 0.096s
  training loss:		2.295602
  validation loss:		2.246122
  validation accuracy:		12.83 %
Epoch 95 of 2000 took 0.096s
  training loss:		2.296968
  validation loss:		2.245380
  validation accuracy:		13.04 %
Epoch 96 of 2000 took 0.096s
  training loss:		2.297302
  validation loss:		2.247304
  validation accuracy:		13.04 %
Epoch 97 of 2000 took 0.097s
  training loss:		2.296207
  validation loss:		2.246157
  validation accuracy:		13.04 %
Epoch 98 of 2000 took 0.096s
  training loss:		2.296554
  validation loss:		2.246981
  validation accuracy:		12.93 %
Epoch 99 of 2000 took 0.096s
  training loss:		2.296106
  validation loss:		2.245029
  validation accuracy:		12.93 %
Epoch 100 of 2000 took 0.096s
  training loss:		2.296166
  validation loss:		2.246174
  validation accuracy:		12.50 %
Epoch 101 of 2000 took 0.096s
  training loss:		2.295754
  validation loss:		2.245260
  validation accuracy:		12.93 %
Epoch 102 of 2000 took 0.096s
  training loss:		2.297166
  validation loss:		2.246056
  validation accuracy:		13.04 %
Epoch 103 of 2000 took 0.096s
  training loss:		2.296002
  validation loss:		2.246435
  validation accuracy:		13.04 %
Epoch 104 of 2000 took 0.096s
  training loss:		2.296515
  validation loss:		2.247275
  validation accuracy:		13.04 %
Epoch 105 of 2000 took 0.096s
  training loss:		2.296182
  validation loss:		2.248151
  validation accuracy:		13.04 %
Epoch 106 of 2000 took 0.096s
  training loss:		2.296279
  validation loss:		2.245911
  validation accuracy:		12.93 %
Epoch 107 of 2000 took 0.096s
  training loss:		2.296018
  validation loss:		2.245484
  validation accuracy:		12.93 %
Epoch 108 of 2000 took 0.096s
  training loss:		2.296066
  validation loss:		2.244013
  validation accuracy:		12.93 %
Epoch 109 of 2000 took 0.096s
  training loss:		2.295766
  validation loss:		2.246362
  validation accuracy:		13.04 %
Epoch 110 of 2000 took 0.096s
  training loss:		2.296390
  validation loss:		2.244757
  validation accuracy:		12.93 %
Epoch 111 of 2000 took 0.096s
  training loss:		2.295350
  validation loss:		2.243087
  validation accuracy:		12.93 %
Epoch 112 of 2000 took 0.096s
  training loss:		2.295701
  validation loss:		2.246229
  validation accuracy:		17.39 %
Epoch 113 of 2000 took 0.096s
  training loss:		2.295706
  validation loss:		2.245106
  validation accuracy:		12.83 %
Epoch 114 of 2000 took 0.096s
  training loss:		2.295954
  validation loss:		2.244770
  validation accuracy:		12.83 %
Epoch 115 of 2000 took 0.096s
  training loss:		2.295637
  validation loss:		2.246118
  validation accuracy:		12.93 %
Epoch 116 of 2000 took 0.096s
  training loss:		2.295489
  validation loss:		2.247400
  validation accuracy:		15.98 %
Epoch 117 of 2000 took 0.099s
  training loss:		2.295033
  validation loss:		2.246792
  validation accuracy:		13.04 %
Epoch 118 of 2000 took 0.096s
  training loss:		2.295079
  validation loss:		2.243680
  validation accuracy:		13.04 %
Epoch 119 of 2000 took 0.096s
  training loss:		2.295670
  validation loss:		2.243036
  validation accuracy:		13.15 %
Epoch 120 of 2000 took 0.096s
  training loss:		2.296280
  validation loss:		2.243656
  validation accuracy:		12.93 %
Epoch 121 of 2000 took 0.096s
  training loss:		2.297128
  validation loss:		2.247214
  validation accuracy:		12.83 %
Epoch 122 of 2000 took 0.096s
  training loss:		2.294924
  validation loss:		2.247744
  validation accuracy:		12.83 %
Epoch 123 of 2000 took 0.096s
  training loss:		2.295199
  validation loss:		2.247053
  validation accuracy:		12.93 %
Epoch 124 of 2000 took 0.096s
  training loss:		2.295359
  validation loss:		2.244656
  validation accuracy:		13.04 %
Epoch 125 of 2000 took 0.096s
  training loss:		2.295619
  validation loss:		2.244981
  validation accuracy:		13.04 %
Epoch 126 of 2000 took 0.096s
  training loss:		2.295570
  validation loss:		2.245341
  validation accuracy:		13.04 %
Epoch 127 of 2000 took 0.096s
  training loss:		2.296125
  validation loss:		2.244953
  validation accuracy:		13.70 %
Epoch 128 of 2000 took 0.097s
  training loss:		2.295651
  validation loss:		2.246220
  validation accuracy:		12.93 %
Epoch 129 of 2000 took 0.096s
  training loss:		2.295179
  validation loss:		2.245788
  validation accuracy:		13.04 %
Epoch 130 of 2000 took 0.096s
  training loss:		2.294897
  validation loss:		2.242156
  validation accuracy:		12.83 %
Epoch 131 of 2000 took 0.096s
  training loss:		2.295311
  validation loss:		2.242395
  validation accuracy:		14.02 %
Epoch 132 of 2000 took 0.096s
  training loss:		2.296202
  validation loss:		2.245815
  validation accuracy:		16.52 %
Epoch 133 of 2000 took 0.096s
  training loss:		2.295581
  validation loss:		2.245727
  validation accuracy:		12.93 %
Epoch 134 of 2000 took 0.096s
  training loss:		2.296038
  validation loss:		2.247023
  validation accuracy:		12.83 %
Epoch 135 of 2000 took 0.096s
  training loss:		2.295104
  validation loss:		2.242590
  validation accuracy:		12.93 %
Epoch 136 of 2000 took 0.096s
  training loss:		2.295070
  validation loss:		2.243898
  validation accuracy:		12.93 %
Epoch 137 of 2000 took 0.096s
  training loss:		2.295578
  validation loss:		2.243829
  validation accuracy:		12.93 %
Epoch 138 of 2000 took 0.096s
  training loss:		2.294813
  validation loss:		2.244460
  validation accuracy:		13.04 %
Epoch 139 of 2000 took 0.096s
  training loss:		2.294318
  validation loss:		2.243036
  validation accuracy:		14.78 %
Epoch 140 of 2000 took 0.096s
  training loss:		2.294439
  validation loss:		2.243447
  validation accuracy:		13.04 %
Epoch 141 of 2000 took 0.096s
  training loss:		2.295624
  validation loss:		2.243852
  validation accuracy:		13.04 %
Epoch 142 of 2000 took 0.096s
  training loss:		2.294619
  validation loss:		2.243794
  validation accuracy:		15.65 %
Epoch 143 of 2000 took 0.096s
  training loss:		2.296277
  validation loss:		2.244807
  validation accuracy:		12.93 %
Epoch 144 of 2000 took 0.096s
  training loss:		2.294999
  validation loss:		2.245995
  validation accuracy:		12.83 %
Epoch 145 of 2000 took 0.096s
  training loss:		2.295564
  validation loss:		2.245463
  validation accuracy:		12.83 %
Epoch 146 of 2000 took 0.096s
  training loss:		2.294870
  validation loss:		2.244031
  validation accuracy:		13.04 %
Epoch 147 of 2000 took 0.096s
  training loss:		2.295328
  validation loss:		2.244514
  validation accuracy:		13.59 %
Epoch 148 of 2000 took 0.096s
  training loss:		2.294860
  validation loss:		2.247940
  validation accuracy:		13.04 %
Epoch 149 of 2000 took 0.096s
  training loss:		2.294157
  validation loss:		2.243957
  validation accuracy:		16.09 %
Epoch 150 of 2000 took 0.096s
  training loss:		2.295443
  validation loss:		2.244206
  validation accuracy:		12.83 %
Epoch 151 of 2000 took 0.096s
  training loss:		2.295724
  validation loss:		2.242208
  validation accuracy:		12.83 %
Epoch 152 of 2000 took 0.096s
  training loss:		2.295122
  validation loss:		2.243981
  validation accuracy:		13.04 %
Epoch 153 of 2000 took 0.096s
  training loss:		2.295377
  validation loss:		2.245282
  validation accuracy:		13.04 %
Epoch 154 of 2000 took 0.096s
  training loss:		2.295340
  validation loss:		2.242248
  validation accuracy:		12.93 %
Epoch 155 of 2000 took 0.096s
  training loss:		2.294872
  validation loss:		2.247031
  validation accuracy:		12.83 %
Epoch 156 of 2000 took 0.097s
  training loss:		2.295296
  validation loss:		2.246828
  validation accuracy:		14.89 %
Epoch 157 of 2000 took 0.096s
  training loss:		2.294743
  validation loss:		2.245092
  validation accuracy:		12.83 %
Epoch 158 of 2000 took 0.096s
  training loss:		2.295607
  validation loss:		2.244221
  validation accuracy:		12.93 %
Epoch 159 of 2000 took 0.096s
  training loss:		2.295193
  validation loss:		2.245376
  validation accuracy:		13.37 %
Epoch 160 of 2000 took 0.096s
  training loss:		2.295358
  validation loss:		2.244097
  validation accuracy:		13.04 %
Epoch 161 of 2000 took 0.096s
  training loss:		2.295213
  validation loss:		2.246539
  validation accuracy:		13.04 %
Epoch 162 of 2000 took 0.096s
  training loss:		2.294924
  validation loss:		2.245651
  validation accuracy:		13.04 %
Epoch 163 of 2000 took 0.096s
  training loss:		2.295329
  validation loss:		2.244033
  validation accuracy:		12.93 %
Epoch 164 of 2000 took 0.096s
  training loss:		2.294338
  validation loss:		2.242766
  validation accuracy:		12.93 %
Epoch 165 of 2000 took 0.096s
  training loss:		2.295329
  validation loss:		2.242797
  validation accuracy:		16.30 %
Epoch 166 of 2000 took 0.096s
  training loss:		2.294495
  validation loss:		2.244342
  validation accuracy:		12.83 %
Epoch 167 of 2000 took 0.096s
  training loss:		2.295126
  validation loss:		2.245407
  validation accuracy:		12.83 %
Epoch 168 of 2000 took 0.096s
  training loss:		2.294168
  validation loss:		2.242944
  validation accuracy:		13.04 %
Epoch 169 of 2000 took 0.096s
  training loss:		2.294511
  validation loss:		2.243583
  validation accuracy:		12.93 %
Epoch 170 of 2000 took 0.096s
  training loss:		2.295597
  validation loss:		2.246453
  validation accuracy:		13.04 %
Epoch 171 of 2000 took 0.096s
  training loss:		2.294774
  validation loss:		2.243536
  validation accuracy:		12.72 %
Epoch 172 of 2000 took 0.096s
  training loss:		2.295413
  validation loss:		2.243685
  validation accuracy:		16.41 %
Epoch 173 of 2000 took 0.096s
  training loss:		2.295197
  validation loss:		2.245706
  validation accuracy:		18.15 %
Epoch 174 of 2000 took 0.096s
  training loss:		2.294472
  validation loss:		2.245022
  validation accuracy:		12.93 %
Epoch 175 of 2000 took 0.097s
  training loss:		2.294127
  validation loss:		2.242464
  validation accuracy:		12.93 %
Epoch 176 of 2000 took 0.096s
  training loss:		2.295433
  validation loss:		2.244843
  validation accuracy:		12.93 %
Epoch 177 of 2000 took 0.099s
  training loss:		2.294588
  validation loss:		2.245104
  validation accuracy:		17.28 %
Epoch 178 of 2000 took 0.097s
  training loss:		2.295293
  validation loss:		2.246382
  validation accuracy:		13.04 %
Epoch 179 of 2000 took 0.121s
  training loss:		2.295207
  validation loss:		2.245220
  validation accuracy:		16.41 %
Epoch 180 of 2000 took 0.151s
  training loss:		2.293868
  validation loss:		2.242062
  validation accuracy:		12.93 %
Epoch 181 of 2000 took 0.101s
  training loss:		2.295013
  validation loss:		2.242847
  validation accuracy:		17.39 %
Epoch 182 of 2000 took 0.103s
  training loss:		2.294332
  validation loss:		2.243528
  validation accuracy:		12.83 %
Epoch 183 of 2000 took 0.099s
  training loss:		2.294536
  validation loss:		2.243094
  validation accuracy:		12.93 %
Epoch 184 of 2000 took 0.102s
  training loss:		2.294791
  validation loss:		2.246726
  validation accuracy:		12.93 %
Epoch 185 of 2000 took 0.103s
  training loss:		2.294629
  validation loss:		2.245653
  validation accuracy:		13.26 %
Epoch 186 of 2000 took 0.102s
  training loss:		2.294580
  validation loss:		2.243128
  validation accuracy:		13.04 %
Epoch 187 of 2000 took 0.101s
  training loss:		2.295046
  validation loss:		2.246566
  validation accuracy:		14.78 %
Epoch 188 of 2000 took 0.100s
  training loss:		2.294500
  validation loss:		2.244946
  validation accuracy:		13.59 %
Epoch 189 of 2000 took 0.100s
  training loss:		2.294476
  validation loss:		2.243639
  validation accuracy:		13.04 %
Epoch 190 of 2000 took 0.100s
  training loss:		2.294496
  validation loss:		2.245413
  validation accuracy:		15.76 %
Epoch 191 of 2000 took 0.099s
  training loss:		2.294921
  validation loss:		2.243190
  validation accuracy:		17.28 %
Epoch 192 of 2000 took 0.100s
  training loss:		2.294991
  validation loss:		2.242624
  validation accuracy:		13.04 %
Epoch 193 of 2000 took 0.100s
  training loss:		2.294443
  validation loss:		2.246365
  validation accuracy:		12.83 %
Epoch 194 of 2000 took 0.100s
  training loss:		2.295052
  validation loss:		2.246875
  validation accuracy:		12.93 %
Epoch 195 of 2000 took 0.100s
  training loss:		2.294700
  validation loss:		2.242425
  validation accuracy:		13.04 %
Epoch 196 of 2000 took 0.100s
  training loss:		2.294896
  validation loss:		2.243974
  validation accuracy:		12.83 %
Epoch 197 of 2000 took 0.100s
  training loss:		2.294336
  validation loss:		2.243367
  validation accuracy:		15.00 %
Epoch 198 of 2000 took 0.100s
  training loss:		2.294413
  validation loss:		2.243246
  validation accuracy:		12.93 %
Epoch 199 of 2000 took 0.100s
  training loss:		2.294787
  validation loss:		2.243713
  validation accuracy:		12.93 %
Epoch 200 of 2000 took 0.100s
  training loss:		2.294362
  validation loss:		2.245138
  validation accuracy:		12.93 %
Epoch 201 of 2000 took 0.100s
  training loss:		2.294613
  validation loss:		2.242052
  validation accuracy:		12.93 %
Epoch 202 of 2000 took 0.100s
  training loss:		2.294140
  validation loss:		2.244026
  validation accuracy:		16.41 %
Epoch 203 of 2000 took 0.101s
  training loss:		2.295556
  validation loss:		2.245049
  validation accuracy:		13.04 %
Epoch 204 of 2000 took 0.099s
  training loss:		2.295179
  validation loss:		2.248412
  validation accuracy:		13.04 %
Epoch 205 of 2000 took 0.100s
  training loss:		2.295173
  validation loss:		2.245555
  validation accuracy:		12.83 %
Epoch 206 of 2000 took 0.100s
  training loss:		2.294827
  validation loss:		2.244197
  validation accuracy:		12.83 %
Epoch 207 of 2000 took 0.100s
  training loss:		2.294142
  validation loss:		2.245056
  validation accuracy:		12.93 %
Epoch 208 of 2000 took 0.100s
  training loss:		2.294207
  validation loss:		2.241474
  validation accuracy:		12.93 %
Epoch 209 of 2000 took 0.099s
  training loss:		2.294049
  validation loss:		2.238729
  validation accuracy:		13.04 %
Epoch 210 of 2000 took 0.100s
  training loss:		2.293773
  validation loss:		2.240210
  validation accuracy:		13.04 %
Epoch 211 of 2000 took 0.099s
  training loss:		2.294296
  validation loss:		2.244431
  validation accuracy:		15.11 %
Epoch 212 of 2000 took 0.100s
  training loss:		2.294412
  validation loss:		2.244476
  validation accuracy:		12.93 %
Epoch 213 of 2000 took 0.100s
  training loss:		2.294682
  validation loss:		2.244898
  validation accuracy:		12.93 %
Epoch 214 of 2000 took 0.100s
  training loss:		2.294407
  validation loss:		2.243859
  validation accuracy:		13.80 %
Epoch 215 of 2000 took 0.100s
  training loss:		2.294405
  validation loss:		2.242401
  validation accuracy:		12.93 %
Epoch 216 of 2000 took 0.100s
  training loss:		2.293772
  validation loss:		2.245729
  validation accuracy:		12.83 %
Epoch 217 of 2000 took 0.100s
  training loss:		2.293727
  validation loss:		2.244073
  validation accuracy:		12.83 %
Epoch 218 of 2000 took 0.100s
  training loss:		2.294318
  validation loss:		2.240726
  validation accuracy:		12.83 %
Epoch 219 of 2000 took 0.102s
  training loss:		2.293530
  validation loss:		2.240358
  validation accuracy:		17.50 %
Epoch 220 of 2000 took 0.131s
  training loss:		2.293548
  validation loss:		2.240836
  validation accuracy:		13.04 %
Epoch 221 of 2000 took 0.144s
  training loss:		2.294805
  validation loss:		2.241312
  validation accuracy:		14.24 %
Epoch 222 of 2000 took 0.100s
  training loss:		2.294594
  validation loss:		2.245999
  validation accuracy:		15.33 %
Epoch 223 of 2000 took 0.099s
  training loss:		2.294167
  validation loss:		2.244551
  validation accuracy:		12.93 %
Epoch 224 of 2000 took 0.100s
  training loss:		2.294190
  validation loss:		2.242672
  validation accuracy:		16.74 %
Epoch 225 of 2000 took 0.104s
  training loss:		2.294551
  validation loss:		2.242625
  validation accuracy:		12.93 %
Epoch 226 of 2000 took 0.107s
  training loss:		2.294455
  validation loss:		2.245526
  validation accuracy:		13.04 %
Epoch 227 of 2000 took 0.100s
  training loss:		2.294445
  validation loss:		2.243134
  validation accuracy:		12.83 %
Epoch 228 of 2000 took 0.099s
  training loss:		2.295093
  validation loss:		2.245913
  validation accuracy:		12.83 %
Epoch 229 of 2000 took 0.100s
  training loss:		2.293673
  validation loss:		2.244850
  validation accuracy:		13.37 %
Epoch 230 of 2000 took 0.099s
  training loss:		2.294575
  validation loss:		2.240282
  validation accuracy:		15.98 %
Epoch 231 of 2000 took 0.100s
  training loss:		2.294635
  validation loss:		2.242669
  validation accuracy:		14.78 %
Epoch 232 of 2000 took 0.100s
  training loss:		2.294191
  validation loss:		2.245545
  validation accuracy:		13.04 %
Epoch 233 of 2000 took 0.103s
  training loss:		2.294049
  validation loss:		2.246267
  validation accuracy:		13.04 %
Epoch 234 of 2000 took 0.100s
  training loss:		2.294478
  validation loss:		2.244167
  validation accuracy:		12.93 %
Epoch 235 of 2000 took 0.100s
  training loss:		2.294254
  validation loss:		2.244152
  validation accuracy:		12.83 %
Epoch 236 of 2000 took 0.100s
  training loss:		2.293890
  validation loss:		2.243678
  validation accuracy:		14.67 %
Epoch 237 of 2000 took 0.102s
  training loss:		2.293550
  validation loss:		2.244777
  validation accuracy:		13.59 %
Epoch 238 of 2000 took 0.102s
  training loss:		2.294856
  validation loss:		2.245154
  validation accuracy:		12.83 %
Epoch 239 of 2000 took 0.100s
  training loss:		2.293829
  validation loss:		2.246068
  validation accuracy:		13.59 %
Epoch 240 of 2000 took 0.100s
  training loss:		2.294617
  validation loss:		2.240670
  validation accuracy:		13.26 %
Epoch 241 of 2000 took 0.103s
  training loss:		2.294705
  validation loss:		2.242550
  validation accuracy:		13.04 %
Epoch 242 of 2000 took 0.102s
  training loss:		2.294623
  validation loss:		2.244048
  validation accuracy:		13.80 %
Epoch 243 of 2000 took 0.102s
  training loss:		2.293833
  validation loss:		2.242357
  validation accuracy:		13.15 %
Epoch 244 of 2000 took 0.102s
  training loss:		2.293614
  validation loss:		2.242194
  validation accuracy:		12.83 %
Epoch 245 of 2000 took 0.102s
  training loss:		2.294040
  validation loss:		2.244217
  validation accuracy:		14.35 %
Epoch 246 of 2000 took 0.102s
  training loss:		2.293721
  validation loss:		2.243952
  validation accuracy:		13.04 %
Epoch 247 of 2000 took 0.102s
  training loss:		2.293945
  validation loss:		2.241715
  validation accuracy:		15.76 %
Epoch 248 of 2000 took 0.102s
  training loss:		2.294770
  validation loss:		2.245526
  validation accuracy:		12.93 %
Epoch 249 of 2000 took 0.103s
  training loss:		2.293626
  validation loss:		2.245354
  validation accuracy:		12.93 %
Epoch 250 of 2000 took 0.102s
  training loss:		2.293559
  validation loss:		2.243192
  validation accuracy:		12.93 %
Epoch 251 of 2000 took 0.102s
  training loss:		2.293575
  validation loss:		2.240152
  validation accuracy:		19.02 %
Epoch 252 of 2000 took 0.103s
  training loss:		2.294134
  validation loss:		2.238597
  validation accuracy:		13.04 %
Epoch 253 of 2000 took 0.102s
  training loss:		2.295416
  validation loss:		2.247635
  validation accuracy:		14.13 %
Epoch 254 of 2000 took 0.102s
  training loss:		2.293606
  validation loss:		2.246178
  validation accuracy:		13.80 %
Epoch 255 of 2000 took 0.102s
  training loss:		2.293406
  validation loss:		2.241602
  validation accuracy:		14.13 %
Epoch 256 of 2000 took 0.102s
  training loss:		2.294703
  validation loss:		2.241962
  validation accuracy:		12.83 %
Epoch 257 of 2000 took 0.102s
  training loss:		2.293681
  validation loss:		2.243399
  validation accuracy:		12.93 %
Epoch 258 of 2000 took 0.102s
  training loss:		2.294164
  validation loss:		2.245385
  validation accuracy:		13.04 %
Epoch 259 of 2000 took 0.102s
  training loss:		2.293558
  validation loss:		2.240611
  validation accuracy:		13.04 %
Epoch 260 of 2000 took 0.102s
  training loss:		2.294804
  validation loss:		2.241280
  validation accuracy:		12.83 %
Epoch 261 of 2000 took 0.102s
  training loss:		2.294647
  validation loss:		2.247293
  validation accuracy:		12.83 %
Epoch 262 of 2000 took 0.102s
  training loss:		2.293380
  validation loss:		2.246125
  validation accuracy:		14.46 %
Epoch 263 of 2000 took 0.102s
  training loss:		2.294491
  validation loss:		2.244178
  validation accuracy:		13.15 %
Epoch 264 of 2000 took 0.102s
  training loss:		2.293678
  validation loss:		2.243752
  validation accuracy:		13.04 %
Epoch 265 of 2000 took 0.102s
  training loss:		2.293211
  validation loss:		2.241059
  validation accuracy:		13.04 %
Epoch 266 of 2000 took 0.102s
  training loss:		2.294394
  validation loss:		2.242631
  validation accuracy:		12.83 %
Epoch 267 of 2000 took 0.102s
  training loss:		2.294804
  validation loss:		2.243815
  validation accuracy:		14.24 %
Epoch 268 of 2000 took 0.102s
  training loss:		2.293160
  validation loss:		2.245012
  validation accuracy:		13.04 %
Epoch 269 of 2000 took 0.102s
  training loss:		2.294007
  validation loss:		2.243860
  validation accuracy:		16.09 %
Epoch 270 of 2000 took 0.102s
  training loss:		2.294659
  validation loss:		2.243179
  validation accuracy:		12.93 %
Epoch 271 of 2000 took 0.102s
  training loss:		2.294207
  validation loss:		2.244035
  validation accuracy:		12.83 %
Epoch 272 of 2000 took 0.102s
  training loss:		2.294518
  validation loss:		2.244437
  validation accuracy:		12.93 %
Epoch 273 of 2000 took 0.102s
  training loss:		2.293320
  validation loss:		2.243816
  validation accuracy:		14.78 %
Epoch 274 of 2000 took 0.102s
  training loss:		2.294360
  validation loss:		2.244600
  validation accuracy:		15.43 %
Epoch 275 of 2000 took 0.102s
  training loss:		2.293982
  validation loss:		2.243441
  validation accuracy:		13.04 %
Epoch 276 of 2000 took 0.102s
  training loss:		2.294050
  validation loss:		2.243862
  validation accuracy:		17.93 %
Epoch 277 of 2000 took 0.102s
  training loss:		2.292888
  validation loss:		2.242375
  validation accuracy:		16.09 %
Epoch 278 of 2000 took 0.103s
  training loss:		2.293164
  validation loss:		2.241231
  validation accuracy:		13.59 %
Epoch 279 of 2000 took 0.102s
  training loss:		2.292635
  validation loss:		2.240077
  validation accuracy:		12.83 %
Epoch 280 of 2000 took 0.102s
  training loss:		2.293071
  validation loss:		2.238848
  validation accuracy:		14.13 %
Epoch 281 of 2000 took 0.102s
  training loss:		2.294760
  validation loss:		2.244518
  validation accuracy:		13.26 %
Epoch 282 of 2000 took 0.102s
  training loss:		2.293868
  validation loss:		2.245392
  validation accuracy:		12.83 %
Epoch 283 of 2000 took 0.102s
  training loss:		2.293656
  validation loss:		2.242399
  validation accuracy:		15.76 %
Epoch 284 of 2000 took 0.102s
  training loss:		2.293639
  validation loss:		2.244428
  validation accuracy:		15.87 %
Epoch 285 of 2000 took 0.102s
  training loss:		2.293118
  validation loss:		2.241835
  validation accuracy:		13.04 %
Epoch 286 of 2000 took 0.102s
  training loss:		2.293514
  validation loss:		2.241651
  validation accuracy:		13.15 %
Epoch 287 of 2000 took 0.102s
  training loss:		2.294090
  validation loss:		2.243051
  validation accuracy:		17.50 %
Epoch 288 of 2000 took 0.102s
  training loss:		2.294022
  validation loss:		2.244716
  validation accuracy:		18.59 %
Epoch 289 of 2000 took 0.102s
  training loss:		2.293900
  validation loss:		2.242640
  validation accuracy:		12.83 %
Epoch 290 of 2000 took 0.102s
  training loss:		2.294057
  validation loss:		2.241584
  validation accuracy:		13.04 %
Epoch 291 of 2000 took 0.102s
  training loss:		2.293713
  validation loss:		2.246061
  validation accuracy:		13.04 %
Epoch 292 of 2000 took 0.102s
  training loss:		2.293857
  validation loss:		2.246643
  validation accuracy:		13.80 %
Epoch 293 of 2000 took 0.102s
  training loss:		2.294531
  validation loss:		2.245832
  validation accuracy:		13.26 %
Epoch 294 of 2000 took 0.102s
  training loss:		2.293785
  validation loss:		2.243462
  validation accuracy:		13.70 %
Epoch 295 of 2000 took 0.102s
  training loss:		2.293940
  validation loss:		2.242727
  validation accuracy:		14.13 %
Epoch 296 of 2000 took 0.102s
  training loss:		2.293727
  validation loss:		2.245315
  validation accuracy:		13.48 %
Epoch 297 of 2000 took 0.102s
  training loss:		2.292931
  validation loss:		2.241675
  validation accuracy:		19.35 %
Epoch 298 of 2000 took 0.102s
  training loss:		2.294182
  validation loss:		2.242372
  validation accuracy:		17.17 %
Epoch 299 of 2000 took 0.102s
  training loss:		2.293470
  validation loss:		2.245582
  validation accuracy:		15.98 %
Epoch 300 of 2000 took 0.102s
  training loss:		2.293773
  validation loss:		2.244364
  validation accuracy:		15.11 %
Epoch 301 of 2000 took 0.102s
  training loss:		2.293342
  validation loss:		2.238777
  validation accuracy:		13.04 %
Epoch 302 of 2000 took 0.102s
  training loss:		2.293499
  validation loss:		2.243562
  validation accuracy:		17.07 %
Epoch 303 of 2000 took 0.102s
  training loss:		2.292235
  validation loss:		2.242642
  validation accuracy:		12.93 %
Epoch 304 of 2000 took 0.102s
  training loss:		2.293587
  validation loss:		2.242802
  validation accuracy:		13.04 %
Epoch 305 of 2000 took 0.102s
  training loss:		2.293912
  validation loss:		2.243336
  validation accuracy:		13.04 %
Epoch 306 of 2000 took 0.102s
  training loss:		2.292765
  validation loss:		2.242273
  validation accuracy:		12.93 %
Epoch 307 of 2000 took 0.102s
  training loss:		2.293643
  validation loss:		2.240679
  validation accuracy:		13.37 %
Epoch 308 of 2000 took 0.103s
  training loss:		2.292388
  validation loss:		2.240677
  validation accuracy:		14.46 %
Epoch 309 of 2000 took 0.102s
  training loss:		2.292750
  validation loss:		2.241687
  validation accuracy:		13.48 %
Epoch 310 of 2000 took 0.102s
  training loss:		2.293822
  validation loss:		2.240325
  validation accuracy:		18.04 %
Epoch 311 of 2000 took 0.102s
  training loss:		2.293475
  validation loss:		2.244780
  validation accuracy:		13.37 %
Epoch 312 of 2000 took 0.102s
  training loss:		2.293315
  validation loss:		2.245804
  validation accuracy:		14.35 %
Epoch 313 of 2000 took 0.103s
  training loss:		2.292957
  validation loss:		2.244259
  validation accuracy:		13.59 %
Epoch 314 of 2000 took 0.102s
  training loss:		2.293336
  validation loss:		2.241528
  validation accuracy:		12.93 %
Epoch 315 of 2000 took 0.102s
  training loss:		2.293287
  validation loss:		2.242999
  validation accuracy:		16.41 %
Epoch 316 of 2000 took 0.102s
  training loss:		2.292353
  validation loss:		2.241474
  validation accuracy:		17.28 %
Epoch 317 of 2000 took 0.102s
  training loss:		2.293300
  validation loss:		2.242154
  validation accuracy:		17.17 %
Epoch 318 of 2000 took 0.102s
  training loss:		2.294342
  validation loss:		2.244449
  validation accuracy:		13.37 %
Epoch 319 of 2000 took 0.102s
  training loss:		2.292653
  validation loss:		2.243688
  validation accuracy:		13.26 %
Epoch 320 of 2000 took 0.102s
  training loss:		2.292678
  validation loss:		2.242190
  validation accuracy:		13.59 %
Epoch 321 of 2000 took 0.102s
  training loss:		2.292412
  validation loss:		2.240520
  validation accuracy:		13.26 %
Epoch 322 of 2000 took 0.103s
  training loss:		2.292690
  validation loss:		2.238286
  validation accuracy:		17.50 %
Epoch 323 of 2000 took 0.103s
  training loss:		2.292018
  validation loss:		2.241629
  validation accuracy:		17.07 %
Epoch 324 of 2000 took 0.102s
  training loss:		2.294023
  validation loss:		2.243247
  validation accuracy:		14.02 %
Epoch 325 of 2000 took 0.104s
  training loss:		2.293305
  validation loss:		2.240727
  validation accuracy:		14.35 %
Epoch 326 of 2000 took 0.102s
  training loss:		2.293266
  validation loss:		2.244075
  validation accuracy:		12.93 %
Epoch 327 of 2000 took 0.102s
  training loss:		2.293002
  validation loss:		2.244442
  validation accuracy:		13.37 %
Epoch 328 of 2000 took 0.102s
  training loss:		2.292894
  validation loss:		2.240813
  validation accuracy:		15.22 %
Epoch 329 of 2000 took 0.102s
  training loss:		2.293046
  validation loss:		2.243618
  validation accuracy:		14.02 %
Epoch 330 of 2000 took 0.102s
  training loss:		2.292550
  validation loss:		2.238679
  validation accuracy:		16.20 %
Epoch 331 of 2000 took 0.102s
  training loss:		2.292515
  validation loss:		2.241522
  validation accuracy:		13.59 %
Epoch 332 of 2000 took 0.102s
  training loss:		2.292045
  validation loss:		2.242180
  validation accuracy:		13.04 %
Epoch 333 of 2000 took 0.102s
  training loss:		2.292966
  validation loss:		2.244288
  validation accuracy:		17.50 %
Epoch 334 of 2000 took 0.102s
  training loss:		2.293475
  validation loss:		2.245801
  validation accuracy:		15.65 %
Epoch 335 of 2000 took 0.102s
  training loss:		2.292272
  validation loss:		2.241920
  validation accuracy:		18.37 %
Epoch 336 of 2000 took 0.102s
  training loss:		2.292160
  validation loss:		2.238840
  validation accuracy:		17.39 %
Epoch 337 of 2000 took 0.103s
  training loss:		2.292711
  validation loss:		2.241757
  validation accuracy:		15.22 %
Epoch 338 of 2000 took 0.102s
  training loss:		2.291612
  validation loss:		2.241197
  validation accuracy:		16.20 %
Epoch 339 of 2000 took 0.102s
  training loss:		2.293620
  validation loss:		2.241730
  validation accuracy:		13.04 %
Epoch 340 of 2000 took 0.103s
  training loss:		2.292875
  validation loss:		2.242130
  validation accuracy:		13.15 %
Epoch 341 of 2000 took 0.102s
  training loss:		2.292096
  validation loss:		2.243810
  validation accuracy:		17.83 %
Epoch 342 of 2000 took 0.102s
  training loss:		2.292810
  validation loss:		2.239941
  validation accuracy:		18.26 %
Epoch 343 of 2000 took 0.102s
  training loss:		2.293158
  validation loss:		2.242370
  validation accuracy:		13.04 %
Epoch 344 of 2000 took 0.102s
  training loss:		2.292144
  validation loss:		2.241297
  validation accuracy:		15.33 %
Epoch 345 of 2000 took 0.102s
  training loss:		2.292914
  validation loss:		2.242469
  validation accuracy:		14.67 %
Epoch 346 of 2000 took 0.102s
  training loss:		2.293881
  validation loss:		2.244347
  validation accuracy:		13.70 %
Epoch 347 of 2000 took 0.102s
  training loss:		2.292480
  validation loss:		2.243470
  validation accuracy:		14.78 %
Epoch 348 of 2000 took 0.102s
  training loss:		2.292547
  validation loss:		2.244532
  validation accuracy:		13.04 %
Epoch 349 of 2000 took 0.102s
  training loss:		2.291746
  validation loss:		2.239572
  validation accuracy:		15.87 %
Epoch 350 of 2000 took 0.102s
  training loss:		2.291917
  validation loss:		2.238825
  validation accuracy:		13.15 %
Epoch 351 of 2000 took 0.102s
  training loss:		2.292122
  validation loss:		2.242675
  validation accuracy:		13.15 %
Epoch 352 of 2000 took 0.102s
  training loss:		2.291548
  validation loss:		2.239683
  validation accuracy:		13.04 %
Epoch 353 of 2000 took 0.102s
  training loss:		2.291920
  validation loss:		2.240353
  validation accuracy:		13.04 %
Epoch 354 of 2000 took 0.102s
  training loss:		2.291659
  validation loss:		2.240103
  validation accuracy:		17.83 %
Epoch 355 of 2000 took 0.102s
  training loss:		2.292725
  validation loss:		2.242250
  validation accuracy:		14.35 %
Epoch 356 of 2000 took 0.102s
  training loss:		2.291002
  validation loss:		2.240998
  validation accuracy:		14.46 %
Epoch 357 of 2000 took 0.103s
  training loss:		2.292028
  validation loss:		2.241783
  validation accuracy:		12.50 %
Epoch 358 of 2000 took 0.102s
  training loss:		2.292694
  validation loss:		2.242985
  validation accuracy:		17.17 %
Epoch 359 of 2000 took 0.102s
  training loss:		2.291521
  validation loss:		2.243848
  validation accuracy:		13.04 %
Epoch 360 of 2000 took 0.102s
  training loss:		2.292194
  validation loss:		2.244144
  validation accuracy:		13.48 %
Epoch 361 of 2000 took 0.102s
  training loss:		2.291806
  validation loss:		2.240973
  validation accuracy:		16.52 %
Epoch 362 of 2000 took 0.102s
  training loss:		2.291608
  validation loss:		2.240127
  validation accuracy:		17.39 %
Epoch 363 of 2000 took 0.102s
  training loss:		2.291202
  validation loss:		2.237760
  validation accuracy:		14.57 %
Epoch 364 of 2000 took 0.102s
  training loss:		2.291802
  validation loss:		2.239024
  validation accuracy:		18.26 %
Epoch 365 of 2000 took 0.102s
  training loss:		2.292381
  validation loss:		2.240424
  validation accuracy:		17.39 %
Epoch 366 of 2000 took 0.102s
  training loss:		2.291152
  validation loss:		2.241175
  validation accuracy:		18.91 %
Epoch 367 of 2000 took 0.103s
  training loss:		2.291069
  validation loss:		2.240756
  validation accuracy:		13.70 %
Epoch 368 of 2000 took 0.102s
  training loss:		2.290307
  validation loss:		2.240016
  validation accuracy:		15.33 %
Epoch 369 of 2000 took 0.103s
  training loss:		2.291568
  validation loss:		2.242406
  validation accuracy:		15.33 %
Epoch 370 of 2000 took 0.102s
  training loss:		2.291784
  validation loss:		2.239255
  validation accuracy:		13.04 %
Epoch 371 of 2000 took 0.102s
  training loss:		2.290894
  validation loss:		2.239430
  validation accuracy:		18.59 %
Epoch 372 of 2000 took 0.102s
  training loss:		2.290649
  validation loss:		2.238346
  validation accuracy:		16.63 %
Epoch 373 of 2000 took 0.102s
  training loss:		2.290766
  validation loss:		2.240158
  validation accuracy:		19.57 %
Epoch 374 of 2000 took 0.105s
  training loss:		2.291468
  validation loss:		2.241421
  validation accuracy:		18.59 %
Epoch 375 of 2000 took 0.106s
  training loss:		2.291581
  validation loss:		2.241616
  validation accuracy:		16.52 %
Epoch 376 of 2000 took 0.105s
  training loss:		2.290527
  validation loss:		2.239052
  validation accuracy:		17.72 %
Epoch 377 of 2000 took 0.106s
  training loss:		2.290247
  validation loss:		2.238810
  validation accuracy:		17.17 %
Epoch 378 of 2000 took 0.105s
  training loss:		2.290027
  validation loss:		2.238159
  validation accuracy:		18.15 %
Epoch 379 of 2000 took 0.103s
  training loss:		2.290512
  validation loss:		2.239016
  validation accuracy:		20.11 %
Epoch 380 of 2000 took 0.102s
  training loss:		2.290841
  validation loss:		2.239176
  validation accuracy:		15.11 %
Epoch 381 of 2000 took 0.102s
  training loss:		2.291060
  validation loss:		2.242485
  validation accuracy:		17.93 %
Epoch 382 of 2000 took 0.102s
  training loss:		2.291618
  validation loss:		2.241226
  validation accuracy:		17.50 %
Epoch 383 of 2000 took 0.102s
  training loss:		2.289791
  validation loss:		2.239759
  validation accuracy:		15.87 %
Epoch 384 of 2000 took 0.102s
  training loss:		2.290289
  validation loss:		2.239858
  validation accuracy:		16.52 %
Epoch 385 of 2000 took 0.102s
  training loss:		2.289918
  validation loss:		2.241088
  validation accuracy:		14.46 %
Epoch 386 of 2000 took 0.102s
  training loss:		2.289914
  validation loss:		2.236033
  validation accuracy:		18.70 %
Epoch 387 of 2000 took 0.102s
  training loss:		2.289795
  validation loss:		2.237884
  validation accuracy:		14.46 %
Epoch 388 of 2000 took 0.103s
  training loss:		2.290582
  validation loss:		2.240344
  validation accuracy:		16.20 %
Epoch 389 of 2000 took 0.102s
  training loss:		2.289430
  validation loss:		2.239780
  validation accuracy:		16.74 %
Epoch 390 of 2000 took 0.102s
  training loss:		2.289522
  validation loss:		2.240774
  validation accuracy:		18.70 %
Epoch 391 of 2000 took 0.106s
  training loss:		2.289685
  validation loss:		2.233609
  validation accuracy:		16.52 %
Epoch 392 of 2000 took 0.102s
  training loss:		2.289547
  validation loss:		2.237965
  validation accuracy:		18.48 %
Epoch 393 of 2000 took 0.102s
  training loss:		2.289404
  validation loss:		2.240537
  validation accuracy:		14.78 %
Epoch 394 of 2000 took 0.102s
  training loss:		2.288863
  validation loss:		2.237891
  validation accuracy:		17.61 %
Epoch 395 of 2000 took 0.102s
  training loss:		2.291027
  validation loss:		2.237846
  validation accuracy:		15.65 %
Epoch 396 of 2000 took 0.103s
  training loss:		2.289746
  validation loss:		2.242000
  validation accuracy:		19.57 %
Epoch 397 of 2000 took 0.102s
  training loss:		2.289810
  validation loss:		2.239720
  validation accuracy:		18.91 %
Epoch 398 of 2000 took 0.102s
  training loss:		2.289536
  validation loss:		2.239518
  validation accuracy:		18.48 %
Epoch 399 of 2000 took 0.102s
  training loss:		2.289021
  validation loss:		2.240575
  validation accuracy:		13.04 %
Epoch 400 of 2000 took 0.102s
  training loss:		2.289040
  validation loss:		2.240146
  validation accuracy:		13.26 %
Epoch 401 of 2000 took 0.102s
  training loss:		2.288978
  validation loss:		2.238612
  validation accuracy:		15.76 %
Epoch 402 of 2000 took 0.102s
  training loss:		2.289420
  validation loss:		2.239986
  validation accuracy:		14.67 %
Epoch 403 of 2000 took 0.102s
  training loss:		2.288345
  validation loss:		2.237854
  validation accuracy:		18.59 %
Epoch 404 of 2000 took 0.102s
  training loss:		2.287889
  validation loss:		2.235551
  validation accuracy:		17.39 %
Epoch 405 of 2000 took 0.102s
  training loss:		2.289335
  validation loss:		2.238999
  validation accuracy:		18.80 %
Epoch 406 of 2000 took 0.102s
  training loss:		2.287634
  validation loss:		2.235604
  validation accuracy:		18.15 %
Epoch 407 of 2000 took 0.102s
  training loss:		2.288325
  validation loss:		2.237785
  validation accuracy:		17.28 %
Epoch 408 of 2000 took 0.102s
  training loss:		2.288411
  validation loss:		2.237354
  validation accuracy:		18.59 %
Epoch 409 of 2000 took 0.102s
  training loss:		2.287302
  validation loss:		2.237514
  validation accuracy:		19.78 %
Epoch 410 of 2000 took 0.102s
  training loss:		2.287720
  validation loss:		2.236432
  validation accuracy:		14.89 %
Epoch 411 of 2000 took 0.102s
  training loss:		2.286536
  validation loss:		2.236683
  validation accuracy:		21.96 %
Epoch 412 of 2000 took 0.103s
  training loss:		2.287730
  validation loss:		2.235187
  validation accuracy:		18.48 %
Epoch 413 of 2000 took 0.102s
  training loss:		2.287232
  validation loss:		2.236735
  validation accuracy:		17.93 %
Epoch 414 of 2000 took 0.102s
  training loss:		2.286684
  validation loss:		2.239047
  validation accuracy:		19.46 %
Epoch 415 of 2000 took 0.102s
  training loss:		2.286813
  validation loss:		2.237084
  validation accuracy:		20.87 %
Epoch 416 of 2000 took 0.108s
  training loss:		2.286776
  validation loss:		2.234817
  validation accuracy:		21.20 %
Epoch 417 of 2000 took 0.106s
  training loss:		2.285827
  validation loss:		2.235200
  validation accuracy:		19.89 %
Epoch 418 of 2000 took 0.104s
  training loss:		2.285605
  validation loss:		2.233509
  validation accuracy:		19.57 %
Epoch 419 of 2000 took 0.102s
  training loss:		2.284991
  validation loss:		2.229224
  validation accuracy:		14.67 %
Epoch 420 of 2000 took 0.102s
  training loss:		2.283982
  validation loss:		2.233329
  validation accuracy:		16.20 %
Epoch 421 of 2000 took 0.102s
  training loss:		2.285274
  validation loss:		2.233175
  validation accuracy:		19.02 %
Epoch 422 of 2000 took 0.102s
  training loss:		2.284873
  validation loss:		2.234091
  validation accuracy:		19.35 %
Epoch 423 of 2000 took 0.102s
  training loss:		2.284539
  validation loss:		2.234309
  validation accuracy:		17.39 %
Epoch 424 of 2000 took 0.102s
  training loss:		2.285209
  validation loss:		2.232764
  validation accuracy:		20.76 %
Epoch 425 of 2000 took 0.103s
  training loss:		2.283988
  validation loss:		2.235065
  validation accuracy:		21.09 %
Epoch 426 of 2000 took 0.102s
  training loss:		2.283658
  validation loss:		2.229863
  validation accuracy:		19.67 %
Epoch 427 of 2000 took 0.102s
  training loss:		2.283502
  validation loss:		2.232340
  validation accuracy:		15.22 %
Epoch 428 of 2000 took 0.102s
  training loss:		2.283477
  validation loss:		2.234351
  validation accuracy:		15.33 %
Epoch 429 of 2000 took 0.102s
  training loss:		2.283463
  validation loss:		2.234093
  validation accuracy:		20.33 %
Epoch 430 of 2000 took 0.104s
  training loss:		2.282825
  validation loss:		2.233006
  validation accuracy:		20.65 %
Epoch 431 of 2000 took 0.102s
  training loss:		2.282660
  validation loss:		2.233182
  validation accuracy:		15.87 %
Epoch 432 of 2000 took 0.102s
  training loss:		2.281298
  validation loss:		2.231864
  validation accuracy:		20.65 %
Epoch 433 of 2000 took 0.102s
  training loss:		2.281833
  validation loss:		2.231561
  validation accuracy:		20.00 %
Epoch 434 of 2000 took 0.102s
  training loss:		2.281351
  validation loss:		2.229908
  validation accuracy:		21.20 %
Epoch 435 of 2000 took 0.102s
  training loss:		2.281393
  validation loss:		2.230764
  validation accuracy:		20.65 %
Epoch 436 of 2000 took 0.102s
  training loss:		2.280730
  validation loss:		2.232406
  validation accuracy:		16.52 %
Epoch 437 of 2000 took 0.102s
  training loss:		2.279236
  validation loss:		2.229590
  validation accuracy:		19.35 %
Epoch 438 of 2000 took 0.102s
  training loss:		2.278864
  validation loss:		2.225065
  validation accuracy:		21.63 %
Epoch 439 of 2000 took 0.102s
  training loss:		2.279001
  validation loss:		2.229243
  validation accuracy:		16.63 %
Epoch 440 of 2000 took 0.102s
  training loss:		2.279164
  validation loss:		2.229109
  validation accuracy:		20.65 %
Epoch 441 of 2000 took 0.102s
  training loss:		2.277040
  validation loss:		2.225735
  validation accuracy:		16.52 %
Epoch 442 of 2000 took 0.102s
  training loss:		2.276548
  validation loss:		2.224674
  validation accuracy:		17.61 %
Epoch 443 of 2000 took 0.102s
  training loss:		2.277080
  validation loss:		2.226671
  validation accuracy:		19.57 %
Epoch 444 of 2000 took 0.102s
  training loss:		2.276317
  validation loss:		2.226652
  validation accuracy:		20.33 %
Epoch 445 of 2000 took 0.102s
  training loss:		2.275669
  validation loss:		2.224567
  validation accuracy:		21.30 %
Epoch 446 of 2000 took 0.102s
  training loss:		2.273925
  validation loss:		2.225664
  validation accuracy:		22.17 %
Epoch 447 of 2000 took 0.102s
  training loss:		2.273123
  validation loss:		2.222905
  validation accuracy:		21.63 %
Epoch 448 of 2000 took 0.102s
  training loss:		2.272526
  validation loss:		2.219068
  validation accuracy:		16.30 %
Epoch 449 of 2000 took 0.102s
  training loss:		2.272300
  validation loss:		2.222593
  validation accuracy:		17.07 %
Epoch 450 of 2000 took 0.102s
  training loss:		2.271093
  validation loss:		2.221695
  validation accuracy:		21.20 %
Epoch 451 of 2000 took 0.102s
  training loss:		2.269584
  validation loss:		2.221960
  validation accuracy:		21.52 %
Epoch 452 of 2000 took 0.102s
  training loss:		2.267890
  validation loss:		2.212026
  validation accuracy:		20.43 %
Epoch 453 of 2000 took 0.102s
  training loss:		2.266527
  validation loss:		2.213103
  validation accuracy:		20.22 %
Epoch 454 of 2000 took 0.102s
  training loss:		2.264975
  validation loss:		2.213346
  validation accuracy:		21.74 %
Epoch 455 of 2000 took 0.103s
  training loss:		2.265087
  validation loss:		2.210634
  validation accuracy:		23.48 %
Epoch 456 of 2000 took 0.102s
  training loss:		2.263226
  validation loss:		2.210435
  validation accuracy:		25.65 %
Epoch 457 of 2000 took 0.102s
  training loss:		2.262101
  validation loss:		2.213133
  validation accuracy:		20.87 %
Epoch 458 of 2000 took 0.102s
  training loss:		2.258578
  validation loss:		2.207589
  validation accuracy:		21.09 %
Epoch 459 of 2000 took 0.102s
  training loss:		2.259567
  validation loss:		2.204276
  validation accuracy:		23.37 %
Epoch 460 of 2000 took 0.102s
  training loss:		2.255287
  validation loss:		2.205862
  validation accuracy:		23.48 %
Epoch 461 of 2000 took 0.102s
  training loss:		2.253178
  validation loss:		2.201311
  validation accuracy:		22.07 %
Epoch 462 of 2000 took 0.102s
  training loss:		2.250067
  validation loss:		2.196462
  validation accuracy:		21.09 %
Epoch 463 of 2000 took 0.102s
  training loss:		2.247780
  validation loss:		2.192869
  validation accuracy:		25.43 %
Epoch 464 of 2000 took 0.102s
  training loss:		2.245728
  validation loss:		2.190979
  validation accuracy:		22.28 %
Epoch 465 of 2000 took 0.102s
  training loss:		2.242331
  validation loss:		2.189122
  validation accuracy:		23.91 %
Epoch 466 of 2000 took 0.102s
  training loss:		2.238475
  validation loss:		2.185254
  validation accuracy:		24.35 %
Epoch 467 of 2000 took 0.102s
  training loss:		2.236144
  validation loss:		2.181519
  validation accuracy:		25.22 %
Epoch 468 of 2000 took 0.102s
  training loss:		2.231685
  validation loss:		2.177090
  validation accuracy:		24.57 %
Epoch 469 of 2000 took 0.102s
  training loss:		2.225302
  validation loss:		2.170918
  validation accuracy:		25.87 %
Epoch 470 of 2000 took 0.102s
  training loss:		2.220971
  validation loss:		2.165059
  validation accuracy:		26.20 %
Epoch 471 of 2000 took 0.102s
  training loss:		2.215238
  validation loss:		2.159062
  validation accuracy:		27.39 %
Epoch 472 of 2000 took 0.102s
  training loss:		2.208424
  validation loss:		2.150096
  validation accuracy:		24.57 %
Epoch 473 of 2000 took 0.102s
  training loss:		2.201410
  validation loss:		2.147134
  validation accuracy:		27.28 %
Epoch 474 of 2000 took 0.102s
  training loss:		2.193447
  validation loss:		2.134282
  validation accuracy:		25.43 %
Epoch 475 of 2000 took 0.102s
  training loss:		2.182492
  validation loss:		2.122484
  validation accuracy:		26.09 %
Epoch 476 of 2000 took 0.103s
  training loss:		2.174515
  validation loss:		2.111934
  validation accuracy:		26.85 %
Epoch 477 of 2000 took 0.102s
  training loss:		2.162139
  validation loss:		2.099212
  validation accuracy:		30.00 %
Epoch 478 of 2000 took 0.102s
  training loss:		2.150182
  validation loss:		2.084574
  validation accuracy:		30.43 %
Epoch 479 of 2000 took 0.102s
  training loss:		2.133640
  validation loss:		2.072432
  validation accuracy:		29.78 %
Epoch 480 of 2000 took 0.102s
  training loss:		2.119139
  validation loss:		2.051368
  validation accuracy:		29.67 %
Epoch 481 of 2000 took 0.102s
  training loss:		2.101197
  validation loss:		2.030323
  validation accuracy:		30.22 %
Epoch 482 of 2000 took 0.102s
  training loss:		2.079643
  validation loss:		2.008570
  validation accuracy:		31.85 %
Epoch 483 of 2000 took 0.102s
  training loss:		2.059797
  validation loss:		1.981755
  validation accuracy:		32.39 %
Epoch 484 of 2000 took 0.103s
  training loss:		2.035676
  validation loss:		1.960508
  validation accuracy:		32.39 %
Epoch 485 of 2000 took 0.102s
  training loss:		2.012161
  validation loss:		1.934399
  validation accuracy:		34.13 %
Epoch 486 of 2000 took 0.102s
  training loss:		1.986420
  validation loss:		1.910129
  validation accuracy:		34.57 %
Epoch 487 of 2000 took 0.102s
  training loss:		1.959059
  validation loss:		1.883668
  validation accuracy:		32.72 %
Epoch 488 of 2000 took 0.102s
  training loss:		1.930369
  validation loss:		1.853969
  validation accuracy:		34.67 %
Epoch 489 of 2000 took 0.102s
  training loss:		1.915413
  validation loss:		1.828107
  validation accuracy:		34.67 %
Epoch 490 of 2000 took 0.102s
  training loss:		1.886838
  validation loss:		1.804082
  validation accuracy:		35.65 %
Epoch 491 of 2000 took 0.103s
  training loss:		1.859438
  validation loss:		1.774868
  validation accuracy:		35.76 %
Epoch 492 of 2000 took 0.102s
  training loss:		1.842495
  validation loss:		1.750808
  validation accuracy:		36.74 %
Epoch 493 of 2000 took 0.102s
  training loss:		1.812777
  validation loss:		1.730749
  validation accuracy:		36.74 %
Epoch 494 of 2000 took 0.102s
  training loss:		1.787938
  validation loss:		1.711890
  validation accuracy:		37.72 %
Epoch 495 of 2000 took 0.102s
  training loss:		1.770174
  validation loss:		1.685642
  validation accuracy:		36.74 %
Epoch 496 of 2000 took 0.102s
  training loss:		1.747032
  validation loss:		1.666169
  validation accuracy:		37.50 %
Epoch 497 of 2000 took 0.102s
  training loss:		1.730741
  validation loss:		1.648088
  validation accuracy:		38.26 %
Epoch 498 of 2000 took 0.102s
  training loss:		1.711529
  validation loss:		1.632541
  validation accuracy:		36.52 %
Epoch 499 of 2000 took 0.102s
  training loss:		1.694092
  validation loss:		1.618837
  validation accuracy:		39.89 %
Epoch 500 of 2000 took 0.102s
  training loss:		1.672174
  validation loss:		1.602873
  validation accuracy:		38.37 %
Epoch 501 of 2000 took 0.102s
  training loss:		1.661649
  validation loss:		1.588501
  validation accuracy:		39.46 %
Epoch 502 of 2000 took 0.102s
  training loss:		1.639779
  validation loss:		1.571485
  validation accuracy:		38.91 %
Epoch 503 of 2000 took 0.102s
  training loss:		1.626420
  validation loss:		1.560027
  validation accuracy:		39.67 %
Epoch 504 of 2000 took 0.102s
  training loss:		1.615744
  validation loss:		1.551325
  validation accuracy:		38.59 %
Epoch 505 of 2000 took 0.102s
  training loss:		1.604579
  validation loss:		1.538514
  validation accuracy:		39.46 %
Epoch 506 of 2000 took 0.102s
  training loss:		1.592036
  validation loss:		1.522869
  validation accuracy:		40.22 %
Epoch 507 of 2000 took 0.103s
  training loss:		1.569531
  validation loss:		1.508827
  validation accuracy:		40.87 %
Epoch 508 of 2000 took 0.102s
  training loss:		1.556287
  validation loss:		1.494081
  validation accuracy:		42.07 %
Epoch 509 of 2000 took 0.102s
  training loss:		1.550445
  validation loss:		1.496970
  validation accuracy:		41.96 %
Epoch 510 of 2000 took 0.102s
  training loss:		1.538565
  validation loss:		1.480808
  validation accuracy:		42.93 %
Epoch 511 of 2000 took 0.102s
  training loss:		1.521598
  validation loss:		1.468217
  validation accuracy:		42.72 %
Epoch 512 of 2000 took 0.102s
  training loss:		1.508150
  validation loss:		1.456831
  validation accuracy:		43.37 %
Epoch 513 of 2000 took 0.102s
  training loss:		1.501636
  validation loss:		1.447810
  validation accuracy:		43.04 %
Epoch 514 of 2000 took 0.103s
  training loss:		1.489361
  validation loss:		1.437459
  validation accuracy:		43.26 %
Epoch 515 of 2000 took 0.102s
  training loss:		1.481293
  validation loss:		1.431014
  validation accuracy:		44.46 %
Epoch 516 of 2000 took 0.102s
  training loss:		1.474651
  validation loss:		1.427396
  validation accuracy:		43.48 %
Epoch 517 of 2000 took 0.102s
  training loss:		1.466189
  validation loss:		1.416162
  validation accuracy:		44.13 %
Epoch 518 of 2000 took 0.102s
  training loss:		1.457116
  validation loss:		1.406843
  validation accuracy:		45.33 %
Epoch 519 of 2000 took 0.102s
  training loss:		1.447413
  validation loss:		1.400259
  validation accuracy:		45.33 %
Epoch 520 of 2000 took 0.102s
  training loss:		1.445528
  validation loss:		1.397225
  validation accuracy:		45.98 %
Epoch 521 of 2000 took 0.102s
  training loss:		1.433365
  validation loss:		1.389215
  validation accuracy:		45.43 %
Epoch 522 of 2000 took 0.102s
  training loss:		1.432037
  validation loss:		1.380618
  validation accuracy:		46.20 %
Epoch 523 of 2000 took 0.102s
  training loss:		1.424001
  validation loss:		1.374080
  validation accuracy:		47.28 %
Epoch 524 of 2000 took 0.102s
  training loss:		1.413931
  validation loss:		1.373004
  validation accuracy:		45.22 %
Epoch 525 of 2000 took 0.103s
  training loss:		1.410640
  validation loss:		1.375066
  validation accuracy:		47.83 %
Epoch 526 of 2000 took 0.102s
  training loss:		1.401571
  validation loss:		1.359826
  validation accuracy:		48.48 %
Epoch 527 of 2000 took 0.102s
  training loss:		1.398542
  validation loss:		1.361376
  validation accuracy:		46.63 %
Epoch 528 of 2000 took 0.102s
  training loss:		1.395882
  validation loss:		1.357600
  validation accuracy:		48.26 %
Epoch 529 of 2000 took 0.102s
  training loss:		1.384182
  validation loss:		1.355356
  validation accuracy:		46.52 %
Epoch 530 of 2000 took 0.103s
  training loss:		1.386216
  validation loss:		1.353070
  validation accuracy:		49.78 %
Epoch 531 of 2000 took 0.102s
  training loss:		1.371783
  validation loss:		1.340844
  validation accuracy:		49.13 %
Epoch 532 of 2000 took 0.102s
  training loss:		1.374210
  validation loss:		1.333465
  validation accuracy:		48.37 %
Epoch 533 of 2000 took 0.103s
  training loss:		1.380718
  validation loss:		1.332156
  validation accuracy:		50.00 %
Epoch 534 of 2000 took 0.102s
  training loss:		1.379420
  validation loss:		1.336598
  validation accuracy:		51.52 %
Epoch 535 of 2000 took 0.102s
  training loss:		1.356113
  validation loss:		1.327846
  validation accuracy:		50.98 %
Epoch 536 of 2000 took 0.102s
  training loss:		1.345129
  validation loss:		1.324697
  validation accuracy:		50.54 %
Epoch 537 of 2000 took 0.102s
  training loss:		1.354040
  validation loss:		1.315552
  validation accuracy:		50.65 %
Epoch 538 of 2000 took 0.102s
  training loss:		1.342554
  validation loss:		1.310980
  validation accuracy:		49.46 %
Epoch 539 of 2000 took 0.102s
  training loss:		1.356099
  validation loss:		1.312095
  validation accuracy:		50.65 %
Epoch 540 of 2000 took 0.102s
  training loss:		1.345400
  validation loss:		1.305349
  validation accuracy:		51.74 %
Epoch 541 of 2000 took 0.102s
  training loss:		1.342752
  validation loss:		1.319542
  validation accuracy:		48.80 %
Epoch 542 of 2000 took 0.102s
  training loss:		1.338960
  validation loss:		1.336515
  validation accuracy:		53.59 %
Epoch 543 of 2000 took 0.103s
  training loss:		1.353599
  validation loss:		1.405204
  validation accuracy:		49.89 %
Epoch 544 of 2000 took 0.102s
  training loss:		1.392941
  validation loss:		1.320598
  validation accuracy:		50.11 %
Epoch 545 of 2000 took 0.102s
  training loss:		1.317708
  validation loss:		1.297356
  validation accuracy:		54.13 %
Epoch 546 of 2000 took 0.102s
  training loss:		1.347815
  validation loss:		1.291628
  validation accuracy:		52.39 %
Epoch 547 of 2000 took 0.102s
  training loss:		1.320642
  validation loss:		1.304967
  validation accuracy:		54.78 %
Epoch 548 of 2000 took 0.102s
  training loss:		1.433330
  validation loss:		1.878936
  validation accuracy:		32.28 %
Epoch 549 of 2000 took 0.103s
  training loss:		1.581793
  validation loss:		1.318750
  validation accuracy:		50.76 %
Epoch 550 of 2000 took 0.102s
  training loss:		1.332079
  validation loss:		1.304177
  validation accuracy:		54.89 %
Epoch 551 of 2000 took 0.102s
  training loss:		1.320646
  validation loss:		1.289171
  validation accuracy:		52.72 %
Epoch 552 of 2000 took 0.102s
  training loss:		1.319425
  validation loss:		1.285455
  validation accuracy:		51.52 %
Epoch 553 of 2000 took 0.102s
  training loss:		1.328359
  validation loss:		1.298535
  validation accuracy:		54.78 %
Epoch 554 of 2000 took 0.102s
  training loss:		1.336131
  validation loss:		1.282548
  validation accuracy:		52.72 %
Epoch 555 of 2000 took 0.102s
  training loss:		1.330586
  validation loss:		1.322167
  validation accuracy:		54.57 %
Epoch 556 of 2000 took 0.102s
  training loss:		1.311515
  validation loss:		1.285775
  validation accuracy:		50.54 %
Epoch 557 of 2000 took 0.102s
  training loss:		1.313373
  validation loss:		1.276641
  validation accuracy:		53.15 %
Epoch 558 of 2000 took 0.102s
  training loss:		1.299730
  validation loss:		1.272821
  validation accuracy:		52.50 %
Epoch 559 of 2000 took 0.102s
  training loss:		1.301028
  validation loss:		1.356352
  validation accuracy:		52.83 %
Epoch 560 of 2000 took 0.102s
  training loss:		1.492946
  validation loss:		1.430028
  validation accuracy:		44.13 %
Epoch 561 of 2000 took 0.102s
  training loss:		1.436552
  validation loss:		1.285727
  validation accuracy:		54.13 %
Epoch 562 of 2000 took 0.102s
  training loss:		1.346940
  validation loss:		1.276508
  validation accuracy:		53.04 %
Epoch 563 of 2000 took 0.102s
  training loss:		1.290721
  validation loss:		1.275917
  validation accuracy:		55.00 %
Epoch 564 of 2000 took 0.102s
  training loss:		1.294961
  validation loss:		1.283995
  validation accuracy:		54.89 %
Epoch 565 of 2000 took 0.102s
  training loss:		1.296516
  validation loss:		1.302195
  validation accuracy:		50.43 %
Epoch 566 of 2000 took 0.102s
  training loss:		1.303103
  validation loss:		1.274317
  validation accuracy:		52.39 %
Epoch 567 of 2000 took 0.102s
  training loss:		1.326454
  validation loss:		1.298848
  validation accuracy:		55.76 %
Epoch 568 of 2000 took 0.102s
  training loss:		1.358936
  validation loss:		1.298232
  validation accuracy:		50.22 %
Epoch 569 of 2000 took 0.103s
  training loss:		1.293587
  validation loss:		1.264262
  validation accuracy:		54.78 %
Epoch 570 of 2000 took 0.102s
  training loss:		1.301365
  validation loss:		1.274719
  validation accuracy:		55.65 %
Epoch 571 of 2000 took 0.102s
  training loss:		1.288994
  validation loss:		1.262766
  validation accuracy:		53.80 %
Epoch 572 of 2000 took 0.103s
  training loss:		1.284961
  validation loss:		1.262902
  validation accuracy:		52.93 %
Epoch 573 of 2000 took 0.102s
  training loss:		1.393229
  validation loss:		1.649716
  validation accuracy:		39.46 %
Epoch 574 of 2000 took 0.102s
  training loss:		1.421487
  validation loss:		1.277269
  validation accuracy:		52.72 %
Epoch 575 of 2000 took 0.102s
  training loss:		1.303758
  validation loss:		1.265256
  validation accuracy:		56.20 %
Epoch 576 of 2000 took 0.102s
  training loss:		1.284331
  validation loss:		1.260844
  validation accuracy:		52.93 %
Epoch 577 of 2000 took 0.102s
  training loss:		1.310110
  validation loss:		1.297621
  validation accuracy:		55.87 %
Epoch 578 of 2000 took 0.102s
  training loss:		1.287830
  validation loss:		1.290857
  validation accuracy:		49.89 %
Epoch 579 of 2000 took 0.102s
  training loss:		1.379898
  validation loss:		1.303826
  validation accuracy:		55.11 %
Epoch 580 of 2000 took 0.102s
  training loss:		1.326457
  validation loss:		1.266452
  validation accuracy:		51.96 %
Epoch 581 of 2000 took 0.102s
  training loss:		1.281211
  validation loss:		1.284253
  validation accuracy:		55.87 %
Epoch 582 of 2000 took 0.102s
  training loss:		1.326994
  validation loss:		1.271482
  validation accuracy:		50.76 %
Epoch 583 of 2000 took 0.102s
  training loss:		1.338973
  validation loss:		1.320193
  validation accuracy:		54.78 %
Epoch 584 of 2000 took 0.102s
  training loss:		1.286590
  validation loss:		1.254017
  validation accuracy:		55.11 %
Epoch 585 of 2000 took 0.102s
  training loss:		1.283931
  validation loss:		1.251629
  validation accuracy:		55.54 %
Epoch 586 of 2000 took 0.102s
  training loss:		1.285457
  validation loss:		1.291877
  validation accuracy:		50.33 %
Epoch 587 of 2000 took 0.103s
  training loss:		1.373146
  validation loss:		1.282374
  validation accuracy:		50.98 %
Epoch 588 of 2000 took 0.102s
  training loss:		1.310781
  validation loss:		1.266971
  validation accuracy:		56.41 %
Epoch 589 of 2000 took 0.102s
  training loss:		1.292588
  validation loss:		1.279177
  validation accuracy:		55.54 %
Epoch 590 of 2000 took 0.102s
  training loss:		1.271195
  validation loss:		1.246359
  validation accuracy:		54.89 %
Epoch 591 of 2000 took 0.102s
  training loss:		1.280831
  validation loss:		1.251072
  validation accuracy:		54.67 %
Epoch 592 of 2000 took 0.102s
  training loss:		1.287211
  validation loss:		1.305003
  validation accuracy:		50.00 %
Epoch 593 of 2000 took 0.102s
  training loss:		1.330897
  validation loss:		1.341838
  validation accuracy:		53.91 %
Epoch 594 of 2000 took 0.102s
  training loss:		1.343617
  validation loss:		1.258751
  validation accuracy:		53.48 %
Epoch 595 of 2000 took 0.102s
  training loss:		1.368092
  validation loss:		1.507989
  validation accuracy:		44.02 %
Epoch 596 of 2000 took 0.102s
  training loss:		1.412765
  validation loss:		1.287458
  validation accuracy:		55.43 %
Epoch 597 of 2000 took 0.102s
  training loss:		1.283172
  validation loss:		1.255118
  validation accuracy:		56.63 %
Epoch 598 of 2000 took 0.102s
  training loss:		1.280949
  validation loss:		1.247667
  validation accuracy:		55.54 %
Epoch 599 of 2000 took 0.102s
  training loss:		1.316464
  validation loss:		1.264011
  validation accuracy:		56.09 %
Epoch 600 of 2000 took 0.102s
  training loss:		1.298272
  validation loss:		1.249932
  validation accuracy:		54.35 %
Epoch 601 of 2000 took 0.102s
  training loss:		1.303522
  validation loss:		1.424325
  validation accuracy:		48.59 %
Epoch 602 of 2000 took 0.103s
  training loss:		1.363028
  validation loss:		1.251564
  validation accuracy:		55.87 %
Epoch 603 of 2000 took 0.102s
  training loss:		1.290052
  validation loss:		1.251035
  validation accuracy:		56.74 %
Epoch 604 of 2000 took 0.102s
  training loss:		1.276794
  validation loss:		1.256844
  validation accuracy:		53.04 %
Epoch 605 of 2000 took 0.102s
  training loss:		1.286634
  validation loss:		1.278147
  validation accuracy:		50.00 %
Epoch 606 of 2000 took 0.103s
  training loss:		1.303791
  validation loss:		1.305955
  validation accuracy:		48.91 %
Epoch 607 of 2000 took 0.103s
  training loss:		1.277462
  validation loss:		1.247782
  validation accuracy:		55.54 %
Epoch 608 of 2000 took 0.103s
  training loss:		1.275757
  validation loss:		1.264329
  validation accuracy:		56.63 %
Epoch 609 of 2000 took 0.102s
  training loss:		1.292196
  validation loss:		1.249758
  validation accuracy:		51.74 %
Epoch 610 of 2000 took 0.102s
  training loss:		1.267201
  validation loss:		1.250382
  validation accuracy:		57.17 %
Epoch 611 of 2000 took 0.108s
  training loss:		1.273933
  validation loss:		1.255709
  validation accuracy:		55.87 %
Epoch 612 of 2000 took 0.106s
  training loss:		1.272693
  validation loss:		1.243353
  validation accuracy:		54.78 %
Epoch 613 of 2000 took 0.106s
  training loss:		1.453130
  validation loss:		1.398442
  validation accuracy:		50.22 %
Epoch 614 of 2000 took 0.105s
  training loss:		1.313392
  validation loss:		1.244429
  validation accuracy:		56.41 %
Epoch 615 of 2000 took 0.106s
  training loss:		1.272010
  validation loss:		1.262548
  validation accuracy:		52.28 %
Epoch 616 of 2000 took 0.106s
  training loss:		1.286074
  validation loss:		1.244738
  validation accuracy:		53.37 %
Epoch 617 of 2000 took 0.106s
  training loss:		1.268145
  validation loss:		1.308377
  validation accuracy:		48.26 %
Epoch 618 of 2000 took 0.106s
  training loss:		1.299029
  validation loss:		1.251149
  validation accuracy:		52.72 %
Epoch 619 of 2000 took 0.106s
  training loss:		1.276273
  validation loss:		1.253312
  validation accuracy:		55.11 %
Epoch 620 of 2000 took 0.106s
  training loss:		1.282086
  validation loss:		1.243766
  validation accuracy:		53.48 %
Epoch 621 of 2000 took 0.106s
  training loss:		1.276165
  validation loss:		1.241593
  validation accuracy:		55.76 %
Epoch 622 of 2000 took 0.105s
  training loss:		1.274606
  validation loss:		1.248033
  validation accuracy:		52.39 %
Epoch 623 of 2000 took 0.105s
  training loss:		1.292557
  validation loss:		1.292958
  validation accuracy:		54.02 %
Epoch 624 of 2000 took 0.106s
  training loss:		1.282241
  validation loss:		1.357073
  validation accuracy:		51.74 %
Epoch 625 of 2000 took 0.106s
  training loss:		1.305650
  validation loss:		1.243193
  validation accuracy:		53.59 %
Epoch 626 of 2000 took 0.105s
  training loss:		1.279447
  validation loss:		1.280162
  validation accuracy:		54.13 %
Epoch 627 of 2000 took 0.106s
  training loss:		1.277091
  validation loss:		1.265298
  validation accuracy:		54.02 %
Epoch 628 of 2000 took 0.106s
  training loss:		1.279287
  validation loss:		1.247996
  validation accuracy:		52.28 %
Epoch 629 of 2000 took 0.105s
  training loss:		1.263226
  validation loss:		1.243137
  validation accuracy:		54.24 %
Epoch 630 of 2000 took 0.106s
  training loss:		1.297561
  validation loss:		1.261881
  validation accuracy:		54.67 %
Epoch 631 of 2000 took 0.106s
  training loss:		1.275279
  validation loss:		1.271408
  validation accuracy:		54.24 %
Epoch 632 of 2000 took 0.106s
  training loss:		1.289909
  validation loss:		1.246645
  validation accuracy:		53.59 %
Epoch 633 of 2000 took 0.105s
  training loss:		1.273302
  validation loss:		1.254714
  validation accuracy:		55.00 %
Epoch 634 of 2000 took 0.106s
  training loss:		1.291573
  validation loss:		1.276019
  validation accuracy:		49.46 %
Epoch 635 of 2000 took 0.105s
  training loss:		1.275291
  validation loss:		1.239252
  validation accuracy:		53.48 %
Epoch 636 of 2000 took 0.105s
  training loss:		1.269872
  validation loss:		1.253439
  validation accuracy:		53.91 %
Epoch 637 of 2000 took 0.105s
  training loss:		1.328055
  validation loss:		1.247680
  validation accuracy:		52.17 %
Epoch 638 of 2000 took 0.105s
  training loss:		1.279243
  validation loss:		1.272678
  validation accuracy:		50.43 %
Epoch 639 of 2000 took 0.105s
  training loss:		1.273823
  validation loss:		1.290357
  validation accuracy:		53.37 %
Epoch 640 of 2000 took 0.105s
  training loss:		1.271437
  validation loss:		1.260954
  validation accuracy:		50.33 %
Epoch 641 of 2000 took 0.106s
  training loss:		1.280186
  validation loss:		1.245363
  validation accuracy:		52.39 %
Epoch 642 of 2000 took 0.105s
  training loss:		1.287634
  validation loss:		1.245444
  validation accuracy:		51.63 %
Epoch 643 of 2000 took 0.105s
  training loss:		1.311413
  validation loss:		1.242670
  validation accuracy:		53.48 %
Epoch 644 of 2000 took 0.106s
  training loss:		1.273562
  validation loss:		1.255458
  validation accuracy:		51.85 %
Epoch 645 of 2000 took 0.105s
  training loss:		1.272179
  validation loss:		1.264057
  validation accuracy:		49.89 %
Epoch 646 of 2000 took 0.106s
  training loss:		1.269920
  validation loss:		1.240836
  validation accuracy:		53.26 %
Epoch 647 of 2000 took 0.106s
  training loss:		1.364219
  validation loss:		1.368057
  validation accuracy:		50.65 %
Epoch 648 of 2000 took 0.106s
  training loss:		1.346964
  validation loss:		1.288605
  validation accuracy:		53.91 %
Epoch 649 of 2000 took 0.103s
  training loss:		1.272969
  validation loss:		1.251721
  validation accuracy:		53.37 %
Epoch 650 of 2000 took 0.102s
  training loss:		1.294337
  validation loss:		1.244976
  validation accuracy:		51.96 %
Epoch 651 of 2000 took 0.102s
  training loss:		1.280258
  validation loss:		1.256962
  validation accuracy:		53.70 %
Epoch 652 of 2000 took 0.102s
  training loss:		1.282091
  validation loss:		1.275668
  validation accuracy:		48.91 %
Epoch 653 of 2000 took 0.102s
  training loss:		1.280916
  validation loss:		1.241703
  validation accuracy:		53.04 %
Epoch 654 of 2000 took 0.102s
  training loss:		1.281719
  validation loss:		1.243581
  validation accuracy:		51.63 %
Epoch 655 of 2000 took 0.102s
  training loss:		1.266483
  validation loss:		1.244810
  validation accuracy:		52.28 %
Epoch 656 of 2000 took 0.102s
  training loss:		1.296472
  validation loss:		1.264950
  validation accuracy:		53.37 %
Epoch 657 of 2000 took 0.102s
  training loss:		1.268065
  validation loss:		1.249874
  validation accuracy:		51.09 %
Epoch 658 of 2000 took 0.102s
  training loss:		1.273768
  validation loss:		1.248390
  validation accuracy:		51.96 %
Epoch 659 of 2000 took 0.103s
  training loss:		1.272950
  validation loss:		1.265661
  validation accuracy:		53.04 %
Epoch 660 of 2000 took 0.103s
  training loss:		1.250972
  validation loss:		1.246555
  validation accuracy:		51.30 %
Epoch 661 of 2000 took 0.103s
  training loss:		1.278942
  validation loss:		1.259398
  validation accuracy:		50.33 %
Epoch 662 of 2000 took 0.102s
  training loss:		1.268832
  validation loss:		1.307575
  validation accuracy:		51.96 %
Epoch 663 of 2000 took 0.102s
  training loss:		1.320986
  validation loss:		1.248931
  validation accuracy:		51.09 %
Epoch 664 of 2000 took 0.103s
  training loss:		1.264062
  validation loss:		1.286511
  validation accuracy:		52.61 %
Epoch 665 of 2000 took 0.102s
  training loss:		1.268189
  validation loss:		1.244991
  validation accuracy:		52.72 %
Epoch 666 of 2000 took 0.102s
  training loss:		1.271144
  validation loss:		1.260592
  validation accuracy:		52.39 %
Epoch 667 of 2000 took 0.102s
  training loss:		1.268076
  validation loss:		1.249186
  validation accuracy:		50.87 %
Epoch 668 of 2000 took 0.102s
  training loss:		1.284313
  validation loss:		1.245738
  validation accuracy:		52.39 %
Epoch 669 of 2000 took 0.103s
  training loss:		1.281668
  validation loss:		1.329600
  validation accuracy:		51.20 %
Epoch 670 of 2000 took 0.102s
  training loss:		1.297822
  validation loss:		1.242139
  validation accuracy:		51.52 %
Epoch 671 of 2000 took 0.103s
  training loss:		1.266615
  validation loss:		1.247804
  validation accuracy:		51.20 %
Epoch 672 of 2000 took 0.103s
  training loss:		1.262687
  validation loss:		1.275156
  validation accuracy:		52.39 %
Epoch 673 of 2000 took 0.102s
  training loss:		1.261428
  validation loss:		1.272534
  validation accuracy:		50.11 %
Epoch 674 of 2000 took 0.102s
  training loss:		1.270700
  validation loss:		1.285365
  validation accuracy:		53.04 %
Epoch 675 of 2000 took 0.102s
  training loss:		1.343078
  validation loss:		1.245047
  validation accuracy:		51.74 %
Epoch 676 of 2000 took 0.102s
  training loss:		1.308165
  validation loss:		1.258926
  validation accuracy:		53.70 %
Epoch 677 of 2000 took 0.102s
  training loss:		1.274661
  validation loss:		1.255783
  validation accuracy:		52.83 %
Epoch 678 of 2000 took 0.102s
  training loss:		1.279616
  validation loss:		1.252647
  validation accuracy:		50.54 %
Epoch 679 of 2000 took 0.102s
  training loss:		1.316522
  validation loss:		1.246304
  validation accuracy:		52.07 %
Epoch 680 of 2000 took 0.102s
  training loss:		1.266224
  validation loss:		1.251655
  validation accuracy:		51.85 %
Epoch 681 of 2000 took 0.102s
  training loss:		1.274383
  validation loss:		1.248624
  validation accuracy:		52.93 %
Epoch 682 of 2000 took 0.102s
  training loss:		1.264137
  validation loss:		1.251324
  validation accuracy:		52.83 %
Epoch 683 of 2000 took 0.103s
  training loss:		1.273325
  validation loss:		1.255495
  validation accuracy:		53.26 %
Epoch 684 of 2000 took 0.102s
  training loss:		1.281902
  validation loss:		1.272318
  validation accuracy:		49.02 %
Epoch 685 of 2000 took 0.103s
  training loss:		1.280197
  validation loss:		1.255245
  validation accuracy:		50.65 %
Epoch 686 of 2000 took 0.102s
  training loss:		1.269114
  validation loss:		1.244391
  validation accuracy:		52.28 %
Epoch 687 of 2000 took 0.102s
  training loss:		1.256241
  validation loss:		1.251635
  validation accuracy:		52.17 %
Epoch 688 of 2000 took 0.102s
  training loss:		1.260680
  validation loss:		1.259669
  validation accuracy:		52.72 %
Epoch 689 of 2000 took 0.103s
  training loss:		1.270635
  validation loss:		1.297621
  validation accuracy:		47.72 %
Epoch 690 of 2000 took 0.102s
  training loss:		1.276884
  validation loss:		1.263463
  validation accuracy:		53.04 %
Epoch 691 of 2000 took 0.102s
  training loss:		1.257803
  validation loss:		1.249645
  validation accuracy:		51.30 %
Epoch 692 of 2000 took 0.102s
  training loss:		1.274185
  validation loss:		1.245052
  validation accuracy:		52.07 %
Epoch 693 of 2000 took 0.102s
  training loss:		1.265296
  validation loss:		1.281053
  validation accuracy:		52.50 %
Epoch 694 of 2000 took 0.102s
  training loss:		1.279615
  validation loss:		1.246202
  validation accuracy:		51.30 %
Epoch 695 of 2000 took 0.102s
  training loss:		1.296250
  validation loss:		1.283057
  validation accuracy:		52.39 %
Epoch 696 of 2000 took 0.102s
  training loss:		1.275293
  validation loss:		1.273560
  validation accuracy:		52.83 %
Epoch 697 of 2000 took 0.102s
  training loss:		1.268929
  validation loss:		1.248743
  validation accuracy:		52.50 %
Epoch 698 of 2000 took 0.102s
  training loss:		1.255175
  validation loss:		1.264005
  validation accuracy:		52.61 %
Epoch 699 of 2000 took 0.102s
  training loss:		1.273432
  validation loss:		1.244534
  validation accuracy:		52.50 %
Epoch 700 of 2000 took 0.102s
  training loss:		1.255524
  validation loss:		1.253902
  validation accuracy:		51.41 %
Epoch 701 of 2000 took 0.102s
  training loss:		1.275423
  validation loss:		1.257972
  validation accuracy:		51.41 %
Epoch 702 of 2000 took 0.102s
  training loss:		1.263070
  validation loss:		1.247889
  validation accuracy:		51.09 %
Epoch 703 of 2000 took 0.102s
  training loss:		1.288782
  validation loss:		1.288020
  validation accuracy:		52.07 %
Epoch 704 of 2000 took 0.102s
  training loss:		1.311439
  validation loss:		1.306482
  validation accuracy:		48.70 %
Epoch 705 of 2000 took 0.102s
  training loss:		1.289311
  validation loss:		1.302630
  validation accuracy:		51.41 %
Epoch 706 of 2000 took 0.102s
  training loss:		1.270661
  validation loss:		1.271743
  validation accuracy:		51.74 %
Epoch 707 of 2000 took 0.102s
  training loss:		1.260809
  validation loss:		1.257148
  validation accuracy:		52.39 %
Epoch 708 of 2000 took 0.102s
  training loss:		1.280739
  validation loss:		1.254109
  validation accuracy:		52.83 %
Epoch 709 of 2000 took 0.103s
  training loss:		1.258493
  validation loss:		1.247614
  validation accuracy:		52.93 %
Epoch 710 of 2000 took 0.103s
  training loss:		1.271920
  validation loss:		1.248007
  validation accuracy:		52.61 %
Epoch 711 of 2000 took 0.102s
  training loss:		1.268808
  validation loss:		1.246146
  validation accuracy:		52.61 %
Epoch 712 of 2000 took 0.102s
  training loss:		1.268565
  validation loss:		1.243184
  validation accuracy:		53.48 %
Epoch 713 of 2000 took 0.102s
  training loss:		1.269985
  validation loss:		1.249725
  validation accuracy:		52.50 %
Epoch 714 of 2000 took 0.102s
  training loss:		1.263207
  validation loss:		1.248812
  validation accuracy:		52.83 %
Epoch 715 of 2000 took 0.102s
  training loss:		1.258186
  validation loss:		1.246927
  validation accuracy:		52.83 %
Epoch 716 of 2000 took 0.102s
  training loss:		1.287275
  validation loss:		1.373107
  validation accuracy:		47.72 %
Epoch 717 of 2000 took 0.102s
  training loss:		1.305512
  validation loss:		1.249485
  validation accuracy:		53.26 %
Epoch 718 of 2000 took 0.103s
  training loss:		1.267411
  validation loss:		1.242929
  validation accuracy:		52.72 %
Epoch 719 of 2000 took 0.103s
  training loss:		1.264514
  validation loss:		1.249033
  validation accuracy:		52.07 %
Epoch 720 of 2000 took 0.102s
  training loss:		1.248529
  validation loss:		1.248550
  validation accuracy:		52.50 %
Epoch 721 of 2000 took 0.102s
  training loss:		1.288179
  validation loss:		1.263175
  validation accuracy:		51.63 %
Epoch 722 of 2000 took 0.102s
  training loss:		1.271580
  validation loss:		1.249847
  validation accuracy:		52.17 %
Epoch 723 of 2000 took 0.102s
  training loss:		1.267972
  validation loss:		1.275360
  validation accuracy:		50.11 %
Epoch 724 of 2000 took 0.103s
  training loss:		1.256597
  validation loss:		1.242847
  validation accuracy:		53.80 %
Epoch 725 of 2000 took 0.102s
  training loss:		1.262117
  validation loss:		1.243570
  validation accuracy:		53.04 %
Epoch 726 of 2000 took 0.102s
  training loss:		1.273951
  validation loss:		1.250564
  validation accuracy:		52.28 %
Epoch 727 of 2000 took 0.102s
  training loss:		1.259788
  validation loss:		1.242612
  validation accuracy:		53.59 %
Epoch 728 of 2000 took 0.102s
  training loss:		1.264607
  validation loss:		1.251737
  validation accuracy:		52.83 %
Epoch 729 of 2000 took 0.102s
  training loss:		1.252324
  validation loss:		1.244733
  validation accuracy:		53.59 %
Epoch 730 of 2000 took 0.102s
  training loss:		1.263023
  validation loss:		1.246529
  validation accuracy:		53.48 %
Epoch 731 of 2000 took 0.102s
  training loss:		1.258850
  validation loss:		1.259193
  validation accuracy:		54.13 %
Epoch 732 of 2000 took 0.102s
  training loss:		1.276847
  validation loss:		1.310269
  validation accuracy:		51.09 %
Epoch 733 of 2000 took 0.102s
  training loss:		1.270161
  validation loss:		1.247649
  validation accuracy:		53.04 %
Epoch 734 of 2000 took 0.102s
  training loss:		1.264201
  validation loss:		1.245609
  validation accuracy:		53.91 %
Epoch 735 of 2000 took 0.102s
  training loss:		1.261034
  validation loss:		1.244240
  validation accuracy:		53.59 %
Epoch 736 of 2000 took 0.102s
  training loss:		1.255429
  validation loss:		1.249526
  validation accuracy:		53.48 %
Epoch 737 of 2000 took 0.102s
  training loss:		1.254998
  validation loss:		1.245228
  validation accuracy:		54.13 %
Epoch 738 of 2000 took 0.102s
  training loss:		1.257562
  validation loss:		1.243517
  validation accuracy:		53.91 %
Epoch 739 of 2000 took 0.103s
  training loss:		1.258543
  validation loss:		1.241010
  validation accuracy:		54.89 %
Epoch 740 of 2000 took 0.102s
  training loss:		1.274091
  validation loss:		1.250333
  validation accuracy:		54.13 %
Epoch 741 of 2000 took 0.102s
  training loss:		1.264010
  validation loss:		1.275373
  validation accuracy:		50.43 %
Epoch 742 of 2000 took 0.102s
  training loss:		1.275249
  validation loss:		1.292752
  validation accuracy:		52.28 %
Epoch 743 of 2000 took 0.102s
  training loss:		1.264120
  validation loss:		1.239994
  validation accuracy:		54.57 %
Epoch 744 of 2000 took 0.102s
  training loss:		1.257083
  validation loss:		1.257474
  validation accuracy:		53.91 %
Epoch 745 of 2000 took 0.102s
  training loss:		1.259415
  validation loss:		1.250385
  validation accuracy:		53.48 %
Epoch 746 of 2000 took 0.102s
  training loss:		1.256821
  validation loss:		1.247226
  validation accuracy:		53.26 %
Epoch 747 of 2000 took 0.102s
  training loss:		1.264315
  validation loss:		1.243777
  validation accuracy:		54.46 %
Epoch 748 of 2000 took 0.103s
  training loss:		1.279601
  validation loss:		1.283102
  validation accuracy:		49.78 %
Epoch 749 of 2000 took 0.102s
  training loss:		1.273923
  validation loss:		1.254895
  validation accuracy:		53.26 %
Epoch 750 of 2000 took 0.102s
  training loss:		1.273074
  validation loss:		1.242603
  validation accuracy:		54.46 %
Epoch 751 of 2000 took 0.102s
  training loss:		1.263823
  validation loss:		1.247014
  validation accuracy:		53.59 %
Epoch 752 of 2000 took 0.102s
  training loss:		1.279391
  validation loss:		1.270400
  validation accuracy:		53.80 %
Epoch 753 of 2000 took 0.102s
  training loss:		1.265146
  validation loss:		1.268111
  validation accuracy:		53.59 %
Epoch 754 of 2000 took 0.102s
  training loss:		1.274943
  validation loss:		1.244210
  validation accuracy:		54.24 %
Epoch 755 of 2000 took 0.102s
  training loss:		1.256372
  validation loss:		1.297588
  validation accuracy:		49.35 %
Epoch 756 of 2000 took 0.102s
  training loss:		1.268709
  validation loss:		1.276758
  validation accuracy:		52.72 %
Epoch 757 of 2000 took 0.102s
  training loss:		1.261567
  validation loss:		1.273089
  validation accuracy:		54.13 %
Epoch 758 of 2000 took 0.102s
  training loss:		1.283271
  validation loss:		1.243316
  validation accuracy:		54.67 %
Epoch 759 of 2000 took 0.102s
  training loss:		1.264286
  validation loss:		1.294010
  validation accuracy:		53.37 %
Epoch 760 of 2000 took 0.102s
  training loss:		1.259930
  validation loss:		1.254145
  validation accuracy:		53.48 %
Epoch 761 of 2000 took 0.102s
  training loss:		1.270122
  validation loss:		1.246891
  validation accuracy:		53.70 %
Epoch 762 of 2000 took 0.102s
  training loss:		1.265022
  validation loss:		1.242873
  validation accuracy:		53.70 %
Epoch 763 of 2000 took 0.103s
  training loss:		1.260322
  validation loss:		1.243445
  validation accuracy:		53.48 %
Epoch 764 of 2000 took 0.102s
  training loss:		1.255879
  validation loss:		1.238872
  validation accuracy:		54.89 %
Epoch 765 of 2000 took 0.102s
  training loss:		1.265708
  validation loss:		1.261185
  validation accuracy:		54.67 %
Epoch 766 of 2000 took 0.102s
  training loss:		1.264107
  validation loss:		1.245565
  validation accuracy:		53.91 %
Epoch 767 of 2000 took 0.103s
  training loss:		1.268895
  validation loss:		1.261398
  validation accuracy:		55.00 %
Epoch 768 of 2000 took 0.102s
  training loss:		1.264296
  validation loss:		1.274676
  validation accuracy:		50.65 %
Epoch 769 of 2000 took 0.102s
  training loss:		1.277149
  validation loss:		1.244534
  validation accuracy:		54.24 %
Epoch 770 of 2000 took 0.102s
  training loss:		1.255015
  validation loss:		1.253080
  validation accuracy:		53.91 %
Epoch 771 of 2000 took 0.102s
  training loss:		1.275633
  validation loss:		1.247263
  validation accuracy:		52.83 %
Epoch 772 of 2000 took 0.102s
  training loss:		1.257157
  validation loss:		1.250211
  validation accuracy:		53.26 %
Epoch 773 of 2000 took 0.102s
  training loss:		1.260737
  validation loss:		1.276102
  validation accuracy:		53.48 %
Epoch 774 of 2000 took 0.102s
  training loss:		1.249813
  validation loss:		1.242731
  validation accuracy:		54.57 %
Epoch 775 of 2000 took 0.102s
  training loss:		1.263639
  validation loss:		1.242115
  validation accuracy:		54.46 %
Epoch 776 of 2000 took 0.102s
  training loss:		1.255718
  validation loss:		1.264532
  validation accuracy:		53.48 %
Epoch 777 of 2000 took 0.103s
  training loss:		1.269848
  validation loss:		1.244437
  validation accuracy:		53.91 %
Epoch 778 of 2000 took 0.102s
  training loss:		1.261405
  validation loss:		1.262457
  validation accuracy:		54.57 %
Epoch 779 of 2000 took 0.102s
  training loss:		1.269745
  validation loss:		1.245292
  validation accuracy:		53.80 %
Epoch 780 of 2000 took 0.102s
  training loss:		1.253275
  validation loss:		1.248132
  validation accuracy:		53.70 %
Epoch 781 of 2000 took 0.102s
  training loss:		1.259835
  validation loss:		1.292500
  validation accuracy:		53.37 %
Epoch 782 of 2000 took 0.102s
  training loss:		1.251883
  validation loss:		1.246684
  validation accuracy:		56.09 %
Epoch 783 of 2000 took 0.102s
  training loss:		1.265446
  validation loss:		1.253569
  validation accuracy:		54.35 %
Epoch 784 of 2000 took 0.102s
  training loss:		1.260957
  validation loss:		1.245447
  validation accuracy:		54.02 %
Epoch 785 of 2000 took 0.102s
  training loss:		1.273860
  validation loss:		1.245932
  validation accuracy:		53.70 %
Epoch 786 of 2000 took 0.102s
  training loss:		1.252795
  validation loss:		1.244901
  validation accuracy:		54.46 %
Epoch 787 of 2000 took 0.102s
  training loss:		1.270310
  validation loss:		1.245936
  validation accuracy:		53.70 %
Epoch 788 of 2000 took 0.102s
  training loss:		1.273119
  validation loss:		1.255856
  validation accuracy:		53.59 %
Epoch 789 of 2000 took 0.102s
  training loss:		1.255934
  validation loss:		1.248419
  validation accuracy:		53.48 %
Epoch 790 of 2000 took 0.102s
  training loss:		1.249530
  validation loss:		1.248933
  validation accuracy:		55.00 %
Epoch 791 of 2000 took 0.102s
  training loss:		1.251231
  validation loss:		1.237505
  validation accuracy:		54.13 %
Epoch 792 of 2000 took 0.102s
  training loss:		1.254518
  validation loss:		1.243231
  validation accuracy:		54.13 %
Epoch 793 of 2000 took 0.102s
  training loss:		1.261438
  validation loss:		1.245374
  validation accuracy:		54.46 %
Epoch 794 of 2000 took 0.102s
  training loss:		1.264219
  validation loss:		1.247273
  validation accuracy:		53.70 %
Epoch 795 of 2000 took 0.102s
  training loss:		1.254437
  validation loss:		1.305376
  validation accuracy:		53.91 %
Epoch 796 of 2000 took 0.102s
  training loss:		1.267383
  validation loss:		1.247574
  validation accuracy:		53.70 %
Epoch 797 of 2000 took 0.102s
  training loss:		1.265481
  validation loss:		1.261379
  validation accuracy:		52.50 %
Epoch 798 of 2000 took 0.102s
  training loss:		1.272067
  validation loss:		1.255941
  validation accuracy:		53.15 %
Epoch 799 of 2000 took 0.102s
  training loss:		1.255385
  validation loss:		1.333040
  validation accuracy:		52.28 %
Epoch 800 of 2000 took 0.102s
  training loss:		1.266301
  validation loss:		1.249256
  validation accuracy:		54.35 %
Epoch 801 of 2000 took 0.103s
  training loss:		1.271525
  validation loss:		1.249892
  validation accuracy:		54.35 %
Epoch 802 of 2000 took 0.103s
  training loss:		1.247429
  validation loss:		1.248402
  validation accuracy:		53.80 %
Epoch 803 of 2000 took 0.102s
  training loss:		1.261489
  validation loss:		1.242497
  validation accuracy:		54.02 %
Epoch 804 of 2000 took 0.102s
  training loss:		1.252296
  validation loss:		1.246595
  validation accuracy:		54.02 %
Epoch 805 of 2000 took 0.102s
  training loss:		1.260628
  validation loss:		1.248036
  validation accuracy:		55.43 %
Epoch 806 of 2000 took 0.103s
  training loss:		1.266221
  validation loss:		1.271263
  validation accuracy:		51.74 %
Epoch 807 of 2000 took 0.104s
  training loss:		1.266891
  validation loss:		1.244966
  validation accuracy:		54.35 %
Epoch 808 of 2000 took 0.102s
  training loss:		1.267987
  validation loss:		1.241103
  validation accuracy:		55.22 %
Epoch 809 of 2000 took 0.102s
  training loss:		1.267212
  validation loss:		1.245467
  validation accuracy:		54.57 %
Epoch 810 of 2000 took 0.102s
  training loss:		1.259027
  validation loss:		1.255922
  validation accuracy:		54.02 %
Epoch 811 of 2000 took 0.102s
  training loss:		1.259964
  validation loss:		1.255330
  validation accuracy:		53.80 %
Epoch 812 of 2000 took 0.102s
  training loss:		1.284138
  validation loss:		1.257619
  validation accuracy:		54.13 %
Epoch 813 of 2000 took 0.102s
  training loss:		1.257064
  validation loss:		1.243162
  validation accuracy:		53.48 %
Epoch 814 of 2000 took 0.102s
  training loss:		1.267447
  validation loss:		1.280893
  validation accuracy:		50.33 %
Epoch 815 of 2000 took 0.102s
  training loss:		1.261468
  validation loss:		1.244457
  validation accuracy:		53.80 %
Epoch 816 of 2000 took 0.102s
  training loss:		1.269953
  validation loss:		1.261414
  validation accuracy:		53.91 %
Epoch 817 of 2000 took 0.102s
  training loss:		1.255555
  validation loss:		1.256881
  validation accuracy:		51.85 %
Epoch 818 of 2000 took 0.102s
  training loss:		1.297995
  validation loss:		1.257083
  validation accuracy:		55.22 %
Epoch 819 of 2000 took 0.102s
  training loss:		1.266631
  validation loss:		1.250739
  validation accuracy:		53.26 %
Epoch 820 of 2000 took 0.102s
  training loss:		1.248458
  validation loss:		1.246178
  validation accuracy:		53.15 %
Epoch 821 of 2000 took 0.102s
  training loss:		1.264694
  validation loss:		1.262044
  validation accuracy:		54.35 %
Epoch 822 of 2000 took 0.102s
  training loss:		1.265305
  validation loss:		1.252047
  validation accuracy:		54.13 %
Epoch 823 of 2000 took 0.102s
  training loss:		1.268354
  validation loss:		1.243920
  validation accuracy:		54.13 %
Epoch 824 of 2000 took 0.102s
  training loss:		1.273707
  validation loss:		1.242736
  validation accuracy:		54.46 %
Epoch 825 of 2000 took 0.102s
  training loss:		1.254494
  validation loss:		1.252350
  validation accuracy:		54.35 %
Epoch 826 of 2000 took 0.102s
  training loss:		1.295054
  validation loss:		1.251695
  validation accuracy:		53.59 %
Epoch 827 of 2000 took 0.102s
  training loss:		1.256102
  validation loss:		1.244480
  validation accuracy:		54.57 %
Epoch 828 of 2000 took 0.102s
  training loss:		1.260255
  validation loss:		1.265097
  validation accuracy:		54.35 %
Epoch 829 of 2000 took 0.102s
  training loss:		1.268267
  validation loss:		1.252429
  validation accuracy:		53.80 %
Epoch 830 of 2000 took 0.102s
  training loss:		1.259132
  validation loss:		1.250943
  validation accuracy:		53.48 %
Epoch 831 of 2000 took 0.102s
  training loss:		1.256221
  validation loss:		1.266694
  validation accuracy:		53.48 %
Epoch 832 of 2000 took 0.102s
  training loss:		1.260011
  validation loss:		1.258606
  validation accuracy:		53.59 %
Epoch 833 of 2000 took 0.102s
  training loss:		1.255222
  validation loss:		1.247134
  validation accuracy:		54.35 %
Epoch 834 of 2000 took 0.102s
  training loss:		1.259471
  validation loss:		1.247750
  validation accuracy:		53.91 %
Epoch 835 of 2000 took 0.102s
  training loss:		1.270759
  validation loss:		1.241667
  validation accuracy:		55.00 %
Epoch 836 of 2000 took 0.103s
  training loss:		1.262159
  validation loss:		1.247569
  validation accuracy:		54.78 %
Epoch 837 of 2000 took 0.102s
  training loss:		1.264618
  validation loss:		1.259496
  validation accuracy:		54.13 %
Epoch 838 of 2000 took 0.102s
  training loss:		1.261212
  validation loss:		1.247733
  validation accuracy:		53.37 %
Epoch 839 of 2000 took 0.102s
  training loss:		1.276587
  validation loss:		1.271814
  validation accuracy:		52.93 %
Epoch 840 of 2000 took 0.102s
  training loss:		1.262391
  validation loss:		1.259051
  validation accuracy:		53.48 %
Epoch 841 of 2000 took 0.103s
  training loss:		1.264807
  validation loss:		1.244560
  validation accuracy:		54.02 %
Epoch 842 of 2000 took 0.102s
  training loss:		1.252013
  validation loss:		1.243587
  validation accuracy:		54.67 %
Epoch 843 of 2000 took 0.102s
  training loss:		1.264843
  validation loss:		1.268945
  validation accuracy:		54.24 %
Epoch 844 of 2000 took 0.102s
  training loss:		1.260528
  validation loss:		1.246040
  validation accuracy:		53.80 %
Epoch 845 of 2000 took 0.102s
  training loss:		1.246509
  validation loss:		1.250223
  validation accuracy:		53.37 %
Epoch 846 of 2000 took 0.102s
  training loss:		1.255668
  validation loss:		1.243280
  validation accuracy:		54.57 %
Epoch 847 of 2000 took 0.102s
  training loss:		1.253754
  validation loss:		1.248798
  validation accuracy:		53.26 %
Epoch 848 of 2000 took 0.102s
  training loss:		1.253818
  validation loss:		1.260384
  validation accuracy:		54.13 %
Epoch 849 of 2000 took 0.102s
  training loss:		1.270772
  validation loss:		1.246545
  validation accuracy:		54.67 %
Epoch 850 of 2000 took 0.102s
  training loss:		1.258810
  validation loss:		1.310071
  validation accuracy:		48.37 %
Epoch 851 of 2000 took 0.102s
  training loss:		1.258373
  validation loss:		1.251150
  validation accuracy:		53.80 %
Epoch 852 of 2000 took 0.102s
  training loss:		1.263164
  validation loss:		1.241651
  validation accuracy:		54.02 %
Epoch 853 of 2000 took 0.102s
  training loss:		1.284785
  validation loss:		1.245598
  validation accuracy:		53.91 %
Epoch 854 of 2000 took 0.102s
  training loss:		1.250127
  validation loss:		1.247185
  validation accuracy:		55.22 %
Epoch 855 of 2000 took 0.102s
  training loss:		1.249268
  validation loss:		1.246122
  validation accuracy:		53.91 %
Epoch 856 of 2000 took 0.102s
  training loss:		1.252414
  validation loss:		1.241342
  validation accuracy:		55.22 %
Epoch 857 of 2000 took 0.102s
  training loss:		1.296040
  validation loss:		1.249274
  validation accuracy:		54.78 %
Epoch 858 of 2000 took 0.102s
  training loss:		1.251831
  validation loss:		1.249390
  validation accuracy:		53.80 %
Epoch 859 of 2000 took 0.102s
  training loss:		1.270819
  validation loss:		1.255549
  validation accuracy:		54.24 %
Epoch 860 of 2000 took 0.102s
  training loss:		1.263947
  validation loss:		1.256546
  validation accuracy:		53.59 %
Epoch 861 of 2000 took 0.102s
  training loss:		1.253943
  validation loss:		1.245800
  validation accuracy:		54.35 %
Epoch 862 of 2000 took 0.103s
  training loss:		1.256587
  validation loss:		1.262195
  validation accuracy:		55.33 %
Epoch 863 of 2000 took 0.102s
  training loss:		1.253084
  validation loss:		1.254530
  validation accuracy:		53.37 %
Epoch 864 of 2000 took 0.102s
  training loss:		1.264708
  validation loss:		1.246398
  validation accuracy:		53.59 %
Epoch 865 of 2000 took 0.103s
  training loss:		1.261257
  validation loss:		1.291513
  validation accuracy:		54.89 %
Epoch 866 of 2000 took 0.103s
  training loss:		1.255571
  validation loss:		1.246196
  validation accuracy:		54.02 %
Epoch 867 of 2000 took 0.102s
  training loss:		1.269001
  validation loss:		1.247066
  validation accuracy:		53.15 %
Epoch 868 of 2000 took 0.102s
  training loss:		1.259630
  validation loss:		1.243570
  validation accuracy:		53.80 %
Epoch 869 of 2000 took 0.102s
  training loss:		1.258541
  validation loss:		1.256545
  validation accuracy:		55.00 %
Epoch 870 of 2000 took 0.102s
  training loss:		1.274510
  validation loss:		1.246422
  validation accuracy:		54.02 %
Epoch 871 of 2000 took 0.102s
  training loss:		1.258365
  validation loss:		1.243647
  validation accuracy:		54.24 %
Epoch 872 of 2000 took 0.102s
  training loss:		1.243589
  validation loss:		1.239271
  validation accuracy:		53.70 %
Epoch 873 of 2000 took 0.102s
  training loss:		1.268682
  validation loss:		1.254570
  validation accuracy:		52.39 %
Epoch 874 of 2000 took 0.102s
  training loss:		1.252374
  validation loss:		1.242021
  validation accuracy:		54.02 %
Epoch 875 of 2000 took 0.102s
  training loss:		1.255253
  validation loss:		1.259751
  validation accuracy:		54.57 %
Epoch 876 of 2000 took 0.102s
  training loss:		1.261429
  validation loss:		1.269186
  validation accuracy:		54.46 %
Epoch 877 of 2000 took 0.102s
  training loss:		1.258172
  validation loss:		1.243506
  validation accuracy:		54.02 %
Epoch 878 of 2000 took 0.102s
  training loss:		1.250642
  validation loss:		1.246621
  validation accuracy:		53.37 %
Epoch 879 of 2000 took 0.103s
  training loss:		1.272974
  validation loss:		1.243537
  validation accuracy:		53.37 %
Epoch 880 of 2000 took 0.103s
  training loss:		1.254480
  validation loss:		1.245179
  validation accuracy:		54.24 %
Epoch 881 of 2000 took 0.102s
  training loss:		1.257574
  validation loss:		1.246722
  validation accuracy:		52.39 %
Epoch 882 of 2000 took 0.102s
  training loss:		1.267155
  validation loss:		1.241124
  validation accuracy:		53.70 %
Epoch 883 of 2000 took 0.105s
  training loss:		1.272341
  validation loss:		1.246200
  validation accuracy:		54.35 %
Epoch 884 of 2000 took 0.109s
  training loss:		1.251128
  validation loss:		1.251486
  validation accuracy:		54.02 %
Epoch 885 of 2000 took 0.109s
  training loss:		1.261892
  validation loss:		1.241438
  validation accuracy:		54.35 %
Epoch 886 of 2000 took 0.109s
  training loss:		1.254741
  validation loss:		1.245315
  validation accuracy:		54.02 %
Epoch 887 of 2000 took 0.109s
  training loss:		1.270771
  validation loss:		1.248287
  validation accuracy:		53.48 %
Epoch 888 of 2000 took 0.109s
  training loss:		1.261313
  validation loss:		1.244556
  validation accuracy:		53.70 %
Epoch 889 of 2000 took 0.109s
  training loss:		1.273081
  validation loss:		1.245194
  validation accuracy:		54.24 %
Epoch 890 of 2000 took 0.109s
  training loss:		1.251699
  validation loss:		1.261090
  validation accuracy:		52.83 %
Epoch 891 of 2000 took 0.109s
  training loss:		1.256715
  validation loss:		1.249200
  validation accuracy:		53.80 %
Epoch 892 of 2000 took 0.109s
  training loss:		1.261114
  validation loss:		1.251490
  validation accuracy:		53.26 %
Epoch 893 of 2000 took 0.109s
  training loss:		1.258279
  validation loss:		1.246271
  validation accuracy:		54.02 %
Epoch 894 of 2000 took 0.109s
  training loss:		1.267346
  validation loss:		1.247304
  validation accuracy:		53.91 %
Epoch 895 of 2000 took 0.109s
  training loss:		1.263120
  validation loss:		1.241926
  validation accuracy:		55.33 %
Epoch 896 of 2000 took 0.109s
  training loss:		1.254329
  validation loss:		1.313290
  validation accuracy:		52.61 %
Epoch 897 of 2000 took 0.109s
  training loss:		1.264734
  validation loss:		1.246536
  validation accuracy:		53.80 %
Epoch 898 of 2000 took 0.109s
  training loss:		1.258662
  validation loss:		1.254930
  validation accuracy:		54.78 %
Epoch 899 of 2000 took 0.109s
  training loss:		1.263718
  validation loss:		1.252292
  validation accuracy:		53.91 %
Epoch 900 of 2000 took 0.109s
  training loss:		1.254118
  validation loss:		1.261828
  validation accuracy:		54.13 %
Epoch 901 of 2000 took 0.109s
  training loss:		1.268857
  validation loss:		1.244888
  validation accuracy:		54.24 %
Epoch 902 of 2000 took 0.109s
  training loss:		1.253151
  validation loss:		1.245945
  validation accuracy:		54.67 %
Epoch 903 of 2000 took 0.109s
  training loss:		1.259224
  validation loss:		1.248681
  validation accuracy:		53.48 %
Epoch 904 of 2000 took 0.109s
  training loss:		1.262259
  validation loss:		1.244461
  validation accuracy:		54.02 %
Epoch 905 of 2000 took 0.109s
  training loss:		1.258289
  validation loss:		1.301633
  validation accuracy:		49.78 %
Epoch 906 of 2000 took 0.109s
  training loss:		1.263974
  validation loss:		1.255589
  validation accuracy:		54.46 %
Epoch 907 of 2000 took 0.109s
  training loss:		1.254344
  validation loss:		1.245871
  validation accuracy:		53.59 %
Epoch 908 of 2000 took 0.109s
  training loss:		1.253895
  validation loss:		1.242884
  validation accuracy:		54.35 %
Epoch 909 of 2000 took 0.109s
  training loss:		1.252660
  validation loss:		1.240399
  validation accuracy:		54.78 %
Epoch 910 of 2000 took 0.109s
  training loss:		1.259955
  validation loss:		1.260775
  validation accuracy:		53.91 %
Epoch 911 of 2000 took 0.109s
  training loss:		1.249943
  validation loss:		1.250325
  validation accuracy:		54.78 %
Epoch 912 of 2000 took 0.109s
  training loss:		1.259777
  validation loss:		1.252290
  validation accuracy:		53.15 %
Epoch 913 of 2000 took 0.109s
  training loss:		1.256111
  validation loss:		1.261452
  validation accuracy:		55.11 %
Epoch 914 of 2000 took 0.109s
  training loss:		1.269836
  validation loss:		1.311096
  validation accuracy:		54.13 %
Epoch 915 of 2000 took 0.109s
  training loss:		1.274648
  validation loss:		1.244509
  validation accuracy:		54.89 %
Epoch 916 of 2000 took 0.110s
  training loss:		1.248020
  validation loss:		1.245959
  validation accuracy:		54.13 %
Epoch 917 of 2000 took 0.111s
  training loss:		1.251025
  validation loss:		1.245834
  validation accuracy:		53.37 %
Epoch 918 of 2000 took 0.109s
  training loss:		1.258226
  validation loss:		1.266431
  validation accuracy:		54.78 %
Epoch 919 of 2000 took 0.109s
  training loss:		1.252488
  validation loss:		1.250739
  validation accuracy:		54.67 %
Epoch 920 of 2000 took 0.107s
  training loss:		1.267998
  validation loss:		1.249378
  validation accuracy:		53.59 %
Epoch 921 of 2000 took 0.106s
  training loss:		1.251887
  validation loss:		1.248136
  validation accuracy:		53.15 %
Epoch 922 of 2000 took 0.106s
  training loss:		1.264756
  validation loss:		1.243538
  validation accuracy:		54.35 %
Epoch 923 of 2000 took 0.105s
  training loss:		1.253956
  validation loss:		1.251356
  validation accuracy:		53.59 %
Epoch 924 of 2000 took 0.106s
  training loss:		1.258946
  validation loss:		1.244900
  validation accuracy:		53.91 %
Epoch 925 of 2000 took 0.105s
  training loss:		1.261006
  validation loss:		1.242875
  validation accuracy:		53.91 %
Epoch 926 of 2000 took 0.105s
  training loss:		1.257949
  validation loss:		1.247721
  validation accuracy:		53.91 %
Epoch 927 of 2000 took 0.106s
  training loss:		1.251833
  validation loss:		1.250707
  validation accuracy:		53.48 %
Epoch 928 of 2000 took 0.106s
  training loss:		1.256019
  validation loss:		1.254888
  validation accuracy:		53.26 %
Epoch 929 of 2000 took 0.105s
  training loss:		1.256900
  validation loss:		1.249399
  validation accuracy:		54.24 %
Epoch 930 of 2000 took 0.105s
  training loss:		1.284742
  validation loss:		1.307931
  validation accuracy:		52.61 %
Epoch 931 of 2000 took 0.106s
  training loss:		1.263368
  validation loss:		1.264826
  validation accuracy:		51.85 %
Epoch 932 of 2000 took 0.106s
  training loss:		1.260966
  validation loss:		1.247377
  validation accuracy:		53.80 %
Epoch 933 of 2000 took 0.106s
  training loss:		1.259562
  validation loss:		1.245260
  validation accuracy:		53.48 %
Epoch 934 of 2000 took 0.106s
  training loss:		1.253540
  validation loss:		1.240800
  validation accuracy:		54.78 %
Epoch 935 of 2000 took 0.106s
  training loss:		1.265864
  validation loss:		1.243450
  validation accuracy:		55.22 %
Epoch 936 of 2000 took 0.106s
  training loss:		1.253950
  validation loss:		1.247937
  validation accuracy:		54.35 %
Epoch 937 of 2000 took 0.105s
  training loss:		1.255753
  validation loss:		1.246043
  validation accuracy:		53.91 %
Epoch 938 of 2000 took 0.105s
  training loss:		1.255347
  validation loss:		1.245443
  validation accuracy:		54.57 %
Epoch 939 of 2000 took 0.105s
  training loss:		1.256649
  validation loss:		1.243793
  validation accuracy:		53.59 %
Epoch 940 of 2000 took 0.106s
  training loss:		1.248609
  validation loss:		1.241429
  validation accuracy:		54.57 %
Epoch 941 of 2000 took 0.106s
  training loss:		1.246971
  validation loss:		1.245895
  validation accuracy:		54.02 %
Epoch 942 of 2000 took 0.105s
  training loss:		1.249446
  validation loss:		1.249246
  validation accuracy:		54.67 %
Epoch 943 of 2000 took 0.106s
  training loss:		1.264470
  validation loss:		1.246298
  validation accuracy:		53.70 %
Epoch 944 of 2000 took 0.105s
  training loss:		1.263341
  validation loss:		1.251060
  validation accuracy:		54.57 %
Epoch 945 of 2000 took 0.105s
  training loss:		1.256813
  validation loss:		1.268113
  validation accuracy:		51.41 %
Epoch 946 of 2000 took 0.105s
  training loss:		1.252558
  validation loss:		1.254156
  validation accuracy:		53.70 %
Epoch 947 of 2000 took 0.105s
  training loss:		1.261137
  validation loss:		1.246823
  validation accuracy:		54.46 %
Epoch 948 of 2000 took 0.105s
  training loss:		1.263256
  validation loss:		1.259438
  validation accuracy:		53.26 %
Epoch 949 of 2000 took 0.105s
  training loss:		1.255764
  validation loss:		1.246930
  validation accuracy:		53.91 %
Epoch 950 of 2000 took 0.106s
  training loss:		1.258380
  validation loss:		1.245687
  validation accuracy:		54.35 %
Epoch 951 of 2000 took 0.102s
  training loss:		1.250197
  validation loss:		1.278499
  validation accuracy:		55.00 %
Epoch 952 of 2000 took 0.102s
  training loss:		1.259325
  validation loss:		1.245534
  validation accuracy:		53.91 %
Epoch 953 of 2000 took 0.102s
  training loss:		1.253063
  validation loss:		1.250422
  validation accuracy:		53.70 %
Epoch 954 of 2000 took 0.102s
  training loss:		1.260533
  validation loss:		1.268056
  validation accuracy:		52.83 %
Epoch 955 of 2000 took 0.103s
  training loss:		1.250980
  validation loss:		1.240890
  validation accuracy:		54.78 %
Epoch 956 of 2000 took 0.102s
  training loss:		1.256920
  validation loss:		1.250220
  validation accuracy:		53.26 %
Epoch 957 of 2000 took 0.102s
  training loss:		1.262236
  validation loss:		1.247811
  validation accuracy:		54.24 %
Epoch 958 of 2000 took 0.102s
  training loss:		1.265469
  validation loss:		1.280616
  validation accuracy:		53.70 %
Epoch 959 of 2000 took 0.103s
  training loss:		1.258925
  validation loss:		1.256648
  validation accuracy:		53.37 %
Epoch 960 of 2000 took 0.102s
  training loss:		1.260214
  validation loss:		1.249982
  validation accuracy:		53.59 %
Epoch 961 of 2000 took 0.102s
  training loss:		1.255261
  validation loss:		1.247598
  validation accuracy:		54.78 %
Epoch 962 of 2000 took 0.102s
  training loss:		1.269590
  validation loss:		1.248215
  validation accuracy:		53.37 %
Epoch 963 of 2000 took 0.102s
  training loss:		1.256316
  validation loss:		1.250124
  validation accuracy:		53.80 %
Epoch 964 of 2000 took 0.102s
  training loss:		1.255051
  validation loss:		1.253176
  validation accuracy:		53.48 %
Epoch 965 of 2000 took 0.102s
  training loss:		1.258759
  validation loss:		1.251711
  validation accuracy:		55.11 %
Epoch 966 of 2000 took 0.102s
  training loss:		1.248841
  validation loss:		1.246312
  validation accuracy:		54.78 %
Epoch 967 of 2000 took 0.102s
  training loss:		1.256261
  validation loss:		1.250517
  validation accuracy:		54.02 %
Epoch 968 of 2000 took 0.102s
  training loss:		1.264196
  validation loss:		1.246240
  validation accuracy:		54.46 %
Epoch 969 of 2000 took 0.102s
  training loss:		1.252971
  validation loss:		1.249571
  validation accuracy:		53.48 %
Epoch 970 of 2000 took 0.102s
  training loss:		1.248765
  validation loss:		1.250890
  validation accuracy:		53.91 %
Epoch 971 of 2000 took 0.102s
  training loss:		1.254242
  validation loss:		1.242348
  validation accuracy:		54.24 %
Epoch 972 of 2000 took 0.102s
  training loss:		1.258885
  validation loss:		1.248161
  validation accuracy:		53.91 %
Epoch 973 of 2000 took 0.103s
  training loss:		1.252313
  validation loss:		1.247755
  validation accuracy:		53.91 %
Epoch 974 of 2000 took 0.102s
  training loss:		1.262419
  validation loss:		1.253285
  validation accuracy:		54.89 %
Epoch 975 of 2000 took 0.102s
  training loss:		1.256860
  validation loss:		1.253835
  validation accuracy:		53.26 %
Epoch 976 of 2000 took 0.102s
  training loss:		1.256661
  validation loss:		1.246465
  validation accuracy:		54.89 %
Epoch 977 of 2000 took 0.102s
  training loss:		1.252075
  validation loss:		1.245339
  validation accuracy:		53.37 %
Epoch 978 of 2000 took 0.102s
  training loss:		1.260666
  validation loss:		1.262755
  validation accuracy:		53.37 %
Epoch 979 of 2000 took 0.102s
  training loss:		1.246750
  validation loss:		1.256919
  validation accuracy:		54.67 %
Epoch 980 of 2000 took 0.103s
  training loss:		1.270331
  validation loss:		1.269932
  validation accuracy:		53.91 %
Epoch 981 of 2000 took 0.102s
  training loss:		1.250042
  validation loss:		1.245155
  validation accuracy:		54.02 %
Epoch 982 of 2000 took 0.102s
  training loss:		1.254086
  validation loss:		1.241913
  validation accuracy:		54.24 %
Epoch 983 of 2000 took 0.102s
  training loss:		1.258380
  validation loss:		1.247723
  validation accuracy:		54.35 %
Epoch 984 of 2000 took 0.102s
  training loss:		1.262691
  validation loss:		1.255144
  validation accuracy:		54.13 %
Epoch 985 of 2000 took 0.102s
  training loss:		1.254887
  validation loss:		1.247119
  validation accuracy:		54.02 %
Epoch 986 of 2000 took 0.102s
  training loss:		1.258462
  validation loss:		1.245975
  validation accuracy:		54.78 %
Epoch 987 of 2000 took 0.102s
  training loss:		1.285081
  validation loss:		1.244817
  validation accuracy:		54.13 %
Epoch 988 of 2000 took 0.102s
  training loss:		1.255718
  validation loss:		1.247737
  validation accuracy:		53.91 %
Epoch 989 of 2000 took 0.102s
  training loss:		1.258613
  validation loss:		1.244415
  validation accuracy:		54.67 %
Epoch 990 of 2000 took 0.102s
  training loss:		1.252706
  validation loss:		1.261450
  validation accuracy:		54.78 %
Epoch 991 of 2000 took 0.102s
  training loss:		1.251423
  validation loss:		1.244327
  validation accuracy:		53.91 %
Epoch 992 of 2000 took 0.102s
  training loss:		1.262943
  validation loss:		1.257292
  validation accuracy:		54.35 %
Epoch 993 of 2000 took 0.102s
  training loss:		1.259614
  validation loss:		1.246735
  validation accuracy:		53.15 %
Epoch 994 of 2000 took 0.103s
  training loss:		1.259186
  validation loss:		1.246467
  validation accuracy:		54.89 %
Epoch 995 of 2000 took 0.102s
  training loss:		1.253589
  validation loss:		1.245357
  validation accuracy:		54.24 %
Epoch 996 of 2000 took 0.102s
  training loss:		1.261298
  validation loss:		1.255394
  validation accuracy:		52.50 %
Epoch 997 of 2000 took 0.102s
  training loss:		1.252550
  validation loss:		1.252872
  validation accuracy:		53.37 %
Epoch 998 of 2000 took 0.103s
  training loss:		1.254497
  validation loss:		1.256489
  validation accuracy:		55.43 %
Epoch 999 of 2000 took 0.103s
  training loss:		1.256386
  validation loss:		1.247431
  validation accuracy:		54.78 %
Epoch 1000 of 2000 took 0.102s
  training loss:		1.250524
  validation loss:		1.247309
  validation accuracy:		54.67 %
Epoch 1001 of 2000 took 0.102s
  training loss:		1.275985
  validation loss:		1.250060
  validation accuracy:		53.91 %
Epoch 1002 of 2000 took 0.102s
  training loss:		1.255208
  validation loss:		1.247648
  validation accuracy:		54.13 %
Epoch 1003 of 2000 took 0.102s
  training loss:		1.264771
  validation loss:		1.253518
  validation accuracy:		53.70 %
Epoch 1004 of 2000 took 0.102s
  training loss:		1.257808
  validation loss:		1.245354
  validation accuracy:		52.93 %
Epoch 1005 of 2000 took 0.102s
  training loss:		1.250822
  validation loss:		1.244647
  validation accuracy:		53.80 %
Epoch 1006 of 2000 took 0.102s
  training loss:		1.254645
  validation loss:		1.243636
  validation accuracy:		54.46 %
Epoch 1007 of 2000 took 0.102s
  training loss:		1.258282
  validation loss:		1.251261
  validation accuracy:		54.78 %
Epoch 1008 of 2000 took 0.103s
  training loss:		1.252465
  validation loss:		1.249962
  validation accuracy:		54.35 %
Epoch 1009 of 2000 took 0.103s
  training loss:		1.258695
  validation loss:		1.254544
  validation accuracy:		53.15 %
Epoch 1010 of 2000 took 0.102s
  training loss:		1.273448
  validation loss:		1.293365
  validation accuracy:		54.02 %
Epoch 1011 of 2000 took 0.102s
  training loss:		1.258669
  validation loss:		1.246844
  validation accuracy:		54.13 %
Epoch 1012 of 2000 took 0.102s
  training loss:		1.255402
  validation loss:		1.245925
  validation accuracy:		54.24 %
Epoch 1013 of 2000 took 0.102s
  training loss:		1.267203
  validation loss:		1.248746
  validation accuracy:		53.70 %
Epoch 1014 of 2000 took 0.102s
  training loss:		1.274589
  validation loss:		1.248585
  validation accuracy:		53.48 %
Epoch 1015 of 2000 took 0.102s
  training loss:		1.258185
  validation loss:		1.241467
  validation accuracy:		54.57 %
Epoch 1016 of 2000 took 0.102s
  training loss:		1.262219
  validation loss:		1.250014
  validation accuracy:		54.35 %
Epoch 1017 of 2000 took 0.102s
  training loss:		1.251583
  validation loss:		1.285077
  validation accuracy:		54.24 %
Epoch 1018 of 2000 took 0.102s
  training loss:		1.257261
  validation loss:		1.261709
  validation accuracy:		51.85 %
Epoch 1019 of 2000 took 0.102s
  training loss:		1.257646
  validation loss:		1.260088
  validation accuracy:		52.61 %
Epoch 1020 of 2000 took 0.102s
  training loss:		1.260183
  validation loss:		1.251791
  validation accuracy:		53.37 %
Epoch 1021 of 2000 took 0.102s
  training loss:		1.259390
  validation loss:		1.273750
  validation accuracy:		54.35 %
Epoch 1022 of 2000 took 0.102s
  training loss:		1.261146
  validation loss:		1.261570
  validation accuracy:		54.46 %
Epoch 1023 of 2000 took 0.102s
  training loss:		1.254826
  validation loss:		1.250028
  validation accuracy:		53.80 %
Epoch 1024 of 2000 took 0.102s
  training loss:		1.250838
  validation loss:		1.247596
  validation accuracy:		53.80 %
Epoch 1025 of 2000 took 0.102s
  training loss:		1.256625
  validation loss:		1.245642
  validation accuracy:		54.78 %
Epoch 1026 of 2000 took 0.102s
  training loss:		1.255444
  validation loss:		1.254242
  validation accuracy:		54.24 %
Epoch 1027 of 2000 took 0.102s
  training loss:		1.259807
  validation loss:		1.244429
  validation accuracy:		53.80 %
Epoch 1028 of 2000 took 0.102s
  training loss:		1.259688
  validation loss:		1.243328
  validation accuracy:		54.89 %
Epoch 1029 of 2000 took 0.102s
  training loss:		1.252437
  validation loss:		1.243664
  validation accuracy:		54.46 %
Epoch 1030 of 2000 took 0.102s
  training loss:		1.255391
  validation loss:		1.247806
  validation accuracy:		54.02 %
Epoch 1031 of 2000 took 0.102s
  training loss:		1.243619
  validation loss:		1.247064
  validation accuracy:		53.70 %
Epoch 1032 of 2000 took 0.102s
  training loss:		1.276952
  validation loss:		1.245594
  validation accuracy:		53.80 %
Epoch 1033 of 2000 took 0.103s
  training loss:		1.253259
  validation loss:		1.245074
  validation accuracy:		54.46 %
Epoch 1034 of 2000 took 0.103s
  training loss:		1.257171
  validation loss:		1.257301
  validation accuracy:		52.07 %
Epoch 1035 of 2000 took 0.102s
  training loss:		1.261669
  validation loss:		1.243645
  validation accuracy:		54.57 %
Epoch 1036 of 2000 took 0.102s
  training loss:		1.256933
  validation loss:		1.246888
  validation accuracy:		53.37 %
Epoch 1037 of 2000 took 0.103s
  training loss:		1.251809
  validation loss:		1.254379
  validation accuracy:		53.70 %
Epoch 1038 of 2000 took 0.105s
  training loss:		1.253126
  validation loss:		1.245303
  validation accuracy:		53.80 %
Epoch 1039 of 2000 took 0.106s
  training loss:		1.259290
  validation loss:		1.264943
  validation accuracy:		54.35 %
Epoch 1040 of 2000 took 0.105s
  training loss:		1.253614
  validation loss:		1.246290
  validation accuracy:		54.24 %
Epoch 1041 of 2000 took 0.105s
  training loss:		1.254653
  validation loss:		1.245451
  validation accuracy:		53.48 %
Epoch 1042 of 2000 took 0.105s
  training loss:		1.250135
  validation loss:		1.245919
  validation accuracy:		54.13 %
Epoch 1043 of 2000 took 0.105s
  training loss:		1.251230
  validation loss:		1.243762
  validation accuracy:		54.78 %
Epoch 1044 of 2000 took 0.105s
  training loss:		1.265466
  validation loss:		1.240282
  validation accuracy:		54.46 %
Epoch 1045 of 2000 took 0.105s
  training loss:		1.256954
  validation loss:		1.247895
  validation accuracy:		54.02 %
Epoch 1046 of 2000 took 0.105s
  training loss:		1.254594
  validation loss:		1.254870
  validation accuracy:		53.80 %
Epoch 1047 of 2000 took 0.105s
  training loss:		1.260493
  validation loss:		1.275777
  validation accuracy:		53.48 %
Epoch 1048 of 2000 took 0.105s
  training loss:		1.261025
  validation loss:		1.256026
  validation accuracy:		52.39 %
Epoch 1049 of 2000 took 0.105s
  training loss:		1.254995
  validation loss:		1.243214
  validation accuracy:		54.24 %
Epoch 1050 of 2000 took 0.105s
  training loss:		1.263353
  validation loss:		1.255495
  validation accuracy:		53.37 %
Epoch 1051 of 2000 took 0.105s
  training loss:		1.256472
  validation loss:		1.249870
  validation accuracy:		53.59 %
Epoch 1052 of 2000 took 0.105s
  training loss:		1.271492
  validation loss:		1.244578
  validation accuracy:		54.78 %
Epoch 1053 of 2000 took 0.105s
  training loss:		1.253782
  validation loss:		1.256788
  validation accuracy:		53.37 %
Epoch 1054 of 2000 took 0.105s
  training loss:		1.256942
  validation loss:		1.247253
  validation accuracy:		54.24 %
Epoch 1055 of 2000 took 0.105s
  training loss:		1.257532
  validation loss:		1.252863
  validation accuracy:		53.37 %
Epoch 1056 of 2000 took 0.105s
  training loss:		1.255810
  validation loss:		1.253852
  validation accuracy:		53.80 %
Epoch 1057 of 2000 took 0.105s
  training loss:		1.264603
  validation loss:		1.246983
  validation accuracy:		54.02 %
Epoch 1058 of 2000 took 0.105s
  training loss:		1.251152
  validation loss:		1.248900
  validation accuracy:		53.80 %
Epoch 1059 of 2000 took 0.105s
  training loss:		1.260493
  validation loss:		1.243311
  validation accuracy:		54.35 %
Epoch 1060 of 2000 took 0.105s
  training loss:		1.250375
  validation loss:		1.245509
  validation accuracy:		55.00 %
Epoch 1061 of 2000 took 0.105s
  training loss:		1.250876
  validation loss:		1.249333
  validation accuracy:		53.37 %
Epoch 1062 of 2000 took 0.105s
  training loss:		1.257038
  validation loss:		1.246977
  validation accuracy:		53.37 %
Epoch 1063 of 2000 took 0.105s
  training loss:		1.253997
  validation loss:		1.253737
  validation accuracy:		53.91 %
Epoch 1064 of 2000 took 0.105s
  training loss:		1.258876
  validation loss:		1.258870
  validation accuracy:		52.07 %
Epoch 1065 of 2000 took 0.105s
  training loss:		1.266672
  validation loss:		1.245448
  validation accuracy:		54.24 %
Epoch 1066 of 2000 took 0.105s
  training loss:		1.266871
  validation loss:		1.247159
  validation accuracy:		54.02 %
Epoch 1067 of 2000 took 0.106s
  training loss:		1.256456
  validation loss:		1.249351
  validation accuracy:		53.59 %
Epoch 1068 of 2000 took 0.105s
  training loss:		1.257360
  validation loss:		1.247612
  validation accuracy:		53.80 %
Epoch 1069 of 2000 took 0.105s
  training loss:		1.253849
  validation loss:		1.247400
  validation accuracy:		53.80 %
Epoch 1070 of 2000 took 0.105s
  training loss:		1.251752
  validation loss:		1.245315
  validation accuracy:		54.02 %
Epoch 1071 of 2000 took 0.105s
  training loss:		1.262662
  validation loss:		1.247590
  validation accuracy:		54.57 %
Epoch 1072 of 2000 took 0.105s
  training loss:		1.249585
  validation loss:		1.280172
  validation accuracy:		53.80 %
Epoch 1073 of 2000 took 0.105s
  training loss:		1.260387
  validation loss:		1.263981
  validation accuracy:		51.74 %
Epoch 1074 of 2000 took 0.104s
  training loss:		1.265414
  validation loss:		1.246640
  validation accuracy:		53.48 %
Epoch 1075 of 2000 took 0.102s
  training loss:		1.258924
  validation loss:		1.268459
  validation accuracy:		54.13 %
Epoch 1076 of 2000 took 0.102s
  training loss:		1.261484
  validation loss:		1.247829
  validation accuracy:		53.37 %
Epoch 1077 of 2000 took 0.102s
  training loss:		1.259684
  validation loss:		1.252876
  validation accuracy:		54.24 %
Epoch 1078 of 2000 took 0.102s
  training loss:		1.260599
  validation loss:		1.248692
  validation accuracy:		54.02 %
Epoch 1079 of 2000 took 0.102s
  training loss:		1.257337
  validation loss:		1.252464
  validation accuracy:		53.15 %
Epoch 1080 of 2000 took 0.102s
  training loss:		1.250955
  validation loss:		1.247452
  validation accuracy:		53.15 %
Epoch 1081 of 2000 took 0.102s
  training loss:		1.255440
  validation loss:		1.254825
  validation accuracy:		54.13 %
Epoch 1082 of 2000 took 0.102s
  training loss:		1.250458
  validation loss:		1.242230
  validation accuracy:		54.35 %
Epoch 1083 of 2000 took 0.102s
  training loss:		1.253689
  validation loss:		1.266252
  validation accuracy:		53.70 %
Epoch 1084 of 2000 took 0.102s
  training loss:		1.260415
  validation loss:		1.249339
  validation accuracy:		53.37 %
Epoch 1085 of 2000 took 0.102s
  training loss:		1.244713
  validation loss:		1.241582
  validation accuracy:		53.80 %
Epoch 1086 of 2000 took 0.103s
  training loss:		1.254259
  validation loss:		1.245929
  validation accuracy:		53.26 %
Epoch 1087 of 2000 took 0.102s
  training loss:		1.257354
  validation loss:		1.252234
  validation accuracy:		53.80 %
Epoch 1088 of 2000 took 0.102s
  training loss:		1.252868
  validation loss:		1.247182
  validation accuracy:		53.70 %
Epoch 1089 of 2000 took 0.103s
  training loss:		1.265499
  validation loss:		1.244149
  validation accuracy:		54.89 %
Epoch 1090 of 2000 took 0.106s
  training loss:		1.248326
  validation loss:		1.247135
  validation accuracy:		54.02 %
Epoch 1091 of 2000 took 0.106s
  training loss:		1.256709
  validation loss:		1.247671
  validation accuracy:		53.70 %
Epoch 1092 of 2000 took 0.105s
  training loss:		1.255290
  validation loss:		1.246540
  validation accuracy:		53.80 %
Epoch 1093 of 2000 took 0.105s
  training loss:		1.251062
  validation loss:		1.250445
  validation accuracy:		54.67 %
Epoch 1094 of 2000 took 0.106s
  training loss:		1.248981
  validation loss:		1.248938
  validation accuracy:		53.15 %
Epoch 1095 of 2000 took 0.106s
  training loss:		1.253934
  validation loss:		1.255782
  validation accuracy:		54.35 %
Epoch 1096 of 2000 took 0.105s
  training loss:		1.264711
  validation loss:		1.251735
  validation accuracy:		53.91 %
Epoch 1097 of 2000 took 0.102s
  training loss:		1.264741
  validation loss:		1.251142
  validation accuracy:		54.24 %
Epoch 1098 of 2000 took 0.102s
  training loss:		1.268280
  validation loss:		1.247864
  validation accuracy:		53.59 %
Epoch 1099 of 2000 took 0.102s
  training loss:		1.262171
  validation loss:		1.252996
  validation accuracy:		53.48 %
Epoch 1100 of 2000 took 0.102s
  training loss:		1.248519
  validation loss:		1.254069
  validation accuracy:		55.11 %
Epoch 1101 of 2000 took 0.102s
  training loss:		1.263117
  validation loss:		1.242886
  validation accuracy:		54.46 %
Epoch 1102 of 2000 took 0.102s
  training loss:		1.256390
  validation loss:		1.256231
  validation accuracy:		53.15 %
Epoch 1103 of 2000 took 0.102s
  training loss:		1.273400
  validation loss:		1.254446
  validation accuracy:		54.78 %
Epoch 1104 of 2000 took 0.102s
  training loss:		1.257706
  validation loss:		1.290711
  validation accuracy:		54.02 %
Epoch 1105 of 2000 took 0.102s
  training loss:		1.245202
  validation loss:		1.249080
  validation accuracy:		54.02 %
Epoch 1106 of 2000 took 0.103s
  training loss:		1.264088
  validation loss:		1.268134
  validation accuracy:		53.70 %
Epoch 1107 of 2000 took 0.102s
  training loss:		1.265817
  validation loss:		1.246910
  validation accuracy:		54.78 %
Epoch 1108 of 2000 took 0.102s
  training loss:		1.260733
  validation loss:		1.246893
  validation accuracy:		54.46 %
Epoch 1109 of 2000 took 0.102s
  training loss:		1.248446
  validation loss:		1.249470
  validation accuracy:		53.80 %
Epoch 1110 of 2000 took 0.103s
  training loss:		1.253060
  validation loss:		1.247188
  validation accuracy:		54.46 %
Epoch 1111 of 2000 took 0.102s
  training loss:		1.255117
  validation loss:		1.245667
  validation accuracy:		54.67 %
Epoch 1112 of 2000 took 0.102s
  training loss:		1.260239
  validation loss:		1.254753
  validation accuracy:		53.59 %
Epoch 1113 of 2000 took 0.102s
  training loss:		1.251532
  validation loss:		1.260726
  validation accuracy:		55.43 %
Epoch 1114 of 2000 took 0.103s
  training loss:		1.257238
  validation loss:		1.244415
  validation accuracy:		54.89 %
Epoch 1115 of 2000 took 0.103s
  training loss:		1.260125
  validation loss:		1.247648
  validation accuracy:		54.57 %
Epoch 1116 of 2000 took 0.102s
  training loss:		1.259128
  validation loss:		1.259102
  validation accuracy:		53.04 %
Epoch 1117 of 2000 took 0.102s
  training loss:		1.258567
  validation loss:		1.257502
  validation accuracy:		53.91 %
Epoch 1118 of 2000 took 0.102s
  training loss:		1.261480
  validation loss:		1.258459
  validation accuracy:		53.59 %
Epoch 1119 of 2000 took 0.102s
  training loss:		1.268398
  validation loss:		1.251012
  validation accuracy:		53.26 %
Epoch 1120 of 2000 took 0.102s
  training loss:		1.256875
  validation loss:		1.252321
  validation accuracy:		54.67 %
Epoch 1121 of 2000 took 0.102s
  training loss:		1.259038
  validation loss:		1.244713
  validation accuracy:		54.13 %
Epoch 1122 of 2000 took 0.102s
  training loss:		1.249519
  validation loss:		1.252848
  validation accuracy:		52.39 %
Epoch 1123 of 2000 took 0.102s
  training loss:		1.262084
  validation loss:		1.241710
  validation accuracy:		53.80 %
Epoch 1124 of 2000 took 0.102s
  training loss:		1.257765
  validation loss:		1.250412
  validation accuracy:		53.91 %
Epoch 1125 of 2000 took 0.102s
  training loss:		1.250282
  validation loss:		1.253974
  validation accuracy:		53.26 %
Epoch 1126 of 2000 took 0.103s
  training loss:		1.257084
  validation loss:		1.244741
  validation accuracy:		53.80 %
Epoch 1127 of 2000 took 0.102s
  training loss:		1.255073
  validation loss:		1.253481
  validation accuracy:		53.37 %
Epoch 1128 of 2000 took 0.102s
  training loss:		1.250997
  validation loss:		1.252172
  validation accuracy:		53.48 %
Epoch 1129 of 2000 took 0.102s
  training loss:		1.253291
  validation loss:		1.242835
  validation accuracy:		53.70 %
Epoch 1130 of 2000 took 0.102s
  training loss:		1.256495
  validation loss:		1.248185
  validation accuracy:		54.24 %
Epoch 1131 of 2000 took 0.102s
  training loss:		1.260983
  validation loss:		1.259786
  validation accuracy:		52.72 %
Epoch 1132 of 2000 took 0.102s
  training loss:		1.261729
  validation loss:		1.245494
  validation accuracy:		53.70 %
Epoch 1133 of 2000 took 0.102s
  training loss:		1.249227
  validation loss:		1.248962
  validation accuracy:		54.02 %
Epoch 1134 of 2000 took 0.102s
  training loss:		1.256687
  validation loss:		1.241145
  validation accuracy:		54.35 %
Epoch 1135 of 2000 took 0.102s
  training loss:		1.248454
  validation loss:		1.250647
  validation accuracy:		53.15 %
Epoch 1136 of 2000 took 0.102s
  training loss:		1.251376
  validation loss:		1.245879
  validation accuracy:		53.91 %
Epoch 1137 of 2000 took 0.102s
  training loss:		1.261790
  validation loss:		1.252205
  validation accuracy:		53.70 %
Epoch 1138 of 2000 took 0.102s
  training loss:		1.256446
  validation loss:		1.244002
  validation accuracy:		54.02 %
Epoch 1139 of 2000 took 0.102s
  training loss:		1.265861
  validation loss:		1.245603
  validation accuracy:		54.57 %
Epoch 1140 of 2000 took 0.102s
  training loss:		1.259528
  validation loss:		1.249090
  validation accuracy:		53.04 %
Epoch 1141 of 2000 took 0.102s
  training loss:		1.270671
  validation loss:		1.258550
  validation accuracy:		53.80 %
Epoch 1142 of 2000 took 0.102s
  training loss:		1.255255
  validation loss:		1.244493
  validation accuracy:		53.80 %
Epoch 1143 of 2000 took 0.102s
  training loss:		1.251260
  validation loss:		1.261731
  validation accuracy:		53.37 %
Epoch 1144 of 2000 took 0.102s
  training loss:		1.257414
  validation loss:		1.245667
  validation accuracy:		53.91 %
Epoch 1145 of 2000 took 0.102s
  training loss:		1.253245
  validation loss:		1.242979
  validation accuracy:		54.02 %
Epoch 1146 of 2000 took 0.102s
  training loss:		1.258048
  validation loss:		1.252756
  validation accuracy:		52.93 %
Epoch 1147 of 2000 took 0.102s
  training loss:		1.259543
  validation loss:		1.241562
  validation accuracy:		54.46 %
Epoch 1148 of 2000 took 0.102s
  training loss:		1.262370
  validation loss:		1.246788
  validation accuracy:		54.24 %
Epoch 1149 of 2000 took 0.103s
  training loss:		1.253009
  validation loss:		1.250977
  validation accuracy:		52.93 %
Epoch 1150 of 2000 took 0.102s
  training loss:		1.250684
  validation loss:		1.248429
  validation accuracy:		54.02 %
Epoch 1151 of 2000 took 0.102s
  training loss:		1.261128
  validation loss:		1.246200
  validation accuracy:		53.80 %
Epoch 1152 of 2000 took 0.102s
  training loss:		1.265506
  validation loss:		1.279312
  validation accuracy:		54.89 %
Epoch 1153 of 2000 took 0.103s
  training loss:		1.261047
  validation loss:		1.245476
  validation accuracy:		53.59 %
Epoch 1154 of 2000 took 0.102s
  training loss:		1.251791
  validation loss:		1.243398
  validation accuracy:		54.89 %
Epoch 1155 of 2000 took 0.103s
  training loss:		1.262510
  validation loss:		1.268732
  validation accuracy:		51.85 %
Epoch 1156 of 2000 took 0.102s
  training loss:		1.262640
  validation loss:		1.249396
  validation accuracy:		53.59 %
Epoch 1157 of 2000 took 0.102s
  training loss:		1.252766
  validation loss:		1.254461
  validation accuracy:		54.78 %
Epoch 1158 of 2000 took 0.102s
  training loss:		1.251051
  validation loss:		1.277298
  validation accuracy:		53.59 %
Epoch 1159 of 2000 took 0.102s
  training loss:		1.270759
  validation loss:		1.249828
  validation accuracy:		54.57 %
Epoch 1160 of 2000 took 0.102s
  training loss:		1.263131
  validation loss:		1.247196
  validation accuracy:		54.02 %
Epoch 1161 of 2000 took 0.103s
  training loss:		1.257489
  validation loss:		1.242416
  validation accuracy:		54.02 %
Epoch 1162 of 2000 took 0.102s
  training loss:		1.252706
  validation loss:		1.268906
  validation accuracy:		51.96 %
Epoch 1163 of 2000 took 0.102s
  training loss:		1.261183
  validation loss:		1.252492
  validation accuracy:		54.13 %
Epoch 1164 of 2000 took 0.102s
  training loss:		1.260213
  validation loss:		1.264892
  validation accuracy:		54.46 %
Epoch 1165 of 2000 took 0.102s
  training loss:		1.261559
  validation loss:		1.245181
  validation accuracy:		52.93 %
Epoch 1166 of 2000 took 0.102s
  training loss:		1.253679
  validation loss:		1.247130
  validation accuracy:		54.13 %
Epoch 1167 of 2000 took 0.103s
  training loss:		1.256455
  validation loss:		1.243515
  validation accuracy:		54.24 %
Epoch 1168 of 2000 took 0.102s
  training loss:		1.257479
  validation loss:		1.246944
  validation accuracy:		53.15 %
Epoch 1169 of 2000 took 0.102s
  training loss:		1.259930
  validation loss:		1.246043
  validation accuracy:		53.59 %
Epoch 1170 of 2000 took 0.102s
  training loss:		1.256984
  validation loss:		1.242940
  validation accuracy:		54.24 %
Epoch 1171 of 2000 took 0.102s
  training loss:		1.255521
  validation loss:		1.250740
  validation accuracy:		53.15 %
Epoch 1172 of 2000 took 0.102s
  training loss:		1.259520
  validation loss:		1.249168
  validation accuracy:		53.48 %
Epoch 1173 of 2000 took 0.102s
  training loss:		1.260963
  validation loss:		1.249850
  validation accuracy:		53.59 %
Epoch 1174 of 2000 took 0.102s
  training loss:		1.249507
  validation loss:		1.245338
  validation accuracy:		53.26 %
Epoch 1175 of 2000 took 0.103s
  training loss:		1.250179
  validation loss:		1.244768
  validation accuracy:		53.80 %
Epoch 1176 of 2000 took 0.102s
  training loss:		1.267341
  validation loss:		1.274264
  validation accuracy:		54.57 %
Epoch 1177 of 2000 took 0.102s
  training loss:		1.264293
  validation loss:		1.245871
  validation accuracy:		54.46 %
Epoch 1178 of 2000 took 0.102s
  training loss:		1.258366
  validation loss:		1.243354
  validation accuracy:		54.13 %
Epoch 1179 of 2000 took 0.102s
  training loss:		1.262027
  validation loss:		1.246711
  validation accuracy:		54.02 %
Epoch 1180 of 2000 took 0.102s
  training loss:		1.258672
  validation loss:		1.244012
  validation accuracy:		53.80 %
Epoch 1181 of 2000 took 0.102s
  training loss:		1.260972
  validation loss:		1.248037
  validation accuracy:		54.35 %
Epoch 1182 of 2000 took 0.102s
  training loss:		1.252944
  validation loss:		1.244734
  validation accuracy:		54.35 %
Epoch 1183 of 2000 took 0.102s
  training loss:		1.247949
  validation loss:		1.245637
  validation accuracy:		54.02 %
Epoch 1184 of 2000 took 0.102s
  training loss:		1.254112
  validation loss:		1.246745
  validation accuracy:		52.72 %
Epoch 1185 of 2000 took 0.103s
  training loss:		1.255051
  validation loss:		1.246193
  validation accuracy:		54.57 %
Epoch 1186 of 2000 took 0.102s
  training loss:		1.257751
  validation loss:		1.247620
  validation accuracy:		53.91 %
Epoch 1187 of 2000 took 0.103s
  training loss:		1.255077
  validation loss:		1.275561
  validation accuracy:		54.57 %
Epoch 1188 of 2000 took 0.103s
  training loss:		1.255302
  validation loss:		1.252967
  validation accuracy:		54.02 %
Epoch 1189 of 2000 took 0.103s
  training loss:		1.252378
  validation loss:		1.248358
  validation accuracy:		53.80 %
Epoch 1190 of 2000 took 0.102s
  training loss:		1.246643
  validation loss:		1.254769
  validation accuracy:		53.91 %
Epoch 1191 of 2000 took 0.102s
  training loss:		1.263383
  validation loss:		1.245608
  validation accuracy:		54.02 %
Epoch 1192 of 2000 took 0.102s
  training loss:		1.271902
  validation loss:		1.247635
  validation accuracy:		53.26 %
Epoch 1193 of 2000 took 0.102s
  training loss:		1.251398
  validation loss:		1.247145
  validation accuracy:		53.59 %
Epoch 1194 of 2000 took 0.102s
  training loss:		1.257044
  validation loss:		1.261055
  validation accuracy:		54.35 %
Epoch 1195 of 2000 took 0.103s
  training loss:		1.255464
  validation loss:		1.248396
  validation accuracy:		53.91 %
Epoch 1196 of 2000 took 0.102s
  training loss:		1.254817
  validation loss:		1.280540
  validation accuracy:		55.00 %
Epoch 1197 of 2000 took 0.102s
  training loss:		1.261888
  validation loss:		1.261728
  validation accuracy:		54.02 %
Epoch 1198 of 2000 took 0.102s
  training loss:		1.269225
  validation loss:		1.263328
  validation accuracy:		53.37 %
Epoch 1199 of 2000 took 0.102s
  training loss:		1.250558
  validation loss:		1.255299
  validation accuracy:		54.02 %
Epoch 1200 of 2000 took 0.102s
  training loss:		1.248684
  validation loss:		1.254431
  validation accuracy:		55.11 %
Epoch 1201 of 2000 took 0.102s
  training loss:		1.259514
  validation loss:		1.267526
  validation accuracy:		53.04 %
Epoch 1202 of 2000 took 0.102s
  training loss:		1.252489
  validation loss:		1.250323
  validation accuracy:		53.70 %
Epoch 1203 of 2000 took 0.102s
  training loss:		1.253340
  validation loss:		1.248400
  validation accuracy:		54.02 %
Epoch 1204 of 2000 took 0.102s
  training loss:		1.254874
  validation loss:		1.248242
  validation accuracy:		53.91 %
Epoch 1205 of 2000 took 0.103s
  training loss:		1.255397
  validation loss:		1.250413
  validation accuracy:		53.37 %
Epoch 1206 of 2000 took 0.102s
  training loss:		1.255642
  validation loss:		1.245338
  validation accuracy:		54.02 %
Epoch 1207 of 2000 took 0.102s
  training loss:		1.254526
  validation loss:		1.260454
  validation accuracy:		54.78 %
Epoch 1208 of 2000 took 0.102s
  training loss:		1.255419
  validation loss:		1.242292
  validation accuracy:		54.78 %
Epoch 1209 of 2000 took 0.102s
  training loss:		1.257890
  validation loss:		1.245913
  validation accuracy:		54.78 %
Epoch 1210 of 2000 took 0.102s
  training loss:		1.252863
  validation loss:		1.247923
  validation accuracy:		54.35 %
Epoch 1211 of 2000 took 0.102s
  training loss:		1.266108
  validation loss:		1.271501
  validation accuracy:		54.57 %
Epoch 1212 of 2000 took 0.102s
  training loss:		1.261173
  validation loss:		1.253015
  validation accuracy:		52.28 %
Epoch 1213 of 2000 took 0.102s
  training loss:		1.254188
  validation loss:		1.254681
  validation accuracy:		53.80 %
Epoch 1214 of 2000 took 0.103s
  training loss:		1.259047
  validation loss:		1.247077
  validation accuracy:		53.80 %
Epoch 1215 of 2000 took 0.102s
  training loss:		1.257546
  validation loss:		1.242134
  validation accuracy:		53.70 %
Epoch 1216 of 2000 took 0.102s
  training loss:		1.263098
  validation loss:		1.243365
  validation accuracy:		53.48 %
Epoch 1217 of 2000 took 0.102s
  training loss:		1.253829
  validation loss:		1.248285
  validation accuracy:		53.80 %
Epoch 1218 of 2000 took 0.102s
  training loss:		1.252949
  validation loss:		1.248399
  validation accuracy:		53.48 %
Epoch 1219 of 2000 took 0.102s
  training loss:		1.256068
  validation loss:		1.242891
  validation accuracy:		54.46 %
Epoch 1220 of 2000 took 0.102s
  training loss:		1.275965
  validation loss:		1.245326
  validation accuracy:		53.59 %
Epoch 1221 of 2000 took 0.102s
  training loss:		1.263748
  validation loss:		1.239738
  validation accuracy:		54.46 %
Epoch 1222 of 2000 took 0.102s
  training loss:		1.259611
  validation loss:		1.249220
  validation accuracy:		54.24 %
Epoch 1223 of 2000 took 0.102s
  training loss:		1.247271
  validation loss:		1.247073
  validation accuracy:		54.24 %
Epoch 1224 of 2000 took 0.102s
  training loss:		1.250720
  validation loss:		1.248875
  validation accuracy:		53.91 %
Epoch 1225 of 2000 took 0.103s
  training loss:		1.258551
  validation loss:		1.249801
  validation accuracy:		53.15 %
Epoch 1226 of 2000 took 0.102s
  training loss:		1.262410
  validation loss:		1.245290
  validation accuracy:		53.37 %
Epoch 1227 of 2000 took 0.103s
  training loss:		1.260627
  validation loss:		1.247041
  validation accuracy:		52.83 %
Epoch 1228 of 2000 took 0.103s
  training loss:		1.258193
  validation loss:		1.253202
  validation accuracy:		53.37 %
Epoch 1229 of 2000 took 0.102s
  training loss:		1.255322
  validation loss:		1.255512
  validation accuracy:		53.37 %
Epoch 1230 of 2000 took 0.102s
  training loss:		1.259670
  validation loss:		1.250584
  validation accuracy:		53.37 %
Epoch 1231 of 2000 took 0.102s
  training loss:		1.261860
  validation loss:		1.245690
  validation accuracy:		54.13 %
Epoch 1232 of 2000 took 0.106s
  training loss:		1.259410
  validation loss:		1.243839
  validation accuracy:		54.35 %
Epoch 1233 of 2000 took 0.106s
  training loss:		1.254171
  validation loss:		1.253131
  validation accuracy:		53.80 %
Epoch 1234 of 2000 took 0.103s
  training loss:		1.257210
  validation loss:		1.251608
  validation accuracy:		53.70 %
Epoch 1235 of 2000 took 0.102s
  training loss:		1.249727
  validation loss:		1.252081
  validation accuracy:		54.13 %
Epoch 1236 of 2000 took 0.102s
  training loss:		1.259138
  validation loss:		1.246247
  validation accuracy:		53.70 %
Epoch 1237 of 2000 took 0.102s
  training loss:		1.254848
  validation loss:		1.257540
  validation accuracy:		53.04 %
Epoch 1238 of 2000 took 0.102s
  training loss:		1.254180
  validation loss:		1.248657
  validation accuracy:		54.02 %
Epoch 1239 of 2000 took 0.102s
  training loss:		1.251501
  validation loss:		1.263594
  validation accuracy:		52.72 %
Epoch 1240 of 2000 took 0.102s
  training loss:		1.260675
  validation loss:		1.246521
  validation accuracy:		53.59 %
Epoch 1241 of 2000 took 0.102s
  training loss:		1.246783
  validation loss:		1.256318
  validation accuracy:		54.02 %
Epoch 1242 of 2000 took 0.102s
  training loss:		1.258188
  validation loss:		1.255940
  validation accuracy:		54.35 %
Epoch 1243 of 2000 took 0.102s
  training loss:		1.254686
  validation loss:		1.241465
  validation accuracy:		53.59 %
Epoch 1244 of 2000 took 0.103s
  training loss:		1.259891
  validation loss:		1.246600
  validation accuracy:		53.15 %
Epoch 1245 of 2000 took 0.102s
  training loss:		1.248936
  validation loss:		1.245618
  validation accuracy:		55.11 %
Epoch 1246 of 2000 took 0.102s
  training loss:		1.252123
  validation loss:		1.247929
  validation accuracy:		53.80 %
Epoch 1247 of 2000 took 0.103s
  training loss:		1.252534
  validation loss:		1.268732
  validation accuracy:		53.48 %
Epoch 1248 of 2000 took 0.102s
  training loss:		1.270123
  validation loss:		1.246408
  validation accuracy:		53.59 %
Epoch 1249 of 2000 took 0.102s
  training loss:		1.260199
  validation loss:		1.253762
  validation accuracy:		53.91 %
Epoch 1250 of 2000 took 0.102s
  training loss:		1.259295
  validation loss:		1.244134
  validation accuracy:		55.22 %
Epoch 1251 of 2000 took 0.103s
  training loss:		1.258857
  validation loss:		1.252127
  validation accuracy:		53.91 %
Epoch 1252 of 2000 took 0.103s
  training loss:		1.253224
  validation loss:		1.257224
  validation accuracy:		52.83 %
Epoch 1253 of 2000 took 0.103s
  training loss:		1.257352
  validation loss:		1.248785
  validation accuracy:		53.26 %
Epoch 1254 of 2000 took 0.102s
  training loss:		1.264061
  validation loss:		1.248222
  validation accuracy:		54.78 %
Epoch 1255 of 2000 took 0.103s
  training loss:		1.251925
  validation loss:		1.248953
  validation accuracy:		55.00 %
Epoch 1256 of 2000 took 0.102s
  training loss:		1.246797
  validation loss:		1.247965
  validation accuracy:		53.70 %
Epoch 1257 of 2000 took 0.102s
  training loss:		1.257745
  validation loss:		1.243573
  validation accuracy:		54.35 %
Epoch 1258 of 2000 took 0.102s
  training loss:		1.251696
  validation loss:		1.261556
  validation accuracy:		52.17 %
Epoch 1259 of 2000 took 0.102s
  training loss:		1.256140
  validation loss:		1.250235
  validation accuracy:		53.48 %
Epoch 1260 of 2000 took 0.102s
  training loss:		1.262754
  validation loss:		1.246955
  validation accuracy:		54.67 %
Epoch 1261 of 2000 took 0.102s
  training loss:		1.263886
  validation loss:		1.262405
  validation accuracy:		53.48 %
Epoch 1262 of 2000 took 0.102s
  training loss:		1.254443
  validation loss:		1.252752
  validation accuracy:		54.78 %
Epoch 1263 of 2000 took 0.102s
  training loss:		1.260122
  validation loss:		1.274189
  validation accuracy:		54.57 %
Epoch 1264 of 2000 took 0.102s
  training loss:		1.260973
  validation loss:		1.244604
  validation accuracy:		54.46 %
Epoch 1265 of 2000 took 0.103s
  training loss:		1.258828
  validation loss:		1.249452
  validation accuracy:		53.15 %
Epoch 1266 of 2000 took 0.102s
  training loss:		1.253112
  validation loss:		1.247236
  validation accuracy:		54.57 %
Epoch 1267 of 2000 took 0.102s
  training loss:		1.246366
  validation loss:		1.246000
  validation accuracy:		53.91 %
Epoch 1268 of 2000 took 0.102s
  training loss:		1.253262
  validation loss:		1.251008
  validation accuracy:		53.59 %
Epoch 1269 of 2000 took 0.102s
  training loss:		1.262466
  validation loss:		1.285110
  validation accuracy:		53.59 %
Epoch 1270 of 2000 took 0.102s
  training loss:		1.262960
  validation loss:		1.249791
  validation accuracy:		54.02 %
Epoch 1271 of 2000 took 0.102s
  training loss:		1.256875
  validation loss:		1.242883
  validation accuracy:		53.26 %
Epoch 1272 of 2000 took 0.103s
  training loss:		1.253902
  validation loss:		1.245345
  validation accuracy:		53.80 %
Epoch 1273 of 2000 took 0.103s
  training loss:		1.253584
  validation loss:		1.244549
  validation accuracy:		53.91 %
Epoch 1274 of 2000 took 0.102s
  training loss:		1.245727
  validation loss:		1.248647
  validation accuracy:		53.26 %
Epoch 1275 of 2000 took 0.102s
  training loss:		1.260493
  validation loss:		1.247795
  validation accuracy:		52.83 %
Epoch 1276 of 2000 took 0.102s
  training loss:		1.252665
  validation loss:		1.242925
  validation accuracy:		54.02 %
Epoch 1277 of 2000 took 0.102s
  training loss:		1.254369
  validation loss:		1.245370
  validation accuracy:		55.00 %
Epoch 1278 of 2000 took 0.102s
  training loss:		1.254083
  validation loss:		1.250036
  validation accuracy:		53.59 %
Epoch 1279 of 2000 took 0.102s
  training loss:		1.254797
  validation loss:		1.249955
  validation accuracy:		53.91 %
Epoch 1280 of 2000 took 0.102s
  training loss:		1.258665
  validation loss:		1.248209
  validation accuracy:		54.13 %
Epoch 1281 of 2000 took 0.102s
  training loss:		1.260480
  validation loss:		1.248956
  validation accuracy:		53.48 %
Epoch 1282 of 2000 took 0.102s
  training loss:		1.251062
  validation loss:		1.243773
  validation accuracy:		54.13 %
Epoch 1283 of 2000 took 0.102s
  training loss:		1.253582
  validation loss:		1.246105
  validation accuracy:		54.24 %
Epoch 1284 of 2000 took 0.102s
  training loss:		1.257375
  validation loss:		1.249980
  validation accuracy:		53.59 %
Epoch 1285 of 2000 took 0.103s
  training loss:		1.258255
  validation loss:		1.248063
  validation accuracy:		54.78 %
Epoch 1286 of 2000 took 0.103s
  training loss:		1.257332
  validation loss:		1.246073
  validation accuracy:		55.00 %
Epoch 1287 of 2000 took 0.102s
  training loss:		1.253306
  validation loss:		1.245688
  validation accuracy:		53.59 %
Epoch 1288 of 2000 took 0.102s
  training loss:		1.256129
  validation loss:		1.249229
  validation accuracy:		53.70 %
Epoch 1289 of 2000 took 0.102s
  training loss:		1.264148
  validation loss:		1.243450
  validation accuracy:		53.91 %
Epoch 1290 of 2000 took 0.102s
  training loss:		1.254093
  validation loss:		1.248835
  validation accuracy:		54.24 %
Epoch 1291 of 2000 took 0.104s
  training loss:		1.260923
  validation loss:		1.251963
  validation accuracy:		55.33 %
Epoch 1292 of 2000 took 0.102s
  training loss:		1.259180
  validation loss:		1.246097
  validation accuracy:		53.59 %
Epoch 1293 of 2000 took 0.102s
  training loss:		1.259047
  validation loss:		1.248206
  validation accuracy:		53.70 %
Epoch 1294 of 2000 took 0.102s
  training loss:		1.254227
  validation loss:		1.257219
  validation accuracy:		53.80 %
Epoch 1295 of 2000 took 0.102s
  training loss:		1.245616
  validation loss:		1.246096
  validation accuracy:		53.80 %
Epoch 1296 of 2000 took 0.102s
  training loss:		1.255299
  validation loss:		1.245340
  validation accuracy:		54.35 %
Epoch 1297 of 2000 took 0.102s
  training loss:		1.244219
  validation loss:		1.243822
  validation accuracy:		54.24 %
Epoch 1298 of 2000 took 0.102s
  training loss:		1.259695
  validation loss:		1.248255
  validation accuracy:		54.02 %
Epoch 1299 of 2000 took 0.102s
  training loss:		1.256674
  validation loss:		1.253278
  validation accuracy:		54.46 %
Epoch 1300 of 2000 took 0.102s
  training loss:		1.256315
  validation loss:		1.250022
  validation accuracy:		53.70 %
Epoch 1301 of 2000 took 0.102s
  training loss:		1.254907
  validation loss:		1.245440
  validation accuracy:		54.89 %
Epoch 1302 of 2000 took 0.103s
  training loss:		1.265627
  validation loss:		1.248044
  validation accuracy:		53.80 %
Epoch 1303 of 2000 took 0.102s
  training loss:		1.269372
  validation loss:		1.253934
  validation accuracy:		53.37 %
Epoch 1304 of 2000 took 0.102s
  training loss:		1.244512
  validation loss:		1.246499
  validation accuracy:		53.91 %
Epoch 1305 of 2000 took 0.102s
  training loss:		1.267056
  validation loss:		1.246626
  validation accuracy:		53.80 %
Epoch 1306 of 2000 took 0.102s
  training loss:		1.257186
  validation loss:		1.251959
  validation accuracy:		54.24 %
Epoch 1307 of 2000 took 0.102s
  training loss:		1.258394
  validation loss:		1.244594
  validation accuracy:		53.70 %
Epoch 1308 of 2000 took 0.102s
  training loss:		1.253316
  validation loss:		1.243514
  validation accuracy:		54.13 %
Epoch 1309 of 2000 took 0.103s
  training loss:		1.253725
  validation loss:		1.249025
  validation accuracy:		53.91 %
Epoch 1310 of 2000 took 0.102s
  training loss:		1.257161
  validation loss:		1.251027
  validation accuracy:		53.91 %
Epoch 1311 of 2000 took 0.102s
  training loss:		1.258844
  validation loss:		1.252193
  validation accuracy:		53.59 %
Epoch 1312 of 2000 took 0.102s
  training loss:		1.246224
  validation loss:		1.244951
  validation accuracy:		53.70 %
Epoch 1313 of 2000 took 0.102s
  training loss:		1.249774
  validation loss:		1.248563
  validation accuracy:		54.67 %
Epoch 1314 of 2000 took 0.102s
  training loss:		1.261293
  validation loss:		1.246971
  validation accuracy:		54.46 %
Epoch 1315 of 2000 took 0.102s
  training loss:		1.251583
  validation loss:		1.253337
  validation accuracy:		52.83 %
Epoch 1316 of 2000 took 0.102s
  training loss:		1.261326
  validation loss:		1.258032
  validation accuracy:		53.37 %
Epoch 1317 of 2000 took 0.102s
  training loss:		1.266708
  validation loss:		1.246592
  validation accuracy:		54.02 %
Epoch 1318 of 2000 took 0.102s
  training loss:		1.267490
  validation loss:		1.244885
  validation accuracy:		55.11 %
Epoch 1319 of 2000 took 0.102s
  training loss:		1.248761
  validation loss:		1.254700
  validation accuracy:		53.37 %
Epoch 1320 of 2000 took 0.102s
  training loss:		1.256088
  validation loss:		1.256521
  validation accuracy:		54.02 %
Epoch 1321 of 2000 took 0.102s
  training loss:		1.263475
  validation loss:		1.246190
  validation accuracy:		54.67 %
Epoch 1322 of 2000 took 0.102s
  training loss:		1.268304
  validation loss:		1.258026
  validation accuracy:		53.70 %
Epoch 1323 of 2000 took 0.102s
  training loss:		1.254582
  validation loss:		1.287054
  validation accuracy:		52.50 %
Epoch 1324 of 2000 took 0.102s
  training loss:		1.256522
  validation loss:		1.278854
  validation accuracy:		51.20 %
Epoch 1325 of 2000 took 0.102s
  training loss:		1.261461
  validation loss:		1.265690
  validation accuracy:		52.28 %
Epoch 1326 of 2000 took 0.102s
  training loss:		1.258362
  validation loss:		1.241241
  validation accuracy:		54.57 %
Epoch 1327 of 2000 took 0.102s
  training loss:		1.261354
  validation loss:		1.248561
  validation accuracy:		55.00 %
Epoch 1328 of 2000 took 0.102s
  training loss:		1.250176
  validation loss:		1.250235
  validation accuracy:		53.04 %
Epoch 1329 of 2000 took 0.102s
  training loss:		1.266466
  validation loss:		1.248212
  validation accuracy:		54.13 %
Epoch 1330 of 2000 took 0.102s
  training loss:		1.257876
  validation loss:		1.253773
  validation accuracy:		53.37 %
Epoch 1331 of 2000 took 0.102s
  training loss:		1.263979
  validation loss:		1.247343
  validation accuracy:		55.11 %
Epoch 1332 of 2000 took 0.103s
  training loss:		1.253789
  validation loss:		1.254678
  validation accuracy:		53.70 %
Epoch 1333 of 2000 took 0.102s
  training loss:		1.250130
  validation loss:		1.243926
  validation accuracy:		53.59 %
Epoch 1334 of 2000 took 0.102s
  training loss:		1.254798
  validation loss:		1.247048
  validation accuracy:		53.80 %
Epoch 1335 of 2000 took 0.102s
  training loss:		1.250827
  validation loss:		1.245108
  validation accuracy:		54.13 %
Epoch 1336 of 2000 took 0.102s
  training loss:		1.260380
  validation loss:		1.243655
  validation accuracy:		54.46 %
Epoch 1337 of 2000 took 0.102s
  training loss:		1.253799
  validation loss:		1.246286
  validation accuracy:		53.15 %
Epoch 1338 of 2000 took 0.102s
  training loss:		1.251960
  validation loss:		1.245157
  validation accuracy:		53.91 %
Epoch 1339 of 2000 took 0.102s
  training loss:		1.252203
  validation loss:		1.248689
  validation accuracy:		53.70 %
Epoch 1340 of 2000 took 0.102s
  training loss:		1.254011
  validation loss:		1.249339
  validation accuracy:		53.15 %
Epoch 1341 of 2000 took 0.102s
  training loss:		1.250436
  validation loss:		1.245115
  validation accuracy:		53.80 %
Epoch 1342 of 2000 took 0.102s
  training loss:		1.266214
  validation loss:		1.249021
  validation accuracy:		53.26 %
Epoch 1343 of 2000 took 0.102s
  training loss:		1.262070
  validation loss:		1.251099
  validation accuracy:		54.78 %
Epoch 1344 of 2000 took 0.102s
  training loss:		1.262237
  validation loss:		1.247888
  validation accuracy:		52.93 %
Epoch 1345 of 2000 took 0.103s
  training loss:		1.262628
  validation loss:		1.245875
  validation accuracy:		54.13 %
Epoch 1346 of 2000 took 0.104s
  training loss:		1.260742
  validation loss:		1.255886
  validation accuracy:		53.91 %
Epoch 1347 of 2000 took 0.105s
  training loss:		1.252827
  validation loss:		1.248847
  validation accuracy:		54.35 %
Epoch 1348 of 2000 took 0.106s
  training loss:		1.252706
  validation loss:		1.254141
  validation accuracy:		54.57 %
Epoch 1349 of 2000 took 0.106s
  training loss:		1.256041
  validation loss:		1.244037
  validation accuracy:		54.89 %
Epoch 1350 of 2000 took 0.106s
  training loss:		1.257712
  validation loss:		1.247575
  validation accuracy:		55.54 %
Epoch 1351 of 2000 took 0.105s
  training loss:		1.249363
  validation loss:		1.246781
  validation accuracy:		55.11 %
Epoch 1352 of 2000 took 0.106s
  training loss:		1.259999
  validation loss:		1.275807
  validation accuracy:		53.37 %
Epoch 1353 of 2000 took 0.105s
  training loss:		1.251390
  validation loss:		1.244723
  validation accuracy:		54.02 %
Epoch 1354 of 2000 took 0.105s
  training loss:		1.253873
  validation loss:		1.245585
  validation accuracy:		53.91 %
Epoch 1355 of 2000 took 0.105s
  training loss:		1.258480
  validation loss:		1.248987
  validation accuracy:		53.15 %
Epoch 1356 of 2000 took 0.105s
  training loss:		1.255695
  validation loss:		1.269555
  validation accuracy:		54.57 %
Epoch 1357 of 2000 took 0.105s
  training loss:		1.266222
  validation loss:		1.246582
  validation accuracy:		54.24 %
Epoch 1358 of 2000 took 0.105s
  training loss:		1.252064
  validation loss:		1.244957
  validation accuracy:		55.00 %
Epoch 1359 of 2000 took 0.106s
  training loss:		1.258947
  validation loss:		1.266411
  validation accuracy:		54.24 %
Epoch 1360 of 2000 took 0.106s
  training loss:		1.248785
  validation loss:		1.245465
  validation accuracy:		54.13 %
Epoch 1361 of 2000 took 0.106s
  training loss:		1.254391
  validation loss:		1.248512
  validation accuracy:		53.91 %
Epoch 1362 of 2000 took 0.106s
  training loss:		1.253623
  validation loss:		1.264220
  validation accuracy:		54.35 %
Epoch 1363 of 2000 took 0.105s
  training loss:		1.259404
  validation loss:		1.257778
  validation accuracy:		54.57 %
Epoch 1364 of 2000 took 0.106s
  training loss:		1.246337
  validation loss:		1.247756
  validation accuracy:		53.91 %
Epoch 1365 of 2000 took 0.105s
  training loss:		1.251697
  validation loss:		1.252610
  validation accuracy:		53.15 %
Epoch 1366 of 2000 took 0.105s
  training loss:		1.254696
  validation loss:		1.248033
  validation accuracy:		54.13 %
Epoch 1367 of 2000 took 0.105s
  training loss:		1.248048
  validation loss:		1.243892
  validation accuracy:		53.80 %
Epoch 1368 of 2000 took 0.105s
  training loss:		1.249693
  validation loss:		1.246644
  validation accuracy:		54.46 %
Epoch 1369 of 2000 took 0.105s
  training loss:		1.257301
  validation loss:		1.249159
  validation accuracy:		53.26 %
Epoch 1370 of 2000 took 0.105s
  training loss:		1.254130
  validation loss:		1.248497
  validation accuracy:		54.35 %
Epoch 1371 of 2000 took 0.105s
  training loss:		1.251727
  validation loss:		1.254567
  validation accuracy:		54.67 %
Epoch 1372 of 2000 took 0.105s
  training loss:		1.255035
  validation loss:		1.251277
  validation accuracy:		53.80 %
Epoch 1373 of 2000 took 0.105s
  training loss:		1.259223
  validation loss:		1.252531
  validation accuracy:		53.59 %
Epoch 1374 of 2000 took 0.105s
  training loss:		1.259107
  validation loss:		1.243854
  validation accuracy:		54.78 %
Epoch 1375 of 2000 took 0.105s
  training loss:		1.250601
  validation loss:		1.258575
  validation accuracy:		53.91 %
Epoch 1376 of 2000 took 0.105s
  training loss:		1.251866
  validation loss:		1.250853
  validation accuracy:		55.00 %
Epoch 1377 of 2000 took 0.105s
  training loss:		1.262696
  validation loss:		1.245948
  validation accuracy:		53.37 %
Epoch 1378 of 2000 took 0.106s
  training loss:		1.262213
  validation loss:		1.275302
  validation accuracy:		53.48 %
Epoch 1379 of 2000 took 0.103s
  training loss:		1.247250
  validation loss:		1.250187
  validation accuracy:		54.35 %
Epoch 1380 of 2000 took 0.102s
  training loss:		1.266708
  validation loss:		1.246887
  validation accuracy:		53.37 %
Epoch 1381 of 2000 took 0.102s
  training loss:		1.254095
  validation loss:		1.247403
  validation accuracy:		53.59 %
Epoch 1382 of 2000 took 0.102s
  training loss:		1.254440
  validation loss:		1.243660
  validation accuracy:		54.02 %
Epoch 1383 of 2000 took 0.102s
  training loss:		1.251842
  validation loss:		1.250767
  validation accuracy:		53.91 %
Epoch 1384 of 2000 took 0.102s
  training loss:		1.258538
  validation loss:		1.245823
  validation accuracy:		54.57 %
Epoch 1385 of 2000 took 0.102s
  training loss:		1.263068
  validation loss:		1.251357
  validation accuracy:		52.93 %
Epoch 1386 of 2000 took 0.102s
  training loss:		1.260009
  validation loss:		1.248340
  validation accuracy:		53.70 %
Epoch 1387 of 2000 took 0.102s
  training loss:		1.252075
  validation loss:		1.243847
  validation accuracy:		54.89 %
Epoch 1388 of 2000 took 0.102s
  training loss:		1.255437
  validation loss:		1.262807
  validation accuracy:		54.35 %
Epoch 1389 of 2000 took 0.102s
  training loss:		1.261254
  validation loss:		1.245136
  validation accuracy:		55.33 %
Epoch 1390 of 2000 took 0.103s
  training loss:		1.256528
  validation loss:		1.248714
  validation accuracy:		53.91 %
Epoch 1391 of 2000 took 0.102s
  training loss:		1.251326
  validation loss:		1.245769
  validation accuracy:		54.35 %
Epoch 1392 of 2000 took 0.102s
  training loss:		1.253749
  validation loss:		1.243177
  validation accuracy:		54.78 %
Epoch 1393 of 2000 took 0.102s
  training loss:		1.254204
  validation loss:		1.243541
  validation accuracy:		54.02 %
Epoch 1394 of 2000 took 0.102s
  training loss:		1.258203
  validation loss:		1.251487
  validation accuracy:		53.04 %
Epoch 1395 of 2000 took 0.102s
  training loss:		1.267391
  validation loss:		1.254800
  validation accuracy:		54.46 %
Epoch 1396 of 2000 took 0.102s
  training loss:		1.254799
  validation loss:		1.257435
  validation accuracy:		54.57 %
Epoch 1397 of 2000 took 0.102s
  training loss:		1.252427
  validation loss:		1.247744
  validation accuracy:		53.48 %
Epoch 1398 of 2000 took 0.102s
  training loss:		1.257708
  validation loss:		1.254531
  validation accuracy:		53.70 %
Epoch 1399 of 2000 took 0.102s
  training loss:		1.250628
  validation loss:		1.247229
  validation accuracy:		53.91 %
Epoch 1400 of 2000 took 0.102s
  training loss:		1.255088
  validation loss:		1.248269
  validation accuracy:		53.70 %
Epoch 1401 of 2000 took 0.103s
  training loss:		1.253349
  validation loss:		1.251110
  validation accuracy:		54.13 %
Epoch 1402 of 2000 took 0.102s
  training loss:		1.251203
  validation loss:		1.247167
  validation accuracy:		53.48 %
Epoch 1403 of 2000 took 0.102s
  training loss:		1.248671
  validation loss:		1.247740
  validation accuracy:		53.80 %
Epoch 1404 of 2000 took 0.102s
  training loss:		1.252298
  validation loss:		1.248660
  validation accuracy:		53.15 %
Epoch 1405 of 2000 took 0.102s
  training loss:		1.253043
  validation loss:		1.260289
  validation accuracy:		52.28 %
Epoch 1406 of 2000 took 0.102s
  training loss:		1.254782
  validation loss:		1.258535
  validation accuracy:		55.00 %
Epoch 1407 of 2000 took 0.102s
  training loss:		1.256971
  validation loss:		1.247519
  validation accuracy:		53.80 %
Epoch 1408 of 2000 took 0.102s
  training loss:		1.261842
  validation loss:		1.253470
  validation accuracy:		55.11 %
Epoch 1409 of 2000 took 0.102s
  training loss:		1.252439
  validation loss:		1.247772
  validation accuracy:		54.78 %
Epoch 1410 of 2000 took 0.102s
  training loss:		1.251790
  validation loss:		1.244752
  validation accuracy:		54.35 %
Epoch 1411 of 2000 took 0.102s
  training loss:		1.255381
  validation loss:		1.283088
  validation accuracy:		53.70 %
Epoch 1412 of 2000 took 0.102s
  training loss:		1.253491
  validation loss:		1.247058
  validation accuracy:		53.59 %
Epoch 1413 of 2000 took 0.102s
  training loss:		1.257971
  validation loss:		1.253066
  validation accuracy:		53.37 %
Epoch 1414 of 2000 took 0.102s
  training loss:		1.258051
  validation loss:		1.258316
  validation accuracy:		53.26 %
Epoch 1415 of 2000 took 0.102s
  training loss:		1.254712
  validation loss:		1.246555
  validation accuracy:		52.93 %
Epoch 1416 of 2000 took 0.102s
  training loss:		1.259104
  validation loss:		1.257297
  validation accuracy:		54.13 %
Epoch 1417 of 2000 took 0.102s
  training loss:		1.262674
  validation loss:		1.246420
  validation accuracy:		54.78 %
Epoch 1418 of 2000 took 0.102s
  training loss:		1.253140
  validation loss:		1.282786
  validation accuracy:		53.26 %
Epoch 1419 of 2000 took 0.103s
  training loss:		1.255217
  validation loss:		1.257718
  validation accuracy:		52.50 %
Epoch 1420 of 2000 took 0.103s
  training loss:		1.254240
  validation loss:		1.248626
  validation accuracy:		53.91 %
Epoch 1421 of 2000 took 0.102s
  training loss:		1.250084
  validation loss:		1.251273
  validation accuracy:		54.78 %
Epoch 1422 of 2000 took 0.104s
  training loss:		1.244655
  validation loss:		1.250350
  validation accuracy:		53.48 %
Epoch 1423 of 2000 took 0.102s
  training loss:		1.254277
  validation loss:		1.257575
  validation accuracy:		55.33 %
Epoch 1424 of 2000 took 0.102s
  training loss:		1.251851
  validation loss:		1.253615
  validation accuracy:		55.22 %
Epoch 1425 of 2000 took 0.102s
  training loss:		1.247239
  validation loss:		1.261769
  validation accuracy:		53.48 %
Epoch 1426 of 2000 took 0.103s
  training loss:		1.256536
  validation loss:		1.249479
  validation accuracy:		54.89 %
Epoch 1427 of 2000 took 0.102s
  training loss:		1.258531
  validation loss:		1.251538
  validation accuracy:		54.24 %
Epoch 1428 of 2000 took 0.103s
  training loss:		1.253898
  validation loss:		1.249355
  validation accuracy:		53.70 %
Epoch 1429 of 2000 took 0.102s
  training loss:		1.253836
  validation loss:		1.244772
  validation accuracy:		53.91 %
Epoch 1430 of 2000 took 0.102s
  training loss:		1.257145
  validation loss:		1.253787
  validation accuracy:		54.35 %
Epoch 1431 of 2000 took 0.102s
  training loss:		1.252749
  validation loss:		1.252013
  validation accuracy:		53.59 %
Epoch 1432 of 2000 took 0.102s
  training loss:		1.262446
  validation loss:		1.249054
  validation accuracy:		54.24 %
Epoch 1433 of 2000 took 0.102s
  training loss:		1.252509
  validation loss:		1.243209
  validation accuracy:		54.89 %
Epoch 1434 of 2000 took 0.102s
  training loss:		1.260773
  validation loss:		1.243970
  validation accuracy:		52.93 %
Epoch 1435 of 2000 took 0.102s
  training loss:		1.253944
  validation loss:		1.251367
  validation accuracy:		55.33 %
Epoch 1436 of 2000 took 0.102s
  training loss:		1.249388
  validation loss:		1.244874
  validation accuracy:		54.46 %
Epoch 1437 of 2000 took 0.102s
  training loss:		1.261613
  validation loss:		1.245734
  validation accuracy:		53.37 %
Epoch 1438 of 2000 took 0.102s
  training loss:		1.258635
  validation loss:		1.251810
  validation accuracy:		53.80 %
Epoch 1439 of 2000 took 0.102s
  training loss:		1.263237
  validation loss:		1.254933
  validation accuracy:		53.48 %
Epoch 1440 of 2000 took 0.102s
  training loss:		1.265762
  validation loss:		1.249553
  validation accuracy:		53.70 %
Epoch 1441 of 2000 took 0.102s
  training loss:		1.251796
  validation loss:		1.246228
  validation accuracy:		53.37 %
Epoch 1442 of 2000 took 0.102s
  training loss:		1.253743
  validation loss:		1.244315
  validation accuracy:		54.46 %
Epoch 1443 of 2000 took 0.102s
  training loss:		1.260040
  validation loss:		1.274026
  validation accuracy:		53.70 %
Epoch 1444 of 2000 took 0.102s
  training loss:		1.265263
  validation loss:		1.247114
  validation accuracy:		53.70 %
Epoch 1445 of 2000 took 0.102s
  training loss:		1.258070
  validation loss:		1.249713
  validation accuracy:		53.59 %
Epoch 1446 of 2000 took 0.102s
  training loss:		1.252356
  validation loss:		1.246944
  validation accuracy:		53.80 %
Epoch 1447 of 2000 took 0.102s
  training loss:		1.253019
  validation loss:		1.248561
  validation accuracy:		53.70 %
Epoch 1448 of 2000 took 0.102s
  training loss:		1.259848
  validation loss:		1.246647
  validation accuracy:		54.46 %
Epoch 1449 of 2000 took 0.103s
  training loss:		1.252439
  validation loss:		1.243903
  validation accuracy:		53.04 %
Epoch 1450 of 2000 took 0.102s
  training loss:		1.261309
  validation loss:		1.243705
  validation accuracy:		54.89 %
Epoch 1451 of 2000 took 0.102s
  training loss:		1.259203
  validation loss:		1.246603
  validation accuracy:		53.70 %
Epoch 1452 of 2000 took 0.102s
  training loss:		1.267495
  validation loss:		1.274061
  validation accuracy:		55.11 %
Epoch 1453 of 2000 took 0.102s
  training loss:		1.252465
  validation loss:		1.247384
  validation accuracy:		53.70 %
Epoch 1454 of 2000 took 0.102s
  training loss:		1.252387
  validation loss:		1.244394
  validation accuracy:		54.89 %
Epoch 1455 of 2000 took 0.102s
  training loss:		1.252117
  validation loss:		1.247689
  validation accuracy:		54.46 %
Epoch 1456 of 2000 took 0.102s
  training loss:		1.256873
  validation loss:		1.246062
  validation accuracy:		53.91 %
Epoch 1457 of 2000 took 0.102s
  training loss:		1.252257
  validation loss:		1.250238
  validation accuracy:		53.80 %
Epoch 1458 of 2000 took 0.103s
  training loss:		1.256067
  validation loss:		1.250159
  validation accuracy:		53.15 %
Epoch 1459 of 2000 took 0.103s
  training loss:		1.247711
  validation loss:		1.257205
  validation accuracy:		53.91 %
Epoch 1460 of 2000 took 0.102s
  training loss:		1.255243
  validation loss:		1.253736
  validation accuracy:		53.37 %
Epoch 1461 of 2000 took 0.103s
  training loss:		1.253478
  validation loss:		1.246112
  validation accuracy:		53.59 %
Epoch 1462 of 2000 took 0.102s
  training loss:		1.257571
  validation loss:		1.248646
  validation accuracy:		53.80 %
Epoch 1463 of 2000 took 0.102s
  training loss:		1.253651
  validation loss:		1.248306
  validation accuracy:		53.91 %
Epoch 1464 of 2000 took 0.102s
  training loss:		1.249767
  validation loss:		1.243303
  validation accuracy:		54.67 %
Epoch 1465 of 2000 took 0.102s
  training loss:		1.256644
  validation loss:		1.246973
  validation accuracy:		53.91 %
Epoch 1466 of 2000 took 0.102s
  training loss:		1.252858
  validation loss:		1.247689
  validation accuracy:		53.37 %
Epoch 1467 of 2000 took 0.102s
  training loss:		1.259948
  validation loss:		1.246569
  validation accuracy:		54.89 %
Epoch 1468 of 2000 took 0.102s
  training loss:		1.245877
  validation loss:		1.249627
  validation accuracy:		54.46 %
Epoch 1469 of 2000 took 0.102s
  training loss:		1.257738
  validation loss:		1.246839
  validation accuracy:		52.93 %
Epoch 1470 of 2000 took 0.102s
  training loss:		1.254113
  validation loss:		1.243962
  validation accuracy:		54.24 %
Epoch 1471 of 2000 took 0.102s
  training loss:		1.247231
  validation loss:		1.243539
  validation accuracy:		54.13 %
Epoch 1472 of 2000 took 0.102s
  training loss:		1.261865
  validation loss:		1.244007
  validation accuracy:		54.02 %
Epoch 1473 of 2000 took 0.102s
  training loss:		1.262016
  validation loss:		1.255247
  validation accuracy:		53.37 %
Epoch 1474 of 2000 took 0.102s
  training loss:		1.249211
  validation loss:		1.247466
  validation accuracy:		53.70 %
Epoch 1475 of 2000 took 0.102s
  training loss:		1.253905
  validation loss:		1.246975
  validation accuracy:		53.48 %
Epoch 1476 of 2000 took 0.102s
  training loss:		1.262111
  validation loss:		1.256750
  validation accuracy:		54.35 %
Epoch 1477 of 2000 took 0.102s
  training loss:		1.252422
  validation loss:		1.257027
  validation accuracy:		53.04 %
Epoch 1478 of 2000 took 0.103s
  training loss:		1.257319
  validation loss:		1.246242
  validation accuracy:		54.24 %
Epoch 1479 of 2000 took 0.103s
  training loss:		1.251532
  validation loss:		1.252115
  validation accuracy:		53.59 %
Epoch 1480 of 2000 took 0.103s
  training loss:		1.250798
  validation loss:		1.250711
  validation accuracy:		52.50 %
Epoch 1481 of 2000 took 0.102s
  training loss:		1.258968
  validation loss:		1.259635
  validation accuracy:		54.02 %
Epoch 1482 of 2000 took 0.103s
  training loss:		1.264948
  validation loss:		1.249580
  validation accuracy:		54.67 %
Epoch 1483 of 2000 took 0.102s
  training loss:		1.251767
  validation loss:		1.249459
  validation accuracy:		53.37 %
Epoch 1484 of 2000 took 0.102s
  training loss:		1.252912
  validation loss:		1.252736
  validation accuracy:		53.80 %
Epoch 1485 of 2000 took 0.102s
  training loss:		1.252805
  validation loss:		1.246657
  validation accuracy:		54.02 %
Epoch 1486 of 2000 took 0.102s
  training loss:		1.255928
  validation loss:		1.246428
  validation accuracy:		53.37 %
Epoch 1487 of 2000 took 0.102s
  training loss:		1.264580
  validation loss:		1.246547
  validation accuracy:		54.67 %
Epoch 1488 of 2000 took 0.102s
  training loss:		1.257301
  validation loss:		1.244944
  validation accuracy:		54.57 %
Epoch 1489 of 2000 took 0.102s
  training loss:		1.258328
  validation loss:		1.246404
  validation accuracy:		55.11 %
Epoch 1490 of 2000 took 0.102s
  training loss:		1.257023
  validation loss:		1.245257
  validation accuracy:		54.67 %
Epoch 1491 of 2000 took 0.102s
  training loss:		1.246906
  validation loss:		1.252467
  validation accuracy:		54.78 %
Epoch 1492 of 2000 took 0.102s
  training loss:		1.255839
  validation loss:		1.244506
  validation accuracy:		53.59 %
Epoch 1493 of 2000 took 0.102s
  training loss:		1.258132
  validation loss:		1.248649
  validation accuracy:		53.70 %
Epoch 1494 of 2000 took 0.102s
  training loss:		1.245761
  validation loss:		1.248902
  validation accuracy:		53.80 %
Epoch 1495 of 2000 took 0.102s
  training loss:		1.251121
  validation loss:		1.248065
  validation accuracy:		53.80 %
Epoch 1496 of 2000 took 0.103s
  training loss:		1.253360
  validation loss:		1.244937
  validation accuracy:		54.57 %
Epoch 1497 of 2000 took 0.102s
  training loss:		1.247561
  validation loss:		1.245001
  validation accuracy:		54.13 %
Epoch 1498 of 2000 took 0.102s
  training loss:		1.258571
  validation loss:		1.246723
  validation accuracy:		55.54 %
Epoch 1499 of 2000 took 0.102s
  training loss:		1.259232
  validation loss:		1.251218
  validation accuracy:		52.28 %
Epoch 1500 of 2000 took 0.102s
  training loss:		1.257830
  validation loss:		1.247586
  validation accuracy:		53.48 %
Epoch 1501 of 2000 took 0.102s
  training loss:		1.255494
  validation loss:		1.263660
  validation accuracy:		53.80 %
Epoch 1502 of 2000 took 0.102s
  training loss:		1.258755
  validation loss:		1.241247
  validation accuracy:		55.87 %
Epoch 1503 of 2000 took 0.103s
  training loss:		1.244436
  validation loss:		1.246799
  validation accuracy:		53.59 %
Epoch 1504 of 2000 took 0.103s
  training loss:		1.258046
  validation loss:		1.248531
  validation accuracy:		53.91 %
Epoch 1505 of 2000 took 0.102s
  training loss:		1.247575
  validation loss:		1.248043
  validation accuracy:		53.80 %
Epoch 1506 of 2000 took 0.102s
  training loss:		1.261712
  validation loss:		1.238409
  validation accuracy:		54.46 %
Epoch 1507 of 2000 took 0.102s
  training loss:		1.258236
  validation loss:		1.242991
  validation accuracy:		54.13 %
Epoch 1508 of 2000 took 0.103s
  training loss:		1.250912
  validation loss:		1.252470
  validation accuracy:		53.48 %
Epoch 1509 of 2000 took 0.102s
  training loss:		1.257322
  validation loss:		1.248263
  validation accuracy:		53.37 %
Epoch 1510 of 2000 took 0.102s
  training loss:		1.252490
  validation loss:		1.257637
  validation accuracy:		54.89 %
Epoch 1511 of 2000 took 0.102s
  training loss:		1.256452
  validation loss:		1.243100
  validation accuracy:		53.91 %
Epoch 1512 of 2000 took 0.103s
  training loss:		1.246656
  validation loss:		1.255862
  validation accuracy:		54.35 %
Epoch 1513 of 2000 took 0.102s
  training loss:		1.249275
  validation loss:		1.245902
  validation accuracy:		54.89 %
Epoch 1514 of 2000 took 0.102s
  training loss:		1.259388
  validation loss:		1.245106
  validation accuracy:		54.57 %
Epoch 1515 of 2000 took 0.102s
  training loss:		1.255085
  validation loss:		1.248531
  validation accuracy:		54.13 %
Epoch 1516 of 2000 took 0.102s
  training loss:		1.254440
  validation loss:		1.252220
  validation accuracy:		53.37 %
Epoch 1517 of 2000 took 0.102s
  training loss:		1.261877
  validation loss:		1.242787
  validation accuracy:		54.24 %
Epoch 1518 of 2000 took 0.102s
  training loss:		1.254978
  validation loss:		1.247716
  validation accuracy:		53.91 %
Epoch 1519 of 2000 took 0.102s
  training loss:		1.261066
  validation loss:		1.243518
  validation accuracy:		53.70 %
Epoch 1520 of 2000 took 0.102s
  training loss:		1.254176
  validation loss:		1.260011
  validation accuracy:		54.02 %
Epoch 1521 of 2000 took 0.102s
  training loss:		1.254170
  validation loss:		1.247314
  validation accuracy:		54.24 %
Epoch 1522 of 2000 took 0.102s
  training loss:		1.251543
  validation loss:		1.246082
  validation accuracy:		54.46 %
Epoch 1523 of 2000 took 0.102s
  training loss:		1.250693
  validation loss:		1.257494
  validation accuracy:		54.24 %
Epoch 1524 of 2000 took 0.102s
  training loss:		1.256692
  validation loss:		1.243111
  validation accuracy:		54.46 %
Epoch 1525 of 2000 took 0.102s
  training loss:		1.260165
  validation loss:		1.244829
  validation accuracy:		54.35 %
Epoch 1526 of 2000 took 0.102s
  training loss:		1.259904
  validation loss:		1.246845
  validation accuracy:		53.80 %
Epoch 1527 of 2000 took 0.102s
  training loss:		1.250080
  validation loss:		1.251726
  validation accuracy:		53.80 %
Epoch 1528 of 2000 took 0.102s
  training loss:		1.257367
  validation loss:		1.246973
  validation accuracy:		55.54 %
Epoch 1529 of 2000 took 0.102s
  training loss:		1.250684
  validation loss:		1.241847
  validation accuracy:		53.91 %
Epoch 1530 of 2000 took 0.102s
  training loss:		1.245322
  validation loss:		1.247758
  validation accuracy:		53.70 %
Epoch 1531 of 2000 took 0.102s
  training loss:		1.252935
  validation loss:		1.247475
  validation accuracy:		54.46 %
Epoch 1532 of 2000 took 0.102s
  training loss:		1.255689
  validation loss:		1.245042
  validation accuracy:		53.15 %
Epoch 1533 of 2000 took 0.102s
  training loss:		1.253888
  validation loss:		1.248908
  validation accuracy:		53.37 %
Epoch 1534 of 2000 took 0.102s
  training loss:		1.250587
  validation loss:		1.248062
  validation accuracy:		54.24 %
Epoch 1535 of 2000 took 0.102s
  training loss:		1.254412
  validation loss:		1.250507
  validation accuracy:		54.46 %
Epoch 1536 of 2000 took 0.102s
  training loss:		1.252650
  validation loss:		1.243265
  validation accuracy:		54.89 %
Epoch 1537 of 2000 took 0.103s
  training loss:		1.255723
  validation loss:		1.261473
  validation accuracy:		53.59 %
Epoch 1538 of 2000 took 0.102s
  training loss:		1.253924
  validation loss:		1.250857
  validation accuracy:		53.48 %
Epoch 1539 of 2000 took 0.103s
  training loss:		1.251388
  validation loss:		1.244670
  validation accuracy:		54.13 %
Epoch 1540 of 2000 took 0.102s
  training loss:		1.259857
  validation loss:		1.248771
  validation accuracy:		52.93 %
Epoch 1541 of 2000 took 0.102s
  training loss:		1.252537
  validation loss:		1.251085
  validation accuracy:		54.57 %
Epoch 1542 of 2000 took 0.102s
  training loss:		1.259265
  validation loss:		1.243604
  validation accuracy:		54.89 %
Epoch 1543 of 2000 took 0.103s
  training loss:		1.253541
  validation loss:		1.247495
  validation accuracy:		55.54 %
Epoch 1544 of 2000 took 0.102s
  training loss:		1.252905
  validation loss:		1.247064
  validation accuracy:		54.13 %
Epoch 1545 of 2000 took 0.102s
  training loss:		1.255251
  validation loss:		1.247433
  validation accuracy:		53.48 %
Epoch 1546 of 2000 took 0.102s
  training loss:		1.256087
  validation loss:		1.258216
  validation accuracy:		53.37 %
Epoch 1547 of 2000 took 0.102s
  training loss:		1.252048
  validation loss:		1.247204
  validation accuracy:		54.57 %
Epoch 1548 of 2000 took 0.102s
  training loss:		1.251915
  validation loss:		1.255406
  validation accuracy:		52.72 %
Epoch 1549 of 2000 took 0.102s
  training loss:		1.254864
  validation loss:		1.240629
  validation accuracy:		54.02 %
Epoch 1550 of 2000 took 0.102s
  training loss:		1.254182
  validation loss:		1.245849
  validation accuracy:		54.24 %
Epoch 1551 of 2000 took 0.102s
  training loss:		1.258011
  validation loss:		1.249005
  validation accuracy:		55.87 %
Epoch 1552 of 2000 took 0.102s
  training loss:		1.255414
  validation loss:		1.263926
  validation accuracy:		54.35 %
Epoch 1553 of 2000 took 0.102s
  training loss:		1.254487
  validation loss:		1.259252
  validation accuracy:		54.46 %
Epoch 1554 of 2000 took 0.102s
  training loss:		1.250633
  validation loss:		1.247413
  validation accuracy:		54.46 %
Epoch 1555 of 2000 took 0.102s
  training loss:		1.253042
  validation loss:		1.250457
  validation accuracy:		53.37 %
Epoch 1556 of 2000 took 0.102s
  training loss:		1.253247
  validation loss:		1.243518
  validation accuracy:		54.02 %
Epoch 1557 of 2000 took 0.102s
  training loss:		1.253086
  validation loss:		1.249381
  validation accuracy:		54.02 %
Epoch 1558 of 2000 took 0.102s
  training loss:		1.253328
  validation loss:		1.247925
  validation accuracy:		53.80 %
Epoch 1559 of 2000 took 0.102s
  training loss:		1.257270
  validation loss:		1.254097
  validation accuracy:		52.93 %
Epoch 1560 of 2000 took 0.102s
  training loss:		1.252798
  validation loss:		1.243670
  validation accuracy:		53.37 %
Epoch 1561 of 2000 took 0.102s
  training loss:		1.256500
  validation loss:		1.246090
  validation accuracy:		53.37 %
Epoch 1562 of 2000 took 0.102s
  training loss:		1.264599
  validation loss:		1.243214
  validation accuracy:		54.46 %
Epoch 1563 of 2000 took 0.102s
  training loss:		1.258408
  validation loss:		1.250349
  validation accuracy:		54.24 %
Epoch 1564 of 2000 took 0.102s
  training loss:		1.255868
  validation loss:		1.247861
  validation accuracy:		53.59 %
Epoch 1565 of 2000 took 0.103s
  training loss:		1.258726
  validation loss:		1.249513
  validation accuracy:		53.91 %
Epoch 1566 of 2000 took 0.103s
  training loss:		1.251427
  validation loss:		1.248042
  validation accuracy:		54.35 %
Epoch 1567 of 2000 took 0.102s
  training loss:		1.253677
  validation loss:		1.246367
  validation accuracy:		53.80 %
Epoch 1568 of 2000 took 0.102s
  training loss:		1.271412
  validation loss:		1.245997
  validation accuracy:		53.91 %
Epoch 1569 of 2000 took 0.102s
  training loss:		1.259874
  validation loss:		1.261648
  validation accuracy:		53.70 %
Epoch 1570 of 2000 took 0.102s
  training loss:		1.259890
  validation loss:		1.246798
  validation accuracy:		53.70 %
Epoch 1571 of 2000 took 0.102s
  training loss:		1.256201
  validation loss:		1.248929
  validation accuracy:		53.48 %
Epoch 1572 of 2000 took 0.102s
  training loss:		1.252175
  validation loss:		1.252295
  validation accuracy:		54.24 %
Epoch 1573 of 2000 took 0.102s
  training loss:		1.251888
  validation loss:		1.246667
  validation accuracy:		54.24 %
Epoch 1574 of 2000 took 0.102s
  training loss:		1.250451
  validation loss:		1.248211
  validation accuracy:		52.93 %
Epoch 1575 of 2000 took 0.102s
  training loss:		1.251699
  validation loss:		1.248843
  validation accuracy:		53.26 %
Epoch 1576 of 2000 took 0.102s
  training loss:		1.252571
  validation loss:		1.244510
  validation accuracy:		54.24 %
Epoch 1577 of 2000 took 0.102s
  training loss:		1.254772
  validation loss:		1.250155
  validation accuracy:		54.13 %
Epoch 1578 of 2000 took 0.102s
  training loss:		1.246860
  validation loss:		1.249939
  validation accuracy:		54.35 %
Epoch 1579 of 2000 took 0.102s
  training loss:		1.249345
  validation loss:		1.244262
  validation accuracy:		54.13 %
Epoch 1580 of 2000 took 0.102s
  training loss:		1.260165
  validation loss:		1.249495
  validation accuracy:		53.48 %
Epoch 1581 of 2000 took 0.102s
  training loss:		1.249281
  validation loss:		1.249783
  validation accuracy:		53.91 %
Epoch 1582 of 2000 took 0.103s
  training loss:		1.250982
  validation loss:		1.253337
  validation accuracy:		54.35 %
Epoch 1583 of 2000 took 0.102s
  training loss:		1.264771
  validation loss:		1.251611
  validation accuracy:		53.15 %
Epoch 1584 of 2000 took 0.102s
  training loss:		1.258205
  validation loss:		1.245340
  validation accuracy:		54.46 %
Epoch 1585 of 2000 took 0.102s
  training loss:		1.248492
  validation loss:		1.244119
  validation accuracy:		54.46 %
Epoch 1586 of 2000 took 0.102s
  training loss:		1.254705
  validation loss:		1.261551
  validation accuracy:		54.35 %
Epoch 1587 of 2000 took 0.102s
  training loss:		1.255634
  validation loss:		1.256158
  validation accuracy:		54.24 %
Epoch 1588 of 2000 took 0.102s
  training loss:		1.252947
  validation loss:		1.257427
  validation accuracy:		54.35 %
Epoch 1589 of 2000 took 0.102s
  training loss:		1.254154
  validation loss:		1.252637
  validation accuracy:		52.72 %
Epoch 1590 of 2000 took 0.102s
  training loss:		1.255895
  validation loss:		1.247283
  validation accuracy:		54.57 %
Epoch 1591 of 2000 took 0.102s
  training loss:		1.254815
  validation loss:		1.247169
  validation accuracy:		55.00 %
Epoch 1592 of 2000 took 0.102s
  training loss:		1.247379
  validation loss:		1.251721
  validation accuracy:		53.26 %
Epoch 1593 of 2000 took 0.103s
  training loss:		1.255644
  validation loss:		1.259935
  validation accuracy:		52.61 %
Epoch 1594 of 2000 took 0.102s
  training loss:		1.264265
  validation loss:		1.246685
  validation accuracy:		54.02 %
Epoch 1595 of 2000 took 0.102s
  training loss:		1.253619
  validation loss:		1.247185
  validation accuracy:		53.70 %
Epoch 1596 of 2000 took 0.103s
  training loss:		1.244954
  validation loss:		1.247126
  validation accuracy:		53.48 %
Epoch 1597 of 2000 took 0.102s
  training loss:		1.250092
  validation loss:		1.247725
  validation accuracy:		54.02 %
Epoch 1598 of 2000 took 0.102s
  training loss:		1.260113
  validation loss:		1.249024
  validation accuracy:		53.70 %
Epoch 1599 of 2000 took 0.102s
  training loss:		1.262334
  validation loss:		1.246326
  validation accuracy:		54.35 %
Epoch 1600 of 2000 took 0.102s
  training loss:		1.256186
  validation loss:		1.244798
  validation accuracy:		54.78 %
Epoch 1601 of 2000 took 0.102s
  training loss:		1.248716
  validation loss:		1.249161
  validation accuracy:		53.80 %
Epoch 1602 of 2000 took 0.102s
  training loss:		1.250746
  validation loss:		1.252808
  validation accuracy:		53.80 %
Epoch 1603 of 2000 took 0.102s
  training loss:		1.262313
  validation loss:		1.247021
  validation accuracy:		54.13 %
Epoch 1604 of 2000 took 0.102s
  training loss:		1.257341
  validation loss:		1.239025
  validation accuracy:		54.02 %
Epoch 1605 of 2000 took 0.102s
  training loss:		1.256320
  validation loss:		1.242515
  validation accuracy:		54.02 %
Epoch 1606 of 2000 took 0.102s
  training loss:		1.257563
  validation loss:		1.253406
  validation accuracy:		52.83 %
Epoch 1607 of 2000 took 0.102s
  training loss:		1.260651
  validation loss:		1.250791
  validation accuracy:		54.89 %
Epoch 1608 of 2000 took 0.102s
  training loss:		1.251821
  validation loss:		1.246185
  validation accuracy:		54.46 %
Epoch 1609 of 2000 took 0.102s
  training loss:		1.256302
  validation loss:		1.248862
  validation accuracy:		52.83 %
Epoch 1610 of 2000 took 0.102s
  training loss:		1.257702
  validation loss:		1.248038
  validation accuracy:		53.80 %
Epoch 1611 of 2000 took 0.102s
  training loss:		1.250883
  validation loss:		1.249069
  validation accuracy:		52.61 %
Epoch 1612 of 2000 took 0.102s
  training loss:		1.256498
  validation loss:		1.247207
  validation accuracy:		53.59 %
Epoch 1613 of 2000 took 0.102s
  training loss:		1.256304
  validation loss:		1.249438
  validation accuracy:		53.80 %
Epoch 1614 of 2000 took 0.102s
  training loss:		1.256332
  validation loss:		1.247032
  validation accuracy:		54.24 %
Epoch 1615 of 2000 took 0.102s
  training loss:		1.245985
  validation loss:		1.246852
  validation accuracy:		54.02 %
Epoch 1616 of 2000 took 0.102s
  training loss:		1.251170
  validation loss:		1.250863
  validation accuracy:		53.48 %
Epoch 1617 of 2000 took 0.102s
  training loss:		1.253881
  validation loss:		1.243774
  validation accuracy:		53.15 %
Epoch 1618 of 2000 took 0.102s
  training loss:		1.256817
  validation loss:		1.246515
  validation accuracy:		54.35 %
Epoch 1619 of 2000 took 0.102s
  training loss:		1.249724
  validation loss:		1.242765
  validation accuracy:		54.13 %
Epoch 1620 of 2000 took 0.102s
  training loss:		1.250449
  validation loss:		1.249458
  validation accuracy:		52.93 %
Epoch 1621 of 2000 took 0.102s
  training loss:		1.255143
  validation loss:		1.268840
  validation accuracy:		53.91 %
Epoch 1622 of 2000 took 0.102s
  training loss:		1.265047
  validation loss:		1.242777
  validation accuracy:		53.70 %
Epoch 1623 of 2000 took 0.102s
  training loss:		1.265636
  validation loss:		1.254406
  validation accuracy:		54.67 %
Epoch 1624 of 2000 took 0.102s
  training loss:		1.251921
  validation loss:		1.246588
  validation accuracy:		53.91 %
Epoch 1625 of 2000 took 0.103s
  training loss:		1.256568
  validation loss:		1.244511
  validation accuracy:		53.37 %
Epoch 1626 of 2000 took 0.102s
  training loss:		1.255705
  validation loss:		1.244449
  validation accuracy:		53.91 %
Epoch 1627 of 2000 took 0.102s
  training loss:		1.257645
  validation loss:		1.263330
  validation accuracy:		53.26 %
Epoch 1628 of 2000 took 0.102s
  training loss:		1.261693
  validation loss:		1.246502
  validation accuracy:		53.48 %
Epoch 1629 of 2000 took 0.102s
  training loss:		1.264272
  validation loss:		1.247516
  validation accuracy:		54.13 %
Epoch 1630 of 2000 took 0.102s
  training loss:		1.261403
  validation loss:		1.244168
  validation accuracy:		54.46 %
Epoch 1631 of 2000 took 0.103s
  training loss:		1.254292
  validation loss:		1.244283
  validation accuracy:		54.46 %
Epoch 1632 of 2000 took 0.103s
  training loss:		1.251465
  validation loss:		1.247973
  validation accuracy:		54.02 %
Epoch 1633 of 2000 took 0.102s
  training loss:		1.257634
  validation loss:		1.248251
  validation accuracy:		53.15 %
Epoch 1634 of 2000 took 0.102s
  training loss:		1.267365
  validation loss:		1.247590
  validation accuracy:		54.02 %
Epoch 1635 of 2000 took 0.102s
  training loss:		1.254115
  validation loss:		1.250939
  validation accuracy:		52.50 %
Epoch 1636 of 2000 took 0.102s
  training loss:		1.259943
  validation loss:		1.247325
  validation accuracy:		54.35 %
Epoch 1637 of 2000 took 0.102s
  training loss:		1.254009
  validation loss:		1.247977
  validation accuracy:		54.35 %
Epoch 1638 of 2000 took 0.102s
  training loss:		1.251710
  validation loss:		1.244811
  validation accuracy:		53.91 %
Epoch 1639 of 2000 took 0.102s
  training loss:		1.249744
  validation loss:		1.244340
  validation accuracy:		54.02 %
Epoch 1640 of 2000 took 0.102s
  training loss:		1.254091
  validation loss:		1.246996
  validation accuracy:		54.13 %
Epoch 1641 of 2000 took 0.102s
  training loss:		1.256264
  validation loss:		1.242788
  validation accuracy:		53.91 %
Epoch 1642 of 2000 took 0.102s
  training loss:		1.259704
  validation loss:		1.252392
  validation accuracy:		54.57 %
Epoch 1643 of 2000 took 0.102s
  training loss:		1.258250
  validation loss:		1.248314
  validation accuracy:		54.13 %
Epoch 1644 of 2000 took 0.102s
  training loss:		1.257448
  validation loss:		1.246114
  validation accuracy:		54.46 %
Epoch 1645 of 2000 took 0.102s
  training loss:		1.270432
  validation loss:		1.244269
  validation accuracy:		53.15 %
Epoch 1646 of 2000 took 0.102s
  training loss:		1.258603
  validation loss:		1.248226
  validation accuracy:		54.13 %
Epoch 1647 of 2000 took 0.102s
  training loss:		1.250688
  validation loss:		1.244150
  validation accuracy:		53.91 %
Epoch 1648 of 2000 took 0.102s
  training loss:		1.255708
  validation loss:		1.242690
  validation accuracy:		54.35 %
Epoch 1649 of 2000 took 0.103s
  training loss:		1.253456
  validation loss:		1.244856
  validation accuracy:		53.80 %
Epoch 1650 of 2000 took 0.102s
  training loss:		1.261935
  validation loss:		1.247723
  validation accuracy:		53.91 %
Epoch 1651 of 2000 took 0.103s
  training loss:		1.259922
  validation loss:		1.243979
  validation accuracy:		53.91 %
Epoch 1652 of 2000 took 0.103s
  training loss:		1.243981
  validation loss:		1.243542
  validation accuracy:		53.59 %
Epoch 1653 of 2000 took 0.103s
  training loss:		1.248408
  validation loss:		1.241752
  validation accuracy:		54.78 %
Epoch 1654 of 2000 took 0.102s
  training loss:		1.257723
  validation loss:		1.258662
  validation accuracy:		53.80 %
Epoch 1655 of 2000 took 0.104s
  training loss:		1.257941
  validation loss:		1.257867
  validation accuracy:		54.02 %
Epoch 1656 of 2000 took 0.102s
  training loss:		1.249935
  validation loss:		1.244757
  validation accuracy:		53.48 %
Epoch 1657 of 2000 took 0.102s
  training loss:		1.253607
  validation loss:		1.249991
  validation accuracy:		54.35 %
Epoch 1658 of 2000 took 0.102s
  training loss:		1.260248
  validation loss:		1.251790
  validation accuracy:		55.43 %
Epoch 1659 of 2000 took 0.104s
  training loss:		1.254400
  validation loss:		1.245796
  validation accuracy:		53.70 %
Epoch 1660 of 2000 took 0.106s
  training loss:		1.252551
  validation loss:		1.257314
  validation accuracy:		53.37 %
Epoch 1661 of 2000 took 0.105s
  training loss:		1.255264
  validation loss:		1.244949
  validation accuracy:		53.70 %
Epoch 1662 of 2000 took 0.105s
  training loss:		1.259807
  validation loss:		1.247085
  validation accuracy:		54.46 %
Epoch 1663 of 2000 took 0.105s
  training loss:		1.245735
  validation loss:		1.247670
  validation accuracy:		53.59 %
Epoch 1664 of 2000 took 0.105s
  training loss:		1.249675
  validation loss:		1.244252
  validation accuracy:		54.13 %
Epoch 1665 of 2000 took 0.105s
  training loss:		1.255474
  validation loss:		1.245499
  validation accuracy:		54.24 %
Epoch 1666 of 2000 took 0.105s
  training loss:		1.244647
  validation loss:		1.247242
  validation accuracy:		53.91 %
Epoch 1667 of 2000 took 0.105s
  training loss:		1.260577
  validation loss:		1.247377
  validation accuracy:		53.91 %
Epoch 1668 of 2000 took 0.105s
  training loss:		1.251564
  validation loss:		1.244092
  validation accuracy:		54.35 %
Epoch 1669 of 2000 took 0.105s
  training loss:		1.258574
  validation loss:		1.255128
  validation accuracy:		53.80 %
Epoch 1670 of 2000 took 0.105s
  training loss:		1.257264
  validation loss:		1.260794
  validation accuracy:		53.80 %
Epoch 1671 of 2000 took 0.106s
  training loss:		1.252285
  validation loss:		1.248301
  validation accuracy:		54.02 %
Epoch 1672 of 2000 took 0.106s
  training loss:		1.256222
  validation loss:		1.248572
  validation accuracy:		54.89 %
Epoch 1673 of 2000 took 0.106s
  training loss:		1.249219
  validation loss:		1.242800
  validation accuracy:		54.46 %
Epoch 1674 of 2000 took 0.105s
  training loss:		1.261079
  validation loss:		1.246192
  validation accuracy:		54.35 %
Epoch 1675 of 2000 took 0.106s
  training loss:		1.244081
  validation loss:		1.247492
  validation accuracy:		53.80 %
Epoch 1676 of 2000 took 0.106s
  training loss:		1.257194
  validation loss:		1.246072
  validation accuracy:		53.80 %
Epoch 1677 of 2000 took 0.105s
  training loss:		1.259121
  validation loss:		1.246805
  validation accuracy:		53.26 %
Epoch 1678 of 2000 took 0.105s
  training loss:		1.249799
  validation loss:		1.267435
  validation accuracy:		55.22 %
Epoch 1679 of 2000 took 0.105s
  training loss:		1.249973
  validation loss:		1.250373
  validation accuracy:		52.72 %
Epoch 1680 of 2000 took 0.105s
  training loss:		1.245262
  validation loss:		1.245179
  validation accuracy:		53.91 %
Epoch 1681 of 2000 took 0.105s
  training loss:		1.264463
  validation loss:		1.250864
  validation accuracy:		54.35 %
Epoch 1682 of 2000 took 0.105s
  training loss:		1.260412
  validation loss:		1.265175
  validation accuracy:		54.13 %
Epoch 1683 of 2000 took 0.106s
  training loss:		1.255782
  validation loss:		1.247193
  validation accuracy:		53.70 %
Epoch 1684 of 2000 took 0.106s
  training loss:		1.256320
  validation loss:		1.245804
  validation accuracy:		53.91 %
Epoch 1685 of 2000 took 0.105s
  training loss:		1.255104
  validation loss:		1.247181
  validation accuracy:		54.02 %
Epoch 1686 of 2000 took 0.105s
  training loss:		1.252521
  validation loss:		1.248285
  validation accuracy:		54.02 %
Epoch 1687 of 2000 took 0.105s
  training loss:		1.256460
  validation loss:		1.246598
  validation accuracy:		54.57 %
Epoch 1688 of 2000 took 0.105s
  training loss:		1.248473
  validation loss:		1.244339
  validation accuracy:		54.46 %
Epoch 1689 of 2000 took 0.105s
  training loss:		1.260635
  validation loss:		1.244651
  validation accuracy:		53.04 %
Epoch 1690 of 2000 took 0.106s
  training loss:		1.246047
  validation loss:		1.251534
  validation accuracy:		53.91 %
Epoch 1691 of 2000 took 0.105s
  training loss:		1.256020
  validation loss:		1.251422
  validation accuracy:		54.89 %
Epoch 1692 of 2000 took 0.105s
  training loss:		1.259520
  validation loss:		1.251419
  validation accuracy:		52.83 %
Epoch 1693 of 2000 took 0.105s
  training loss:		1.256190
  validation loss:		1.248734
  validation accuracy:		53.59 %
Epoch 1694 of 2000 took 0.106s
  training loss:		1.251975
  validation loss:		1.242280
  validation accuracy:		53.37 %
Epoch 1695 of 2000 took 0.106s
  training loss:		1.258037
  validation loss:		1.241537
  validation accuracy:		54.24 %
Epoch 1696 of 2000 took 0.105s
  training loss:		1.263892
  validation loss:		1.254535
  validation accuracy:		53.48 %
Epoch 1697 of 2000 took 0.104s
  training loss:		1.257568
  validation loss:		1.246321
  validation accuracy:		53.59 %
Epoch 1698 of 2000 took 0.102s
  training loss:		1.254200
  validation loss:		1.242630
  validation accuracy:		54.78 %
Epoch 1699 of 2000 took 0.102s
  training loss:		1.257076
  validation loss:		1.246728
  validation accuracy:		53.48 %
Epoch 1700 of 2000 took 0.102s
  training loss:		1.255743
  validation loss:		1.253913
  validation accuracy:		53.59 %
Epoch 1701 of 2000 took 0.102s
  training loss:		1.261322
  validation loss:		1.247546
  validation accuracy:		53.70 %
Epoch 1702 of 2000 took 0.102s
  training loss:		1.253688
  validation loss:		1.250208
  validation accuracy:		53.15 %
Epoch 1703 of 2000 took 0.102s
  training loss:		1.256047
  validation loss:		1.253390
  validation accuracy:		54.67 %
Epoch 1704 of 2000 took 0.102s
  training loss:		1.255544
  validation loss:		1.251933
  validation accuracy:		53.48 %
Epoch 1705 of 2000 took 0.102s
  training loss:		1.256334
  validation loss:		1.259729
  validation accuracy:		54.24 %
Epoch 1706 of 2000 took 0.102s
  training loss:		1.254787
  validation loss:		1.247289
  validation accuracy:		54.46 %
Epoch 1707 of 2000 took 0.102s
  training loss:		1.251098
  validation loss:		1.249375
  validation accuracy:		52.83 %
Epoch 1708 of 2000 took 0.102s
  training loss:		1.250067
  validation loss:		1.243408
  validation accuracy:		54.02 %
Epoch 1709 of 2000 took 0.102s
  training loss:		1.254577
  validation loss:		1.247466
  validation accuracy:		53.37 %
Epoch 1710 of 2000 took 0.102s
  training loss:		1.247982
  validation loss:		1.246236
  validation accuracy:		55.11 %
Epoch 1711 of 2000 took 0.102s
  training loss:		1.252873
  validation loss:		1.253273
  validation accuracy:		53.59 %
Epoch 1712 of 2000 took 0.102s
  training loss:		1.251971
  validation loss:		1.248151
  validation accuracy:		53.80 %
Epoch 1713 of 2000 took 0.103s
  training loss:		1.254702
  validation loss:		1.247194
  validation accuracy:		53.80 %
Epoch 1714 of 2000 took 0.102s
  training loss:		1.256095
  validation loss:		1.250842
  validation accuracy:		53.15 %
Epoch 1715 of 2000 took 0.102s
  training loss:		1.254764
  validation loss:		1.255888
  validation accuracy:		53.59 %
Epoch 1716 of 2000 took 0.102s
  training loss:		1.254319
  validation loss:		1.253826
  validation accuracy:		53.59 %
Epoch 1717 of 2000 took 0.102s
  training loss:		1.251263
  validation loss:		1.249500
  validation accuracy:		53.80 %
Epoch 1718 of 2000 took 0.102s
  training loss:		1.253268
  validation loss:		1.243744
  validation accuracy:		54.24 %
Epoch 1719 of 2000 took 0.102s
  training loss:		1.248656
  validation loss:		1.256849
  validation accuracy:		54.13 %
Epoch 1720 of 2000 took 0.102s
  training loss:		1.252228
  validation loss:		1.245394
  validation accuracy:		54.24 %
Epoch 1721 of 2000 took 0.102s
  training loss:		1.263864
  validation loss:		1.284378
  validation accuracy:		53.70 %
Epoch 1722 of 2000 took 0.102s
  training loss:		1.260492
  validation loss:		1.244673
  validation accuracy:		54.46 %
Epoch 1723 of 2000 took 0.102s
  training loss:		1.259027
  validation loss:		1.250457
  validation accuracy:		54.24 %
Epoch 1724 of 2000 took 0.102s
  training loss:		1.256907
  validation loss:		1.258838
  validation accuracy:		54.02 %
Epoch 1725 of 2000 took 0.102s
  training loss:		1.254913
  validation loss:		1.245256
  validation accuracy:		53.59 %
Epoch 1726 of 2000 took 0.102s
  training loss:		1.253781
  validation loss:		1.250018
  validation accuracy:		53.15 %
Epoch 1727 of 2000 took 0.102s
  training loss:		1.253549
  validation loss:		1.247704
  validation accuracy:		53.48 %
Epoch 1728 of 2000 took 0.102s
  training loss:		1.258909
  validation loss:		1.243133
  validation accuracy:		54.35 %
Epoch 1729 of 2000 took 0.103s
  training loss:		1.251587
  validation loss:		1.246308
  validation accuracy:		53.91 %
Epoch 1730 of 2000 took 0.102s
  training loss:		1.266802
  validation loss:		1.271537
  validation accuracy:		52.93 %
Epoch 1731 of 2000 took 0.102s
  training loss:		1.265098
  validation loss:		1.249642
  validation accuracy:		55.00 %
Epoch 1732 of 2000 took 0.102s
  training loss:		1.252167
  validation loss:		1.248813
  validation accuracy:		53.48 %
Epoch 1733 of 2000 took 0.102s
  training loss:		1.259061
  validation loss:		1.253282
  validation accuracy:		54.13 %
Epoch 1734 of 2000 took 0.103s
  training loss:		1.250508
  validation loss:		1.250710
  validation accuracy:		53.37 %
Epoch 1735 of 2000 took 0.103s
  training loss:		1.257165
  validation loss:		1.247946
  validation accuracy:		55.22 %
Epoch 1736 of 2000 took 0.102s
  training loss:		1.256174
  validation loss:		1.250472
  validation accuracy:		53.80 %
Epoch 1737 of 2000 took 0.103s
  training loss:		1.253255
  validation loss:		1.248354
  validation accuracy:		53.59 %
Epoch 1738 of 2000 took 0.102s
  training loss:		1.251884
  validation loss:		1.246244
  validation accuracy:		53.80 %
Epoch 1739 of 2000 took 0.102s
  training loss:		1.250172
  validation loss:		1.244637
  validation accuracy:		54.46 %
Epoch 1740 of 2000 took 0.102s
  training loss:		1.250984
  validation loss:		1.243135
  validation accuracy:		53.70 %
Epoch 1741 of 2000 took 0.102s
  training loss:		1.251980
  validation loss:		1.265509
  validation accuracy:		54.78 %
Epoch 1742 of 2000 took 0.103s
  training loss:		1.258980
  validation loss:		1.245624
  validation accuracy:		54.46 %
Epoch 1743 of 2000 took 0.102s
  training loss:		1.256430
  validation loss:		1.246596
  validation accuracy:		53.15 %
Epoch 1744 of 2000 took 0.102s
  training loss:		1.260224
  validation loss:		1.269927
  validation accuracy:		53.48 %
Epoch 1745 of 2000 took 0.102s
  training loss:		1.258906
  validation loss:		1.246120
  validation accuracy:		53.37 %
Epoch 1746 of 2000 took 0.102s
  training loss:		1.245053
  validation loss:		1.248434
  validation accuracy:		52.83 %
Epoch 1747 of 2000 took 0.102s
  training loss:		1.259462
  validation loss:		1.242798
  validation accuracy:		54.13 %
Epoch 1748 of 2000 took 0.102s
  training loss:		1.254837
  validation loss:		1.245545
  validation accuracy:		53.59 %
Epoch 1749 of 2000 took 0.102s
  training loss:		1.259015
  validation loss:		1.244888
  validation accuracy:		55.11 %
Epoch 1750 of 2000 took 0.102s
  training loss:		1.256506
  validation loss:		1.250637
  validation accuracy:		53.37 %
Epoch 1751 of 2000 took 0.103s
  training loss:		1.251356
  validation loss:		1.256974
  validation accuracy:		53.70 %
Epoch 1752 of 2000 took 0.102s
  training loss:		1.258714
  validation loss:		1.257092
  validation accuracy:		53.91 %
Epoch 1753 of 2000 took 0.102s
  training loss:		1.257343
  validation loss:		1.248567
  validation accuracy:		54.24 %
Epoch 1754 of 2000 took 0.102s
  training loss:		1.257526
  validation loss:		1.248385
  validation accuracy:		53.15 %
Epoch 1755 of 2000 took 0.102s
  training loss:		1.254083
  validation loss:		1.242790
  validation accuracy:		54.24 %
Epoch 1756 of 2000 took 0.102s
  training loss:		1.254940
  validation loss:		1.243221
  validation accuracy:		53.91 %
Epoch 1757 of 2000 took 0.102s
  training loss:		1.259320
  validation loss:		1.244689
  validation accuracy:		53.91 %
Epoch 1758 of 2000 took 0.102s
  training loss:		1.255133
  validation loss:		1.242683
  validation accuracy:		54.24 %
Epoch 1759 of 2000 took 0.102s
  training loss:		1.247656
  validation loss:		1.245155
  validation accuracy:		54.02 %
Epoch 1760 of 2000 took 0.102s
  training loss:		1.256081
  validation loss:		1.247023
  validation accuracy:		53.70 %
Epoch 1761 of 2000 took 0.102s
  training loss:		1.251454
  validation loss:		1.244095
  validation accuracy:		54.24 %
Epoch 1762 of 2000 took 0.102s
  training loss:		1.259522
  validation loss:		1.245724
  validation accuracy:		53.91 %
Epoch 1763 of 2000 took 0.102s
  training loss:		1.258156
  validation loss:		1.246653
  validation accuracy:		53.15 %
Epoch 1764 of 2000 took 0.102s
  training loss:		1.250453
  validation loss:		1.248426
  validation accuracy:		53.70 %
Epoch 1765 of 2000 took 0.102s
  training loss:		1.248663
  validation loss:		1.256591
  validation accuracy:		53.26 %
Epoch 1766 of 2000 took 0.102s
  training loss:		1.243811
  validation loss:		1.241778
  validation accuracy:		53.80 %
Epoch 1767 of 2000 took 0.102s
  training loss:		1.255147
  validation loss:		1.244689
  validation accuracy:		53.59 %
Epoch 1768 of 2000 took 0.102s
  training loss:		1.255238
  validation loss:		1.246768
  validation accuracy:		54.35 %
Epoch 1769 of 2000 took 0.102s
  training loss:		1.251182
  validation loss:		1.252090
  validation accuracy:		53.48 %
Epoch 1770 of 2000 took 0.102s
  training loss:		1.255717
  validation loss:		1.255903
  validation accuracy:		54.78 %
Epoch 1771 of 2000 took 0.103s
  training loss:		1.258903
  validation loss:		1.248428
  validation accuracy:		54.24 %
Epoch 1772 of 2000 took 0.104s
  training loss:		1.258786
  validation loss:		1.250203
  validation accuracy:		53.37 %
Epoch 1773 of 2000 took 0.103s
  training loss:		1.253457
  validation loss:		1.249208
  validation accuracy:		53.04 %
Epoch 1774 of 2000 took 0.102s
  training loss:		1.256933
  validation loss:		1.245032
  validation accuracy:		55.54 %
Epoch 1775 of 2000 took 0.102s
  training loss:		1.251806
  validation loss:		1.252148
  validation accuracy:		53.80 %
Epoch 1776 of 2000 took 0.102s
  training loss:		1.255905
  validation loss:		1.247104
  validation accuracy:		53.59 %
Epoch 1777 of 2000 took 0.102s
  training loss:		1.251222
  validation loss:		1.275872
  validation accuracy:		53.15 %
Epoch 1778 of 2000 took 0.102s
  training loss:		1.251588
  validation loss:		1.241612
  validation accuracy:		55.00 %
Epoch 1779 of 2000 took 0.102s
  training loss:		1.260497
  validation loss:		1.250840
  validation accuracy:		53.91 %
Epoch 1780 of 2000 took 0.102s
  training loss:		1.244387
  validation loss:		1.248286
  validation accuracy:		53.15 %
Epoch 1781 of 2000 took 0.102s
  training loss:		1.257169
  validation loss:		1.244449
  validation accuracy:		53.59 %
Epoch 1782 of 2000 took 0.102s
  training loss:		1.252971
  validation loss:		1.245433
  validation accuracy:		53.59 %
Epoch 1783 of 2000 took 0.102s
  training loss:		1.258221
  validation loss:		1.247339
  validation accuracy:		53.59 %
Epoch 1784 of 2000 took 0.102s
  training loss:		1.251718
  validation loss:		1.244349
  validation accuracy:		53.59 %
Epoch 1785 of 2000 took 0.102s
  training loss:		1.255301
  validation loss:		1.246162
  validation accuracy:		54.02 %
Epoch 1786 of 2000 took 0.102s
  training loss:		1.253664
  validation loss:		1.248800
  validation accuracy:		53.04 %
Epoch 1787 of 2000 took 0.102s
  training loss:		1.266249
  validation loss:		1.245022
  validation accuracy:		53.91 %
Epoch 1788 of 2000 took 0.102s
  training loss:		1.261857
  validation loss:		1.254416
  validation accuracy:		53.91 %
Epoch 1789 of 2000 took 0.102s
  training loss:		1.265926
  validation loss:		1.253084
  validation accuracy:		54.67 %
Epoch 1790 of 2000 took 0.102s
  training loss:		1.263518
  validation loss:		1.245643
  validation accuracy:		54.02 %
Epoch 1791 of 2000 took 0.102s
  training loss:		1.248031
  validation loss:		1.244716
  validation accuracy:		53.15 %
Epoch 1792 of 2000 took 0.102s
  training loss:		1.250247
  validation loss:		1.245425
  validation accuracy:		53.80 %
Epoch 1793 of 2000 took 0.102s
  training loss:		1.251425
  validation loss:		1.244483
  validation accuracy:		53.80 %
Epoch 1794 of 2000 took 0.102s
  training loss:		1.247286
  validation loss:		1.256255
  validation accuracy:		53.59 %
Epoch 1795 of 2000 took 0.102s
  training loss:		1.254282
  validation loss:		1.252468
  validation accuracy:		52.61 %
Epoch 1796 of 2000 took 0.102s
  training loss:		1.258605
  validation loss:		1.245245
  validation accuracy:		55.00 %
Epoch 1797 of 2000 took 0.102s
  training loss:		1.248660
  validation loss:		1.254269
  validation accuracy:		54.89 %
Epoch 1798 of 2000 took 0.103s
  training loss:		1.253436
  validation loss:		1.247348
  validation accuracy:		54.24 %
Epoch 1799 of 2000 took 0.103s
  training loss:		1.251173
  validation loss:		1.252635
  validation accuracy:		53.80 %
Epoch 1800 of 2000 took 0.102s
  training loss:		1.250010
  validation loss:		1.245697
  validation accuracy:		53.91 %
Epoch 1801 of 2000 took 0.103s
  training loss:		1.252179
  validation loss:		1.248395
  validation accuracy:		54.78 %
Epoch 1802 of 2000 took 0.102s
  training loss:		1.262490
  validation loss:		1.245258
  validation accuracy:		53.37 %
Epoch 1803 of 2000 took 0.102s
  training loss:		1.250136
  validation loss:		1.244822
  validation accuracy:		54.02 %
Epoch 1804 of 2000 took 0.102s
  training loss:		1.237901
  validation loss:		1.250136
  validation accuracy:		53.26 %
Epoch 1805 of 2000 took 0.102s
  training loss:		1.256717
  validation loss:		1.244059
  validation accuracy:		54.57 %
Epoch 1806 of 2000 took 0.102s
  training loss:		1.245690
  validation loss:		1.246327
  validation accuracy:		53.70 %
Epoch 1807 of 2000 took 0.102s
  training loss:		1.258673
  validation loss:		1.253352
  validation accuracy:		52.93 %
Epoch 1808 of 2000 took 0.102s
  training loss:		1.253427
  validation loss:		1.251203
  validation accuracy:		53.48 %
Epoch 1809 of 2000 took 0.102s
  training loss:		1.249427
  validation loss:		1.248548
  validation accuracy:		53.37 %
Epoch 1810 of 2000 took 0.102s
  training loss:		1.250324
  validation loss:		1.246813
  validation accuracy:		54.67 %
Epoch 1811 of 2000 took 0.102s
  training loss:		1.253844
  validation loss:		1.247408
  validation accuracy:		54.78 %
Epoch 1812 of 2000 took 0.102s
  training loss:		1.263830
  validation loss:		1.244471
  validation accuracy:		54.24 %
Epoch 1813 of 2000 took 0.102s
  training loss:		1.250360
  validation loss:		1.264588
  validation accuracy:		51.96 %
Epoch 1814 of 2000 took 0.102s
  training loss:		1.256093
  validation loss:		1.244907
  validation accuracy:		53.91 %
Epoch 1815 of 2000 took 0.107s
  training loss:		1.250004
  validation loss:		1.248158
  validation accuracy:		53.37 %
Epoch 1816 of 2000 took 0.109s
  training loss:		1.245412
  validation loss:		1.253823
  validation accuracy:		53.48 %
Epoch 1817 of 2000 took 0.109s
  training loss:		1.252794
  validation loss:		1.247961
  validation accuracy:		54.46 %
Epoch 1818 of 2000 took 0.109s
  training loss:		1.251586
  validation loss:		1.247341
  validation accuracy:		53.59 %
Epoch 1819 of 2000 took 0.109s
  training loss:		1.249602
  validation loss:		1.247330
  validation accuracy:		53.15 %
Epoch 1820 of 2000 took 0.109s
  training loss:		1.255584
  validation loss:		1.253827
  validation accuracy:		53.91 %
Epoch 1821 of 2000 took 0.109s
  training loss:		1.248970
  validation loss:		1.254735
  validation accuracy:		53.37 %
Epoch 1822 of 2000 took 0.109s
  training loss:		1.264353
  validation loss:		1.245964
  validation accuracy:		54.46 %
Epoch 1823 of 2000 took 0.109s
  training loss:		1.254096
  validation loss:		1.245317
  validation accuracy:		53.04 %
Epoch 1824 of 2000 took 0.109s
  training loss:		1.257087
  validation loss:		1.242874
  validation accuracy:		53.91 %
Epoch 1825 of 2000 took 0.109s
  training loss:		1.251191
  validation loss:		1.245239
  validation accuracy:		53.70 %
Epoch 1826 of 2000 took 0.109s
  training loss:		1.257747
  validation loss:		1.250717
  validation accuracy:		53.80 %
Epoch 1827 of 2000 took 0.109s
  training loss:		1.257442
  validation loss:		1.245883
  validation accuracy:		54.57 %
Epoch 1828 of 2000 took 0.109s
  training loss:		1.259227
  validation loss:		1.246709
  validation accuracy:		54.24 %
Epoch 1829 of 2000 took 0.109s
  training loss:		1.252011
  validation loss:		1.244433
  validation accuracy:		54.02 %
Epoch 1830 of 2000 took 0.109s
  training loss:		1.249715
  validation loss:		1.245096
  validation accuracy:		54.13 %
Epoch 1831 of 2000 took 0.109s
  training loss:		1.251209
  validation loss:		1.250765
  validation accuracy:		55.33 %
Epoch 1832 of 2000 took 0.109s
  training loss:		1.251543
  validation loss:		1.248013
  validation accuracy:		54.24 %
Epoch 1833 of 2000 took 0.109s
  training loss:		1.258445
  validation loss:		1.249408
  validation accuracy:		53.80 %
Epoch 1834 of 2000 took 0.109s
  training loss:		1.250830
  validation loss:		1.246040
  validation accuracy:		53.26 %
Epoch 1835 of 2000 took 0.109s
  training loss:		1.252256
  validation loss:		1.239807
  validation accuracy:		54.46 %
Epoch 1836 of 2000 took 0.109s
  training loss:		1.248373
  validation loss:		1.249099
  validation accuracy:		55.22 %
Epoch 1837 of 2000 took 0.109s
  training loss:		1.256431
  validation loss:		1.284984
  validation accuracy:		53.59 %
Epoch 1838 of 2000 took 0.109s
  training loss:		1.255743
  validation loss:		1.242973
  validation accuracy:		53.26 %
Epoch 1839 of 2000 took 0.109s
  training loss:		1.249810
  validation loss:		1.247496
  validation accuracy:		54.78 %
Epoch 1840 of 2000 took 0.109s
  training loss:		1.256701
  validation loss:		1.268883
  validation accuracy:		53.59 %
Epoch 1841 of 2000 took 0.109s
  training loss:		1.263247
  validation loss:		1.247917
  validation accuracy:		53.48 %
Epoch 1842 of 2000 took 0.109s
  training loss:		1.254162
  validation loss:		1.247081
  validation accuracy:		52.93 %
Epoch 1843 of 2000 took 0.109s
  training loss:		1.258554
  validation loss:		1.242684
  validation accuracy:		53.80 %
Epoch 1844 of 2000 took 0.109s
  training loss:		1.256506
  validation loss:		1.240314
  validation accuracy:		54.57 %
Epoch 1845 of 2000 took 0.109s
  training loss:		1.254660
  validation loss:		1.247882
  validation accuracy:		53.80 %
Epoch 1846 of 2000 took 0.109s
  training loss:		1.254996
  validation loss:		1.247459
  validation accuracy:		54.24 %
Epoch 1847 of 2000 took 0.109s
  training loss:		1.249131
  validation loss:		1.244355
  validation accuracy:		53.80 %
Epoch 1848 of 2000 took 0.109s
  training loss:		1.260823
  validation loss:		1.247918
  validation accuracy:		54.13 %
Epoch 1849 of 2000 took 0.109s
  training loss:		1.263757
  validation loss:		1.247438
  validation accuracy:		54.13 %
Epoch 1850 of 2000 took 0.109s
  training loss:		1.248078
  validation loss:		1.249549
  validation accuracy:		54.35 %
Epoch 1851 of 2000 took 0.108s
  training loss:		1.258083
  validation loss:		1.244772
  validation accuracy:		53.70 %
Epoch 1852 of 2000 took 0.105s
  training loss:		1.255713
  validation loss:		1.248189
  validation accuracy:		53.37 %
Epoch 1853 of 2000 took 0.105s
  training loss:		1.250713
  validation loss:		1.246982
  validation accuracy:		54.02 %
Epoch 1854 of 2000 took 0.105s
  training loss:		1.255084
  validation loss:		1.248145
  validation accuracy:		52.72 %
Epoch 1855 of 2000 took 0.105s
  training loss:		1.257224
  validation loss:		1.248039
  validation accuracy:		53.48 %
Epoch 1856 of 2000 took 0.105s
  training loss:		1.259000
  validation loss:		1.241893
  validation accuracy:		54.02 %
Epoch 1857 of 2000 took 0.107s
  training loss:		1.249198
  validation loss:		1.249475
  validation accuracy:		53.26 %
Epoch 1858 of 2000 took 0.107s
  training loss:		1.254822
  validation loss:		1.248091
  validation accuracy:		54.46 %
Epoch 1859 of 2000 took 0.144s
  training loss:		1.251515
  validation loss:		1.247845
  validation accuracy:		52.93 %
Epoch 1860 of 2000 took 0.100s
  training loss:		1.255520
  validation loss:		1.243447
  validation accuracy:		54.35 %
Epoch 1861 of 2000 took 0.100s
  training loss:		1.253449
  validation loss:		1.244898
  validation accuracy:		53.80 %
Epoch 1862 of 2000 took 0.104s
  training loss:		1.254663
  validation loss:		1.252984
  validation accuracy:		53.04 %
Epoch 1863 of 2000 took 0.106s
  training loss:		1.247302
  validation loss:		1.249217
  validation accuracy:		53.59 %
Epoch 1864 of 2000 took 0.102s
  training loss:		1.247414
  validation loss:		1.244943
  validation accuracy:		55.22 %
Epoch 1865 of 2000 took 0.106s
  training loss:		1.256001
  validation loss:		1.245176
  validation accuracy:		54.89 %
Epoch 1866 of 2000 took 0.106s
  training loss:		1.267192
  validation loss:		1.246835
  validation accuracy:		53.48 %
Epoch 1867 of 2000 took 0.100s
  training loss:		1.257524
  validation loss:		1.251004
  validation accuracy:		52.83 %
Epoch 1868 of 2000 took 0.100s
  training loss:		1.246788
  validation loss:		1.248439
  validation accuracy:		53.37 %
Epoch 1869 of 2000 took 0.099s
  training loss:		1.257398
  validation loss:		1.248448
  validation accuracy:		53.15 %
Epoch 1870 of 2000 took 0.100s
  training loss:		1.248986
  validation loss:		1.246239
  validation accuracy:		54.67 %
Epoch 1871 of 2000 took 0.100s
  training loss:		1.252526
  validation loss:		1.246834
  validation accuracy:		53.59 %
Epoch 1872 of 2000 took 0.100s
  training loss:		1.251701
  validation loss:		1.246482
  validation accuracy:		54.13 %
Epoch 1873 of 2000 took 0.100s
  training loss:		1.260249
  validation loss:		1.246331
  validation accuracy:		54.57 %
Epoch 1874 of 2000 took 0.100s
  training loss:		1.255015
  validation loss:		1.242844
  validation accuracy:		54.02 %
Epoch 1875 of 2000 took 0.100s
  training loss:		1.255700
  validation loss:		1.245307
  validation accuracy:		54.24 %
Epoch 1876 of 2000 took 0.099s
  training loss:		1.254363
  validation loss:		1.250332
  validation accuracy:		53.80 %
Epoch 1877 of 2000 took 0.100s
  training loss:		1.263029
  validation loss:		1.269185
  validation accuracy:		53.15 %
Epoch 1878 of 2000 took 0.100s
  training loss:		1.255832
  validation loss:		1.250159
  validation accuracy:		54.35 %
Epoch 1879 of 2000 took 0.100s
  training loss:		1.256936
  validation loss:		1.250907
  validation accuracy:		52.93 %
Epoch 1880 of 2000 took 0.100s
  training loss:		1.247943
  validation loss:		1.249295
  validation accuracy:		52.93 %
Epoch 1881 of 2000 took 0.100s
  training loss:		1.255139
  validation loss:		1.253198
  validation accuracy:		54.24 %
Epoch 1882 of 2000 took 0.100s
  training loss:		1.253573
  validation loss:		1.253108
  validation accuracy:		54.67 %
Epoch 1883 of 2000 took 0.100s
  training loss:		1.251050
  validation loss:		1.250861
  validation accuracy:		53.70 %
Epoch 1884 of 2000 took 0.100s
  training loss:		1.255372
  validation loss:		1.247626
  validation accuracy:		53.70 %
Epoch 1885 of 2000 took 0.100s
  training loss:		1.254523
  validation loss:		1.248101
  validation accuracy:		55.00 %
Epoch 1886 of 2000 took 0.100s
  training loss:		1.252559
  validation loss:		1.247838
  validation accuracy:		53.48 %
Epoch 1887 of 2000 took 0.101s
  training loss:		1.258636
  validation loss:		1.253046
  validation accuracy:		54.57 %
Epoch 1888 of 2000 took 0.100s
  training loss:		1.252495
  validation loss:		1.246031
  validation accuracy:		54.13 %
Epoch 1889 of 2000 took 0.100s
  training loss:		1.259841
  validation loss:		1.245085
  validation accuracy:		53.37 %
Epoch 1890 of 2000 took 0.099s
  training loss:		1.253841
  validation loss:		1.251771
  validation accuracy:		52.28 %
Epoch 1891 of 2000 took 0.100s
  training loss:		1.256401
  validation loss:		1.246517
  validation accuracy:		54.02 %
Epoch 1892 of 2000 took 0.100s
  training loss:		1.251109
  validation loss:		1.249099
  validation accuracy:		54.24 %
Epoch 1893 of 2000 took 0.101s
  training loss:		1.249013
  validation loss:		1.249597
  validation accuracy:		53.37 %
Epoch 1894 of 2000 took 0.100s
  training loss:		1.258320
  validation loss:		1.256868
  validation accuracy:		52.28 %
Epoch 1895 of 2000 took 0.101s
  training loss:		1.258997
  validation loss:		1.250829
  validation accuracy:		54.24 %
Epoch 1896 of 2000 took 0.100s
  training loss:		1.262572
  validation loss:		1.260186
  validation accuracy:		54.02 %
Epoch 1897 of 2000 took 0.100s
  training loss:		1.258456
  validation loss:		1.247420
  validation accuracy:		53.80 %
Epoch 1898 of 2000 took 0.102s
  training loss:		1.257972
  validation loss:		1.247008
  validation accuracy:		54.24 %
Epoch 1899 of 2000 took 0.143s
  training loss:		1.254950
  validation loss:		1.255359
  validation accuracy:		53.70 %
Epoch 1900 of 2000 took 0.166s
  training loss:		1.258600
  validation loss:		1.243463
  validation accuracy:		54.67 %
Epoch 1901 of 2000 took 0.166s
  training loss:		1.256456
  validation loss:		1.245562
  validation accuracy:		53.48 %
Epoch 1902 of 2000 took 0.165s
  training loss:		1.258222
  validation loss:		1.243496
  validation accuracy:		54.67 %
Epoch 1903 of 2000 took 0.101s
  training loss:		1.249163
  validation loss:		1.250875
  validation accuracy:		53.04 %
Epoch 1904 of 2000 took 0.099s
  training loss:		1.248290
  validation loss:		1.248653
  validation accuracy:		53.59 %
Epoch 1905 of 2000 took 0.103s
  training loss:		1.252245
  validation loss:		1.245858
  validation accuracy:		54.24 %
Epoch 1906 of 2000 took 0.103s
  training loss:		1.251960
  validation loss:		1.255843
  validation accuracy:		52.61 %
Epoch 1907 of 2000 took 0.108s
  training loss:		1.257211
  validation loss:		1.247392
  validation accuracy:		54.57 %
Epoch 1908 of 2000 took 0.100s
  training loss:		1.256360
  validation loss:		1.243860
  validation accuracy:		53.91 %
Epoch 1909 of 2000 took 0.100s
  training loss:		1.255433
  validation loss:		1.242801
  validation accuracy:		54.13 %
Epoch 1910 of 2000 took 0.101s
  training loss:		1.252155
  validation loss:		1.251446
  validation accuracy:		53.15 %
Epoch 1911 of 2000 took 0.100s
  training loss:		1.253342
  validation loss:		1.254365
  validation accuracy:		52.83 %
Epoch 1912 of 2000 took 0.099s
  training loss:		1.258386
  validation loss:		1.253943
  validation accuracy:		54.57 %
Epoch 1913 of 2000 took 0.100s
  training loss:		1.263730
  validation loss:		1.246417
  validation accuracy:		54.02 %
Epoch 1914 of 2000 took 0.100s
  training loss:		1.255936
  validation loss:		1.247798
  validation accuracy:		53.26 %
Epoch 1915 of 2000 took 0.100s
  training loss:		1.255591
  validation loss:		1.244401
  validation accuracy:		53.37 %
Epoch 1916 of 2000 took 0.100s
  training loss:		1.257697
  validation loss:		1.244862
  validation accuracy:		54.13 %
Epoch 1917 of 2000 took 0.100s
  training loss:		1.251980
  validation loss:		1.242108
  validation accuracy:		54.57 %
Epoch 1918 of 2000 took 0.104s
  training loss:		1.249900
  validation loss:		1.241753
  validation accuracy:		53.91 %
Epoch 1919 of 2000 took 0.103s
  training loss:		1.259333
  validation loss:		1.246354
  validation accuracy:		53.59 %
Epoch 1920 of 2000 took 0.100s
  training loss:		1.255382
  validation loss:		1.247657
  validation accuracy:		53.37 %
Epoch 1921 of 2000 took 0.102s
  training loss:		1.261457
  validation loss:		1.257289
  validation accuracy:		53.26 %
Epoch 1922 of 2000 took 0.102s
  training loss:		1.260554
  validation loss:		1.255631
  validation accuracy:		53.91 %
Epoch 1923 of 2000 took 0.102s
  training loss:		1.248980
  validation loss:		1.244535
  validation accuracy:		53.80 %
Epoch 1924 of 2000 took 0.103s
  training loss:		1.258140
  validation loss:		1.255599
  validation accuracy:		53.48 %
Epoch 1925 of 2000 took 0.103s
  training loss:		1.261812
  validation loss:		1.249554
  validation accuracy:		52.83 %
Epoch 1926 of 2000 took 0.105s
  training loss:		1.252435
  validation loss:		1.251922
  validation accuracy:		53.80 %
Epoch 1927 of 2000 took 0.103s
  training loss:		1.251681
  validation loss:		1.270244
  validation accuracy:		54.24 %
Epoch 1928 of 2000 took 0.102s
  training loss:		1.255770
  validation loss:		1.244866
  validation accuracy:		53.91 %
Epoch 1929 of 2000 took 0.102s
  training loss:		1.253977
  validation loss:		1.249912
  validation accuracy:		52.83 %
Epoch 1930 of 2000 took 0.102s
  training loss:		1.259845
  validation loss:		1.247432
  validation accuracy:		53.80 %
Epoch 1931 of 2000 took 0.102s
  training loss:		1.254143
  validation loss:		1.244242
  validation accuracy:		54.57 %
Epoch 1932 of 2000 took 0.102s
  training loss:		1.261016
  validation loss:		1.249565
  validation accuracy:		53.70 %
Epoch 1933 of 2000 took 0.102s
  training loss:		1.261617
  validation loss:		1.246622
  validation accuracy:		53.91 %
Epoch 1934 of 2000 took 0.102s
  training loss:		1.251006
  validation loss:		1.247498
  validation accuracy:		53.48 %
Epoch 1935 of 2000 took 0.102s
  training loss:		1.258185
  validation loss:		1.255357
  validation accuracy:		52.93 %
Epoch 1936 of 2000 took 0.102s
  training loss:		1.260619
  validation loss:		1.246841
  validation accuracy:		53.26 %
Epoch 1937 of 2000 took 0.102s
  training loss:		1.250943
  validation loss:		1.247763
  validation accuracy:		54.02 %
Epoch 1938 of 2000 took 0.102s
  training loss:		1.245547
  validation loss:		1.246002
  validation accuracy:		53.26 %
Epoch 1939 of 2000 took 0.102s
  training loss:		1.253811
  validation loss:		1.247980
  validation accuracy:		54.57 %
Epoch 1940 of 2000 took 0.102s
  training loss:		1.251136
  validation loss:		1.245799
  validation accuracy:		52.83 %
Epoch 1941 of 2000 took 0.102s
  training loss:		1.260287
  validation loss:		1.241991
  validation accuracy:		54.13 %
Epoch 1942 of 2000 took 0.103s
  training loss:		1.248872
  validation loss:		1.251887
  validation accuracy:		53.59 %
Epoch 1943 of 2000 took 0.102s
  training loss:		1.252995
  validation loss:		1.247583
  validation accuracy:		53.91 %
Epoch 1944 of 2000 took 0.103s
  training loss:		1.255333
  validation loss:		1.262965
  validation accuracy:		53.04 %
Epoch 1945 of 2000 took 0.102s
  training loss:		1.248857
  validation loss:		1.252378
  validation accuracy:		53.80 %
Epoch 1946 of 2000 took 0.102s
  training loss:		1.243928
  validation loss:		1.250315
  validation accuracy:		54.13 %
Epoch 1947 of 2000 took 0.102s
  training loss:		1.246972
  validation loss:		1.258803
  validation accuracy:		54.35 %
Epoch 1948 of 2000 took 0.102s
  training loss:		1.253097
  validation loss:		1.243767
  validation accuracy:		54.78 %
Epoch 1949 of 2000 took 0.102s
  training loss:		1.266043
  validation loss:		1.247291
  validation accuracy:		54.89 %
Epoch 1950 of 2000 took 0.102s
  training loss:		1.250125
  validation loss:		1.256323
  validation accuracy:		52.61 %
Epoch 1951 of 2000 took 0.102s
  training loss:		1.255296
  validation loss:		1.249182
  validation accuracy:		54.57 %
Epoch 1952 of 2000 took 0.102s
  training loss:		1.253611
  validation loss:		1.276534
  validation accuracy:		54.46 %
Epoch 1953 of 2000 took 0.102s
  training loss:		1.255858
  validation loss:		1.242046
  validation accuracy:		54.67 %
Epoch 1954 of 2000 took 0.102s
  training loss:		1.251438
  validation loss:		1.248562
  validation accuracy:		53.70 %
Epoch 1955 of 2000 took 0.102s
  training loss:		1.255688
  validation loss:		1.249039
  validation accuracy:		53.91 %
Epoch 1956 of 2000 took 0.102s
  training loss:		1.251585
  validation loss:		1.247425
  validation accuracy:		53.59 %
Epoch 1957 of 2000 took 0.102s
  training loss:		1.255433
  validation loss:		1.253534
  validation accuracy:		53.26 %
Epoch 1958 of 2000 took 0.102s
  training loss:		1.265856
  validation loss:		1.246333
  validation accuracy:		53.48 %
Epoch 1959 of 2000 took 0.102s
  training loss:		1.257091
  validation loss:		1.253147
  validation accuracy:		53.26 %
Epoch 1960 of 2000 took 0.102s
  training loss:		1.257055
  validation loss:		1.257294
  validation accuracy:		54.67 %
Epoch 1961 of 2000 took 0.102s
  training loss:		1.247004
  validation loss:		1.242115
  validation accuracy:		53.15 %
Epoch 1962 of 2000 took 0.102s
  training loss:		1.258931
  validation loss:		1.249135
  validation accuracy:		54.35 %
Epoch 1963 of 2000 took 0.102s
  training loss:		1.257844
  validation loss:		1.245503
  validation accuracy:		53.48 %
Epoch 1964 of 2000 took 0.103s
  training loss:		1.261904
  validation loss:		1.271234
  validation accuracy:		54.35 %
Epoch 1965 of 2000 took 0.102s
  training loss:		1.259236
  validation loss:		1.257875
  validation accuracy:		53.70 %
Epoch 1966 of 2000 took 0.102s
  training loss:		1.252538
  validation loss:		1.246441
  validation accuracy:		53.48 %
Epoch 1967 of 2000 took 0.102s
  training loss:		1.249253
  validation loss:		1.248441
  validation accuracy:		53.26 %
Epoch 1968 of 2000 took 0.102s
  training loss:		1.258421
  validation loss:		1.251257
  validation accuracy:		54.89 %
Epoch 1969 of 2000 took 0.102s
  training loss:		1.251075
  validation loss:		1.247391
  validation accuracy:		54.35 %
Epoch 1970 of 2000 took 0.102s
  training loss:		1.247789
  validation loss:		1.244431
  validation accuracy:		55.43 %
Epoch 1971 of 2000 took 0.102s
  training loss:		1.255773
  validation loss:		1.241698
  validation accuracy:		54.89 %
Epoch 1972 of 2000 took 0.103s
  training loss:		1.250945
  validation loss:		1.245740
  validation accuracy:		53.59 %
Epoch 1973 of 2000 took 0.104s
  training loss:		1.254478
  validation loss:		1.250461
  validation accuracy:		52.93 %
Epoch 1974 of 2000 took 0.102s
  training loss:		1.258526
  validation loss:		1.247722
  validation accuracy:		53.26 %
Epoch 1975 of 2000 took 0.102s
  training loss:		1.252602
  validation loss:		1.254897
  validation accuracy:		53.15 %
Epoch 1976 of 2000 took 0.102s
  training loss:		1.260290
  validation loss:		1.245799
  validation accuracy:		54.02 %
Epoch 1977 of 2000 took 0.102s
  training loss:		1.242148
  validation loss:		1.247576
  validation accuracy:		53.26 %
Epoch 1978 of 2000 took 0.102s
  training loss:		1.253995
  validation loss:		1.242970
  validation accuracy:		53.91 %
Epoch 1979 of 2000 took 0.102s
  training loss:		1.252598
  validation loss:		1.248289
  validation accuracy:		53.48 %
Epoch 1980 of 2000 took 0.102s
  training loss:		1.249853
  validation loss:		1.249594
  validation accuracy:		54.24 %
Epoch 1981 of 2000 took 0.103s
  training loss:		1.255357
  validation loss:		1.247772
  validation accuracy:		53.59 %
Epoch 1982 of 2000 took 0.102s
  training loss:		1.251529
  validation loss:		1.246214
  validation accuracy:		53.70 %
Epoch 1983 of 2000 took 0.102s
  training loss:		1.261478
  validation loss:		1.242747
  validation accuracy:		54.57 %
Epoch 1984 of 2000 took 0.102s
  training loss:		1.250344
  validation loss:		1.243734
  validation accuracy:		54.57 %
Epoch 1985 of 2000 took 0.102s
  training loss:		1.253033
  validation loss:		1.242828
  validation accuracy:		53.91 %
Epoch 1986 of 2000 took 0.102s
  training loss:		1.261653
  validation loss:		1.246039
  validation accuracy:		54.46 %
Epoch 1987 of 2000 took 0.102s
  training loss:		1.249790
  validation loss:		1.261018
  validation accuracy:		53.15 %
Epoch 1988 of 2000 took 0.102s
  training loss:		1.257399
  validation loss:		1.247510
  validation accuracy:		54.02 %
Epoch 1989 of 2000 took 0.102s
  training loss:		1.258881
  validation loss:		1.245882
  validation accuracy:		53.59 %
Epoch 1990 of 2000 took 0.102s
  training loss:		1.253098
  validation loss:		1.252199
  validation accuracy:		53.26 %
Epoch 1991 of 2000 took 0.102s
  training loss:		1.255326
  validation loss:		1.258044
  validation accuracy:		54.46 %
Epoch 1992 of 2000 took 0.102s
  training loss:		1.259597
  validation loss:		1.246988
  validation accuracy:		55.00 %
Epoch 1993 of 2000 took 0.102s
  training loss:		1.252574
  validation loss:		1.261602
  validation accuracy:		52.72 %
Epoch 1994 of 2000 took 0.102s
  training loss:		1.257576
  validation loss:		1.250909
  validation accuracy:		53.26 %
Epoch 1995 of 2000 took 0.102s
  training loss:		1.249898
  validation loss:		1.247307
  validation accuracy:		53.48 %
Epoch 1996 of 2000 took 0.102s
  training loss:		1.257346
  validation loss:		1.241449
  validation accuracy:		54.57 %
Epoch 1997 of 2000 took 0.102s
  training loss:		1.261822
  validation loss:		1.246724
  validation accuracy:		53.26 %
Epoch 1998 of 2000 took 0.102s
  training loss:		1.258270
  validation loss:		1.256909
  validation accuracy:		52.28 %
Epoch 1999 of 2000 took 0.102s
  training loss:		1.256610
  validation loss:		1.247943
  validation accuracy:		53.91 %
Epoch 2000 of 2000 took 0.102s
  training loss:		1.250202
  validation loss:		1.240137
  validation accuracy:		53.91 %
Final results:
  test loss:			1.381324
  test accuracy:		50.68 %
