Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
w=(50, 50),b=(50,)
decomposing tensor W of shape (3, 50, 50)...
decomposing tensor B of shape (3, 50)...
Starting training...
Epoch 1 of 2000 took 0.104s
  training loss:		2.999798
  validation loss:		2.978558
  validation accuracy:		13.15 %
Epoch 2 of 2000 took 0.101s
  training loss:		2.970430
  validation loss:		2.936700
  validation accuracy:		12.93 %
Epoch 3 of 2000 took 0.103s
  training loss:		2.934337
  validation loss:		2.893981
  validation accuracy:		12.93 %
Epoch 4 of 2000 took 0.102s
  training loss:		2.898595
  validation loss:		2.853147
  validation accuracy:		12.93 %
Epoch 5 of 2000 took 0.102s
  training loss:		2.865108
  validation loss:		2.814458
  validation accuracy:		13.04 %
Epoch 6 of 2000 took 0.102s
  training loss:		2.832751
  validation loss:		2.777455
  validation accuracy:		12.93 %
Epoch 7 of 2000 took 0.102s
  training loss:		2.802238
  validation loss:		2.741538
  validation accuracy:		12.93 %
Epoch 8 of 2000 took 0.103s
  training loss:		2.772643
  validation loss:		2.706707
  validation accuracy:		12.93 %
Epoch 9 of 2000 took 0.102s
  training loss:		2.744407
  validation loss:		2.672250
  validation accuracy:		12.93 %
Epoch 10 of 2000 took 0.103s
  training loss:		2.715841
  validation loss:		2.638545
  validation accuracy:		12.93 %
Epoch 11 of 2000 took 0.102s
  training loss:		2.685900
  validation loss:		2.604929
  validation accuracy:		12.93 %
Epoch 12 of 2000 took 0.102s
  training loss:		2.656439
  validation loss:		2.571355
  validation accuracy:		12.93 %
Epoch 13 of 2000 took 0.103s
  training loss:		2.629336
  validation loss:		2.538526
  validation accuracy:		12.93 %
Epoch 14 of 2000 took 0.102s
  training loss:		2.601889
  validation loss:		2.506745
  validation accuracy:		12.93 %
Epoch 15 of 2000 took 0.102s
  training loss:		2.573818
  validation loss:		2.476541
  validation accuracy:		12.93 %
Epoch 16 of 2000 took 0.102s
  training loss:		2.546572
  validation loss:		2.447501
  validation accuracy:		12.93 %
Epoch 17 of 2000 took 0.103s
  training loss:		2.518632
  validation loss:		2.419521
  validation accuracy:		12.93 %
Epoch 18 of 2000 took 0.103s
  training loss:		2.494663
  validation loss:		2.393960
  validation accuracy:		12.93 %
Epoch 19 of 2000 took 0.102s
  training loss:		2.472727
  validation loss:		2.371242
  validation accuracy:		12.93 %
Epoch 20 of 2000 took 0.102s
  training loss:		2.449866
  validation loss:		2.350836
  validation accuracy:		12.93 %
Epoch 21 of 2000 took 0.103s
  training loss:		2.428981
  validation loss:		2.333210
  validation accuracy:		12.93 %
Epoch 22 of 2000 took 0.103s
  training loss:		2.410812
  validation loss:		2.318219
  validation accuracy:		12.93 %
Epoch 23 of 2000 took 0.102s
  training loss:		2.395713
  validation loss:		2.305497
  validation accuracy:		12.93 %
Epoch 24 of 2000 took 0.102s
  training loss:		2.381983
  validation loss:		2.294802
  validation accuracy:		12.93 %
Epoch 25 of 2000 took 0.102s
  training loss:		2.369215
  validation loss:		2.287565
  validation accuracy:		12.93 %
Epoch 26 of 2000 took 0.102s
  training loss:		2.360228
  validation loss:		2.282992
  validation accuracy:		12.93 %
Epoch 27 of 2000 took 0.102s
  training loss:		2.349060
  validation loss:		2.277675
  validation accuracy:		12.93 %
Epoch 28 of 2000 took 0.103s
  training loss:		2.343139
  validation loss:		2.273691
  validation accuracy:		12.93 %
Epoch 29 of 2000 took 0.102s
  training loss:		2.335423
  validation loss:		2.271205
  validation accuracy:		13.04 %
Epoch 30 of 2000 took 0.102s
  training loss:		2.328433
  validation loss:		2.266738
  validation accuracy:		13.15 %
Epoch 31 of 2000 took 0.102s
  training loss:		2.324398
  validation loss:		2.262762
  validation accuracy:		13.48 %
Epoch 32 of 2000 took 0.103s
  training loss:		2.319801
  validation loss:		2.258987
  validation accuracy:		13.59 %
Epoch 33 of 2000 took 0.105s
  training loss:		2.318434
  validation loss:		2.257853
  validation accuracy:		13.70 %
Epoch 34 of 2000 took 0.103s
  training loss:		2.315446
  validation loss:		2.256390
  validation accuracy:		14.35 %
Epoch 35 of 2000 took 0.102s
  training loss:		2.312446
  validation loss:		2.256060
  validation accuracy:		14.46 %
Epoch 36 of 2000 took 0.102s
  training loss:		2.311462
  validation loss:		2.255798
  validation accuracy:		13.80 %
Epoch 37 of 2000 took 0.102s
  training loss:		2.309147
  validation loss:		2.255997
  validation accuracy:		13.59 %
Epoch 38 of 2000 took 0.102s
  training loss:		2.306905
  validation loss:		2.254153
  validation accuracy:		15.11 %
Epoch 39 of 2000 took 0.102s
  training loss:		2.305928
  validation loss:		2.251263
  validation accuracy:		16.09 %
Epoch 40 of 2000 took 0.102s
  training loss:		2.304433
  validation loss:		2.250582
  validation accuracy:		15.11 %
Epoch 41 of 2000 took 0.102s
  training loss:		2.303434
  validation loss:		2.249243
  validation accuracy:		15.33 %
Epoch 42 of 2000 took 0.103s
  training loss:		2.301882
  validation loss:		2.247408
  validation accuracy:		13.70 %
Epoch 43 of 2000 took 0.103s
  training loss:		2.301083
  validation loss:		2.245803
  validation accuracy:		13.70 %
Epoch 44 of 2000 took 0.103s
  training loss:		2.300580
  validation loss:		2.247019
  validation accuracy:		13.37 %
Epoch 45 of 2000 took 0.103s
  training loss:		2.300275
  validation loss:		2.245890
  validation accuracy:		13.26 %
Epoch 46 of 2000 took 0.103s
  training loss:		2.299493
  validation loss:		2.246858
  validation accuracy:		14.67 %
Epoch 47 of 2000 took 0.102s
  training loss:		2.297684
  validation loss:		2.246227
  validation accuracy:		15.22 %
Epoch 48 of 2000 took 0.102s
  training loss:		2.298158
  validation loss:		2.246198
  validation accuracy:		13.59 %
Epoch 49 of 2000 took 0.103s
  training loss:		2.297375
  validation loss:		2.245541
  validation accuracy:		15.33 %
Epoch 50 of 2000 took 0.102s
  training loss:		2.296893
  validation loss:		2.243232
  validation accuracy:		18.04 %
Epoch 51 of 2000 took 0.102s
  training loss:		2.296404
  validation loss:		2.243394
  validation accuracy:		13.15 %
Epoch 52 of 2000 took 0.103s
  training loss:		2.295470
  validation loss:		2.242295
  validation accuracy:		13.59 %
Epoch 53 of 2000 took 0.102s
  training loss:		2.295693
  validation loss:		2.240619
  validation accuracy:		18.04 %
Epoch 54 of 2000 took 0.102s
  training loss:		2.295206
  validation loss:		2.244626
  validation accuracy:		15.87 %
Epoch 55 of 2000 took 0.102s
  training loss:		2.295237
  validation loss:		2.244411
  validation accuracy:		15.11 %
Epoch 56 of 2000 took 0.102s
  training loss:		2.294510
  validation loss:		2.242135
  validation accuracy:		15.87 %
Epoch 57 of 2000 took 0.103s
  training loss:		2.293766
  validation loss:		2.239477
  validation accuracy:		13.26 %
Epoch 58 of 2000 took 0.102s
  training loss:		2.292967
  validation loss:		2.238979
  validation accuracy:		13.48 %
Epoch 59 of 2000 took 0.102s
  training loss:		2.293938
  validation loss:		2.243215
  validation accuracy:		16.52 %
Epoch 60 of 2000 took 0.103s
  training loss:		2.292952
  validation loss:		2.241037
  validation accuracy:		16.30 %
Epoch 61 of 2000 took 0.103s
  training loss:		2.292900
  validation loss:		2.241263
  validation accuracy:		15.00 %
Epoch 62 of 2000 took 0.102s
  training loss:		2.292257
  validation loss:		2.240846
  validation accuracy:		14.89 %
Epoch 63 of 2000 took 0.102s
  training loss:		2.291348
  validation loss:		2.239721
  validation accuracy:		17.50 %
Epoch 64 of 2000 took 0.102s
  training loss:		2.291626
  validation loss:		2.237677
  validation accuracy:		15.22 %
Epoch 65 of 2000 took 0.102s
  training loss:		2.290502
  validation loss:		2.237322
  validation accuracy:		17.83 %
Epoch 66 of 2000 took 0.102s
  training loss:		2.290297
  validation loss:		2.238894
  validation accuracy:		15.65 %
Epoch 67 of 2000 took 0.103s
  training loss:		2.289863
  validation loss:		2.238702
  validation accuracy:		14.46 %
Epoch 68 of 2000 took 0.102s
  training loss:		2.288768
  validation loss:		2.234498
  validation accuracy:		18.91 %
Epoch 69 of 2000 took 0.102s
  training loss:		2.289877
  validation loss:		2.236594
  validation accuracy:		16.85 %
Epoch 70 of 2000 took 0.102s
  training loss:		2.289181
  validation loss:		2.235227
  validation accuracy:		17.72 %
Epoch 71 of 2000 took 0.102s
  training loss:		2.287969
  validation loss:		2.233617
  validation accuracy:		15.76 %
Epoch 72 of 2000 took 0.102s
  training loss:		2.288440
  validation loss:		2.233959
  validation accuracy:		15.87 %
Epoch 73 of 2000 took 0.102s
  training loss:		2.286675
  validation loss:		2.232395
  validation accuracy:		16.96 %
Epoch 74 of 2000 took 0.103s
  training loss:		2.286664
  validation loss:		2.232559
  validation accuracy:		19.35 %
Epoch 75 of 2000 took 0.103s
  training loss:		2.286793
  validation loss:		2.232110
  validation accuracy:		18.48 %
Epoch 76 of 2000 took 0.102s
  training loss:		2.286262
  validation loss:		2.233666
  validation accuracy:		14.78 %
Epoch 77 of 2000 took 0.102s
  training loss:		2.285343
  validation loss:		2.233383
  validation accuracy:		15.43 %
Epoch 78 of 2000 took 0.102s
  training loss:		2.285137
  validation loss:		2.231200
  validation accuracy:		18.26 %
Epoch 79 of 2000 took 0.102s
  training loss:		2.283508
  validation loss:		2.229347
  validation accuracy:		22.07 %
Epoch 80 of 2000 took 0.102s
  training loss:		2.284484
  validation loss:		2.229937
  validation accuracy:		21.63 %
Epoch 81 of 2000 took 0.103s
  training loss:		2.283861
  validation loss:		2.231999
  validation accuracy:		15.87 %
Epoch 82 of 2000 took 0.102s
  training loss:		2.282470
  validation loss:		2.228888
  validation accuracy:		19.35 %
Epoch 83 of 2000 took 0.102s
  training loss:		2.282512
  validation loss:		2.229333
  validation accuracy:		18.70 %
Epoch 84 of 2000 took 0.103s
  training loss:		2.280854
  validation loss:		2.227219
  validation accuracy:		20.65 %
Epoch 85 of 2000 took 0.102s
  training loss:		2.279633
  validation loss:		2.224752
  validation accuracy:		21.09 %
Epoch 86 of 2000 took 0.103s
  training loss:		2.279510
  validation loss:		2.225407
  validation accuracy:		20.43 %
Epoch 87 of 2000 took 0.103s
  training loss:		2.277645
  validation loss:		2.223663
  validation accuracy:		18.48 %
Epoch 88 of 2000 took 0.103s
  training loss:		2.277818
  validation loss:		2.223416
  validation accuracy:		23.91 %
Epoch 89 of 2000 took 0.102s
  training loss:		2.277035
  validation loss:		2.220409
  validation accuracy:		18.37 %
Epoch 90 of 2000 took 0.105s
  training loss:		2.275277
  validation loss:		2.220182
  validation accuracy:		19.89 %
Epoch 91 of 2000 took 0.103s
  training loss:		2.274490
  validation loss:		2.219341
  validation accuracy:		22.07 %
Epoch 92 of 2000 took 0.102s
  training loss:		2.272064
  validation loss:		2.215558
  validation accuracy:		21.74 %
Epoch 93 of 2000 took 0.102s
  training loss:		2.272687
  validation loss:		2.215884
  validation accuracy:		23.80 %
Epoch 94 of 2000 took 0.102s
  training loss:		2.271245
  validation loss:		2.215330
  validation accuracy:		23.80 %
Epoch 95 of 2000 took 0.103s
  training loss:		2.269535
  validation loss:		2.213194
  validation accuracy:		25.22 %
Epoch 96 of 2000 took 0.103s
  training loss:		2.268198
  validation loss:		2.208056
  validation accuracy:		24.89 %
Epoch 97 of 2000 took 0.102s
  training loss:		2.267559
  validation loss:		2.210457
  validation accuracy:		23.59 %
Epoch 98 of 2000 took 0.103s
  training loss:		2.265156
  validation loss:		2.207440
  validation accuracy:		25.87 %
Epoch 99 of 2000 took 0.103s
  training loss:		2.261891
  validation loss:		2.205858
  validation accuracy:		24.67 %
Epoch 100 of 2000 took 0.103s
  training loss:		2.260611
  validation loss:		2.203210
  validation accuracy:		24.78 %
Epoch 101 of 2000 took 0.102s
  training loss:		2.258034
  validation loss:		2.200653
  validation accuracy:		24.13 %
Epoch 102 of 2000 took 0.102s
  training loss:		2.254719
  validation loss:		2.196183
  validation accuracy:		25.76 %
Epoch 103 of 2000 took 0.102s
  training loss:		2.251740
  validation loss:		2.191783
  validation accuracy:		23.91 %
Epoch 104 of 2000 took 0.102s
  training loss:		2.250042
  validation loss:		2.189349
  validation accuracy:		25.87 %
Epoch 105 of 2000 took 0.102s
  training loss:		2.246795
  validation loss:		2.184225
  validation accuracy:		26.96 %
Epoch 106 of 2000 took 0.103s
  training loss:		2.241518
  validation loss:		2.179284
  validation accuracy:		25.22 %
Epoch 107 of 2000 took 0.103s
  training loss:		2.238514
  validation loss:		2.178826
  validation accuracy:		24.35 %
Epoch 108 of 2000 took 0.103s
  training loss:		2.232818
  validation loss:		2.168425
  validation accuracy:		26.41 %
Epoch 109 of 2000 took 0.102s
  training loss:		2.228108
  validation loss:		2.162493
  validation accuracy:		25.98 %
Epoch 110 of 2000 took 0.103s
  training loss:		2.221367
  validation loss:		2.154647
  validation accuracy:		25.33 %
Epoch 111 of 2000 took 0.103s
  training loss:		2.214423
  validation loss:		2.146146
  validation accuracy:		27.17 %
Epoch 112 of 2000 took 0.102s
  training loss:		2.206130
  validation loss:		2.137365
  validation accuracy:		26.41 %
Epoch 113 of 2000 took 0.102s
  training loss:		2.198818
  validation loss:		2.124755
  validation accuracy:		25.65 %
Epoch 114 of 2000 took 0.102s
  training loss:		2.184678
  validation loss:		2.108829
  validation accuracy:		26.20 %
Epoch 115 of 2000 took 0.102s
  training loss:		2.173855
  validation loss:		2.094690
  validation accuracy:		25.98 %
Epoch 116 of 2000 took 0.103s
  training loss:		2.161161
  validation loss:		2.081897
  validation accuracy:		27.61 %
Epoch 117 of 2000 took 0.102s
  training loss:		2.142091
  validation loss:		2.061567
  validation accuracy:		27.28 %
Epoch 118 of 2000 took 0.102s
  training loss:		2.123816
  validation loss:		2.038991
  validation accuracy:		28.37 %
Epoch 119 of 2000 took 0.102s
  training loss:		2.106341
  validation loss:		2.013323
  validation accuracy:		27.50 %
Epoch 120 of 2000 took 0.102s
  training loss:		2.083403
  validation loss:		1.987657
  validation accuracy:		29.67 %
Epoch 121 of 2000 took 0.102s
  training loss:		2.054785
  validation loss:		1.958163
  validation accuracy:		29.57 %
Epoch 122 of 2000 took 0.102s
  training loss:		2.029469
  validation loss:		1.926668
  validation accuracy:		29.57 %
Epoch 123 of 2000 took 0.102s
  training loss:		1.998014
  validation loss:		1.902615
  validation accuracy:		30.87 %
Epoch 124 of 2000 took 0.102s
  training loss:		1.975170
  validation loss:		1.872720
  validation accuracy:		32.83 %
Epoch 125 of 2000 took 0.102s
  training loss:		1.950653
  validation loss:		1.844404
  validation accuracy:		33.15 %
Epoch 126 of 2000 took 0.102s
  training loss:		1.924589
  validation loss:		1.816611
  validation accuracy:		33.59 %
Epoch 127 of 2000 took 0.103s
  training loss:		1.895475
  validation loss:		1.795061
  validation accuracy:		34.02 %
Epoch 128 of 2000 took 0.102s
  training loss:		1.870457
  validation loss:		1.772886
  validation accuracy:		34.35 %
Epoch 129 of 2000 took 0.103s
  training loss:		1.853009
  validation loss:		1.752386
  validation accuracy:		35.43 %
Epoch 130 of 2000 took 0.103s
  training loss:		1.827900
  validation loss:		1.730690
  validation accuracy:		35.98 %
Epoch 131 of 2000 took 0.103s
  training loss:		1.809592
  validation loss:		1.713163
  validation accuracy:		36.30 %
Epoch 132 of 2000 took 0.102s
  training loss:		1.791066
  validation loss:		1.699714
  validation accuracy:		37.07 %
Epoch 133 of 2000 took 0.103s
  training loss:		1.767821
  validation loss:		1.686473
  validation accuracy:		38.26 %
Epoch 134 of 2000 took 0.103s
  training loss:		1.750159
  validation loss:		1.665766
  validation accuracy:		38.48 %
Epoch 135 of 2000 took 0.103s
  training loss:		1.734287
  validation loss:		1.648525
  validation accuracy:		38.91 %
Epoch 136 of 2000 took 0.102s
  training loss:		1.724804
  validation loss:		1.634697
  validation accuracy:		38.80 %
Epoch 137 of 2000 took 0.102s
  training loss:		1.699668
  validation loss:		1.621101
  validation accuracy:		38.26 %
Epoch 138 of 2000 took 0.103s
  training loss:		1.687605
  validation loss:		1.602247
  validation accuracy:		39.67 %
Epoch 139 of 2000 took 0.103s
  training loss:		1.673201
  validation loss:		1.584929
  validation accuracy:		41.20 %
Epoch 140 of 2000 took 0.102s
  training loss:		1.654843
  validation loss:		1.570384
  validation accuracy:		42.17 %
Epoch 141 of 2000 took 0.102s
  training loss:		1.640155
  validation loss:		1.555482
  validation accuracy:		42.50 %
Epoch 142 of 2000 took 0.102s
  training loss:		1.630850
  validation loss:		1.547661
  validation accuracy:		42.83 %
Epoch 143 of 2000 took 0.102s
  training loss:		1.615803
  validation loss:		1.539507
  validation accuracy:		43.15 %
Epoch 144 of 2000 took 0.102s
  training loss:		1.593707
  validation loss:		1.517145
  validation accuracy:		42.83 %
Epoch 145 of 2000 took 0.103s
  training loss:		1.580109
  validation loss:		1.503628
  validation accuracy:		42.93 %
Epoch 146 of 2000 took 0.103s
  training loss:		1.569215
  validation loss:		1.492050
  validation accuracy:		42.83 %
Epoch 147 of 2000 took 0.103s
  training loss:		1.560847
  validation loss:		1.487273
  validation accuracy:		41.63 %
Epoch 148 of 2000 took 0.102s
  training loss:		1.548354
  validation loss:		1.477459
  validation accuracy:		44.35 %
Epoch 149 of 2000 took 0.103s
  training loss:		1.539068
  validation loss:		1.459721
  validation accuracy:		43.91 %
Epoch 150 of 2000 took 0.102s
  training loss:		1.521781
  validation loss:		1.447725
  validation accuracy:		44.24 %
Epoch 151 of 2000 took 0.103s
  training loss:		1.508895
  validation loss:		1.445731
  validation accuracy:		44.02 %
Epoch 152 of 2000 took 0.102s
  training loss:		1.519329
  validation loss:		1.436054
  validation accuracy:		44.24 %
Epoch 153 of 2000 took 0.103s
  training loss:		1.496614
  validation loss:		1.442416
  validation accuracy:		45.33 %
Epoch 154 of 2000 took 0.103s
  training loss:		1.482497
  validation loss:		1.432320
  validation accuracy:		44.35 %
Epoch 155 of 2000 took 0.103s
  training loss:		1.486554
  validation loss:		1.414718
  validation accuracy:		45.33 %
Epoch 156 of 2000 took 0.103s
  training loss:		1.468716
  validation loss:		1.406586
  validation accuracy:		44.89 %
Epoch 157 of 2000 took 0.103s
  training loss:		1.458965
  validation loss:		1.398157
  validation accuracy:		46.20 %
Epoch 158 of 2000 took 0.103s
  training loss:		1.452179
  validation loss:		1.396207
  validation accuracy:		47.17 %
Epoch 159 of 2000 took 0.103s
  training loss:		1.462852
  validation loss:		1.400204
  validation accuracy:		46.52 %
Epoch 160 of 2000 took 0.103s
  training loss:		1.440913
  validation loss:		1.383202
  validation accuracy:		47.72 %
Epoch 161 of 2000 took 0.103s
  training loss:		1.443004
  validation loss:		1.380547
  validation accuracy:		47.61 %
Epoch 162 of 2000 took 0.103s
  training loss:		1.435835
  validation loss:		1.493744
  validation accuracy:		41.96 %
Epoch 163 of 2000 took 0.103s
  training loss:		1.595610
  validation loss:		1.534070
  validation accuracy:		41.74 %
Epoch 164 of 2000 took 0.103s
  training loss:		1.504820
  validation loss:		1.388088
  validation accuracy:		48.26 %
Epoch 165 of 2000 took 0.102s
  training loss:		1.424484
  validation loss:		1.380308
  validation accuracy:		46.85 %
Epoch 166 of 2000 took 0.103s
  training loss:		1.448556
  validation loss:		1.391250
  validation accuracy:		45.98 %
Epoch 167 of 2000 took 0.103s
  training loss:		1.457450
  validation loss:		1.385520
  validation accuracy:		46.41 %
Epoch 168 of 2000 took 0.103s
  training loss:		1.407972
  validation loss:		1.373620
  validation accuracy:		47.72 %
Epoch 169 of 2000 took 0.103s
  training loss:		1.434160
  validation loss:		1.584241
  validation accuracy:		39.46 %
Epoch 170 of 2000 took 0.103s
  training loss:		1.600504
  validation loss:		1.396755
  validation accuracy:		46.09 %
Epoch 171 of 2000 took 0.102s
  training loss:		1.418127
  validation loss:		1.361321
  validation accuracy:		50.22 %
Epoch 172 of 2000 took 0.103s
  training loss:		1.411382
  validation loss:		1.349994
  validation accuracy:		50.54 %
Epoch 173 of 2000 took 0.102s
  training loss:		1.409594
  validation loss:		1.371928
  validation accuracy:		46.85 %
Epoch 174 of 2000 took 0.103s
  training loss:		1.395402
  validation loss:		1.349118
  validation accuracy:		49.78 %
Epoch 175 of 2000 took 0.104s
  training loss:		1.377989
  validation loss:		1.345311
  validation accuracy:		50.00 %
Epoch 176 of 2000 took 0.105s
  training loss:		1.455812
  validation loss:		1.438042
  validation accuracy:		45.22 %
Epoch 177 of 2000 took 0.103s
  training loss:		1.537366
  validation loss:		1.442816
  validation accuracy:		43.70 %
Epoch 178 of 2000 took 0.103s
  training loss:		1.408599
  validation loss:		1.356229
  validation accuracy:		49.89 %
Epoch 179 of 2000 took 0.102s
  training loss:		1.387862
  validation loss:		1.353505
  validation accuracy:		48.80 %
Epoch 180 of 2000 took 0.103s
  training loss:		1.376396
  validation loss:		1.356442
  validation accuracy:		49.35 %
Epoch 181 of 2000 took 0.102s
  training loss:		1.423346
  validation loss:		1.368291
  validation accuracy:		48.48 %
Epoch 182 of 2000 took 0.103s
  training loss:		1.448422
  validation loss:		1.487726
  validation accuracy:		43.26 %
Epoch 183 of 2000 took 0.102s
  training loss:		1.452678
  validation loss:		1.350128
  validation accuracy:		50.22 %
Epoch 184 of 2000 took 0.103s
  training loss:		1.365976
  validation loss:		1.332067
  validation accuracy:		51.63 %
Epoch 185 of 2000 took 0.103s
  training loss:		1.372204
  validation loss:		1.351155
  validation accuracy:		50.11 %
Epoch 186 of 2000 took 0.103s
  training loss:		1.449282
  validation loss:		1.397325
  validation accuracy:		46.74 %
Epoch 187 of 2000 took 0.103s
  training loss:		1.509693
  validation loss:		1.397719
  validation accuracy:		48.70 %
Epoch 188 of 2000 took 0.103s
  training loss:		1.410880
  validation loss:		1.350805
  validation accuracy:		49.46 %
Epoch 189 of 2000 took 0.103s
  training loss:		1.401427
  validation loss:		1.340554
  validation accuracy:		49.67 %
Epoch 190 of 2000 took 0.102s
  training loss:		1.385314
  validation loss:		1.365486
  validation accuracy:		50.33 %
Epoch 191 of 2000 took 0.103s
  training loss:		1.393260
  validation loss:		1.326196
  validation accuracy:		52.83 %
Epoch 192 of 2000 took 0.102s
  training loss:		1.371430
  validation loss:		1.354123
  validation accuracy:		50.87 %
Epoch 193 of 2000 took 0.103s
  training loss:		1.375751
  validation loss:		1.336024
  validation accuracy:		50.33 %
Epoch 194 of 2000 took 0.102s
  training loss:		1.440052
  validation loss:		1.570097
  validation accuracy:		41.20 %
Epoch 195 of 2000 took 0.103s
  training loss:		1.429734
  validation loss:		1.343081
  validation accuracy:		50.43 %
Epoch 196 of 2000 took 0.102s
  training loss:		1.371734
  validation loss:		1.373991
  validation accuracy:		48.70 %
Epoch 197 of 2000 took 0.103s
  training loss:		1.372469
  validation loss:		1.390522
  validation accuracy:		49.13 %
Epoch 198 of 2000 took 0.103s
  training loss:		1.362810
  validation loss:		1.316617
  validation accuracy:		53.80 %
Epoch 199 of 2000 took 0.102s
  training loss:		1.349596
  validation loss:		1.314578
  validation accuracy:		52.93 %
Epoch 200 of 2000 took 0.102s
  training loss:		1.347937
  validation loss:		1.318866
  validation accuracy:		53.80 %
Epoch 201 of 2000 took 0.102s
  training loss:		1.439860
  validation loss:		1.515099
  validation accuracy:		43.70 %
Epoch 202 of 2000 took 0.102s
  training loss:		1.531491
  validation loss:		1.441687
  validation accuracy:		45.54 %
Epoch 203 of 2000 took 0.103s
  training loss:		1.402613
  validation loss:		1.359052
  validation accuracy:		50.65 %
Epoch 204 of 2000 took 0.103s
  training loss:		1.357869
  validation loss:		1.314655
  validation accuracy:		53.59 %
Epoch 205 of 2000 took 0.103s
  training loss:		1.353618
  validation loss:		1.367609
  validation accuracy:		50.43 %
Epoch 206 of 2000 took 0.102s
  training loss:		1.372973
  validation loss:		1.347887
  validation accuracy:		51.74 %
Epoch 207 of 2000 took 0.103s
  training loss:		1.357199
  validation loss:		1.369550
  validation accuracy:		50.43 %
Epoch 208 of 2000 took 0.103s
  training loss:		1.346147
  validation loss:		1.318878
  validation accuracy:		52.83 %
Epoch 209 of 2000 took 0.102s
  training loss:		1.348503
  validation loss:		1.317269
  validation accuracy:		54.02 %
Epoch 210 of 2000 took 0.102s
  training loss:		1.350826
  validation loss:		1.323869
  validation accuracy:		51.74 %
Epoch 211 of 2000 took 0.103s
  training loss:		1.396625
  validation loss:		1.405431
  validation accuracy:		48.59 %
Epoch 212 of 2000 took 0.102s
  training loss:		1.405898
  validation loss:		1.377960
  validation accuracy:		48.26 %
Epoch 213 of 2000 took 0.103s
  training loss:		1.435632
  validation loss:		1.397445
  validation accuracy:		48.80 %
Epoch 214 of 2000 took 0.102s
  training loss:		1.408291
  validation loss:		1.363422
  validation accuracy:		48.70 %
Epoch 215 of 2000 took 0.103s
  training loss:		1.387316
  validation loss:		1.468258
  validation accuracy:		45.76 %
Epoch 216 of 2000 took 0.103s
  training loss:		1.378511
  validation loss:		1.341644
  validation accuracy:		49.02 %
Epoch 217 of 2000 took 0.103s
  training loss:		1.345976
  validation loss:		1.371192
  validation accuracy:		48.48 %
Epoch 218 of 2000 took 0.102s
  training loss:		1.444532
  validation loss:		1.382020
  validation accuracy:		49.67 %
Epoch 219 of 2000 took 0.102s
  training loss:		1.351270
  validation loss:		1.321476
  validation accuracy:		54.89 %
Epoch 220 of 2000 took 0.102s
  training loss:		1.341782
  validation loss:		1.311534
  validation accuracy:		53.59 %
Epoch 221 of 2000 took 0.102s
  training loss:		1.348263
  validation loss:		1.352201
  validation accuracy:		51.63 %
Epoch 222 of 2000 took 0.103s
  training loss:		1.348032
  validation loss:		1.306808
  validation accuracy:		54.02 %
Epoch 223 of 2000 took 0.104s
  training loss:		1.332689
  validation loss:		1.317945
  validation accuracy:		53.80 %
Epoch 224 of 2000 took 0.126s
  training loss:		1.352422
  validation loss:		1.306015
  validation accuracy:		54.35 %
Epoch 225 of 2000 took 0.122s
  training loss:		1.366404
  validation loss:		1.424720
  validation accuracy:		48.37 %
Epoch 226 of 2000 took 0.102s
  training loss:		1.385449
  validation loss:		1.425285
  validation accuracy:		45.65 %
Epoch 227 of 2000 took 0.103s
  training loss:		1.396940
  validation loss:		1.491986
  validation accuracy:		42.61 %
Epoch 228 of 2000 took 0.099s
  training loss:		1.405413
  validation loss:		1.320243
  validation accuracy:		54.46 %
Epoch 229 of 2000 took 0.102s
  training loss:		1.338338
  validation loss:		1.307858
  validation accuracy:		54.67 %
Epoch 230 of 2000 took 0.103s
  training loss:		1.357232
  validation loss:		1.413827
  validation accuracy:		46.20 %
Epoch 231 of 2000 took 0.103s
  training loss:		1.377631
  validation loss:		1.399950
  validation accuracy:		49.89 %
Epoch 232 of 2000 took 0.101s
  training loss:		1.362294
  validation loss:		1.312742
  validation accuracy:		55.00 %
Epoch 233 of 2000 took 0.101s
  training loss:		1.339634
  validation loss:		1.304613
  validation accuracy:		53.80 %
Epoch 234 of 2000 took 0.100s
  training loss:		1.346306
  validation loss:		1.393584
  validation accuracy:		50.00 %
Epoch 235 of 2000 took 0.100s
  training loss:		1.377324
  validation loss:		1.323091
  validation accuracy:		52.50 %
Epoch 236 of 2000 took 0.100s
  training loss:		1.380110
  validation loss:		1.310140
  validation accuracy:		55.11 %
Epoch 237 of 2000 took 0.101s
  training loss:		1.342301
  validation loss:		1.326646
  validation accuracy:		53.48 %
Epoch 238 of 2000 took 0.100s
  training loss:		1.354227
  validation loss:		1.315575
  validation accuracy:		52.39 %
Epoch 239 of 2000 took 0.100s
  training loss:		1.333092
  validation loss:		1.300069
  validation accuracy:		54.46 %
Epoch 240 of 2000 took 0.100s
  training loss:		1.327784
  validation loss:		1.304731
  validation accuracy:		55.33 %
Epoch 241 of 2000 took 0.100s
  training loss:		1.347117
  validation loss:		1.350310
  validation accuracy:		50.22 %
Epoch 242 of 2000 took 0.100s
  training loss:		1.488538
  validation loss:		1.397168
  validation accuracy:		49.89 %
Epoch 243 of 2000 took 0.100s
  training loss:		1.342632
  validation loss:		1.316723
  validation accuracy:		54.78 %
Epoch 244 of 2000 took 0.101s
  training loss:		1.334048
  validation loss:		1.301737
  validation accuracy:		54.57 %
Epoch 245 of 2000 took 0.100s
  training loss:		1.337662
  validation loss:		1.300328
  validation accuracy:		54.46 %
Epoch 246 of 2000 took 0.101s
  training loss:		1.336817
  validation loss:		1.346929
  validation accuracy:		51.74 %
Epoch 247 of 2000 took 0.101s
  training loss:		1.366718
  validation loss:		1.317842
  validation accuracy:		54.13 %
Epoch 248 of 2000 took 0.100s
  training loss:		1.336471
  validation loss:		1.305432
  validation accuracy:		55.43 %
Epoch 249 of 2000 took 0.100s
  training loss:		1.426739
  validation loss:		1.455009
  validation accuracy:		47.39 %
Epoch 250 of 2000 took 0.100s
  training loss:		1.423277
  validation loss:		1.336768
  validation accuracy:		52.39 %
Epoch 251 of 2000 took 0.100s
  training loss:		1.367191
  validation loss:		1.312469
  validation accuracy:		54.89 %
Epoch 252 of 2000 took 0.101s
  training loss:		1.335717
  validation loss:		1.317913
  validation accuracy:		54.02 %
Epoch 253 of 2000 took 0.100s
  training loss:		1.343278
  validation loss:		1.332559
  validation accuracy:		53.15 %
Epoch 254 of 2000 took 0.100s
  training loss:		1.337051
  validation loss:		1.324322
  validation accuracy:		54.02 %
Epoch 255 of 2000 took 0.101s
  training loss:		1.336631
  validation loss:		1.342640
  validation accuracy:		52.50 %
Epoch 256 of 2000 took 0.100s
  training loss:		1.343004
  validation loss:		1.303742
  validation accuracy:		54.35 %
Epoch 257 of 2000 took 0.100s
  training loss:		1.336110
  validation loss:		1.305848
  validation accuracy:		55.00 %
Epoch 258 of 2000 took 0.101s
  training loss:		1.331050
  validation loss:		1.301425
  validation accuracy:		55.33 %
Epoch 259 of 2000 took 0.099s
  training loss:		1.332850
  validation loss:		1.336865
  validation accuracy:		53.04 %
Epoch 260 of 2000 took 0.101s
  training loss:		1.336436
  validation loss:		1.306255
  validation accuracy:		54.35 %
Epoch 261 of 2000 took 0.101s
  training loss:		1.327925
  validation loss:		1.307656
  validation accuracy:		53.91 %
Epoch 262 of 2000 took 0.100s
  training loss:		1.331111
  validation loss:		1.326847
  validation accuracy:		52.39 %
Epoch 263 of 2000 took 0.101s
  training loss:		1.333226
  validation loss:		1.325312
  validation accuracy:		53.26 %
Epoch 264 of 2000 took 0.102s
  training loss:		1.512734
  validation loss:		1.357770
  validation accuracy:		51.30 %
Epoch 265 of 2000 took 0.139s
  training loss:		1.350820
  validation loss:		1.344121
  validation accuracy:		51.96 %
Epoch 266 of 2000 took 0.164s
  training loss:		1.325767
  validation loss:		1.303881
  validation accuracy:		54.57 %
Epoch 267 of 2000 took 0.165s
  training loss:		1.368444
  validation loss:		1.324844
  validation accuracy:		51.85 %
Epoch 268 of 2000 took 0.165s
  training loss:		1.397438
  validation loss:		1.374909
  validation accuracy:		51.63 %
Epoch 269 of 2000 took 0.165s
  training loss:		1.374276
  validation loss:		1.308940
  validation accuracy:		54.57 %
Epoch 270 of 2000 took 0.120s
  training loss:		1.331999
  validation loss:		1.361756
  validation accuracy:		52.17 %
Epoch 271 of 2000 took 0.100s
  training loss:		1.371456
  validation loss:		1.304522
  validation accuracy:		54.78 %
Epoch 272 of 2000 took 0.101s
  training loss:		1.328383
  validation loss:		1.318423
  validation accuracy:		52.61 %
Epoch 273 of 2000 took 0.103s
  training loss:		1.338358
  validation loss:		1.302305
  validation accuracy:		55.65 %
Epoch 274 of 2000 took 0.108s
  training loss:		1.328244
  validation loss:		1.313206
  validation accuracy:		53.48 %
Epoch 275 of 2000 took 0.104s
  training loss:		1.356922
  validation loss:		1.313846
  validation accuracy:		53.04 %
Epoch 276 of 2000 took 0.102s
  training loss:		1.321243
  validation loss:		1.304355
  validation accuracy:		54.89 %
Epoch 277 of 2000 took 0.100s
  training loss:		1.335851
  validation loss:		1.305546
  validation accuracy:		54.67 %
Epoch 278 of 2000 took 0.100s
  training loss:		1.350679
  validation loss:		1.305799
  validation accuracy:		55.33 %
Epoch 279 of 2000 took 0.100s
  training loss:		1.323981
  validation loss:		1.312951
  validation accuracy:		53.91 %
Epoch 280 of 2000 took 0.100s
  training loss:		1.347430
  validation loss:		1.308673
  validation accuracy:		54.13 %
Epoch 281 of 2000 took 0.100s
  training loss:		1.322183
  validation loss:		1.310615
  validation accuracy:		55.11 %
Epoch 282 of 2000 took 0.100s
  training loss:		1.328165
  validation loss:		1.306255
  validation accuracy:		54.67 %
Epoch 283 of 2000 took 0.101s
  training loss:		1.333961
  validation loss:		1.326351
  validation accuracy:		53.26 %
Epoch 284 of 2000 took 0.100s
  training loss:		1.353711
  validation loss:		1.392197
  validation accuracy:		48.48 %
Epoch 285 of 2000 took 0.101s
  training loss:		1.368716
  validation loss:		1.324077
  validation accuracy:		53.37 %
Epoch 286 of 2000 took 0.104s
  training loss:		1.332285
  validation loss:		1.303803
  validation accuracy:		55.87 %
Epoch 287 of 2000 took 0.100s
  training loss:		1.345268
  validation loss:		1.310034
  validation accuracy:		53.48 %
Epoch 288 of 2000 took 0.101s
  training loss:		1.339015
  validation loss:		1.330150
  validation accuracy:		52.17 %
Epoch 289 of 2000 took 0.103s
  training loss:		1.351181
  validation loss:		1.311747
  validation accuracy:		52.83 %
Epoch 290 of 2000 took 0.103s
  training loss:		1.365964
  validation loss:		1.333231
  validation accuracy:		53.26 %
Epoch 291 of 2000 took 0.103s
  training loss:		1.342013
  validation loss:		1.307341
  validation accuracy:		54.78 %
Epoch 292 of 2000 took 0.103s
  training loss:		1.361741
  validation loss:		1.314211
  validation accuracy:		53.37 %
Epoch 293 of 2000 took 0.103s
  training loss:		1.333220
  validation loss:		1.304356
  validation accuracy:		54.57 %
Epoch 294 of 2000 took 0.102s
  training loss:		1.333087
  validation loss:		1.302093
  validation accuracy:		54.89 %
Epoch 295 of 2000 took 0.102s
  training loss:		1.338618
  validation loss:		1.306620
  validation accuracy:		55.65 %
Epoch 296 of 2000 took 0.102s
  training loss:		1.333281
  validation loss:		1.310621
  validation accuracy:		54.13 %
Epoch 297 of 2000 took 0.102s
  training loss:		1.360563
  validation loss:		1.342550
  validation accuracy:		51.20 %
Epoch 298 of 2000 took 0.102s
  training loss:		1.404831
  validation loss:		1.307068
  validation accuracy:		55.33 %
Epoch 299 of 2000 took 0.102s
  training loss:		1.360289
  validation loss:		1.309003
  validation accuracy:		54.78 %
Epoch 300 of 2000 took 0.102s
  training loss:		1.332863
  validation loss:		1.307233
  validation accuracy:		55.00 %
Epoch 301 of 2000 took 0.102s
  training loss:		1.347237
  validation loss:		1.303415
  validation accuracy:		55.43 %
Epoch 302 of 2000 took 0.102s
  training loss:		1.332028
  validation loss:		1.306383
  validation accuracy:		54.67 %
Epoch 303 of 2000 took 0.103s
  training loss:		1.337151
  validation loss:		1.322732
  validation accuracy:		53.70 %
Epoch 304 of 2000 took 0.102s
  training loss:		1.333402
  validation loss:		1.318097
  validation accuracy:		52.93 %
Epoch 305 of 2000 took 0.103s
  training loss:		1.357464
  validation loss:		1.374172
  validation accuracy:		51.85 %
Epoch 306 of 2000 took 0.102s
  training loss:		1.341230
  validation loss:		1.301138
  validation accuracy:		54.78 %
Epoch 307 of 2000 took 0.102s
  training loss:		1.375499
  validation loss:		1.302132
  validation accuracy:		55.33 %
Epoch 308 of 2000 took 0.102s
  training loss:		1.323624
  validation loss:		1.308646
  validation accuracy:		54.35 %
Epoch 309 of 2000 took 0.102s
  training loss:		1.339101
  validation loss:		1.302281
  validation accuracy:		55.00 %
Epoch 310 of 2000 took 0.102s
  training loss:		1.375555
  validation loss:		1.349612
  validation accuracy:		51.63 %
Epoch 311 of 2000 took 0.102s
  training loss:		1.336932
  validation loss:		1.306713
  validation accuracy:		54.57 %
Epoch 312 of 2000 took 0.102s
  training loss:		1.346931
  validation loss:		1.305399
  validation accuracy:		54.57 %
Epoch 313 of 2000 took 0.102s
  training loss:		1.325364
  validation loss:		1.301061
  validation accuracy:		55.22 %
Epoch 314 of 2000 took 0.102s
  training loss:		1.316526
  validation loss:		1.302084
  validation accuracy:		55.00 %
Epoch 315 of 2000 took 0.102s
  training loss:		1.332829
  validation loss:		1.304441
  validation accuracy:		55.22 %
Epoch 316 of 2000 took 0.102s
  training loss:		1.335316
  validation loss:		1.309867
  validation accuracy:		55.11 %
Epoch 317 of 2000 took 0.102s
  training loss:		1.329995
  validation loss:		1.304715
  validation accuracy:		55.11 %
Epoch 318 of 2000 took 0.102s
  training loss:		1.323232
  validation loss:		1.301027
  validation accuracy:		54.89 %
Epoch 319 of 2000 took 0.103s
  training loss:		1.329932
  validation loss:		1.305503
  validation accuracy:		55.11 %
Epoch 320 of 2000 took 0.102s
  training loss:		1.323180
  validation loss:		1.302054
  validation accuracy:		54.89 %
Epoch 321 of 2000 took 0.102s
  training loss:		1.318590
  validation loss:		1.306020
  validation accuracy:		54.35 %
Epoch 322 of 2000 took 0.103s
  training loss:		1.324777
  validation loss:		1.300152
  validation accuracy:		55.11 %
Epoch 323 of 2000 took 0.102s
  training loss:		1.327187
  validation loss:		1.301112
  validation accuracy:		56.09 %
Epoch 324 of 2000 took 0.102s
  training loss:		1.336097
  validation loss:		1.412028
  validation accuracy:		49.78 %
Epoch 325 of 2000 took 0.102s
  training loss:		1.372710
  validation loss:		1.303879
  validation accuracy:		54.78 %
Epoch 326 of 2000 took 0.102s
  training loss:		1.339200
  validation loss:		1.329450
  validation accuracy:		54.13 %
Epoch 327 of 2000 took 0.103s
  training loss:		1.341911
  validation loss:		1.361968
  validation accuracy:		50.43 %
Epoch 328 of 2000 took 0.102s
  training loss:		1.395645
  validation loss:		1.312907
  validation accuracy:		54.67 %
Epoch 329 of 2000 took 0.102s
  training loss:		1.323256
  validation loss:		1.330657
  validation accuracy:		52.83 %
Epoch 330 of 2000 took 0.103s
  training loss:		1.335598
  validation loss:		1.315568
  validation accuracy:		54.13 %
Epoch 331 of 2000 took 0.102s
  training loss:		1.344337
  validation loss:		1.410380
  validation accuracy:		50.11 %
Epoch 332 of 2000 took 0.103s
  training loss:		1.363242
  validation loss:		1.309116
  validation accuracy:		55.22 %
Epoch 333 of 2000 took 0.102s
  training loss:		1.328679
  validation loss:		1.340703
  validation accuracy:		52.83 %
Epoch 334 of 2000 took 0.102s
  training loss:		1.336499
  validation loss:		1.328086
  validation accuracy:		54.24 %
Epoch 335 of 2000 took 0.102s
  training loss:		1.325968
  validation loss:		1.312346
  validation accuracy:		55.00 %
Epoch 336 of 2000 took 0.102s
  training loss:		1.327504
  validation loss:		1.306140
  validation accuracy:		54.67 %
Epoch 337 of 2000 took 0.102s
  training loss:		1.329105
  validation loss:		1.316841
  validation accuracy:		54.57 %
Epoch 338 of 2000 took 0.102s
  training loss:		1.347356
  validation loss:		1.303751
  validation accuracy:		55.11 %
Epoch 339 of 2000 took 0.102s
  training loss:		1.320629
  validation loss:		1.297262
  validation accuracy:		55.76 %
Epoch 340 of 2000 took 0.102s
  training loss:		1.344736
  validation loss:		1.301409
  validation accuracy:		55.11 %
Epoch 341 of 2000 took 0.102s
  training loss:		1.334800
  validation loss:		1.319918
  validation accuracy:		54.46 %
Epoch 342 of 2000 took 0.102s
  training loss:		1.330967
  validation loss:		1.345515
  validation accuracy:		51.85 %
Epoch 343 of 2000 took 0.102s
  training loss:		1.336085
  validation loss:		1.316087
  validation accuracy:		54.46 %
Epoch 344 of 2000 took 0.102s
  training loss:		1.322129
  validation loss:		1.327412
  validation accuracy:		54.02 %
Epoch 345 of 2000 took 0.102s
  training loss:		1.351631
  validation loss:		1.308367
  validation accuracy:		54.67 %
Epoch 346 of 2000 took 0.102s
  training loss:		1.332671
  validation loss:		1.306981
  validation accuracy:		55.76 %
Epoch 347 of 2000 took 0.102s
  training loss:		1.327176
  validation loss:		1.302617
  validation accuracy:		55.00 %
Epoch 348 of 2000 took 0.103s
  training loss:		1.310843
  validation loss:		1.296694
  validation accuracy:		54.89 %
Epoch 349 of 2000 took 0.102s
  training loss:		1.323462
  validation loss:		1.341231
  validation accuracy:		53.48 %
Epoch 350 of 2000 took 0.103s
  training loss:		1.335467
  validation loss:		1.307563
  validation accuracy:		54.78 %
Epoch 351 of 2000 took 0.103s
  training loss:		1.327921
  validation loss:		1.300034
  validation accuracy:		55.87 %
Epoch 352 of 2000 took 0.103s
  training loss:		1.327641
  validation loss:		1.300820
  validation accuracy:		55.76 %
Epoch 353 of 2000 took 0.103s
  training loss:		1.323717
  validation loss:		1.306377
  validation accuracy:		54.78 %
Epoch 354 of 2000 took 0.102s
  training loss:		1.328738
  validation loss:		1.317261
  validation accuracy:		54.24 %
Epoch 355 of 2000 took 0.102s
  training loss:		1.337845
  validation loss:		1.299623
  validation accuracy:		54.89 %
Epoch 356 of 2000 took 0.103s
  training loss:		1.318988
  validation loss:		1.302227
  validation accuracy:		55.22 %
Epoch 357 of 2000 took 0.103s
  training loss:		1.319889
  validation loss:		1.314820
  validation accuracy:		53.70 %
Epoch 358 of 2000 took 0.102s
  training loss:		1.327664
  validation loss:		1.308485
  validation accuracy:		54.24 %
Epoch 359 of 2000 took 0.102s
  training loss:		1.359374
  validation loss:		1.335936
  validation accuracy:		53.04 %
Epoch 360 of 2000 took 0.102s
  training loss:		1.330397
  validation loss:		1.305853
  validation accuracy:		54.89 %
Epoch 361 of 2000 took 0.103s
  training loss:		1.322780
  validation loss:		1.304222
  validation accuracy:		56.30 %
Epoch 362 of 2000 took 0.103s
  training loss:		1.325363
  validation loss:		1.334782
  validation accuracy:		53.80 %
Epoch 363 of 2000 took 0.103s
  training loss:		1.332459
  validation loss:		1.299647
  validation accuracy:		55.65 %
Epoch 364 of 2000 took 0.103s
  training loss:		1.326104
  validation loss:		1.322949
  validation accuracy:		54.13 %
Epoch 365 of 2000 took 0.102s
  training loss:		1.339845
  validation loss:		1.346848
  validation accuracy:		53.37 %
Epoch 366 of 2000 took 0.103s
  training loss:		1.342471
  validation loss:		1.304748
  validation accuracy:		55.76 %
Epoch 367 of 2000 took 0.103s
  training loss:		1.320515
  validation loss:		1.303819
  validation accuracy:		55.11 %
Epoch 368 of 2000 took 0.102s
  training loss:		1.327361
  validation loss:		1.298761
  validation accuracy:		55.11 %
Epoch 369 of 2000 took 0.103s
  training loss:		1.329070
  validation loss:		1.296922
  validation accuracy:		55.76 %
Epoch 370 of 2000 took 0.102s
  training loss:		1.330889
  validation loss:		1.302234
  validation accuracy:		55.00 %
Epoch 371 of 2000 took 0.103s
  training loss:		1.337686
  validation loss:		1.307149
  validation accuracy:		55.43 %
Epoch 372 of 2000 took 0.102s
  training loss:		1.326554
  validation loss:		1.302817
  validation accuracy:		54.89 %
Epoch 373 of 2000 took 0.102s
  training loss:		1.329033
  validation loss:		1.304673
  validation accuracy:		55.00 %
Epoch 374 of 2000 took 0.102s
  training loss:		1.328693
  validation loss:		1.300269
  validation accuracy:		55.22 %
Epoch 375 of 2000 took 0.102s
  training loss:		1.319679
  validation loss:		1.300276
  validation accuracy:		55.43 %
Epoch 376 of 2000 took 0.102s
  training loss:		1.317919
  validation loss:		1.301643
  validation accuracy:		54.13 %
Epoch 377 of 2000 took 0.102s
  training loss:		1.320896
  validation loss:		1.307392
  validation accuracy:		54.35 %
Epoch 378 of 2000 took 0.103s
  training loss:		1.326397
  validation loss:		1.300569
  validation accuracy:		55.33 %
Epoch 379 of 2000 took 0.102s
  training loss:		1.332197
  validation loss:		1.308415
  validation accuracy:		55.76 %
Epoch 380 of 2000 took 0.102s
  training loss:		1.355570
  validation loss:		1.300305
  validation accuracy:		55.43 %
Epoch 381 of 2000 took 0.103s
  training loss:		1.327932
  validation loss:		1.339282
  validation accuracy:		54.13 %
Epoch 382 of 2000 took 0.102s
  training loss:		1.336684
  validation loss:		1.311169
  validation accuracy:		54.02 %
Epoch 383 of 2000 took 0.102s
  training loss:		1.320162
  validation loss:		1.319971
  validation accuracy:		54.35 %
Epoch 384 of 2000 took 0.102s
  training loss:		1.321576
  validation loss:		1.305621
  validation accuracy:		54.89 %
Epoch 385 of 2000 took 0.102s
  training loss:		1.321752
  validation loss:		1.318536
  validation accuracy:		54.35 %
Epoch 386 of 2000 took 0.102s
  training loss:		1.321629
  validation loss:		1.302257
  validation accuracy:		54.89 %
Epoch 387 of 2000 took 0.102s
  training loss:		1.315025
  validation loss:		1.301952
  validation accuracy:		55.76 %
Epoch 388 of 2000 took 0.102s
  training loss:		1.313061
  validation loss:		1.308606
  validation accuracy:		54.35 %
Epoch 389 of 2000 took 0.102s
  training loss:		1.317490
  validation loss:		1.302905
  validation accuracy:		54.57 %
Epoch 390 of 2000 took 0.102s
  training loss:		1.319054
  validation loss:		1.308416
  validation accuracy:		56.20 %
Epoch 391 of 2000 took 0.102s
  training loss:		1.325817
  validation loss:		1.298225
  validation accuracy:		54.78 %
Epoch 392 of 2000 took 0.102s
  training loss:		1.322637
  validation loss:		1.298609
  validation accuracy:		55.43 %
Epoch 393 of 2000 took 0.102s
  training loss:		1.331041
  validation loss:		1.344289
  validation accuracy:		53.37 %
Epoch 394 of 2000 took 0.102s
  training loss:		1.321699
  validation loss:		1.337375
  validation accuracy:		54.13 %
Epoch 395 of 2000 took 0.102s
  training loss:		1.359899
  validation loss:		1.335355
  validation accuracy:		53.48 %
Epoch 396 of 2000 took 0.103s
  training loss:		1.321639
  validation loss:		1.303285
  validation accuracy:		55.65 %
Epoch 397 of 2000 took 0.102s
  training loss:		1.329818
  validation loss:		1.303807
  validation accuracy:		56.74 %
Epoch 398 of 2000 took 0.102s
  training loss:		1.338416
  validation loss:		1.304328
  validation accuracy:		56.85 %
Epoch 399 of 2000 took 0.105s
  training loss:		1.323242
  validation loss:		1.300057
  validation accuracy:		55.11 %
Epoch 400 of 2000 took 0.103s
  training loss:		1.328977
  validation loss:		1.308228
  validation accuracy:		56.85 %
Epoch 401 of 2000 took 0.102s
  training loss:		1.321434
  validation loss:		1.298741
  validation accuracy:		55.43 %
Epoch 402 of 2000 took 0.102s
  training loss:		1.329909
  validation loss:		1.304373
  validation accuracy:		55.33 %
Epoch 403 of 2000 took 0.102s
  training loss:		1.316284
  validation loss:		1.296232
  validation accuracy:		55.76 %
Epoch 404 of 2000 took 0.102s
  training loss:		1.322686
  validation loss:		1.297143
  validation accuracy:		55.22 %
Epoch 405 of 2000 took 0.103s
  training loss:		1.321999
  validation loss:		1.300750
  validation accuracy:		55.33 %
Epoch 406 of 2000 took 0.102s
  training loss:		1.322114
  validation loss:		1.296477
  validation accuracy:		55.11 %
Epoch 407 of 2000 took 0.103s
  training loss:		1.319778
  validation loss:		1.308779
  validation accuracy:		55.00 %
Epoch 408 of 2000 took 0.103s
  training loss:		1.323830
  validation loss:		1.296443
  validation accuracy:		55.43 %
Epoch 409 of 2000 took 0.103s
  training loss:		1.321508
  validation loss:		1.305477
  validation accuracy:		54.89 %
Epoch 410 of 2000 took 0.103s
  training loss:		1.328075
  validation loss:		1.298883
  validation accuracy:		55.65 %
Epoch 411 of 2000 took 0.102s
  training loss:		1.317374
  validation loss:		1.299632
  validation accuracy:		55.87 %
Epoch 412 of 2000 took 0.102s
  training loss:		1.318386
  validation loss:		1.318284
  validation accuracy:		55.22 %
Epoch 413 of 2000 took 0.102s
  training loss:		1.334461
  validation loss:		1.305189
  validation accuracy:		57.17 %
Epoch 414 of 2000 took 0.102s
  training loss:		1.319216
  validation loss:		1.334852
  validation accuracy:		53.80 %
Epoch 415 of 2000 took 0.102s
  training loss:		1.336909
  validation loss:		1.298796
  validation accuracy:		56.63 %
Epoch 416 of 2000 took 0.102s
  training loss:		1.320099
  validation loss:		1.315476
  validation accuracy:		54.67 %
Epoch 417 of 2000 took 0.102s
  training loss:		1.327832
  validation loss:		1.315195
  validation accuracy:		55.98 %
Epoch 418 of 2000 took 0.103s
  training loss:		1.323512
  validation loss:		1.295758
  validation accuracy:		55.87 %
Epoch 419 of 2000 took 0.103s
  training loss:		1.320766
  validation loss:		1.295135
  validation accuracy:		55.98 %
Epoch 420 of 2000 took 0.103s
  training loss:		1.315647
  validation loss:		1.295100
  validation accuracy:		55.76 %
Epoch 421 of 2000 took 0.103s
  training loss:		1.326657
  validation loss:		1.296481
  validation accuracy:		55.76 %
Epoch 422 of 2000 took 0.102s
  training loss:		1.329058
  validation loss:		1.317794
  validation accuracy:		54.67 %
Epoch 423 of 2000 took 0.102s
  training loss:		1.318930
  validation loss:		1.300959
  validation accuracy:		55.98 %
Epoch 424 of 2000 took 0.102s
  training loss:		1.318289
  validation loss:		1.311652
  validation accuracy:		56.74 %
Epoch 425 of 2000 took 0.103s
  training loss:		1.325148
  validation loss:		1.318770
  validation accuracy:		54.46 %
Epoch 426 of 2000 took 0.102s
  training loss:		1.317041
  validation loss:		1.296639
  validation accuracy:		55.87 %
Epoch 427 of 2000 took 0.102s
  training loss:		1.319849
  validation loss:		1.297712
  validation accuracy:		55.33 %
Epoch 428 of 2000 took 0.102s
  training loss:		1.321012
  validation loss:		1.306954
  validation accuracy:		55.11 %
Epoch 429 of 2000 took 0.103s
  training loss:		1.328123
  validation loss:		1.293694
  validation accuracy:		56.52 %
Epoch 430 of 2000 took 0.102s
  training loss:		1.327322
  validation loss:		1.301219
  validation accuracy:		56.74 %
Epoch 431 of 2000 took 0.102s
  training loss:		1.328217
  validation loss:		1.310955
  validation accuracy:		57.39 %
Epoch 432 of 2000 took 0.102s
  training loss:		1.325471
  validation loss:		1.294849
  validation accuracy:		56.85 %
Epoch 433 of 2000 took 0.102s
  training loss:		1.316111
  validation loss:		1.297417
  validation accuracy:		55.43 %
Epoch 434 of 2000 took 0.102s
  training loss:		1.312382
  validation loss:		1.296154
  validation accuracy:		55.54 %
Epoch 435 of 2000 took 0.103s
  training loss:		1.310076
  validation loss:		1.321056
  validation accuracy:		55.22 %
Epoch 436 of 2000 took 0.103s
  training loss:		1.335433
  validation loss:		1.298544
  validation accuracy:		55.65 %
Epoch 437 of 2000 took 0.103s
  training loss:		1.328492
  validation loss:		1.295023
  validation accuracy:		56.85 %
Epoch 438 of 2000 took 0.102s
  training loss:		1.305552
  validation loss:		1.295319
  validation accuracy:		55.43 %
Epoch 439 of 2000 took 0.103s
  training loss:		1.315537
  validation loss:		1.316737
  validation accuracy:		56.30 %
Epoch 440 of 2000 took 0.102s
  training loss:		1.318811
  validation loss:		1.297331
  validation accuracy:		55.33 %
Epoch 441 of 2000 took 0.102s
  training loss:		1.312343
  validation loss:		1.288446
  validation accuracy:		56.74 %
Epoch 442 of 2000 took 0.102s
  training loss:		1.318853
  validation loss:		1.302496
  validation accuracy:		55.54 %
Epoch 443 of 2000 took 0.102s
  training loss:		1.312702
  validation loss:		1.290090
  validation accuracy:		56.85 %
Epoch 444 of 2000 took 0.103s
  training loss:		1.310719
  validation loss:		1.291171
  validation accuracy:		56.41 %
Epoch 445 of 2000 took 0.103s
  training loss:		1.314091
  validation loss:		1.335701
  validation accuracy:		54.24 %
Epoch 446 of 2000 took 0.102s
  training loss:		1.324536
  validation loss:		1.293176
  validation accuracy:		56.09 %
Epoch 447 of 2000 took 0.103s
  training loss:		1.307583
  validation loss:		1.296239
  validation accuracy:		55.87 %
Epoch 448 of 2000 took 0.106s
  training loss:		1.303082
  validation loss:		1.296447
  validation accuracy:		56.09 %
Epoch 449 of 2000 took 0.106s
  training loss:		1.310538
  validation loss:		1.286376
  validation accuracy:		57.50 %
Epoch 450 of 2000 took 0.106s
  training loss:		1.319076
  validation loss:		1.302050
  validation accuracy:		55.87 %
Epoch 451 of 2000 took 0.106s
  training loss:		1.310797
  validation loss:		1.286439
  validation accuracy:		55.76 %
Epoch 452 of 2000 took 0.106s
  training loss:		1.325294
  validation loss:		1.334979
  validation accuracy:		54.46 %
Epoch 453 of 2000 took 0.106s
  training loss:		1.320090
  validation loss:		1.289719
  validation accuracy:		57.28 %
Epoch 454 of 2000 took 0.106s
  training loss:		1.297150
  validation loss:		1.283629
  validation accuracy:		56.96 %
Epoch 455 of 2000 took 0.106s
  training loss:		1.315442
  validation loss:		1.308578
  validation accuracy:		55.00 %
Epoch 456 of 2000 took 0.106s
  training loss:		1.296599
  validation loss:		1.287487
  validation accuracy:		57.17 %
Epoch 457 of 2000 took 0.106s
  training loss:		1.304740
  validation loss:		1.358461
  validation accuracy:		53.26 %
Epoch 458 of 2000 took 0.106s
  training loss:		1.309471
  validation loss:		1.292387
  validation accuracy:		55.87 %
Epoch 459 of 2000 took 0.106s
  training loss:		1.341055
  validation loss:		1.286434
  validation accuracy:		57.39 %
Epoch 460 of 2000 took 0.106s
  training loss:		1.309461
  validation loss:		1.285374
  validation accuracy:		56.20 %
Epoch 461 of 2000 took 0.106s
  training loss:		1.309039
  validation loss:		1.328001
  validation accuracy:		54.67 %
Epoch 462 of 2000 took 0.106s
  training loss:		1.331094
  validation loss:		1.304112
  validation accuracy:		56.09 %
Epoch 463 of 2000 took 0.106s
  training loss:		1.296127
  validation loss:		1.281071
  validation accuracy:		57.28 %
Epoch 464 of 2000 took 0.106s
  training loss:		1.297894
  validation loss:		1.291889
  validation accuracy:		56.20 %
Epoch 465 of 2000 took 0.106s
  training loss:		1.293424
  validation loss:		1.289042
  validation accuracy:		57.39 %
Epoch 466 of 2000 took 0.106s
  training loss:		1.316296
  validation loss:		1.296142
  validation accuracy:		56.30 %
Epoch 467 of 2000 took 0.106s
  training loss:		1.288889
  validation loss:		1.285766
  validation accuracy:		56.41 %
Epoch 468 of 2000 took 0.106s
  training loss:		1.301889
  validation loss:		1.280198
  validation accuracy:		57.07 %
Epoch 469 of 2000 took 0.106s
  training loss:		1.304809
  validation loss:		1.285923
  validation accuracy:		57.28 %
Epoch 470 of 2000 took 0.106s
  training loss:		1.301785
  validation loss:		1.280319
  validation accuracy:		56.96 %
Epoch 471 of 2000 took 0.106s
  training loss:		1.303151
  validation loss:		1.271681
  validation accuracy:		57.28 %
Epoch 472 of 2000 took 0.106s
  training loss:		1.299539
  validation loss:		1.285445
  validation accuracy:		57.17 %
Epoch 473 of 2000 took 0.106s
  training loss:		1.305954
  validation loss:		1.280007
  validation accuracy:		57.50 %
Epoch 474 of 2000 took 0.106s
  training loss:		1.307762
  validation loss:		1.304570
  validation accuracy:		55.43 %
Epoch 475 of 2000 took 0.106s
  training loss:		1.317458
  validation loss:		1.275885
  validation accuracy:		57.61 %
Epoch 476 of 2000 took 0.106s
  training loss:		1.296075
  validation loss:		1.274181
  validation accuracy:		57.61 %
Epoch 477 of 2000 took 0.106s
  training loss:		1.275076
  validation loss:		1.264456
  validation accuracy:		57.39 %
Epoch 478 of 2000 took 0.106s
  training loss:		1.304696
  validation loss:		1.267460
  validation accuracy:		57.61 %
Epoch 479 of 2000 took 0.106s
  training loss:		1.304131
  validation loss:		1.272108
  validation accuracy:		58.37 %
Epoch 480 of 2000 took 0.106s
  training loss:		1.289317
  validation loss:		1.261563
  validation accuracy:		57.93 %
Epoch 481 of 2000 took 0.106s
  training loss:		1.285721
  validation loss:		1.291944
  validation accuracy:		56.41 %
Epoch 482 of 2000 took 0.106s
  training loss:		1.292418
  validation loss:		1.263184
  validation accuracy:		57.83 %
Epoch 483 of 2000 took 0.106s
  training loss:		1.303635
  validation loss:		1.272527
  validation accuracy:		57.72 %
Epoch 484 of 2000 took 0.106s
  training loss:		1.294893
  validation loss:		1.273430
  validation accuracy:		57.83 %
Epoch 485 of 2000 took 0.106s
  training loss:		1.280763
  validation loss:		1.291477
  validation accuracy:		58.70 %
Epoch 486 of 2000 took 0.106s
  training loss:		1.291444
  validation loss:		1.252950
  validation accuracy:		58.15 %
Epoch 487 of 2000 took 0.106s
  training loss:		1.271813
  validation loss:		1.250583
  validation accuracy:		58.48 %
Epoch 488 of 2000 took 0.106s
  training loss:		1.276507
  validation loss:		1.255232
  validation accuracy:		59.24 %
Epoch 489 of 2000 took 0.106s
  training loss:		1.278203
  validation loss:		1.250653
  validation accuracy:		59.24 %
Epoch 490 of 2000 took 0.106s
  training loss:		1.266339
  validation loss:		1.241923
  validation accuracy:		59.35 %
Epoch 491 of 2000 took 0.106s
  training loss:		1.267855
  validation loss:		1.238482
  validation accuracy:		59.67 %
Epoch 492 of 2000 took 0.106s
  training loss:		1.270549
  validation loss:		1.269356
  validation accuracy:		58.15 %
Epoch 493 of 2000 took 0.106s
  training loss:		1.266728
  validation loss:		1.266140
  validation accuracy:		58.15 %
Epoch 494 of 2000 took 0.106s
  training loss:		1.264681
  validation loss:		1.236303
  validation accuracy:		60.65 %
Epoch 495 of 2000 took 0.106s
  training loss:		1.268045
  validation loss:		1.236539
  validation accuracy:		60.65 %
Epoch 496 of 2000 took 0.106s
  training loss:		1.256087
  validation loss:		1.222628
  validation accuracy:		61.63 %
Epoch 497 of 2000 took 0.106s
  training loss:		1.241860
  validation loss:		1.227510
  validation accuracy:		61.96 %
Epoch 498 of 2000 took 0.106s
  training loss:		1.252668
  validation loss:		1.230389
  validation accuracy:		60.54 %
Epoch 499 of 2000 took 0.106s
  training loss:		1.262339
  validation loss:		1.208618
  validation accuracy:		63.26 %
Epoch 500 of 2000 took 0.106s
  training loss:		1.239967
  validation loss:		1.220018
  validation accuracy:		61.30 %
Epoch 501 of 2000 took 0.106s
  training loss:		1.242674
  validation loss:		1.197811
  validation accuracy:		63.48 %
Epoch 502 of 2000 took 0.106s
  training loss:		1.230738
  validation loss:		1.187484
  validation accuracy:		63.04 %
Epoch 503 of 2000 took 0.106s
  training loss:		1.219443
  validation loss:		1.181125
  validation accuracy:		62.93 %
Epoch 504 of 2000 took 0.106s
  training loss:		1.211795
  validation loss:		1.172247
  validation accuracy:		65.00 %
Epoch 505 of 2000 took 0.106s
  training loss:		1.206883
  validation loss:		1.162005
  validation accuracy:		64.89 %
Epoch 506 of 2000 took 0.106s
  training loss:		1.202171
  validation loss:		1.153021
  validation accuracy:		65.11 %
Epoch 507 of 2000 took 0.106s
  training loss:		1.186635
  validation loss:		1.150227
  validation accuracy:		66.20 %
Epoch 508 of 2000 took 0.106s
  training loss:		1.186380
  validation loss:		1.137098
  validation accuracy:		65.98 %
Epoch 509 of 2000 took 0.106s
  training loss:		1.183454
  validation loss:		1.136878
  validation accuracy:		65.98 %
Epoch 510 of 2000 took 0.106s
  training loss:		1.174525
  validation loss:		1.123573
  validation accuracy:		66.74 %
Epoch 511 of 2000 took 0.106s
  training loss:		1.169589
  validation loss:		1.108028
  validation accuracy:		67.83 %
Epoch 512 of 2000 took 0.106s
  training loss:		1.153753
  validation loss:		1.096094
  validation accuracy:		68.59 %
Epoch 513 of 2000 took 0.106s
  training loss:		1.145673
  validation loss:		1.091207
  validation accuracy:		68.59 %
Epoch 514 of 2000 took 0.106s
  training loss:		1.133329
  validation loss:		1.084996
  validation accuracy:		68.37 %
Epoch 515 of 2000 took 0.106s
  training loss:		1.129328
  validation loss:		1.064299
  validation accuracy:		69.46 %
Epoch 516 of 2000 took 0.106s
  training loss:		1.114772
  validation loss:		1.050379
  validation accuracy:		70.11 %
Epoch 517 of 2000 took 0.106s
  training loss:		1.102327
  validation loss:		1.045703
  validation accuracy:		69.67 %
Epoch 518 of 2000 took 0.106s
  training loss:		1.088787
  validation loss:		1.027832
  validation accuracy:		70.98 %
Epoch 519 of 2000 took 0.106s
  training loss:		1.079743
  validation loss:		1.014889
  validation accuracy:		71.20 %
Epoch 520 of 2000 took 0.106s
  training loss:		1.063721
  validation loss:		1.003967
  validation accuracy:		72.17 %
Epoch 521 of 2000 took 0.106s
  training loss:		1.057464
  validation loss:		0.999653
  validation accuracy:		72.50 %
Epoch 522 of 2000 took 0.106s
  training loss:		1.055782
  validation loss:		0.975981
  validation accuracy:		73.15 %
Epoch 523 of 2000 took 0.107s
  training loss:		1.043291
  validation loss:		0.967809
  validation accuracy:		72.93 %
Epoch 524 of 2000 took 0.106s
  training loss:		1.027780
  validation loss:		0.954977
  validation accuracy:		73.26 %
Epoch 525 of 2000 took 0.106s
  training loss:		1.009974
  validation loss:		0.942161
  validation accuracy:		74.02 %
Epoch 526 of 2000 took 0.106s
  training loss:		1.011995
  validation loss:		0.938370
  validation accuracy:		73.59 %
Epoch 527 of 2000 took 0.106s
  training loss:		0.995662
  validation loss:		0.917848
  validation accuracy:		74.35 %
Epoch 528 of 2000 took 0.106s
  training loss:		0.983372
  validation loss:		0.911093
  validation accuracy:		74.35 %
Epoch 529 of 2000 took 0.106s
  training loss:		0.970109
  validation loss:		0.911017
  validation accuracy:		75.11 %
Epoch 530 of 2000 took 0.106s
  training loss:		0.970156
  validation loss:		0.922354
  validation accuracy:		74.89 %
Epoch 531 of 2000 took 0.106s
  training loss:		0.959379
  validation loss:		0.885569
  validation accuracy:		75.65 %
Epoch 532 of 2000 took 0.106s
  training loss:		0.951099
  validation loss:		0.872678
  validation accuracy:		75.65 %
Epoch 533 of 2000 took 0.106s
  training loss:		0.942779
  validation loss:		0.884550
  validation accuracy:		74.57 %
Epoch 534 of 2000 took 0.106s
  training loss:		0.927455
  validation loss:		0.865558
  validation accuracy:		75.98 %
Epoch 535 of 2000 took 0.106s
  training loss:		0.912692
  validation loss:		0.883738
  validation accuracy:		74.67 %
Epoch 536 of 2000 took 0.106s
  training loss:		0.925044
  validation loss:		0.845190
  validation accuracy:		76.09 %
Epoch 537 of 2000 took 0.106s
  training loss:		0.904828
  validation loss:		0.830321
  validation accuracy:		76.20 %
Epoch 538 of 2000 took 0.106s
  training loss:		0.905097
  validation loss:		0.821036
  validation accuracy:		76.74 %
Epoch 539 of 2000 took 0.106s
  training loss:		0.898073
  validation loss:		0.844797
  validation accuracy:		75.76 %
Epoch 540 of 2000 took 0.106s
  training loss:		0.890025
  validation loss:		0.818018
  validation accuracy:		76.63 %
Epoch 541 of 2000 took 0.106s
  training loss:		0.877297
  validation loss:		0.807249
  validation accuracy:		77.17 %
Epoch 542 of 2000 took 0.106s
  training loss:		0.874692
  validation loss:		0.794828
  validation accuracy:		77.93 %
Epoch 543 of 2000 took 0.106s
  training loss:		0.862550
  validation loss:		0.792611
  validation accuracy:		77.93 %
Epoch 544 of 2000 took 0.108s
  training loss:		0.846693
  validation loss:		0.784901
  validation accuracy:		77.83 %
Epoch 545 of 2000 took 0.106s
  training loss:		0.841917
  validation loss:		0.784532
  validation accuracy:		77.83 %
Epoch 546 of 2000 took 0.106s
  training loss:		0.832756
  validation loss:		0.784963
  validation accuracy:		77.61 %
Epoch 547 of 2000 took 0.106s
  training loss:		0.832373
  validation loss:		0.765496
  validation accuracy:		78.70 %
Epoch 548 of 2000 took 0.106s
  training loss:		0.826582
  validation loss:		0.794041
  validation accuracy:		77.39 %
Epoch 549 of 2000 took 0.106s
  training loss:		0.820124
  validation loss:		0.750995
  validation accuracy:		79.24 %
Epoch 550 of 2000 took 0.106s
  training loss:		0.806980
  validation loss:		0.763272
  validation accuracy:		78.04 %
Epoch 551 of 2000 took 0.106s
  training loss:		0.803513
  validation loss:		0.757798
  validation accuracy:		77.83 %
Epoch 552 of 2000 took 0.106s
  training loss:		0.790154
  validation loss:		0.742683
  validation accuracy:		78.80 %
Epoch 553 of 2000 took 0.106s
  training loss:		0.796834
  validation loss:		0.727887
  validation accuracy:		79.78 %
Epoch 554 of 2000 took 0.106s
  training loss:		0.773673
  validation loss:		0.726839
  validation accuracy:		79.57 %
Epoch 555 of 2000 took 0.106s
  training loss:		0.783073
  validation loss:		0.744780
  validation accuracy:		78.80 %
Epoch 556 of 2000 took 0.106s
  training loss:		0.771136
  validation loss:		0.715746
  validation accuracy:		79.13 %
Epoch 557 of 2000 took 0.106s
  training loss:		0.766248
  validation loss:		0.712704
  validation accuracy:		80.00 %
Epoch 558 of 2000 took 0.106s
  training loss:		0.745445
  validation loss:		0.710879
  validation accuracy:		79.57 %
Epoch 559 of 2000 took 0.106s
  training loss:		0.753307
  validation loss:		0.713324
  validation accuracy:		79.46 %
Epoch 560 of 2000 took 0.106s
  training loss:		0.746407
  validation loss:		0.716437
  validation accuracy:		78.70 %
Epoch 561 of 2000 took 0.103s
  training loss:		0.744682
  validation loss:		0.693978
  validation accuracy:		80.11 %
Epoch 562 of 2000 took 0.103s
  training loss:		0.736977
  validation loss:		0.706589
  validation accuracy:		80.11 %
Epoch 563 of 2000 took 0.103s
  training loss:		0.731522
  validation loss:		0.699844
  validation accuracy:		79.67 %
Epoch 564 of 2000 took 0.103s
  training loss:		0.724544
  validation loss:		0.702106
  validation accuracy:		79.46 %
Epoch 565 of 2000 took 0.102s
  training loss:		0.725859
  validation loss:		0.683960
  validation accuracy:		79.67 %
Epoch 566 of 2000 took 0.102s
  training loss:		0.717311
  validation loss:		0.690326
  validation accuracy:		78.91 %
Epoch 567 of 2000 took 0.102s
  training loss:		0.705847
  validation loss:		0.692008
  validation accuracy:		78.91 %
Epoch 568 of 2000 took 0.103s
  training loss:		0.703682
  validation loss:		0.681256
  validation accuracy:		80.00 %
Epoch 569 of 2000 took 0.102s
  training loss:		0.699737
  validation loss:		0.679989
  validation accuracy:		79.89 %
Epoch 570 of 2000 took 0.102s
  training loss:		0.704757
  validation loss:		0.674333
  validation accuracy:		80.11 %
Epoch 571 of 2000 took 0.102s
  training loss:		0.692233
  validation loss:		0.666568
  validation accuracy:		80.11 %
Epoch 572 of 2000 took 0.103s
  training loss:		0.694196
  validation loss:		0.670247
  validation accuracy:		79.89 %
Epoch 573 of 2000 took 0.103s
  training loss:		0.686807
  validation loss:		0.678172
  validation accuracy:		79.24 %
Epoch 574 of 2000 took 0.102s
  training loss:		0.690464
  validation loss:		0.685716
  validation accuracy:		78.37 %
Epoch 575 of 2000 took 0.102s
  training loss:		0.680843
  validation loss:		0.664219
  validation accuracy:		79.67 %
Epoch 576 of 2000 took 0.102s
  training loss:		0.678702
  validation loss:		0.660186
  validation accuracy:		80.43 %
Epoch 577 of 2000 took 0.103s
  training loss:		0.669094
  validation loss:		0.665019
  validation accuracy:		80.22 %
Epoch 578 of 2000 took 0.103s
  training loss:		0.674339
  validation loss:		0.659090
  validation accuracy:		79.24 %
Epoch 579 of 2000 took 0.102s
  training loss:		0.685176
  validation loss:		0.668201
  validation accuracy:		79.57 %
Epoch 580 of 2000 took 0.103s
  training loss:		0.673129
  validation loss:		0.662813
  validation accuracy:		80.11 %
Epoch 581 of 2000 took 0.102s
  training loss:		0.673099
  validation loss:		0.692155
  validation accuracy:		79.02 %
Epoch 582 of 2000 took 0.103s
  training loss:		0.667072
  validation loss:		0.674312
  validation accuracy:		79.24 %
Epoch 583 of 2000 took 0.102s
  training loss:		0.663053
  validation loss:		0.653772
  validation accuracy:		80.87 %
Epoch 584 of 2000 took 0.102s
  training loss:		0.674017
  validation loss:		0.665746
  validation accuracy:		79.46 %
Epoch 585 of 2000 took 0.102s
  training loss:		0.661072
  validation loss:		0.648921
  validation accuracy:		80.54 %
Epoch 586 of 2000 took 0.102s
  training loss:		0.671318
  validation loss:		0.680014
  validation accuracy:		79.13 %
Epoch 587 of 2000 took 0.102s
  training loss:		0.662924
  validation loss:		0.653570
  validation accuracy:		80.54 %
Epoch 588 of 2000 took 0.102s
  training loss:		0.654756
  validation loss:		0.643587
  validation accuracy:		80.65 %
Epoch 589 of 2000 took 0.102s
  training loss:		0.654491
  validation loss:		0.646402
  validation accuracy:		81.41 %
Epoch 590 of 2000 took 0.102s
  training loss:		0.649588
  validation loss:		0.650013
  validation accuracy:		80.65 %
Epoch 591 of 2000 took 0.103s
  training loss:		0.651798
  validation loss:		0.648902
  validation accuracy:		80.00 %
Epoch 592 of 2000 took 0.103s
  training loss:		0.647090
  validation loss:		0.657477
  validation accuracy:		79.67 %
Epoch 593 of 2000 took 0.102s
  training loss:		0.640152
  validation loss:		0.636139
  validation accuracy:		80.76 %
Epoch 594 of 2000 took 0.102s
  training loss:		0.633971
  validation loss:		0.642692
  validation accuracy:		80.54 %
Epoch 595 of 2000 took 0.102s
  training loss:		0.660446
  validation loss:		0.648228
  validation accuracy:		80.76 %
Epoch 596 of 2000 took 0.102s
  training loss:		0.648774
  validation loss:		0.655293
  validation accuracy:		79.67 %
Epoch 597 of 2000 took 0.103s
  training loss:		0.663458
  validation loss:		0.626237
  validation accuracy:		80.54 %
Epoch 598 of 2000 took 0.103s
  training loss:		0.641624
  validation loss:		0.637254
  validation accuracy:		80.98 %
Epoch 599 of 2000 took 0.103s
  training loss:		0.651093
  validation loss:		0.629890
  validation accuracy:		79.89 %
Epoch 600 of 2000 took 0.103s
  training loss:		0.649380
  validation loss:		0.628173
  validation accuracy:		80.43 %
Epoch 601 of 2000 took 0.103s
  training loss:		0.651367
  validation loss:		0.638379
  validation accuracy:		80.76 %
Epoch 602 of 2000 took 0.102s
  training loss:		0.633351
  validation loss:		0.674857
  validation accuracy:		79.57 %
Epoch 603 of 2000 took 0.103s
  training loss:		0.632829
  validation loss:		0.646139
  validation accuracy:		79.89 %
Epoch 604 of 2000 took 0.102s
  training loss:		0.640798
  validation loss:		0.635376
  validation accuracy:		79.46 %
Epoch 605 of 2000 took 0.103s
  training loss:		0.628205
  validation loss:		0.638424
  validation accuracy:		80.65 %
Epoch 606 of 2000 took 0.102s
  training loss:		0.625683
  validation loss:		0.653419
  validation accuracy:		80.87 %
Epoch 607 of 2000 took 0.102s
  training loss:		0.627921
  validation loss:		0.660184
  validation accuracy:		79.35 %
Epoch 608 of 2000 took 0.103s
  training loss:		0.632700
  validation loss:		0.617107
  validation accuracy:		80.76 %
Epoch 609 of 2000 took 0.103s
  training loss:		0.627543
  validation loss:		0.624465
  validation accuracy:		80.65 %
Epoch 610 of 2000 took 0.102s
  training loss:		0.624026
  validation loss:		0.635571
  validation accuracy:		80.65 %
Epoch 611 of 2000 took 0.103s
  training loss:		0.627449
  validation loss:		0.621242
  validation accuracy:		81.52 %
Epoch 612 of 2000 took 0.102s
  training loss:		0.620519
  validation loss:		0.647472
  validation accuracy:		80.43 %
Epoch 613 of 2000 took 0.102s
  training loss:		0.613505
  validation loss:		0.669509
  validation accuracy:		79.24 %
Epoch 614 of 2000 took 0.102s
  training loss:		0.625429
  validation loss:		0.645581
  validation accuracy:		80.22 %
Epoch 615 of 2000 took 0.102s
  training loss:		0.633896
  validation loss:		0.645940
  validation accuracy:		80.98 %
Epoch 616 of 2000 took 0.102s
  training loss:		0.622811
  validation loss:		0.631482
  validation accuracy:		81.52 %
Epoch 617 of 2000 took 0.102s
  training loss:		0.609177
  validation loss:		0.611431
  validation accuracy:		81.52 %
Epoch 618 of 2000 took 0.103s
  training loss:		0.616165
  validation loss:		0.610306
  validation accuracy:		82.07 %
Epoch 619 of 2000 took 0.103s
  training loss:		0.615839
  validation loss:		0.617147
  validation accuracy:		81.41 %
Epoch 620 of 2000 took 0.103s
  training loss:		0.604326
  validation loss:		0.619780
  validation accuracy:		80.87 %
Epoch 621 of 2000 took 0.103s
  training loss:		0.615447
  validation loss:		0.617060
  validation accuracy:		81.52 %
Epoch 622 of 2000 took 0.102s
  training loss:		0.603247
  validation loss:		0.635626
  validation accuracy:		80.98 %
Epoch 623 of 2000 took 0.102s
  training loss:		0.614708
  validation loss:		0.607787
  validation accuracy:		82.28 %
Epoch 624 of 2000 took 0.102s
  training loss:		0.623316
  validation loss:		0.615111
  validation accuracy:		81.74 %
Epoch 625 of 2000 took 0.102s
  training loss:		0.611480
  validation loss:		0.615062
  validation accuracy:		82.07 %
Epoch 626 of 2000 took 0.102s
  training loss:		0.613422
  validation loss:		0.620949
  validation accuracy:		81.09 %
Epoch 627 of 2000 took 0.103s
  training loss:		0.612765
  validation loss:		0.619492
  validation accuracy:		81.74 %
Epoch 628 of 2000 took 0.102s
  training loss:		0.605793
  validation loss:		0.602057
  validation accuracy:		82.17 %
Epoch 629 of 2000 took 0.102s
  training loss:		0.597403
  validation loss:		0.618423
  validation accuracy:		81.09 %
Epoch 630 of 2000 took 0.103s
  training loss:		0.603716
  validation loss:		0.637099
  validation accuracy:		81.20 %
Epoch 631 of 2000 took 0.103s
  training loss:		0.626100
  validation loss:		0.821970
  validation accuracy:		75.22 %
Epoch 632 of 2000 took 0.102s
  training loss:		0.833692
  validation loss:		0.636322
  validation accuracy:		81.63 %
Epoch 633 of 2000 took 0.102s
  training loss:		0.612690
  validation loss:		0.618910
  validation accuracy:		81.96 %
Epoch 634 of 2000 took 0.102s
  training loss:		0.614098
  validation loss:		0.605996
  validation accuracy:		81.74 %
Epoch 635 of 2000 took 0.102s
  training loss:		0.599737
  validation loss:		0.623482
  validation accuracy:		81.20 %
Epoch 636 of 2000 took 0.103s
  training loss:		0.594015
  validation loss:		0.623300
  validation accuracy:		81.20 %
Epoch 637 of 2000 took 0.102s
  training loss:		0.598553
  validation loss:		0.605683
  validation accuracy:		81.20 %
Epoch 638 of 2000 took 0.102s
  training loss:		0.603304
  validation loss:		0.628987
  validation accuracy:		80.87 %
Epoch 639 of 2000 took 0.104s
  training loss:		0.598138
  validation loss:		0.624471
  validation accuracy:		80.54 %
Epoch 640 of 2000 took 0.103s
  training loss:		0.590560
  validation loss:		0.598965
  validation accuracy:		82.17 %
Epoch 641 of 2000 took 0.102s
  training loss:		0.598294
  validation loss:		0.603537
  validation accuracy:		82.17 %
Epoch 642 of 2000 took 0.102s
  training loss:		0.595936
  validation loss:		0.599091
  validation accuracy:		82.17 %
Epoch 643 of 2000 took 0.102s
  training loss:		0.588255
  validation loss:		0.618059
  validation accuracy:		81.74 %
Epoch 644 of 2000 took 0.102s
  training loss:		0.594634
  validation loss:		0.598162
  validation accuracy:		82.93 %
Epoch 645 of 2000 took 0.102s
  training loss:		0.596652
  validation loss:		0.715027
  validation accuracy:		78.37 %
Epoch 646 of 2000 took 0.102s
  training loss:		0.669331
  validation loss:		0.607782
  validation accuracy:		81.41 %
Epoch 647 of 2000 took 0.102s
  training loss:		0.617168
  validation loss:		0.605068
  validation accuracy:		82.83 %
Epoch 648 of 2000 took 0.102s
  training loss:		0.592787
  validation loss:		0.610171
  validation accuracy:		81.20 %
Epoch 649 of 2000 took 0.102s
  training loss:		0.597336
  validation loss:		0.655908
  validation accuracy:		80.11 %
Epoch 650 of 2000 took 0.102s
  training loss:		0.582792
  validation loss:		0.620029
  validation accuracy:		80.65 %
Epoch 651 of 2000 took 0.102s
  training loss:		0.589662
  validation loss:		0.605126
  validation accuracy:		81.41 %
Epoch 652 of 2000 took 0.102s
  training loss:		0.593532
  validation loss:		0.592508
  validation accuracy:		82.39 %
Epoch 653 of 2000 took 0.103s
  training loss:		0.587299
  validation loss:		0.670820
  validation accuracy:		79.35 %
Epoch 654 of 2000 took 0.102s
  training loss:		0.606843
  validation loss:		0.669660
  validation accuracy:		79.78 %
Epoch 655 of 2000 took 0.102s
  training loss:		0.583655
  validation loss:		0.627793
  validation accuracy:		81.41 %
Epoch 656 of 2000 took 0.102s
  training loss:		0.578566
  validation loss:		0.593036
  validation accuracy:		82.61 %
Epoch 657 of 2000 took 0.102s
  training loss:		0.615992
  validation loss:		0.608081
  validation accuracy:		81.30 %
Epoch 658 of 2000 took 0.102s
  training loss:		0.596251
  validation loss:		0.649448
  validation accuracy:		81.20 %
Epoch 659 of 2000 took 0.102s
  training loss:		0.602804
  validation loss:		0.677465
  validation accuracy:		79.57 %
Epoch 660 of 2000 took 0.102s
  training loss:		0.622131
  validation loss:		0.586927
  validation accuracy:		82.83 %
Epoch 661 of 2000 took 0.102s
  training loss:		0.630311
  validation loss:		0.706713
  validation accuracy:		79.35 %
Epoch 662 of 2000 took 0.102s
  training loss:		0.596611
  validation loss:		0.600545
  validation accuracy:		81.85 %
Epoch 663 of 2000 took 0.102s
  training loss:		0.616457
  validation loss:		0.663356
  validation accuracy:		79.67 %
Epoch 664 of 2000 took 0.102s
  training loss:		0.575031
  validation loss:		0.604267
  validation accuracy:		82.39 %
Epoch 665 of 2000 took 0.102s
  training loss:		0.625693
  validation loss:		0.731330
  validation accuracy:		77.93 %
Epoch 666 of 2000 took 0.103s
  training loss:		0.614364
  validation loss:		0.589256
  validation accuracy:		82.50 %
Epoch 667 of 2000 took 0.102s
  training loss:		0.595467
  validation loss:		0.588197
  validation accuracy:		82.50 %
Epoch 668 of 2000 took 0.103s
  training loss:		0.577823
  validation loss:		0.596883
  validation accuracy:		81.63 %
Epoch 669 of 2000 took 0.102s
  training loss:		0.589578
  validation loss:		0.599028
  validation accuracy:		82.39 %
Epoch 670 of 2000 took 0.102s
  training loss:		0.623422
  validation loss:		0.673072
  validation accuracy:		79.35 %
Epoch 671 of 2000 took 0.102s
  training loss:		0.591272
  validation loss:		0.613009
  validation accuracy:		80.98 %
Epoch 672 of 2000 took 0.102s
  training loss:		0.575978
  validation loss:		0.628994
  validation accuracy:		81.52 %
Epoch 673 of 2000 took 0.102s
  training loss:		0.570304
  validation loss:		0.599921
  validation accuracy:		82.28 %
Epoch 674 of 2000 took 0.102s
  training loss:		0.593354
  validation loss:		0.597223
  validation accuracy:		82.72 %
Epoch 675 of 2000 took 0.103s
  training loss:		0.581863
  validation loss:		0.594574
  validation accuracy:		82.72 %
Epoch 676 of 2000 took 0.102s
  training loss:		0.594053
  validation loss:		0.590875
  validation accuracy:		82.93 %
Epoch 677 of 2000 took 0.102s
  training loss:		0.583260
  validation loss:		0.591964
  validation accuracy:		82.83 %
Epoch 678 of 2000 took 0.103s
  training loss:		0.739562
  validation loss:		0.628541
  validation accuracy:		80.33 %
Epoch 679 of 2000 took 0.102s
  training loss:		0.581147
  validation loss:		0.602193
  validation accuracy:		81.30 %
Epoch 680 of 2000 took 0.102s
  training loss:		0.578771
  validation loss:		0.583009
  validation accuracy:		82.28 %
Epoch 681 of 2000 took 0.102s
  training loss:		0.620631
  validation loss:		0.765626
  validation accuracy:		77.17 %
Epoch 682 of 2000 took 0.102s
  training loss:		0.589076
  validation loss:		0.597055
  validation accuracy:		82.39 %
Epoch 683 of 2000 took 0.102s
  training loss:		0.588336
  validation loss:		0.743339
  validation accuracy:		77.17 %
Epoch 684 of 2000 took 0.102s
  training loss:		0.624144
  validation loss:		0.591727
  validation accuracy:		82.83 %
Epoch 685 of 2000 took 0.102s
  training loss:		0.591554
  validation loss:		0.581785
  validation accuracy:		82.28 %
Epoch 686 of 2000 took 0.102s
  training loss:		0.575338
  validation loss:		0.614260
  validation accuracy:		81.96 %
Epoch 687 of 2000 took 0.102s
  training loss:		0.590725
  validation loss:		0.650391
  validation accuracy:		80.54 %
Epoch 688 of 2000 took 0.102s
  training loss:		0.581026
  validation loss:		0.594712
  validation accuracy:		82.83 %
Epoch 689 of 2000 took 0.102s
  training loss:		0.591905
  validation loss:		0.592977
  validation accuracy:		81.52 %
Epoch 690 of 2000 took 0.102s
  training loss:		0.576661
  validation loss:		0.586524
  validation accuracy:		82.50 %
Epoch 691 of 2000 took 0.102s
  training loss:		0.574306
  validation loss:		0.592328
  validation accuracy:		81.20 %
Epoch 692 of 2000 took 0.102s
  training loss:		0.581071
  validation loss:		0.687297
  validation accuracy:		79.24 %
Epoch 693 of 2000 took 0.102s
  training loss:		0.620252
  validation loss:		0.631192
  validation accuracy:		80.87 %
Epoch 694 of 2000 took 0.102s
  training loss:		0.597439
  validation loss:		0.602713
  validation accuracy:		81.41 %
Epoch 695 of 2000 took 0.102s
  training loss:		0.573682
  validation loss:		0.597222
  validation accuracy:		82.61 %
Epoch 696 of 2000 took 0.102s
  training loss:		0.596101
  validation loss:		0.604866
  validation accuracy:		80.98 %
Epoch 697 of 2000 took 0.103s
  training loss:		0.574843
  validation loss:		0.582470
  validation accuracy:		81.85 %
Epoch 698 of 2000 took 0.103s
  training loss:		0.575772
  validation loss:		0.587453
  validation accuracy:		81.96 %
Epoch 699 of 2000 took 0.103s
  training loss:		0.571712
  validation loss:		0.621083
  validation accuracy:		81.74 %
Epoch 700 of 2000 took 0.102s
  training loss:		0.585815
  validation loss:		0.592691
  validation accuracy:		82.83 %
Epoch 701 of 2000 took 0.103s
  training loss:		0.574596
  validation loss:		0.599820
  validation accuracy:		81.30 %
Epoch 702 of 2000 took 0.102s
  training loss:		0.568641
  validation loss:		0.592937
  validation accuracy:		81.41 %
Epoch 703 of 2000 took 0.103s
  training loss:		0.584536
  validation loss:		0.646515
  validation accuracy:		80.22 %
Epoch 704 of 2000 took 0.102s
  training loss:		0.589783
  validation loss:		0.577755
  validation accuracy:		82.93 %
Epoch 705 of 2000 took 0.102s
  training loss:		0.569125
  validation loss:		0.603319
  validation accuracy:		82.07 %
Epoch 706 of 2000 took 0.103s
  training loss:		0.571640
  validation loss:		0.636912
  validation accuracy:		80.43 %
Epoch 707 of 2000 took 0.102s
  training loss:		0.557697
  validation loss:		0.626361
  validation accuracy:		80.33 %
Epoch 708 of 2000 took 0.102s
  training loss:		0.579679
  validation loss:		0.595700
  validation accuracy:		82.93 %
Epoch 709 of 2000 took 0.103s
  training loss:		0.579904
  validation loss:		0.585459
  validation accuracy:		81.96 %
Epoch 710 of 2000 took 0.102s
  training loss:		0.567463
  validation loss:		0.581140
  validation accuracy:		82.83 %
Epoch 711 of 2000 took 0.102s
  training loss:		0.621919
  validation loss:		0.592653
  validation accuracy:		82.83 %
Epoch 712 of 2000 took 0.102s
  training loss:		0.579725
  validation loss:		0.579783
  validation accuracy:		83.15 %
Epoch 713 of 2000 took 0.102s
  training loss:		0.575481
  validation loss:		0.636439
  validation accuracy:		81.09 %
Epoch 714 of 2000 took 0.103s
  training loss:		0.573989
  validation loss:		0.606833
  validation accuracy:		82.07 %
Epoch 715 of 2000 took 0.102s
  training loss:		0.569499
  validation loss:		0.584066
  validation accuracy:		82.83 %
Epoch 716 of 2000 took 0.102s
  training loss:		0.577974
  validation loss:		0.596262
  validation accuracy:		82.07 %
Epoch 717 of 2000 took 0.104s
  training loss:		0.617858
  validation loss:		0.586974
  validation accuracy:		82.17 %
Epoch 718 of 2000 took 0.106s
  training loss:		0.566479
  validation loss:		0.690401
  validation accuracy:		79.35 %
Epoch 719 of 2000 took 0.106s
  training loss:		0.561654
  validation loss:		0.579181
  validation accuracy:		82.07 %
Epoch 720 of 2000 took 0.106s
  training loss:		0.575281
  validation loss:		0.575670
  validation accuracy:		82.07 %
Epoch 721 of 2000 took 0.108s
  training loss:		0.590057
  validation loss:		0.598463
  validation accuracy:		81.30 %
Epoch 722 of 2000 took 0.106s
  training loss:		0.621146
  validation loss:		0.601091
  validation accuracy:		81.96 %
Epoch 723 of 2000 took 0.105s
  training loss:		0.568064
  validation loss:		0.589065
  validation accuracy:		83.37 %
Epoch 724 of 2000 took 0.106s
  training loss:		0.578700
  validation loss:		0.746138
  validation accuracy:		76.85 %
Epoch 725 of 2000 took 0.105s
  training loss:		0.599319
  validation loss:		0.581703
  validation accuracy:		81.96 %
Epoch 726 of 2000 took 0.106s
  training loss:		0.600350
  validation loss:		0.952715
  validation accuracy:		72.28 %
Epoch 727 of 2000 took 0.106s
  training loss:		0.656366
  validation loss:		0.594173
  validation accuracy:		82.17 %
Epoch 728 of 2000 took 0.106s
  training loss:		0.579346
  validation loss:		0.614012
  validation accuracy:		82.17 %
Epoch 729 of 2000 took 0.106s
  training loss:		0.572611
  validation loss:		0.615937
  validation accuracy:		81.52 %
Epoch 730 of 2000 took 0.106s
  training loss:		0.610305
  validation loss:		0.584842
  validation accuracy:		82.50 %
Epoch 731 of 2000 took 0.106s
  training loss:		0.575483
  validation loss:		0.579009
  validation accuracy:		82.50 %
Epoch 732 of 2000 took 0.106s
  training loss:		0.581569
  validation loss:		0.581351
  validation accuracy:		81.85 %
Epoch 733 of 2000 took 0.106s
  training loss:		0.573773
  validation loss:		0.583645
  validation accuracy:		82.28 %
Epoch 734 of 2000 took 0.106s
  training loss:		0.561888
  validation loss:		0.573687
  validation accuracy:		82.72 %
Epoch 735 of 2000 took 0.106s
  training loss:		0.578599
  validation loss:		0.701496
  validation accuracy:		78.37 %
Epoch 736 of 2000 took 0.106s
  training loss:		0.582526
  validation loss:		0.648969
  validation accuracy:		80.87 %
Epoch 737 of 2000 took 0.106s
  training loss:		0.579764
  validation loss:		0.588631
  validation accuracy:		82.39 %
Epoch 738 of 2000 took 0.106s
  training loss:		0.569183
  validation loss:		0.599557
  validation accuracy:		81.30 %
Epoch 739 of 2000 took 0.106s
  training loss:		0.572622
  validation loss:		0.587411
  validation accuracy:		82.50 %
Epoch 740 of 2000 took 0.106s
  training loss:		0.572827
  validation loss:		0.572361
  validation accuracy:		83.04 %
Epoch 741 of 2000 took 0.106s
  training loss:		0.567124
  validation loss:		0.586428
  validation accuracy:		81.85 %
Epoch 742 of 2000 took 0.106s
  training loss:		0.580811
  validation loss:		0.599791
  validation accuracy:		81.85 %
Epoch 743 of 2000 took 0.106s
  training loss:		0.594301
  validation loss:		0.612670
  validation accuracy:		81.20 %
Epoch 744 of 2000 took 0.106s
  training loss:		0.561040
  validation loss:		0.584835
  validation accuracy:		82.83 %
Epoch 745 of 2000 took 0.106s
  training loss:		0.596394
  validation loss:		0.586555
  validation accuracy:		82.28 %
Epoch 746 of 2000 took 0.106s
  training loss:		0.566245
  validation loss:		0.575021
  validation accuracy:		82.72 %
Epoch 747 of 2000 took 0.106s
  training loss:		0.572361
  validation loss:		0.575431
  validation accuracy:		82.07 %
Epoch 748 of 2000 took 0.106s
  training loss:		0.560319
  validation loss:		0.594845
  validation accuracy:		81.74 %
Epoch 749 of 2000 took 0.105s
  training loss:		0.567630
  validation loss:		0.593021
  validation accuracy:		82.50 %
Epoch 750 of 2000 took 0.106s
  training loss:		0.574657
  validation loss:		0.577330
  validation accuracy:		82.93 %
Epoch 751 of 2000 took 0.106s
  training loss:		0.593899
  validation loss:		0.616513
  validation accuracy:		80.98 %
Epoch 752 of 2000 took 0.106s
  training loss:		0.577461
  validation loss:		0.584073
  validation accuracy:		82.83 %
Epoch 753 of 2000 took 0.106s
  training loss:		0.617182
  validation loss:		0.697302
  validation accuracy:		77.61 %
Epoch 754 of 2000 took 0.106s
  training loss:		0.600622
  validation loss:		0.580077
  validation accuracy:		83.15 %
Epoch 755 of 2000 took 0.105s
  training loss:		0.577358
  validation loss:		0.600051
  validation accuracy:		81.96 %
Epoch 756 of 2000 took 0.103s
  training loss:		0.567343
  validation loss:		0.610617
  validation accuracy:		81.63 %
Epoch 757 of 2000 took 0.103s
  training loss:		0.632246
  validation loss:		0.580969
  validation accuracy:		82.28 %
Epoch 758 of 2000 took 0.103s
  training loss:		0.573111
  validation loss:		0.644884
  validation accuracy:		81.09 %
Epoch 759 of 2000 took 0.102s
  training loss:		0.572051
  validation loss:		0.600361
  validation accuracy:		81.85 %
Epoch 760 of 2000 took 0.103s
  training loss:		0.567422
  validation loss:		0.576373
  validation accuracy:		83.04 %
Epoch 761 of 2000 took 0.103s
  training loss:		0.587766
  validation loss:		0.595130
  validation accuracy:		82.07 %
Epoch 762 of 2000 took 0.103s
  training loss:		0.577842
  validation loss:		0.583176
  validation accuracy:		81.85 %
Epoch 763 of 2000 took 0.103s
  training loss:		0.562871
  validation loss:		0.620372
  validation accuracy:		81.52 %
Epoch 764 of 2000 took 0.103s
  training loss:		0.567254
  validation loss:		0.581524
  validation accuracy:		82.17 %
Epoch 765 of 2000 took 0.102s
  training loss:		0.571080
  validation loss:		0.576641
  validation accuracy:		83.37 %
Epoch 766 of 2000 took 0.103s
  training loss:		0.570158
  validation loss:		0.601543
  validation accuracy:		81.85 %
Epoch 767 of 2000 took 0.102s
  training loss:		0.566478
  validation loss:		0.577258
  validation accuracy:		83.26 %
Epoch 768 of 2000 took 0.102s
  training loss:		0.569407
  validation loss:		0.575118
  validation accuracy:		82.72 %
Epoch 769 of 2000 took 0.103s
  training loss:		0.575907
  validation loss:		0.728572
  validation accuracy:		78.59 %
Epoch 770 of 2000 took 0.102s
  training loss:		0.590313
  validation loss:		0.673690
  validation accuracy:		79.13 %
Epoch 771 of 2000 took 0.103s
  training loss:		0.588200
  validation loss:		0.585948
  validation accuracy:		82.50 %
Epoch 772 of 2000 took 0.102s
  training loss:		0.569171
  validation loss:		0.575621
  validation accuracy:		82.50 %
Epoch 773 of 2000 took 0.102s
  training loss:		0.560333
  validation loss:		0.575415
  validation accuracy:		82.61 %
Epoch 774 of 2000 took 0.104s
  training loss:		0.569643
  validation loss:		0.571279
  validation accuracy:		82.61 %
Epoch 775 of 2000 took 0.109s
  training loss:		0.576671
  validation loss:		0.578038
  validation accuracy:		82.61 %
Epoch 776 of 2000 took 0.109s
  training loss:		0.572277
  validation loss:		0.571734
  validation accuracy:		82.72 %
Epoch 777 of 2000 took 0.109s
  training loss:		0.564329
  validation loss:		0.577398
  validation accuracy:		83.26 %
Epoch 778 of 2000 took 0.109s
  training loss:		0.574890
  validation loss:		0.573599
  validation accuracy:		83.26 %
Epoch 779 of 2000 took 0.109s
  training loss:		0.569247
  validation loss:		0.591235
  validation accuracy:		81.96 %
Epoch 780 of 2000 took 0.109s
  training loss:		0.572759
  validation loss:		0.606942
  validation accuracy:		81.63 %
Epoch 781 of 2000 took 0.109s
  training loss:		0.564021
  validation loss:		0.633065
  validation accuracy:		81.09 %
Epoch 782 of 2000 took 0.107s
  training loss:		0.571524
  validation loss:		0.568977
  validation accuracy:		83.26 %
Epoch 783 of 2000 took 0.106s
  training loss:		0.575888
  validation loss:		0.580171
  validation accuracy:		82.72 %
Epoch 784 of 2000 took 0.106s
  training loss:		0.571441
  validation loss:		0.576450
  validation accuracy:		82.61 %
Epoch 785 of 2000 took 0.106s
  training loss:		0.561745
  validation loss:		0.588235
  validation accuracy:		82.39 %
Epoch 786 of 2000 took 0.106s
  training loss:		0.566213
  validation loss:		0.590529
  validation accuracy:		81.96 %
Epoch 787 of 2000 took 0.106s
  training loss:		0.564374
  validation loss:		0.604943
  validation accuracy:		81.63 %
Epoch 788 of 2000 took 0.106s
  training loss:		0.570645
  validation loss:		0.592764
  validation accuracy:		82.07 %
Epoch 789 of 2000 took 0.106s
  training loss:		0.567152
  validation loss:		0.578492
  validation accuracy:		82.39 %
Epoch 790 of 2000 took 0.103s
  training loss:		0.571195
  validation loss:		0.673000
  validation accuracy:		79.35 %
Epoch 791 of 2000 took 0.102s
  training loss:		0.580738
  validation loss:		0.569300
  validation accuracy:		82.61 %
Epoch 792 of 2000 took 0.103s
  training loss:		0.570376
  validation loss:		0.622210
  validation accuracy:		81.09 %
Epoch 793 of 2000 took 0.103s
  training loss:		0.573135
  validation loss:		0.599864
  validation accuracy:		81.85 %
Epoch 794 of 2000 took 0.102s
  training loss:		0.569340
  validation loss:		0.602448
  validation accuracy:		81.74 %
Epoch 795 of 2000 took 0.102s
  training loss:		0.573643
  validation loss:		0.625502
  validation accuracy:		81.09 %
Epoch 796 of 2000 took 0.102s
  training loss:		0.578858
  validation loss:		0.574287
  validation accuracy:		83.26 %
Epoch 797 of 2000 took 0.102s
  training loss:		0.563418
  validation loss:		0.585518
  validation accuracy:		82.93 %
Epoch 798 of 2000 took 0.103s
  training loss:		0.569379
  validation loss:		0.735032
  validation accuracy:		76.63 %
Epoch 799 of 2000 took 0.102s
  training loss:		0.647311
  validation loss:		0.589994
  validation accuracy:		82.61 %
Epoch 800 of 2000 took 0.102s
  training loss:		0.566575
  validation loss:		0.581181
  validation accuracy:		81.96 %
Epoch 801 of 2000 took 0.102s
  training loss:		0.581740
  validation loss:		0.601771
  validation accuracy:		81.63 %
Epoch 802 of 2000 took 0.102s
  training loss:		0.574380
  validation loss:		0.573871
  validation accuracy:		83.37 %
Epoch 803 of 2000 took 0.102s
  training loss:		0.562696
  validation loss:		0.569746
  validation accuracy:		83.04 %
Epoch 804 of 2000 took 0.103s
  training loss:		0.558319
  validation loss:		0.578113
  validation accuracy:		82.83 %
Epoch 805 of 2000 took 0.102s
  training loss:		0.579522
  validation loss:		0.636704
  validation accuracy:		80.65 %
Epoch 806 of 2000 took 0.102s
  training loss:		0.583812
  validation loss:		0.586297
  validation accuracy:		82.39 %
Epoch 807 of 2000 took 0.102s
  training loss:		0.578675
  validation loss:		0.623484
  validation accuracy:		81.41 %
Epoch 808 of 2000 took 0.102s
  training loss:		0.568268
  validation loss:		0.614059
  validation accuracy:		81.63 %
Epoch 809 of 2000 took 0.102s
  training loss:		0.561174
  validation loss:		0.574757
  validation accuracy:		82.93 %
Epoch 810 of 2000 took 0.102s
  training loss:		0.565290
  validation loss:		0.625980
  validation accuracy:		81.09 %
Epoch 811 of 2000 took 0.102s
  training loss:		0.565136
  validation loss:		0.573080
  validation accuracy:		83.37 %
Epoch 812 of 2000 took 0.102s
  training loss:		0.551571
  validation loss:		0.588650
  validation accuracy:		82.39 %
Epoch 813 of 2000 took 0.103s
  training loss:		0.568392
  validation loss:		0.583957
  validation accuracy:		82.61 %
Epoch 814 of 2000 took 0.103s
  training loss:		0.575416
  validation loss:		0.581346
  validation accuracy:		82.61 %
Epoch 815 of 2000 took 0.102s
  training loss:		0.573484
  validation loss:		0.591987
  validation accuracy:		82.39 %
Epoch 816 of 2000 took 0.102s
  training loss:		0.562436
  validation loss:		0.660139
  validation accuracy:		79.57 %
Epoch 817 of 2000 took 0.102s
  training loss:		0.574816
  validation loss:		0.570133
  validation accuracy:		83.48 %
Epoch 818 of 2000 took 0.102s
  training loss:		0.567268
  validation loss:		0.618976
  validation accuracy:		81.30 %
Epoch 819 of 2000 took 0.102s
  training loss:		0.583870
  validation loss:		0.594826
  validation accuracy:		81.96 %
Epoch 820 of 2000 took 0.102s
  training loss:		0.554909
  validation loss:		0.577921
  validation accuracy:		83.15 %
Epoch 821 of 2000 took 0.102s
  training loss:		0.557064
  validation loss:		0.607884
  validation accuracy:		81.41 %
Epoch 822 of 2000 took 0.102s
  training loss:		0.563491
  validation loss:		0.569931
  validation accuracy:		83.04 %
Epoch 823 of 2000 took 0.102s
  training loss:		0.569008
  validation loss:		0.621748
  validation accuracy:		80.87 %
Epoch 824 of 2000 took 0.102s
  training loss:		0.583826
  validation loss:		0.569215
  validation accuracy:		83.15 %
Epoch 825 of 2000 took 0.102s
  training loss:		0.556927
  validation loss:		0.582719
  validation accuracy:		82.83 %
Epoch 826 of 2000 took 0.102s
  training loss:		0.562391
  validation loss:		0.582505
  validation accuracy:		82.50 %
Epoch 827 of 2000 took 0.102s
  training loss:		0.615328
  validation loss:		0.777818
  validation accuracy:		75.76 %
Epoch 828 of 2000 took 0.102s
  training loss:		0.593741
  validation loss:		0.572719
  validation accuracy:		82.83 %
Epoch 829 of 2000 took 0.103s
  training loss:		0.580151
  validation loss:		0.579616
  validation accuracy:		82.28 %
Epoch 830 of 2000 took 0.102s
  training loss:		0.560017
  validation loss:		0.581771
  validation accuracy:		82.72 %
Epoch 831 of 2000 took 0.102s
  training loss:		0.567472
  validation loss:		0.580541
  validation accuracy:		83.26 %
Epoch 832 of 2000 took 0.104s
  training loss:		0.570402
  validation loss:		0.573722
  validation accuracy:		83.15 %
Epoch 833 of 2000 took 0.106s
  training loss:		0.570427
  validation loss:		0.580012
  validation accuracy:		82.28 %
Epoch 834 of 2000 took 0.106s
  training loss:		0.565656
  validation loss:		0.573402
  validation accuracy:		82.50 %
Epoch 835 of 2000 took 0.106s
  training loss:		0.577219
  validation loss:		0.618061
  validation accuracy:		80.98 %
Epoch 836 of 2000 took 0.106s
  training loss:		0.556202
  validation loss:		0.641091
  validation accuracy:		80.22 %
Epoch 837 of 2000 took 0.106s
  training loss:		0.568354
  validation loss:		0.571419
  validation accuracy:		83.26 %
Epoch 838 of 2000 took 0.105s
  training loss:		0.581172
  validation loss:		0.591192
  validation accuracy:		82.39 %
Epoch 839 of 2000 took 0.106s
  training loss:		0.573403
  validation loss:		0.614887
  validation accuracy:		81.74 %
Epoch 840 of 2000 took 0.106s
  training loss:		0.567515
  validation loss:		0.586312
  validation accuracy:		83.04 %
Epoch 841 of 2000 took 0.106s
  training loss:		0.569353
  validation loss:		0.581720
  validation accuracy:		83.26 %
Epoch 842 of 2000 took 0.106s
  training loss:		0.553026
  validation loss:		0.595463
  validation accuracy:		82.83 %
Epoch 843 of 2000 took 0.106s
  training loss:		0.561114
  validation loss:		0.594491
  validation accuracy:		82.39 %
Epoch 844 of 2000 took 0.106s
  training loss:		0.580264
  validation loss:		0.573169
  validation accuracy:		83.04 %
Epoch 845 of 2000 took 0.106s
  training loss:		0.560161
  validation loss:		0.627683
  validation accuracy:		80.33 %
Epoch 846 of 2000 took 0.106s
  training loss:		0.564271
  validation loss:		0.601908
  validation accuracy:		82.39 %
Epoch 847 of 2000 took 0.106s
  training loss:		0.568524
  validation loss:		0.570692
  validation accuracy:		82.72 %
Epoch 848 of 2000 took 0.106s
  training loss:		0.556914
  validation loss:		0.572872
  validation accuracy:		82.72 %
Epoch 849 of 2000 took 0.106s
  training loss:		0.574067
  validation loss:		0.588641
  validation accuracy:		82.83 %
Epoch 850 of 2000 took 0.106s
  training loss:		0.564737
  validation loss:		0.616545
  validation accuracy:		81.52 %
Epoch 851 of 2000 took 0.106s
  training loss:		0.565668
  validation loss:		0.576246
  validation accuracy:		83.26 %
Epoch 852 of 2000 took 0.106s
  training loss:		0.564996
  validation loss:		0.579412
  validation accuracy:		82.93 %
Epoch 853 of 2000 took 0.106s
  training loss:		0.568209
  validation loss:		0.579757
  validation accuracy:		83.26 %
Epoch 854 of 2000 took 0.106s
  training loss:		0.558027
  validation loss:		0.570518
  validation accuracy:		82.93 %
Epoch 855 of 2000 took 0.106s
  training loss:		0.559043
  validation loss:		0.578554
  validation accuracy:		82.93 %
Epoch 856 of 2000 took 0.106s
  training loss:		0.574113
  validation loss:		0.586772
  validation accuracy:		82.72 %
Epoch 857 of 2000 took 0.106s
  training loss:		0.584542
  validation loss:		0.575142
  validation accuracy:		82.93 %
Epoch 858 of 2000 took 0.106s
  training loss:		0.578488
  validation loss:		0.570089
  validation accuracy:		82.93 %
Epoch 859 of 2000 took 0.106s
  training loss:		0.563718
  validation loss:		0.576413
  validation accuracy:		83.15 %
Epoch 860 of 2000 took 0.106s
  training loss:		0.561817
  validation loss:		0.569018
  validation accuracy:		83.26 %
Epoch 861 of 2000 took 0.106s
  training loss:		0.561412
  validation loss:		0.575670
  validation accuracy:		83.04 %
Epoch 862 of 2000 took 0.106s
  training loss:		0.571813
  validation loss:		0.582256
  validation accuracy:		83.59 %
Epoch 863 of 2000 took 0.106s
  training loss:		0.576729
  validation loss:		0.633030
  validation accuracy:		80.54 %
Epoch 864 of 2000 took 0.106s
  training loss:		0.566856
  validation loss:		0.574136
  validation accuracy:		83.04 %
Epoch 865 of 2000 took 0.106s
  training loss:		0.562574
  validation loss:		0.623234
  validation accuracy:		80.87 %
Epoch 866 of 2000 took 0.106s
  training loss:		0.560218
  validation loss:		0.575152
  validation accuracy:		83.04 %
Epoch 867 of 2000 took 0.107s
  training loss:		0.559927
  validation loss:		0.568710
  validation accuracy:		83.48 %
Epoch 868 of 2000 took 0.106s
  training loss:		0.553347
  validation loss:		0.609260
  validation accuracy:		82.28 %
Epoch 869 of 2000 took 0.106s
  training loss:		0.563876
  validation loss:		0.577035
  validation accuracy:		83.70 %
Epoch 870 of 2000 took 0.104s
  training loss:		0.570912
  validation loss:		0.572254
  validation accuracy:		83.04 %
Epoch 871 of 2000 took 0.103s
  training loss:		0.566656
  validation loss:		0.577838
  validation accuracy:		83.15 %
Epoch 872 of 2000 took 0.102s
  training loss:		0.572048
  validation loss:		0.599755
  validation accuracy:		82.93 %
Epoch 873 of 2000 took 0.102s
  training loss:		0.571271
  validation loss:		0.570559
  validation accuracy:		83.48 %
Epoch 874 of 2000 took 0.102s
  training loss:		0.573465
  validation loss:		0.587648
  validation accuracy:		83.04 %
Epoch 875 of 2000 took 0.103s
  training loss:		0.563790
  validation loss:		0.573105
  validation accuracy:		83.48 %
Epoch 876 of 2000 took 0.102s
  training loss:		0.565531
  validation loss:		0.632834
  validation accuracy:		80.76 %
Epoch 877 of 2000 took 0.102s
  training loss:		0.562647
  validation loss:		0.570180
  validation accuracy:		83.37 %
Epoch 878 of 2000 took 0.102s
  training loss:		0.578220
  validation loss:		0.577949
  validation accuracy:		82.93 %
Epoch 879 of 2000 took 0.102s
  training loss:		0.574849
  validation loss:		0.596600
  validation accuracy:		83.15 %
Epoch 880 of 2000 took 0.102s
  training loss:		0.565716
  validation loss:		0.581042
  validation accuracy:		82.83 %
Epoch 881 of 2000 took 0.103s
  training loss:		0.573134
  validation loss:		0.571509
  validation accuracy:		83.15 %
Epoch 882 of 2000 took 0.102s
  training loss:		0.582956
  validation loss:		0.588832
  validation accuracy:		81.96 %
Epoch 883 of 2000 took 0.102s
  training loss:		0.565765
  validation loss:		0.587218
  validation accuracy:		82.39 %
Epoch 884 of 2000 took 0.102s
  training loss:		0.565924
  validation loss:		0.583135
  validation accuracy:		83.04 %
Epoch 885 of 2000 took 0.102s
  training loss:		0.568482
  validation loss:		0.566941
  validation accuracy:		83.26 %
Epoch 886 of 2000 took 0.102s
  training loss:		0.576728
  validation loss:		0.644153
  validation accuracy:		80.33 %
Epoch 887 of 2000 took 0.102s
  training loss:		0.544649
  validation loss:		0.580360
  validation accuracy:		82.93 %
Epoch 888 of 2000 took 0.102s
  training loss:		0.564299
  validation loss:		0.580377
  validation accuracy:		83.37 %
Epoch 889 of 2000 took 0.102s
  training loss:		0.567062
  validation loss:		0.590864
  validation accuracy:		83.37 %
Epoch 890 of 2000 took 0.102s
  training loss:		0.562799
  validation loss:		0.630452
  validation accuracy:		80.76 %
Epoch 891 of 2000 took 0.103s
  training loss:		0.578085
  validation loss:		0.580111
  validation accuracy:		82.93 %
Epoch 892 of 2000 took 0.102s
  training loss:		0.565903
  validation loss:		0.567771
  validation accuracy:		82.93 %
Epoch 893 of 2000 took 0.102s
  training loss:		0.559134
  validation loss:		0.598171
  validation accuracy:		82.72 %
Epoch 894 of 2000 took 0.102s
  training loss:		0.555263
  validation loss:		0.614328
  validation accuracy:		81.41 %
Epoch 895 of 2000 took 0.102s
  training loss:		0.558423
  validation loss:		0.626343
  validation accuracy:		80.87 %
Epoch 896 of 2000 took 0.102s
  training loss:		0.569804
  validation loss:		0.568317
  validation accuracy:		83.48 %
Epoch 897 of 2000 took 0.102s
  training loss:		0.558647
  validation loss:		0.589588
  validation accuracy:		82.72 %
Epoch 898 of 2000 took 0.102s
  training loss:		0.570852
  validation loss:		0.588625
  validation accuracy:		82.50 %
Epoch 899 of 2000 took 0.102s
  training loss:		0.560777
  validation loss:		0.613663
  validation accuracy:		82.39 %
Epoch 900 of 2000 took 0.103s
  training loss:		0.560070
  validation loss:		0.601470
  validation accuracy:		81.96 %
Epoch 901 of 2000 took 0.102s
  training loss:		0.563327
  validation loss:		0.584701
  validation accuracy:		83.26 %
Epoch 902 of 2000 took 0.102s
  training loss:		0.570562
  validation loss:		0.605280
  validation accuracy:		82.50 %
Epoch 903 of 2000 took 0.102s
  training loss:		0.568983
  validation loss:		0.574777
  validation accuracy:		83.04 %
Epoch 904 of 2000 took 0.102s
  training loss:		0.557260
  validation loss:		0.577876
  validation accuracy:		82.93 %
Epoch 905 of 2000 took 0.102s
  training loss:		0.571591
  validation loss:		0.580805
  validation accuracy:		83.37 %
Epoch 906 of 2000 took 0.103s
  training loss:		0.566465
  validation loss:		0.595353
  validation accuracy:		83.59 %
Epoch 907 of 2000 took 0.102s
  training loss:		0.571542
  validation loss:		0.601427
  validation accuracy:		82.72 %
Epoch 908 of 2000 took 0.103s
  training loss:		0.554685
  validation loss:		0.576306
  validation accuracy:		83.37 %
Epoch 909 of 2000 took 0.103s
  training loss:		0.565783
  validation loss:		0.624540
  validation accuracy:		80.98 %
Epoch 910 of 2000 took 0.103s
  training loss:		0.568686
  validation loss:		0.592943
  validation accuracy:		83.04 %
Epoch 911 of 2000 took 0.102s
  training loss:		0.572850
  validation loss:		0.599881
  validation accuracy:		82.28 %
Epoch 912 of 2000 took 0.102s
  training loss:		0.572048
  validation loss:		0.584406
  validation accuracy:		82.93 %
Epoch 913 of 2000 took 0.102s
  training loss:		0.558434
  validation loss:		0.580399
  validation accuracy:		83.15 %
Epoch 914 of 2000 took 0.102s
  training loss:		0.557944
  validation loss:		0.572933
  validation accuracy:		83.04 %
Epoch 915 of 2000 took 0.102s
  training loss:		0.564273
  validation loss:		0.581287
  validation accuracy:		83.26 %
Epoch 916 of 2000 took 0.102s
  training loss:		0.558862
  validation loss:		0.634620
  validation accuracy:		80.76 %
Epoch 917 of 2000 took 0.102s
  training loss:		0.554850
  validation loss:		0.609764
  validation accuracy:		81.74 %
Epoch 918 of 2000 took 0.102s
  training loss:		0.583286
  validation loss:		0.647361
  validation accuracy:		80.33 %
Epoch 919 of 2000 took 0.102s
  training loss:		0.565746
  validation loss:		0.574715
  validation accuracy:		82.39 %
Epoch 920 of 2000 took 0.102s
  training loss:		0.564937
  validation loss:		0.571397
  validation accuracy:		82.93 %
Epoch 921 of 2000 took 0.102s
  training loss:		0.551519
  validation loss:		0.590119
  validation accuracy:		82.93 %
Epoch 922 of 2000 took 0.102s
  training loss:		0.568058
  validation loss:		0.578814
  validation accuracy:		83.48 %
Epoch 923 of 2000 took 0.102s
  training loss:		0.553568
  validation loss:		0.599738
  validation accuracy:		82.39 %
Epoch 924 of 2000 took 0.102s
  training loss:		0.574708
  validation loss:		0.581677
  validation accuracy:		83.26 %
Epoch 925 of 2000 took 0.102s
  training loss:		0.569187
  validation loss:		0.667445
  validation accuracy:		80.22 %
Epoch 926 of 2000 took 0.102s
  training loss:		0.561978
  validation loss:		0.598871
  validation accuracy:		82.07 %
Epoch 927 of 2000 took 0.102s
  training loss:		0.567270
  validation loss:		0.581684
  validation accuracy:		83.37 %
Epoch 928 of 2000 took 0.102s
  training loss:		0.559926
  validation loss:		0.619388
  validation accuracy:		81.09 %
Epoch 929 of 2000 took 0.103s
  training loss:		0.561321
  validation loss:		0.609101
  validation accuracy:		81.85 %
Epoch 930 of 2000 took 0.103s
  training loss:		0.572576
  validation loss:		0.608753
  validation accuracy:		81.85 %
Epoch 931 of 2000 took 0.105s
  training loss:		0.567379
  validation loss:		0.582544
  validation accuracy:		82.72 %
Epoch 932 of 2000 took 0.103s
  training loss:		0.572605
  validation loss:		0.573861
  validation accuracy:		82.83 %
Epoch 933 of 2000 took 0.102s
  training loss:		0.544393
  validation loss:		0.570371
  validation accuracy:		83.04 %
Epoch 934 of 2000 took 0.102s
  training loss:		0.582515
  validation loss:		0.689550
  validation accuracy:		78.80 %
Epoch 935 of 2000 took 0.102s
  training loss:		0.568820
  validation loss:		0.578836
  validation accuracy:		83.37 %
Epoch 936 of 2000 took 0.102s
  training loss:		0.555023
  validation loss:		0.591970
  validation accuracy:		82.72 %
Epoch 937 of 2000 took 0.102s
  training loss:		0.573099
  validation loss:		0.646184
  validation accuracy:		79.89 %
Epoch 938 of 2000 took 0.102s
  training loss:		0.579992
  validation loss:		0.621951
  validation accuracy:		81.52 %
Epoch 939 of 2000 took 0.102s
  training loss:		0.566431
  validation loss:		0.589020
  validation accuracy:		83.59 %
Epoch 940 of 2000 took 0.103s
  training loss:		0.556344
  validation loss:		0.636178
  validation accuracy:		80.33 %
Epoch 941 of 2000 took 0.102s
  training loss:		0.553331
  validation loss:		0.576597
  validation accuracy:		82.93 %
Epoch 942 of 2000 took 0.102s
  training loss:		0.557402
  validation loss:		0.621696
  validation accuracy:		81.30 %
Epoch 943 of 2000 took 0.103s
  training loss:		0.572669
  validation loss:		0.578659
  validation accuracy:		82.17 %
Epoch 944 of 2000 took 0.102s
  training loss:		0.570111
  validation loss:		0.580999
  validation accuracy:		83.37 %
Epoch 945 of 2000 took 0.102s
  training loss:		0.606847
  validation loss:		0.673521
  validation accuracy:		79.67 %
Epoch 946 of 2000 took 0.102s
  training loss:		0.565086
  validation loss:		0.641682
  validation accuracy:		80.54 %
Epoch 947 of 2000 took 0.102s
  training loss:		0.564517
  validation loss:		0.587761
  validation accuracy:		83.70 %
Epoch 948 of 2000 took 0.102s
  training loss:		0.567952
  validation loss:		0.583387
  validation accuracy:		83.48 %
Epoch 949 of 2000 took 0.102s
  training loss:		0.569636
  validation loss:		0.590069
  validation accuracy:		82.39 %
Epoch 950 of 2000 took 0.102s
  training loss:		0.559119
  validation loss:		0.573232
  validation accuracy:		82.83 %
Epoch 951 of 2000 took 0.102s
  training loss:		0.568988
  validation loss:		0.603369
  validation accuracy:		81.85 %
Epoch 952 of 2000 took 0.102s
  training loss:		0.563652
  validation loss:		0.592954
  validation accuracy:		83.70 %
Epoch 953 of 2000 took 0.102s
  training loss:		0.554156
  validation loss:		0.632352
  validation accuracy:		81.20 %
Epoch 954 of 2000 took 0.102s
  training loss:		0.561279
  validation loss:		0.615395
  validation accuracy:		81.52 %
Epoch 955 of 2000 took 0.102s
  training loss:		0.556186
  validation loss:		0.579733
  validation accuracy:		83.48 %
Epoch 956 of 2000 took 0.102s
  training loss:		0.573379
  validation loss:		0.639277
  validation accuracy:		80.43 %
Epoch 957 of 2000 took 0.102s
  training loss:		0.564802
  validation loss:		0.607084
  validation accuracy:		81.85 %
Epoch 958 of 2000 took 0.102s
  training loss:		0.569300
  validation loss:		0.596363
  validation accuracy:		82.50 %
Epoch 959 of 2000 took 0.103s
  training loss:		0.560033
  validation loss:		0.618351
  validation accuracy:		81.20 %
Epoch 960 of 2000 took 0.102s
  training loss:		0.559335
  validation loss:		0.575749
  validation accuracy:		83.04 %
Epoch 961 of 2000 took 0.102s
  training loss:		0.558940
  validation loss:		0.586181
  validation accuracy:		82.93 %
Epoch 962 of 2000 took 0.102s
  training loss:		0.561814
  validation loss:		0.572454
  validation accuracy:		83.48 %
Epoch 963 of 2000 took 0.102s
  training loss:		0.571590
  validation loss:		0.578774
  validation accuracy:		83.37 %
Epoch 964 of 2000 took 0.102s
  training loss:		0.573500
  validation loss:		0.586740
  validation accuracy:		83.15 %
Epoch 965 of 2000 took 0.102s
  training loss:		0.558998
  validation loss:		0.580127
  validation accuracy:		83.48 %
Epoch 966 of 2000 took 0.102s
  training loss:		0.563950
  validation loss:		0.602279
  validation accuracy:		82.72 %
Epoch 967 of 2000 took 0.102s
  training loss:		0.570959
  validation loss:		0.610456
  validation accuracy:		81.96 %
Epoch 968 of 2000 took 0.102s
  training loss:		0.554690
  validation loss:		0.665823
  validation accuracy:		80.00 %
Epoch 969 of 2000 took 0.102s
  training loss:		0.565298
  validation loss:		0.611173
  validation accuracy:		81.52 %
Epoch 970 of 2000 took 0.102s
  training loss:		0.568671
  validation loss:		0.589249
  validation accuracy:		82.72 %
Epoch 971 of 2000 took 0.102s
  training loss:		0.558799
  validation loss:		0.576783
  validation accuracy:		82.39 %
Epoch 972 of 2000 took 0.102s
  training loss:		0.566924
  validation loss:		0.572038
  validation accuracy:		83.37 %
Epoch 973 of 2000 took 0.102s
  training loss:		0.559987
  validation loss:		0.634173
  validation accuracy:		81.09 %
Epoch 974 of 2000 took 0.102s
  training loss:		0.556892
  validation loss:		0.583818
  validation accuracy:		83.70 %
Epoch 975 of 2000 took 0.103s
  training loss:		0.574721
  validation loss:		0.575241
  validation accuracy:		83.04 %
Epoch 976 of 2000 took 0.102s
  training loss:		0.548465
  validation loss:		0.573877
  validation accuracy:		83.37 %
Epoch 977 of 2000 took 0.103s
  training loss:		0.555170
  validation loss:		0.577347
  validation accuracy:		82.83 %
Epoch 978 of 2000 took 0.103s
  training loss:		0.562809
  validation loss:		0.585974
  validation accuracy:		83.48 %
Epoch 979 of 2000 took 0.103s
  training loss:		0.575819
  validation loss:		0.613732
  validation accuracy:		82.17 %
Epoch 980 of 2000 took 0.102s
  training loss:		0.555175
  validation loss:		0.580067
  validation accuracy:		82.72 %
Epoch 981 of 2000 took 0.102s
  training loss:		0.556602
  validation loss:		0.608270
  validation accuracy:		82.72 %
Epoch 982 of 2000 took 0.102s
  training loss:		0.567233
  validation loss:		0.608158
  validation accuracy:		82.61 %
Epoch 983 of 2000 took 0.102s
  training loss:		0.573238
  validation loss:		0.578768
  validation accuracy:		83.48 %
Epoch 984 of 2000 took 0.103s
  training loss:		0.566692
  validation loss:		0.581981
  validation accuracy:		82.93 %
Epoch 985 of 2000 took 0.102s
  training loss:		0.575675
  validation loss:		0.576465
  validation accuracy:		83.04 %
Epoch 986 of 2000 took 0.102s
  training loss:		0.561613
  validation loss:		0.595333
  validation accuracy:		82.39 %
Epoch 987 of 2000 took 0.102s
  training loss:		0.571630
  validation loss:		0.601346
  validation accuracy:		83.04 %
Epoch 988 of 2000 took 0.103s
  training loss:		0.565727
  validation loss:		0.568497
  validation accuracy:		83.04 %
Epoch 989 of 2000 took 0.102s
  training loss:		0.565547
  validation loss:		0.582689
  validation accuracy:		82.83 %
Epoch 990 of 2000 took 0.102s
  training loss:		0.556373
  validation loss:		0.638074
  validation accuracy:		80.22 %
Epoch 991 of 2000 took 0.102s
  training loss:		0.556719
  validation loss:		0.576892
  validation accuracy:		82.93 %
Epoch 992 of 2000 took 0.102s
  training loss:		0.554752
  validation loss:		0.606140
  validation accuracy:		83.26 %
Epoch 993 of 2000 took 0.102s
  training loss:		0.560660
  validation loss:		0.603660
  validation accuracy:		83.48 %
Epoch 994 of 2000 took 0.102s
  training loss:		0.554177
  validation loss:		0.601719
  validation accuracy:		82.28 %
Epoch 995 of 2000 took 0.102s
  training loss:		0.556930
  validation loss:		0.574876
  validation accuracy:		82.72 %
Epoch 996 of 2000 took 0.102s
  training loss:		0.563999
  validation loss:		0.623992
  validation accuracy:		81.96 %
Epoch 997 of 2000 took 0.102s
  training loss:		0.566647
  validation loss:		0.608805
  validation accuracy:		82.17 %
Epoch 998 of 2000 took 0.102s
  training loss:		0.563876
  validation loss:		0.588952
  validation accuracy:		83.59 %
Epoch 999 of 2000 took 0.102s
  training loss:		0.549553
  validation loss:		0.588996
  validation accuracy:		83.37 %
Epoch 1000 of 2000 took 0.102s
  training loss:		0.569871
  validation loss:		0.605308
  validation accuracy:		82.17 %
Epoch 1001 of 2000 took 0.102s
  training loss:		0.563076
  validation loss:		0.627258
  validation accuracy:		81.09 %
Epoch 1002 of 2000 took 0.102s
  training loss:		0.565051
  validation loss:		0.587991
  validation accuracy:		82.93 %
Epoch 1003 of 2000 took 0.102s
  training loss:		0.573755
  validation loss:		0.618075
  validation accuracy:		81.96 %
Epoch 1004 of 2000 took 0.102s
  training loss:		0.568791
  validation loss:		0.571497
  validation accuracy:		83.15 %
Epoch 1005 of 2000 took 0.102s
  training loss:		0.571692
  validation loss:		0.590568
  validation accuracy:		83.26 %
Epoch 1006 of 2000 took 0.103s
  training loss:		0.553852
  validation loss:		0.621309
  validation accuracy:		81.96 %
Epoch 1007 of 2000 took 0.102s
  training loss:		0.554758
  validation loss:		0.611103
  validation accuracy:		81.96 %
Epoch 1008 of 2000 took 0.103s
  training loss:		0.566068
  validation loss:		0.593474
  validation accuracy:		83.37 %
Epoch 1009 of 2000 took 0.102s
  training loss:		0.554152
  validation loss:		0.583988
  validation accuracy:		83.59 %
Epoch 1010 of 2000 took 0.102s
  training loss:		0.555289
  validation loss:		0.589134
  validation accuracy:		83.26 %
Epoch 1011 of 2000 took 0.102s
  training loss:		0.553036
  validation loss:		0.586147
  validation accuracy:		83.26 %
Epoch 1012 of 2000 took 0.102s
  training loss:		0.552604
  validation loss:		0.608700
  validation accuracy:		81.74 %
Epoch 1013 of 2000 took 0.102s
  training loss:		0.564542
  validation loss:		0.584515
  validation accuracy:		82.93 %
Epoch 1014 of 2000 took 0.102s
  training loss:		0.569167
  validation loss:		0.577848
  validation accuracy:		83.26 %
Epoch 1015 of 2000 took 0.102s
  training loss:		0.557950
  validation loss:		0.602145
  validation accuracy:		82.93 %
Epoch 1016 of 2000 took 0.102s
  training loss:		0.546181
  validation loss:		0.577446
  validation accuracy:		83.04 %
Epoch 1017 of 2000 took 0.102s
  training loss:		0.554903
  validation loss:		0.602777
  validation accuracy:		82.07 %
Epoch 1018 of 2000 took 0.104s
  training loss:		0.567249
  validation loss:		0.579550
  validation accuracy:		83.37 %
Epoch 1019 of 2000 took 0.104s
  training loss:		0.556514
  validation loss:		0.574101
  validation accuracy:		82.83 %
Epoch 1020 of 2000 took 0.103s
  training loss:		0.561262
  validation loss:		0.614820
  validation accuracy:		81.74 %
Epoch 1021 of 2000 took 0.103s
  training loss:		0.563657
  validation loss:		0.576025
  validation accuracy:		82.83 %
Epoch 1022 of 2000 took 0.102s
  training loss:		0.557669
  validation loss:		0.573253
  validation accuracy:		83.04 %
Epoch 1023 of 2000 took 0.103s
  training loss:		0.568231
  validation loss:		0.571232
  validation accuracy:		83.15 %
Epoch 1024 of 2000 took 0.102s
  training loss:		0.555481
  validation loss:		0.573397
  validation accuracy:		83.37 %
Epoch 1025 of 2000 took 0.102s
  training loss:		0.565678
  validation loss:		0.600154
  validation accuracy:		82.07 %
Epoch 1026 of 2000 took 0.103s
  training loss:		0.563132
  validation loss:		0.588226
  validation accuracy:		82.83 %
Epoch 1027 of 2000 took 0.102s
  training loss:		0.555175
  validation loss:		0.620687
  validation accuracy:		81.09 %
Epoch 1028 of 2000 took 0.102s
  training loss:		0.568749
  validation loss:		0.579271
  validation accuracy:		82.93 %
Epoch 1029 of 2000 took 0.102s
  training loss:		0.555353
  validation loss:		0.594426
  validation accuracy:		82.50 %
Epoch 1030 of 2000 took 0.102s
  training loss:		0.558128
  validation loss:		0.567630
  validation accuracy:		82.93 %
Epoch 1031 of 2000 took 0.102s
  training loss:		0.572238
  validation loss:		0.608568
  validation accuracy:		81.63 %
Epoch 1032 of 2000 took 0.102s
  training loss:		0.561183
  validation loss:		0.601123
  validation accuracy:		81.74 %
Epoch 1033 of 2000 took 0.102s
  training loss:		0.565182
  validation loss:		0.617889
  validation accuracy:		81.96 %
Epoch 1034 of 2000 took 0.102s
  training loss:		0.559159
  validation loss:		0.626249
  validation accuracy:		81.09 %
Epoch 1035 of 2000 took 0.102s
  training loss:		0.564423
  validation loss:		0.659913
  validation accuracy:		79.57 %
Epoch 1036 of 2000 took 0.102s
  training loss:		0.572341
  validation loss:		0.584902
  validation accuracy:		83.37 %
Epoch 1037 of 2000 took 0.102s
  training loss:		0.557183
  validation loss:		0.578903
  validation accuracy:		83.37 %
Epoch 1038 of 2000 took 0.102s
  training loss:		0.569316
  validation loss:		0.584137
  validation accuracy:		82.83 %
Epoch 1039 of 2000 took 0.102s
  training loss:		0.564571
  validation loss:		0.615411
  validation accuracy:		82.07 %
Epoch 1040 of 2000 took 0.102s
  training loss:		0.559197
  validation loss:		0.577817
  validation accuracy:		83.04 %
Epoch 1041 of 2000 took 0.103s
  training loss:		0.563438
  validation loss:		0.618100
  validation accuracy:		81.20 %
Epoch 1042 of 2000 took 0.102s
  training loss:		0.549586
  validation loss:		0.588911
  validation accuracy:		82.39 %
Epoch 1043 of 2000 took 0.102s
  training loss:		0.561256
  validation loss:		0.587512
  validation accuracy:		83.48 %
Epoch 1044 of 2000 took 0.102s
  training loss:		0.552990
  validation loss:		0.574192
  validation accuracy:		82.93 %
Epoch 1045 of 2000 took 0.102s
  training loss:		0.548848
  validation loss:		0.584067
  validation accuracy:		82.93 %
Epoch 1046 of 2000 took 0.102s
  training loss:		0.550957
  validation loss:		0.580446
  validation accuracy:		83.70 %
Epoch 1047 of 2000 took 0.103s
  training loss:		0.553493
  validation loss:		0.585688
  validation accuracy:		82.39 %
Epoch 1048 of 2000 took 0.102s
  training loss:		0.570211
  validation loss:		0.674528
  validation accuracy:		79.35 %
Epoch 1049 of 2000 took 0.102s
  training loss:		0.561719
  validation loss:		0.580729
  validation accuracy:		83.37 %
Epoch 1050 of 2000 took 0.102s
  training loss:		0.559441
  validation loss:		0.584831
  validation accuracy:		83.15 %
Epoch 1051 of 2000 took 0.102s
  training loss:		0.554994
  validation loss:		0.591912
  validation accuracy:		83.26 %
Epoch 1052 of 2000 took 0.102s
  training loss:		0.551572
  validation loss:		0.586509
  validation accuracy:		82.28 %
Epoch 1053 of 2000 took 0.102s
  training loss:		0.560173
  validation loss:		0.599267
  validation accuracy:		82.28 %
Epoch 1054 of 2000 took 0.102s
  training loss:		0.572710
  validation loss:		0.650883
  validation accuracy:		79.89 %
Epoch 1055 of 2000 took 0.102s
  training loss:		0.581977
  validation loss:		0.574528
  validation accuracy:		83.15 %
Epoch 1056 of 2000 took 0.102s
  training loss:		0.562650
  validation loss:		0.579242
  validation accuracy:		83.48 %
Epoch 1057 of 2000 took 0.102s
  training loss:		0.551733
  validation loss:		0.605673
  validation accuracy:		81.85 %
Epoch 1058 of 2000 took 0.102s
  training loss:		0.572638
  validation loss:		0.593569
  validation accuracy:		83.04 %
Epoch 1059 of 2000 took 0.102s
  training loss:		0.554356
  validation loss:		0.616486
  validation accuracy:		81.30 %
Epoch 1060 of 2000 took 0.102s
  training loss:		0.554284
  validation loss:		0.581956
  validation accuracy:		83.04 %
Epoch 1061 of 2000 took 0.102s
  training loss:		0.552911
  validation loss:		0.589785
  validation accuracy:		82.72 %
Epoch 1062 of 2000 took 0.103s
  training loss:		0.571217
  validation loss:		0.574649
  validation accuracy:		83.37 %
Epoch 1063 of 2000 took 0.102s
  training loss:		0.561721
  validation loss:		0.607715
  validation accuracy:		81.96 %
Epoch 1064 of 2000 took 0.103s
  training loss:		0.576174
  validation loss:		0.588233
  validation accuracy:		83.26 %
Epoch 1065 of 2000 took 0.106s
  training loss:		0.558392
  validation loss:		0.646680
  validation accuracy:		80.00 %
Epoch 1066 of 2000 took 0.106s
  training loss:		0.568896
  validation loss:		0.596416
  validation accuracy:		83.15 %
Epoch 1067 of 2000 took 0.106s
  training loss:		0.562776
  validation loss:		0.623344
  validation accuracy:		80.65 %
Epoch 1068 of 2000 took 0.105s
  training loss:		0.553833
  validation loss:		0.574650
  validation accuracy:		83.48 %
Epoch 1069 of 2000 took 0.105s
  training loss:		0.569409
  validation loss:		0.582035
  validation accuracy:		82.83 %
Epoch 1070 of 2000 took 0.105s
  training loss:		0.565030
  validation loss:		0.598583
  validation accuracy:		82.28 %
Epoch 1071 of 2000 took 0.105s
  training loss:		0.569004
  validation loss:		0.590712
  validation accuracy:		83.04 %
Epoch 1072 of 2000 took 0.105s
  training loss:		0.560335
  validation loss:		0.597561
  validation accuracy:		83.04 %
Epoch 1073 of 2000 took 0.106s
  training loss:		0.561890
  validation loss:		0.592073
  validation accuracy:		83.04 %
Epoch 1074 of 2000 took 0.106s
  training loss:		0.552884
  validation loss:		0.573016
  validation accuracy:		83.37 %
Epoch 1075 of 2000 took 0.106s
  training loss:		0.551608
  validation loss:		0.571863
  validation accuracy:		83.15 %
Epoch 1076 of 2000 took 0.106s
  training loss:		0.550791
  validation loss:		0.588370
  validation accuracy:		82.72 %
Epoch 1077 of 2000 took 0.106s
  training loss:		0.578316
  validation loss:		0.574190
  validation accuracy:		83.37 %
Epoch 1078 of 2000 took 0.105s
  training loss:		0.562244
  validation loss:		0.566114
  validation accuracy:		83.59 %
Epoch 1079 of 2000 took 0.105s
  training loss:		0.565712
  validation loss:		0.600365
  validation accuracy:		82.72 %
Epoch 1080 of 2000 took 0.105s
  training loss:		0.561293
  validation loss:		0.573501
  validation accuracy:		83.37 %
Epoch 1081 of 2000 took 0.105s
  training loss:		0.561160
  validation loss:		0.611145
  validation accuracy:		81.52 %
Epoch 1082 of 2000 took 0.106s
  training loss:		0.569310
  validation loss:		0.571642
  validation accuracy:		82.93 %
Epoch 1083 of 2000 took 0.106s
  training loss:		0.569815
  validation loss:		0.581831
  validation accuracy:		83.15 %
Epoch 1084 of 2000 took 0.106s
  training loss:		0.562537
  validation loss:		0.604219
  validation accuracy:		82.72 %
Epoch 1085 of 2000 took 0.106s
  training loss:		0.566347
  validation loss:		0.581090
  validation accuracy:		83.59 %
Epoch 1086 of 2000 took 0.106s
  training loss:		0.551442
  validation loss:		0.595715
  validation accuracy:		82.83 %
Epoch 1087 of 2000 took 0.106s
  training loss:		0.548066
  validation loss:		0.573199
  validation accuracy:		83.37 %
Epoch 1088 of 2000 took 0.105s
  training loss:		0.556451
  validation loss:		0.583633
  validation accuracy:		83.37 %
Epoch 1089 of 2000 took 0.105s
  training loss:		0.556121
  validation loss:		0.568277
  validation accuracy:		83.26 %
Epoch 1090 of 2000 took 0.105s
  training loss:		0.550902
  validation loss:		0.588785
  validation accuracy:		83.37 %
Epoch 1091 of 2000 took 0.105s
  training loss:		0.560712
  validation loss:		0.613691
  validation accuracy:		81.74 %
Epoch 1092 of 2000 took 0.106s
  training loss:		0.556240
  validation loss:		0.598175
  validation accuracy:		81.85 %
Epoch 1093 of 2000 took 0.106s
  training loss:		0.557314
  validation loss:		0.587101
  validation accuracy:		83.04 %
Epoch 1094 of 2000 took 0.106s
  training loss:		0.562770
  validation loss:		0.591592
  validation accuracy:		83.26 %
Epoch 1095 of 2000 took 0.106s
  training loss:		0.560945
  validation loss:		0.591055
  validation accuracy:		83.59 %
Epoch 1096 of 2000 took 0.106s
  training loss:		0.563919
  validation loss:		0.584281
  validation accuracy:		83.15 %
Epoch 1097 of 2000 took 0.105s
  training loss:		0.559498
  validation loss:		0.579043
  validation accuracy:		82.93 %
Epoch 1098 of 2000 took 0.105s
  training loss:		0.552890
  validation loss:		0.578499
  validation accuracy:		83.26 %
Epoch 1099 of 2000 took 0.105s
  training loss:		0.557888
  validation loss:		0.569739
  validation accuracy:		82.72 %
Epoch 1100 of 2000 took 0.106s
  training loss:		0.565580
  validation loss:		0.617485
  validation accuracy:		82.07 %
Epoch 1101 of 2000 took 0.105s
  training loss:		0.565389
  validation loss:		0.575925
  validation accuracy:		83.15 %
Epoch 1102 of 2000 took 0.106s
  training loss:		0.558182
  validation loss:		0.585361
  validation accuracy:		82.72 %
Epoch 1103 of 2000 took 0.106s
  training loss:		0.562173
  validation loss:		0.593163
  validation accuracy:		83.48 %
Epoch 1104 of 2000 took 0.106s
  training loss:		0.558519
  validation loss:		0.644724
  validation accuracy:		80.11 %
Epoch 1105 of 2000 took 0.106s
  training loss:		0.567080
  validation loss:		0.632382
  validation accuracy:		81.09 %
Epoch 1106 of 2000 took 0.106s
  training loss:		0.573137
  validation loss:		0.576424
  validation accuracy:		82.50 %
Epoch 1107 of 2000 took 0.106s
  training loss:		0.551105
  validation loss:		0.585570
  validation accuracy:		82.93 %
Epoch 1108 of 2000 took 0.106s
  training loss:		0.557502
  validation loss:		0.597641
  validation accuracy:		83.15 %
Epoch 1109 of 2000 took 0.106s
  training loss:		0.564185
  validation loss:		0.598861
  validation accuracy:		82.72 %
Epoch 1110 of 2000 took 0.106s
  training loss:		0.561626
  validation loss:		0.569646
  validation accuracy:		83.37 %
Epoch 1111 of 2000 took 0.105s
  training loss:		0.558314
  validation loss:		0.577303
  validation accuracy:		83.04 %
Epoch 1112 of 2000 took 0.105s
  training loss:		0.564186
  validation loss:		0.584508
  validation accuracy:		83.48 %
Epoch 1113 of 2000 took 0.105s
  training loss:		0.555399
  validation loss:		0.625882
  validation accuracy:		81.30 %
Epoch 1114 of 2000 took 0.106s
  training loss:		0.559692
  validation loss:		0.607719
  validation accuracy:		82.07 %
Epoch 1115 of 2000 took 0.106s
  training loss:		0.560077
  validation loss:		0.594960
  validation accuracy:		82.83 %
Epoch 1116 of 2000 took 0.105s
  training loss:		0.560951
  validation loss:		0.568982
  validation accuracy:		83.04 %
Epoch 1117 of 2000 took 0.106s
  training loss:		0.566378
  validation loss:		0.609843
  validation accuracy:		81.74 %
Epoch 1118 of 2000 took 0.106s
  training loss:		0.559109
  validation loss:		0.593235
  validation accuracy:		82.93 %
Epoch 1119 of 2000 took 0.106s
  training loss:		0.564129
  validation loss:		0.572783
  validation accuracy:		83.15 %
Epoch 1120 of 2000 took 0.106s
  training loss:		0.550304
  validation loss:		0.584707
  validation accuracy:		83.26 %
Epoch 1121 of 2000 took 0.106s
  training loss:		0.552306
  validation loss:		0.600220
  validation accuracy:		82.39 %
Epoch 1122 of 2000 took 0.106s
  training loss:		0.559444
  validation loss:		0.614275
  validation accuracy:		81.20 %
Epoch 1123 of 2000 took 0.106s
  training loss:		0.559604
  validation loss:		0.594802
  validation accuracy:		82.61 %
Epoch 1124 of 2000 took 0.106s
  training loss:		0.551935
  validation loss:		0.581674
  validation accuracy:		83.37 %
Epoch 1125 of 2000 took 0.105s
  training loss:		0.568341
  validation loss:		0.572906
  validation accuracy:		83.59 %
Epoch 1126 of 2000 took 0.105s
  training loss:		0.560585
  validation loss:		0.608235
  validation accuracy:		82.07 %
Epoch 1127 of 2000 took 0.106s
  training loss:		0.559804
  validation loss:		0.602118
  validation accuracy:		82.28 %
Epoch 1128 of 2000 took 0.106s
  training loss:		0.564184
  validation loss:		0.593680
  validation accuracy:		83.59 %
Epoch 1129 of 2000 took 0.105s
  training loss:		0.557573
  validation loss:		0.586755
  validation accuracy:		83.04 %
Epoch 1130 of 2000 took 0.106s
  training loss:		0.556569
  validation loss:		0.580461
  validation accuracy:		82.93 %
Epoch 1131 of 2000 took 0.106s
  training loss:		0.566079
  validation loss:		0.598384
  validation accuracy:		82.72 %
Epoch 1132 of 2000 took 0.106s
  training loss:		0.556013
  validation loss:		0.578338
  validation accuracy:		83.26 %
Epoch 1133 of 2000 took 0.106s
  training loss:		0.546605
  validation loss:		0.573592
  validation accuracy:		82.93 %
Epoch 1134 of 2000 took 0.106s
  training loss:		0.560866
  validation loss:		0.596031
  validation accuracy:		83.26 %
Epoch 1135 of 2000 took 0.106s
  training loss:		0.554004
  validation loss:		0.603582
  validation accuracy:		82.28 %
Epoch 1136 of 2000 took 0.106s
  training loss:		0.553909
  validation loss:		0.620888
  validation accuracy:		81.52 %
Epoch 1137 of 2000 took 0.106s
  training loss:		0.556459
  validation loss:		0.581302
  validation accuracy:		83.37 %
Epoch 1138 of 2000 took 0.106s
  training loss:		0.560105
  validation loss:		0.585663
  validation accuracy:		82.93 %
Epoch 1139 of 2000 took 0.105s
  training loss:		0.555750
  validation loss:		0.581852
  validation accuracy:		83.26 %
Epoch 1140 of 2000 took 0.105s
  training loss:		0.570348
  validation loss:		0.587266
  validation accuracy:		83.15 %
Epoch 1141 of 2000 took 0.102s
  training loss:		0.560002
  validation loss:		0.580859
  validation accuracy:		82.93 %
Epoch 1142 of 2000 took 0.102s
  training loss:		0.557534
  validation loss:		0.622238
  validation accuracy:		81.20 %
Epoch 1143 of 2000 took 0.102s
  training loss:		0.553330
  validation loss:		0.583028
  validation accuracy:		83.26 %
Epoch 1144 of 2000 took 0.102s
  training loss:		0.559229
  validation loss:		0.599236
  validation accuracy:		82.83 %
Epoch 1145 of 2000 took 0.102s
  training loss:		0.576988
  validation loss:		0.584029
  validation accuracy:		83.04 %
Epoch 1146 of 2000 took 0.102s
  training loss:		0.565464
  validation loss:		0.584932
  validation accuracy:		83.37 %
Epoch 1147 of 2000 took 0.102s
  training loss:		0.558934
  validation loss:		0.583744
  validation accuracy:		83.04 %
Epoch 1148 of 2000 took 0.102s
  training loss:		0.569090
  validation loss:		0.571099
  validation accuracy:		83.04 %
Epoch 1149 of 2000 took 0.102s
  training loss:		0.566124
  validation loss:		0.609142
  validation accuracy:		82.17 %
Epoch 1150 of 2000 took 0.102s
  training loss:		0.555513
  validation loss:		0.593258
  validation accuracy:		83.37 %
Epoch 1151 of 2000 took 0.102s
  training loss:		0.558433
  validation loss:		0.601244
  validation accuracy:		82.93 %
Epoch 1152 of 2000 took 0.102s
  training loss:		0.559431
  validation loss:		0.586586
  validation accuracy:		83.37 %
Epoch 1153 of 2000 took 0.102s
  training loss:		0.562151
  validation loss:		0.595080
  validation accuracy:		82.83 %
Epoch 1154 of 2000 took 0.102s
  training loss:		0.570591
  validation loss:		0.589143
  validation accuracy:		83.37 %
Epoch 1155 of 2000 took 0.102s
  training loss:		0.546538
  validation loss:		0.585478
  validation accuracy:		83.15 %
Epoch 1156 of 2000 took 0.102s
  training loss:		0.558633
  validation loss:		0.615075
  validation accuracy:		81.63 %
Epoch 1157 of 2000 took 0.103s
  training loss:		0.562351
  validation loss:		0.579208
  validation accuracy:		82.72 %
Epoch 1158 of 2000 took 0.103s
  training loss:		0.559101
  validation loss:		0.589567
  validation accuracy:		82.93 %
Epoch 1159 of 2000 took 0.102s
  training loss:		0.555673
  validation loss:		0.605873
  validation accuracy:		82.17 %
Epoch 1160 of 2000 took 0.102s
  training loss:		0.561304
  validation loss:		0.598868
  validation accuracy:		82.61 %
Epoch 1161 of 2000 took 0.102s
  training loss:		0.559322
  validation loss:		0.578066
  validation accuracy:		83.04 %
Epoch 1162 of 2000 took 0.103s
  training loss:		0.553366
  validation loss:		0.571905
  validation accuracy:		83.26 %
Epoch 1163 of 2000 took 0.102s
  training loss:		0.547600
  validation loss:		0.585518
  validation accuracy:		83.04 %
Epoch 1164 of 2000 took 0.102s
  training loss:		0.560612
  validation loss:		0.596712
  validation accuracy:		82.93 %
Epoch 1165 of 2000 took 0.103s
  training loss:		0.561370
  validation loss:		0.586352
  validation accuracy:		83.37 %
Epoch 1166 of 2000 took 0.102s
  training loss:		0.549318
  validation loss:		0.580506
  validation accuracy:		83.48 %
Epoch 1167 of 2000 took 0.102s
  training loss:		0.558891
  validation loss:		0.595357
  validation accuracy:		83.15 %
Epoch 1168 of 2000 took 0.102s
  training loss:		0.545580
  validation loss:		0.590443
  validation accuracy:		82.50 %
Epoch 1169 of 2000 took 0.102s
  training loss:		0.558073
  validation loss:		0.571703
  validation accuracy:		83.04 %
Epoch 1170 of 2000 took 0.102s
  training loss:		0.557980
  validation loss:		0.580104
  validation accuracy:		83.15 %
Epoch 1171 of 2000 took 0.102s
  training loss:		0.557318
  validation loss:		0.586055
  validation accuracy:		82.72 %
Epoch 1172 of 2000 took 0.102s
  training loss:		0.552122
  validation loss:		0.620804
  validation accuracy:		81.52 %
Epoch 1173 of 2000 took 0.102s
  training loss:		0.554395
  validation loss:		0.579052
  validation accuracy:		82.61 %
Epoch 1174 of 2000 took 0.102s
  training loss:		0.557573
  validation loss:		0.569513
  validation accuracy:		83.37 %
Epoch 1175 of 2000 took 0.102s
  training loss:		0.565396
  validation loss:		0.606440
  validation accuracy:		82.28 %
Epoch 1176 of 2000 took 0.102s
  training loss:		0.558961
  validation loss:		0.587530
  validation accuracy:		83.04 %
Epoch 1177 of 2000 took 0.103s
  training loss:		0.544478
  validation loss:		0.589359
  validation accuracy:		82.83 %
Epoch 1178 of 2000 took 0.102s
  training loss:		0.560466
  validation loss:		0.611742
  validation accuracy:		81.63 %
Epoch 1179 of 2000 took 0.102s
  training loss:		0.569916
  validation loss:		0.606170
  validation accuracy:		82.17 %
Epoch 1180 of 2000 took 0.102s
  training loss:		0.563463
  validation loss:		0.578667
  validation accuracy:		82.83 %
Epoch 1181 of 2000 took 0.102s
  training loss:		0.570359
  validation loss:		0.602828
  validation accuracy:		82.28 %
Epoch 1182 of 2000 took 0.102s
  training loss:		0.554258
  validation loss:		0.573132
  validation accuracy:		83.48 %
Epoch 1183 of 2000 took 0.102s
  training loss:		0.573330
  validation loss:		0.687229
  validation accuracy:		79.13 %
Epoch 1184 of 2000 took 0.102s
  training loss:		0.561930
  validation loss:		0.577828
  validation accuracy:		83.26 %
Epoch 1185 of 2000 took 0.105s
  training loss:		0.564717
  validation loss:		0.620528
  validation accuracy:		81.30 %
Epoch 1186 of 2000 took 0.102s
  training loss:		0.554369
  validation loss:		0.573965
  validation accuracy:		83.15 %
Epoch 1187 of 2000 took 0.102s
  training loss:		0.567445
  validation loss:		0.571898
  validation accuracy:		83.15 %
Epoch 1188 of 2000 took 0.102s
  training loss:		0.557881
  validation loss:		0.591988
  validation accuracy:		83.15 %
Epoch 1189 of 2000 took 0.102s
  training loss:		0.558131
  validation loss:		0.604834
  validation accuracy:		82.72 %
Epoch 1190 of 2000 took 0.102s
  training loss:		0.561650
  validation loss:		0.587785
  validation accuracy:		83.70 %
Epoch 1191 of 2000 took 0.103s
  training loss:		0.563166
  validation loss:		0.579034
  validation accuracy:		83.37 %
Epoch 1192 of 2000 took 0.103s
  training loss:		0.563588
  validation loss:		0.579092
  validation accuracy:		83.26 %
Epoch 1193 of 2000 took 0.102s
  training loss:		0.556241
  validation loss:		0.595200
  validation accuracy:		83.04 %
Epoch 1194 of 2000 took 0.102s
  training loss:		0.556347
  validation loss:		0.571094
  validation accuracy:		83.15 %
Epoch 1195 of 2000 took 0.102s
  training loss:		0.559350
  validation loss:		0.593959
  validation accuracy:		83.26 %
Epoch 1196 of 2000 took 0.102s
  training loss:		0.546952
  validation loss:		0.578268
  validation accuracy:		82.72 %
Epoch 1197 of 2000 took 0.102s
  training loss:		0.552457
  validation loss:		0.618576
  validation accuracy:		81.85 %
Epoch 1198 of 2000 took 0.102s
  training loss:		0.554928
  validation loss:		0.570871
  validation accuracy:		83.48 %
Epoch 1199 of 2000 took 0.102s
  training loss:		0.561600
  validation loss:		0.595376
  validation accuracy:		83.37 %
Epoch 1200 of 2000 took 0.102s
  training loss:		0.560690
  validation loss:		0.580914
  validation accuracy:		83.48 %
Epoch 1201 of 2000 took 0.103s
  training loss:		0.567309
  validation loss:		0.587910
  validation accuracy:		82.72 %
Epoch 1202 of 2000 took 0.102s
  training loss:		0.570066
  validation loss:		0.587836
  validation accuracy:		82.72 %
Epoch 1203 of 2000 took 0.102s
  training loss:		0.550491
  validation loss:		0.573831
  validation accuracy:		83.26 %
Epoch 1204 of 2000 took 0.102s
  training loss:		0.565321
  validation loss:		0.633376
  validation accuracy:		80.76 %
Epoch 1205 of 2000 took 0.102s
  training loss:		0.560248
  validation loss:		0.574622
  validation accuracy:		83.26 %
Epoch 1206 of 2000 took 0.102s
  training loss:		0.559717
  validation loss:		0.614992
  validation accuracy:		81.96 %
Epoch 1207 of 2000 took 0.102s
  training loss:		0.551453
  validation loss:		0.579790
  validation accuracy:		83.15 %
Epoch 1208 of 2000 took 0.102s
  training loss:		0.555151
  validation loss:		0.582675
  validation accuracy:		82.50 %
Epoch 1209 of 2000 took 0.102s
  training loss:		0.567241
  validation loss:		0.572369
  validation accuracy:		83.26 %
Epoch 1210 of 2000 took 0.102s
  training loss:		0.563297
  validation loss:		0.569002
  validation accuracy:		83.26 %
Epoch 1211 of 2000 took 0.102s
  training loss:		0.563914
  validation loss:		0.570154
  validation accuracy:		83.26 %
Epoch 1212 of 2000 took 0.103s
  training loss:		0.556397
  validation loss:		0.572875
  validation accuracy:		83.26 %
Epoch 1213 of 2000 took 0.102s
  training loss:		0.564976
  validation loss:		0.604982
  validation accuracy:		82.61 %
Epoch 1214 of 2000 took 0.102s
  training loss:		0.563235
  validation loss:		0.573992
  validation accuracy:		83.37 %
Epoch 1215 of 2000 took 0.102s
  training loss:		0.551109
  validation loss:		0.612763
  validation accuracy:		81.74 %
Epoch 1216 of 2000 took 0.103s
  training loss:		0.562853
  validation loss:		0.587595
  validation accuracy:		83.26 %
Epoch 1217 of 2000 took 0.102s
  training loss:		0.570182
  validation loss:		0.571284
  validation accuracy:		83.15 %
Epoch 1218 of 2000 took 0.102s
  training loss:		0.561403
  validation loss:		0.585691
  validation accuracy:		82.72 %
Epoch 1219 of 2000 took 0.103s
  training loss:		0.563500
  validation loss:		0.578626
  validation accuracy:		82.93 %
Epoch 1220 of 2000 took 0.103s
  training loss:		0.559311
  validation loss:		0.577888
  validation accuracy:		82.93 %
Epoch 1221 of 2000 took 0.104s
  training loss:		0.563267
  validation loss:		0.622206
  validation accuracy:		81.74 %
Epoch 1222 of 2000 took 0.103s
  training loss:		0.548828
  validation loss:		0.572573
  validation accuracy:		83.15 %
Epoch 1223 of 2000 took 0.102s
  training loss:		0.565307
  validation loss:		0.572409
  validation accuracy:		83.15 %
Epoch 1224 of 2000 took 0.102s
  training loss:		0.557048
  validation loss:		0.593723
  validation accuracy:		83.59 %
Epoch 1225 of 2000 took 0.102s
  training loss:		0.562965
  validation loss:		0.590675
  validation accuracy:		83.37 %
Epoch 1226 of 2000 took 0.102s
  training loss:		0.546415
  validation loss:		0.598227
  validation accuracy:		83.26 %
Epoch 1227 of 2000 took 0.102s
  training loss:		0.558919
  validation loss:		0.574640
  validation accuracy:		83.04 %
Epoch 1228 of 2000 took 0.103s
  training loss:		0.557731
  validation loss:		0.601536
  validation accuracy:		82.17 %
Epoch 1229 of 2000 took 0.102s
  training loss:		0.562553
  validation loss:		0.572354
  validation accuracy:		83.26 %
Epoch 1230 of 2000 took 0.102s
  training loss:		0.550355
  validation loss:		0.608708
  validation accuracy:		81.85 %
Epoch 1231 of 2000 took 0.102s
  training loss:		0.547728
  validation loss:		0.581335
  validation accuracy:		83.04 %
Epoch 1232 of 2000 took 0.102s
  training loss:		0.558269
  validation loss:		0.575930
  validation accuracy:		82.83 %
Epoch 1233 of 2000 took 0.103s
  training loss:		0.561743
  validation loss:		0.598084
  validation accuracy:		82.93 %
Epoch 1234 of 2000 took 0.102s
  training loss:		0.567846
  validation loss:		0.596063
  validation accuracy:		83.04 %
Epoch 1235 of 2000 took 0.102s
  training loss:		0.550465
  validation loss:		0.575735
  validation accuracy:		83.15 %
Epoch 1236 of 2000 took 0.102s
  training loss:		0.556221
  validation loss:		0.616223
  validation accuracy:		81.63 %
Epoch 1237 of 2000 took 0.102s
  training loss:		0.549279
  validation loss:		0.593078
  validation accuracy:		82.72 %
Epoch 1238 of 2000 took 0.102s
  training loss:		0.554304
  validation loss:		0.571821
  validation accuracy:		83.15 %
Epoch 1239 of 2000 took 0.102s
  training loss:		0.551641
  validation loss:		0.572821
  validation accuracy:		83.37 %
Epoch 1240 of 2000 took 0.102s
  training loss:		0.563307
  validation loss:		0.585511
  validation accuracy:		83.80 %
Epoch 1241 of 2000 took 0.102s
  training loss:		0.546594
  validation loss:		0.592693
  validation accuracy:		83.15 %
Epoch 1242 of 2000 took 0.102s
  training loss:		0.558419
  validation loss:		0.602443
  validation accuracy:		81.52 %
Epoch 1243 of 2000 took 0.102s
  training loss:		0.559181
  validation loss:		0.578068
  validation accuracy:		82.72 %
Epoch 1244 of 2000 took 0.102s
  training loss:		0.552290
  validation loss:		0.586309
  validation accuracy:		83.70 %
Epoch 1245 of 2000 took 0.102s
  training loss:		0.555230
  validation loss:		0.616510
  validation accuracy:		81.96 %
Epoch 1246 of 2000 took 0.102s
  training loss:		0.554093
  validation loss:		0.600416
  validation accuracy:		82.93 %
Epoch 1247 of 2000 took 0.102s
  training loss:		0.562526
  validation loss:		0.571835
  validation accuracy:		83.26 %
Epoch 1248 of 2000 took 0.102s
  training loss:		0.562709
  validation loss:		0.640250
  validation accuracy:		80.87 %
Epoch 1249 of 2000 took 0.102s
  training loss:		0.559397
  validation loss:		0.574413
  validation accuracy:		83.37 %
Epoch 1250 of 2000 took 0.102s
  training loss:		0.564508
  validation loss:		0.590846
  validation accuracy:		82.72 %
Epoch 1251 of 2000 took 0.103s
  training loss:		0.557838
  validation loss:		0.583042
  validation accuracy:		82.72 %
Epoch 1252 of 2000 took 0.102s
  training loss:		0.559212
  validation loss:		0.584562
  validation accuracy:		82.93 %
Epoch 1253 of 2000 took 0.102s
  training loss:		0.573798
  validation loss:		0.582460
  validation accuracy:		83.15 %
Epoch 1254 of 2000 took 0.102s
  training loss:		0.552202
  validation loss:		0.615477
  validation accuracy:		81.52 %
Epoch 1255 of 2000 took 0.103s
  training loss:		0.546843
  validation loss:		0.577341
  validation accuracy:		82.93 %
Epoch 1256 of 2000 took 0.102s
  training loss:		0.553579
  validation loss:		0.571387
  validation accuracy:		83.26 %
Epoch 1257 of 2000 took 0.102s
  training loss:		0.556991
  validation loss:		0.576342
  validation accuracy:		82.93 %
Epoch 1258 of 2000 took 0.103s
  training loss:		0.561598
  validation loss:		0.595905
  validation accuracy:		82.93 %
Epoch 1259 of 2000 took 0.102s
  training loss:		0.563779
  validation loss:		0.577854
  validation accuracy:		83.26 %
Epoch 1260 of 2000 took 0.102s
  training loss:		0.554539
  validation loss:		0.590981
  validation accuracy:		82.93 %
Epoch 1261 of 2000 took 0.102s
  training loss:		0.560858
  validation loss:		0.578332
  validation accuracy:		83.26 %
Epoch 1262 of 2000 took 0.102s
  training loss:		0.551840
  validation loss:		0.573720
  validation accuracy:		82.93 %
Epoch 1263 of 2000 took 0.102s
  training loss:		0.550529
  validation loss:		0.572559
  validation accuracy:		83.04 %
Epoch 1264 of 2000 took 0.102s
  training loss:		0.559348
  validation loss:		0.632210
  validation accuracy:		81.09 %
Epoch 1265 of 2000 took 0.102s
  training loss:		0.563308
  validation loss:		0.588257
  validation accuracy:		82.72 %
Epoch 1266 of 2000 took 0.102s
  training loss:		0.563703
  validation loss:		0.611331
  validation accuracy:		81.20 %
Epoch 1267 of 2000 took 0.102s
  training loss:		0.559609
  validation loss:		0.573887
  validation accuracy:		83.26 %
Epoch 1268 of 2000 took 0.102s
  training loss:		0.555313
  validation loss:		0.569789
  validation accuracy:		82.83 %
Epoch 1269 of 2000 took 0.102s
  training loss:		0.559894
  validation loss:		0.580846
  validation accuracy:		83.37 %
Epoch 1270 of 2000 took 0.102s
  training loss:		0.557575
  validation loss:		0.578951
  validation accuracy:		83.15 %
Epoch 1271 of 2000 took 0.102s
  training loss:		0.561128
  validation loss:		0.588718
  validation accuracy:		83.15 %
Epoch 1272 of 2000 took 0.102s
  training loss:		0.564086
  validation loss:		0.627529
  validation accuracy:		81.09 %
Epoch 1273 of 2000 took 0.102s
  training loss:		0.554337
  validation loss:		0.638984
  validation accuracy:		80.43 %
Epoch 1274 of 2000 took 0.102s
  training loss:		0.553672
  validation loss:		0.583116
  validation accuracy:		83.37 %
Epoch 1275 of 2000 took 0.102s
  training loss:		0.560054
  validation loss:		0.578427
  validation accuracy:		82.93 %
Epoch 1276 of 2000 took 0.102s
  training loss:		0.559856
  validation loss:		0.578177
  validation accuracy:		83.15 %
Epoch 1277 of 2000 took 0.102s
  training loss:		0.554560
  validation loss:		0.581600
  validation accuracy:		83.15 %
Epoch 1278 of 2000 took 0.102s
  training loss:		0.557087
  validation loss:		0.570705
  validation accuracy:		82.83 %
Epoch 1279 of 2000 took 0.102s
  training loss:		0.556016
  validation loss:		0.590289
  validation accuracy:		83.04 %
Epoch 1280 of 2000 took 0.103s
  training loss:		0.554852
  validation loss:		0.617731
  validation accuracy:		81.20 %
Epoch 1281 of 2000 took 0.102s
  training loss:		0.563737
  validation loss:		0.594043
  validation accuracy:		83.15 %
Epoch 1282 of 2000 took 0.102s
  training loss:		0.555775
  validation loss:		0.575958
  validation accuracy:		83.26 %
Epoch 1283 of 2000 took 0.102s
  training loss:		0.560534
  validation loss:		0.598699
  validation accuracy:		82.17 %
Epoch 1284 of 2000 took 0.102s
  training loss:		0.567814
  validation loss:		0.571135
  validation accuracy:		82.93 %
Epoch 1285 of 2000 took 0.102s
  training loss:		0.561047
  validation loss:		0.576537
  validation accuracy:		83.26 %
Epoch 1286 of 2000 took 0.102s
  training loss:		0.548284
  validation loss:		0.612896
  validation accuracy:		81.63 %
Epoch 1287 of 2000 took 0.103s
  training loss:		0.553810
  validation loss:		0.599642
  validation accuracy:		82.61 %
Epoch 1288 of 2000 took 0.102s
  training loss:		0.559829
  validation loss:		0.608955
  validation accuracy:		81.74 %
Epoch 1289 of 2000 took 0.103s
  training loss:		0.549223
  validation loss:		0.590062
  validation accuracy:		83.26 %
Epoch 1290 of 2000 took 0.102s
  training loss:		0.565681
  validation loss:		0.578307
  validation accuracy:		83.04 %
Epoch 1291 of 2000 took 0.103s
  training loss:		0.554293
  validation loss:		0.592288
  validation accuracy:		83.37 %
Epoch 1292 of 2000 took 0.102s
  training loss:		0.558642
  validation loss:		0.572575
  validation accuracy:		83.04 %
Epoch 1293 of 2000 took 0.102s
  training loss:		0.561739
  validation loss:		0.591175
  validation accuracy:		82.61 %
Epoch 1294 of 2000 took 0.103s
  training loss:		0.568526
  validation loss:		0.578111
  validation accuracy:		83.59 %
Epoch 1295 of 2000 took 0.102s
  training loss:		0.556958
  validation loss:		0.594172
  validation accuracy:		82.39 %
Epoch 1296 of 2000 took 0.102s
  training loss:		0.549364
  validation loss:		0.581317
  validation accuracy:		83.04 %
Epoch 1297 of 2000 took 0.103s
  training loss:		0.557555
  validation loss:		0.580733
  validation accuracy:		83.37 %
Epoch 1298 of 2000 took 0.102s
  training loss:		0.563336
  validation loss:		0.591435
  validation accuracy:		83.26 %
Epoch 1299 of 2000 took 0.102s
  training loss:		0.558483
  validation loss:		0.618437
  validation accuracy:		81.85 %
Epoch 1300 of 2000 took 0.102s
  training loss:		0.555397
  validation loss:		0.585248
  validation accuracy:		83.37 %
Epoch 1301 of 2000 took 0.102s
  training loss:		0.558133
  validation loss:		0.587277
  validation accuracy:		83.26 %
Epoch 1302 of 2000 took 0.102s
  training loss:		0.564498
  validation loss:		0.571283
  validation accuracy:		83.04 %
Epoch 1303 of 2000 took 0.102s
  training loss:		0.560148
  validation loss:		0.573696
  validation accuracy:		83.15 %
Epoch 1304 of 2000 took 0.102s
  training loss:		0.570468
  validation loss:		0.597600
  validation accuracy:		83.37 %
Epoch 1305 of 2000 took 0.102s
  training loss:		0.560776
  validation loss:		0.585470
  validation accuracy:		83.59 %
Epoch 1306 of 2000 took 0.102s
  training loss:		0.559820
  validation loss:		0.590477
  validation accuracy:		83.15 %
Epoch 1307 of 2000 took 0.102s
  training loss:		0.550110
  validation loss:		0.592623
  validation accuracy:		83.26 %
Epoch 1308 of 2000 took 0.102s
  training loss:		0.563858
  validation loss:		0.579819
  validation accuracy:		82.83 %
Epoch 1309 of 2000 took 0.103s
  training loss:		0.552745
  validation loss:		0.589843
  validation accuracy:		83.70 %
Epoch 1310 of 2000 took 0.102s
  training loss:		0.551069
  validation loss:		0.572865
  validation accuracy:		83.37 %
Epoch 1311 of 2000 took 0.102s
  training loss:		0.554303
  validation loss:		0.577792
  validation accuracy:		83.15 %
Epoch 1312 of 2000 took 0.102s
  training loss:		0.552284
  validation loss:		0.583722
  validation accuracy:		82.83 %
Epoch 1313 of 2000 took 0.102s
  training loss:		0.556802
  validation loss:		0.598711
  validation accuracy:		82.50 %
Epoch 1314 of 2000 took 0.102s
  training loss:		0.563208
  validation loss:		0.614559
  validation accuracy:		81.74 %
Epoch 1315 of 2000 took 0.102s
  training loss:		0.558255
  validation loss:		0.603083
  validation accuracy:		82.61 %
Epoch 1316 of 2000 took 0.102s
  training loss:		0.550150
  validation loss:		0.602086
  validation accuracy:		82.39 %
Epoch 1317 of 2000 took 0.102s
  training loss:		0.556316
  validation loss:		0.579019
  validation accuracy:		83.48 %
Epoch 1318 of 2000 took 0.102s
  training loss:		0.557638
  validation loss:		0.595869
  validation accuracy:		82.61 %
Epoch 1319 of 2000 took 0.102s
  training loss:		0.553052
  validation loss:		0.594390
  validation accuracy:		83.15 %
Epoch 1320 of 2000 took 0.102s
  training loss:		0.557640
  validation loss:		0.600213
  validation accuracy:		82.83 %
Epoch 1321 of 2000 took 0.102s
  training loss:		0.547316
  validation loss:		0.606420
  validation accuracy:		81.74 %
Epoch 1322 of 2000 took 0.102s
  training loss:		0.562641
  validation loss:		0.632558
  validation accuracy:		81.09 %
Epoch 1323 of 2000 took 0.102s
  training loss:		0.556429
  validation loss:		0.601644
  validation accuracy:		82.39 %
Epoch 1324 of 2000 took 0.102s
  training loss:		0.558091
  validation loss:		0.583703
  validation accuracy:		82.93 %
Epoch 1325 of 2000 took 0.102s
  training loss:		0.562968
  validation loss:		0.576268
  validation accuracy:		83.26 %
Epoch 1326 of 2000 took 0.102s
  training loss:		0.550805
  validation loss:		0.590536
  validation accuracy:		83.04 %
Epoch 1327 of 2000 took 0.102s
  training loss:		0.561742
  validation loss:		0.581118
  validation accuracy:		83.48 %
Epoch 1328 of 2000 took 0.103s
  training loss:		0.546976
  validation loss:		0.592746
  validation accuracy:		82.61 %
Epoch 1329 of 2000 took 0.102s
  training loss:		0.549305
  validation loss:		0.602728
  validation accuracy:		82.61 %
Epoch 1330 of 2000 took 0.103s
  training loss:		0.550899
  validation loss:		0.585445
  validation accuracy:		82.39 %
Epoch 1331 of 2000 took 0.103s
  training loss:		0.556668
  validation loss:		0.609153
  validation accuracy:		81.96 %
Epoch 1332 of 2000 took 0.102s
  training loss:		0.551395
  validation loss:		0.594375
  validation accuracy:		83.26 %
Epoch 1333 of 2000 took 0.103s
  training loss:		0.547079
  validation loss:		0.579273
  validation accuracy:		83.04 %
Epoch 1334 of 2000 took 0.102s
  training loss:		0.557411
  validation loss:		0.647604
  validation accuracy:		80.22 %
Epoch 1335 of 2000 took 0.102s
  training loss:		0.553637
  validation loss:		0.596208
  validation accuracy:		82.39 %
Epoch 1336 of 2000 took 0.103s
  training loss:		0.560599
  validation loss:		0.589330
  validation accuracy:		83.37 %
Epoch 1337 of 2000 took 0.102s
  training loss:		0.558201
  validation loss:		0.581101
  validation accuracy:		83.15 %
Epoch 1338 of 2000 took 0.102s
  training loss:		0.549997
  validation loss:		0.578992
  validation accuracy:		83.04 %
Epoch 1339 of 2000 took 0.103s
  training loss:		0.560907
  validation loss:		0.582505
  validation accuracy:		83.04 %
Epoch 1340 of 2000 took 0.102s
  training loss:		0.558247
  validation loss:		0.596151
  validation accuracy:		83.37 %
Epoch 1341 of 2000 took 0.102s
  training loss:		0.571704
  validation loss:		0.578392
  validation accuracy:		82.50 %
Epoch 1342 of 2000 took 0.102s
  training loss:		0.563222
  validation loss:		0.638384
  validation accuracy:		80.87 %
Epoch 1343 of 2000 took 0.102s
  training loss:		0.565445
  validation loss:		0.603717
  validation accuracy:		82.17 %
Epoch 1344 of 2000 took 0.102s
  training loss:		0.560517
  validation loss:		0.604676
  validation accuracy:		82.39 %
Epoch 1345 of 2000 took 0.102s
  training loss:		0.555774
  validation loss:		0.591819
  validation accuracy:		82.93 %
Epoch 1346 of 2000 took 0.102s
  training loss:		0.563836
  validation loss:		0.578892
  validation accuracy:		83.37 %
Epoch 1347 of 2000 took 0.102s
  training loss:		0.551591
  validation loss:		0.584700
  validation accuracy:		82.93 %
Epoch 1348 of 2000 took 0.102s
  training loss:		0.556699
  validation loss:		0.579617
  validation accuracy:		83.04 %
Epoch 1349 of 2000 took 0.102s
  training loss:		0.556815
  validation loss:		0.624351
  validation accuracy:		81.20 %
Epoch 1350 of 2000 took 0.102s
  training loss:		0.561379
  validation loss:		0.569493
  validation accuracy:		83.04 %
Epoch 1351 of 2000 took 0.102s
  training loss:		0.559411
  validation loss:		0.590992
  validation accuracy:		83.48 %
Epoch 1352 of 2000 took 0.102s
  training loss:		0.546744
  validation loss:		0.602776
  validation accuracy:		82.72 %
Epoch 1353 of 2000 took 0.102s
  training loss:		0.549165
  validation loss:		0.595493
  validation accuracy:		83.04 %
Epoch 1354 of 2000 took 0.102s
  training loss:		0.550708
  validation loss:		0.581615
  validation accuracy:		82.61 %
Epoch 1355 of 2000 took 0.102s
  training loss:		0.557294
  validation loss:		0.570531
  validation accuracy:		83.26 %
Epoch 1356 of 2000 took 0.102s
  training loss:		0.560541
  validation loss:		0.580127
  validation accuracy:		82.83 %
Epoch 1357 of 2000 took 0.105s
  training loss:		0.555606
  validation loss:		0.580577
  validation accuracy:		82.83 %
Epoch 1358 of 2000 took 0.109s
  training loss:		0.555896
  validation loss:		0.608542
  validation accuracy:		81.96 %
Epoch 1359 of 2000 took 0.109s
  training loss:		0.559027
  validation loss:		0.569863
  validation accuracy:		82.93 %
Epoch 1360 of 2000 took 0.109s
  training loss:		0.553982
  validation loss:		0.575018
  validation accuracy:		83.26 %
Epoch 1361 of 2000 took 0.109s
  training loss:		0.557459
  validation loss:		0.584663
  validation accuracy:		83.04 %
Epoch 1362 of 2000 took 0.109s
  training loss:		0.553343
  validation loss:		0.585387
  validation accuracy:		83.37 %
Epoch 1363 of 2000 took 0.109s
  training loss:		0.550850
  validation loss:		0.581135
  validation accuracy:		82.93 %
Epoch 1364 of 2000 took 0.109s
  training loss:		0.560795
  validation loss:		0.579670
  validation accuracy:		83.37 %
Epoch 1365 of 2000 took 0.106s
  training loss:		0.544202
  validation loss:		0.587233
  validation accuracy:		83.48 %
Epoch 1366 of 2000 took 0.106s
  training loss:		0.558585
  validation loss:		0.578544
  validation accuracy:		83.15 %
Epoch 1367 of 2000 took 0.106s
  training loss:		0.553707
  validation loss:		0.581135
  validation accuracy:		83.15 %
Epoch 1368 of 2000 took 0.106s
  training loss:		0.559237
  validation loss:		0.603579
  validation accuracy:		82.50 %
Epoch 1369 of 2000 took 0.106s
  training loss:		0.555659
  validation loss:		0.615082
  validation accuracy:		81.41 %
Epoch 1370 of 2000 took 0.106s
  training loss:		0.553465
  validation loss:		0.586213
  validation accuracy:		83.15 %
Epoch 1371 of 2000 took 0.106s
  training loss:		0.567907
  validation loss:		0.580537
  validation accuracy:		82.93 %
Epoch 1372 of 2000 took 0.106s
  training loss:		0.554780
  validation loss:		0.610738
  validation accuracy:		82.07 %
Epoch 1373 of 2000 took 0.106s
  training loss:		0.552329
  validation loss:		0.609206
  validation accuracy:		82.50 %
Epoch 1374 of 2000 took 0.106s
  training loss:		0.555682
  validation loss:		0.579023
  validation accuracy:		82.93 %
Epoch 1375 of 2000 took 0.106s
  training loss:		0.553095
  validation loss:		0.593785
  validation accuracy:		83.37 %
Epoch 1376 of 2000 took 0.106s
  training loss:		0.567319
  validation loss:		0.632298
  validation accuracy:		80.87 %
Epoch 1377 of 2000 took 0.106s
  training loss:		0.561938
  validation loss:		0.598454
  validation accuracy:		83.26 %
Epoch 1378 of 2000 took 0.106s
  training loss:		0.547428
  validation loss:		0.610835
  validation accuracy:		81.74 %
Epoch 1379 of 2000 took 0.106s
  training loss:		0.562607
  validation loss:		0.586871
  validation accuracy:		82.93 %
Epoch 1380 of 2000 took 0.106s
  training loss:		0.563242
  validation loss:		0.580090
  validation accuracy:		82.83 %
Epoch 1381 of 2000 took 0.105s
  training loss:		0.557868
  validation loss:		0.572706
  validation accuracy:		83.26 %
Epoch 1382 of 2000 took 0.106s
  training loss:		0.559095
  validation loss:		0.586284
  validation accuracy:		82.93 %
Epoch 1383 of 2000 took 0.106s
  training loss:		0.562283
  validation loss:		0.593837
  validation accuracy:		83.37 %
Epoch 1384 of 2000 took 0.106s
  training loss:		0.558488
  validation loss:		0.579570
  validation accuracy:		83.48 %
Epoch 1385 of 2000 took 0.105s
  training loss:		0.545813
  validation loss:		0.586635
  validation accuracy:		82.93 %
Epoch 1386 of 2000 took 0.102s
  training loss:		0.548782
  validation loss:		0.606437
  validation accuracy:		82.17 %
Epoch 1387 of 2000 took 0.102s
  training loss:		0.556702
  validation loss:		0.581515
  validation accuracy:		83.26 %
Epoch 1388 of 2000 took 0.102s
  training loss:		0.571548
  validation loss:		0.585891
  validation accuracy:		83.48 %
Epoch 1389 of 2000 took 0.102s
  training loss:		0.557362
  validation loss:		0.612090
  validation accuracy:		81.74 %
Epoch 1390 of 2000 took 0.102s
  training loss:		0.561839
  validation loss:		0.599513
  validation accuracy:		82.61 %
Epoch 1391 of 2000 took 0.102s
  training loss:		0.540232
  validation loss:		0.578842
  validation accuracy:		83.37 %
Epoch 1392 of 2000 took 0.102s
  training loss:		0.560059
  validation loss:		0.605988
  validation accuracy:		81.85 %
Epoch 1393 of 2000 took 0.102s
  training loss:		0.558721
  validation loss:		0.573251
  validation accuracy:		82.93 %
Epoch 1394 of 2000 took 0.102s
  training loss:		0.556579
  validation loss:		0.590788
  validation accuracy:		82.83 %
Epoch 1395 of 2000 took 0.103s
  training loss:		0.558207
  validation loss:		0.585150
  validation accuracy:		83.37 %
Epoch 1396 of 2000 took 0.103s
  training loss:		0.559807
  validation loss:		0.606828
  validation accuracy:		82.61 %
Epoch 1397 of 2000 took 0.103s
  training loss:		0.559106
  validation loss:		0.592572
  validation accuracy:		82.39 %
Epoch 1398 of 2000 took 0.102s
  training loss:		0.542016
  validation loss:		0.591758
  validation accuracy:		82.93 %
Epoch 1399 of 2000 took 0.102s
  training loss:		0.553509
  validation loss:		0.580524
  validation accuracy:		82.61 %
Epoch 1400 of 2000 took 0.102s
  training loss:		0.554991
  validation loss:		0.583166
  validation accuracy:		82.72 %
Epoch 1401 of 2000 took 0.102s
  training loss:		0.565013
  validation loss:		0.587786
  validation accuracy:		83.04 %
Epoch 1402 of 2000 took 0.102s
  training loss:		0.552615
  validation loss:		0.575485
  validation accuracy:		83.26 %
Epoch 1403 of 2000 took 0.102s
  training loss:		0.560195
  validation loss:		0.583714
  validation accuracy:		83.26 %
Epoch 1404 of 2000 took 0.102s
  training loss:		0.560350
  validation loss:		0.573032
  validation accuracy:		83.15 %
Epoch 1405 of 2000 took 0.103s
  training loss:		0.554702
  validation loss:		0.588315
  validation accuracy:		82.72 %
Epoch 1406 of 2000 took 0.102s
  training loss:		0.554939
  validation loss:		0.600040
  validation accuracy:		82.83 %
Epoch 1407 of 2000 took 0.102s
  training loss:		0.547652
  validation loss:		0.602570
  validation accuracy:		82.93 %
Epoch 1408 of 2000 took 0.102s
  training loss:		0.560357
  validation loss:		0.588245
  validation accuracy:		83.48 %
Epoch 1409 of 2000 took 0.102s
  training loss:		0.555313
  validation loss:		0.580584
  validation accuracy:		82.72 %
Epoch 1410 of 2000 took 0.103s
  training loss:		0.565886
  validation loss:		0.597716
  validation accuracy:		82.17 %
Epoch 1411 of 2000 took 0.102s
  training loss:		0.552126
  validation loss:		0.624422
  validation accuracy:		81.74 %
Epoch 1412 of 2000 took 0.102s
  training loss:		0.550782
  validation loss:		0.586712
  validation accuracy:		83.04 %
Epoch 1413 of 2000 took 0.104s
  training loss:		0.562196
  validation loss:		0.589471
  validation accuracy:		83.26 %
Epoch 1414 of 2000 took 0.106s
  training loss:		0.561855
  validation loss:		0.578148
  validation accuracy:		83.26 %
Epoch 1415 of 2000 took 0.106s
  training loss:		0.554481
  validation loss:		0.615814
  validation accuracy:		81.63 %
Epoch 1416 of 2000 took 0.106s
  training loss:		0.554537
  validation loss:		0.589956
  validation accuracy:		83.15 %
Epoch 1417 of 2000 took 0.106s
  training loss:		0.561957
  validation loss:		0.601846
  validation accuracy:		82.61 %
Epoch 1418 of 2000 took 0.106s
  training loss:		0.550075
  validation loss:		0.574592
  validation accuracy:		83.04 %
Epoch 1419 of 2000 took 0.106s
  training loss:		0.552891
  validation loss:		0.590499
  validation accuracy:		83.04 %
Epoch 1420 of 2000 took 0.106s
  training loss:		0.560539
  validation loss:		0.627406
  validation accuracy:		80.98 %
Epoch 1421 of 2000 took 0.106s
  training loss:		0.561319
  validation loss:		0.577875
  validation accuracy:		83.37 %
Epoch 1422 of 2000 took 0.106s
  training loss:		0.561573
  validation loss:		0.588781
  validation accuracy:		83.15 %
Epoch 1423 of 2000 took 0.106s
  training loss:		0.548020
  validation loss:		0.578360
  validation accuracy:		83.04 %
Epoch 1424 of 2000 took 0.106s
  training loss:		0.554832
  validation loss:		0.655519
  validation accuracy:		80.11 %
Epoch 1425 of 2000 took 0.106s
  training loss:		0.561860
  validation loss:		0.578138
  validation accuracy:		83.15 %
Epoch 1426 of 2000 took 0.106s
  training loss:		0.557872
  validation loss:		0.596879
  validation accuracy:		82.83 %
Epoch 1427 of 2000 took 0.106s
  training loss:		0.559063
  validation loss:		0.597004
  validation accuracy:		82.93 %
Epoch 1428 of 2000 took 0.106s
  training loss:		0.549603
  validation loss:		0.579617
  validation accuracy:		82.83 %
Epoch 1429 of 2000 took 0.106s
  training loss:		0.562384
  validation loss:		0.574916
  validation accuracy:		83.48 %
Epoch 1430 of 2000 took 0.106s
  training loss:		0.557774
  validation loss:		0.590546
  validation accuracy:		82.83 %
Epoch 1431 of 2000 took 0.106s
  training loss:		0.558823
  validation loss:		0.579998
  validation accuracy:		83.15 %
Epoch 1432 of 2000 took 0.106s
  training loss:		0.549581
  validation loss:		0.581152
  validation accuracy:		82.83 %
Epoch 1433 of 2000 took 0.106s
  training loss:		0.561742
  validation loss:		0.606887
  validation accuracy:		82.39 %
Epoch 1434 of 2000 took 0.106s
  training loss:		0.555475
  validation loss:		0.574335
  validation accuracy:		82.93 %
Epoch 1435 of 2000 took 0.106s
  training loss:		0.560721
  validation loss:		0.577160
  validation accuracy:		82.72 %
Epoch 1436 of 2000 took 0.106s
  training loss:		0.563930
  validation loss:		0.661764
  validation accuracy:		79.78 %
Epoch 1437 of 2000 took 0.106s
  training loss:		0.564905
  validation loss:		0.573897
  validation accuracy:		83.15 %
Epoch 1438 of 2000 took 0.106s
  training loss:		0.555393
  validation loss:		0.597964
  validation accuracy:		82.39 %
Epoch 1439 of 2000 took 0.106s
  training loss:		0.555312
  validation loss:		0.586049
  validation accuracy:		83.37 %
Epoch 1440 of 2000 took 0.106s
  training loss:		0.552199
  validation loss:		0.580653
  validation accuracy:		83.04 %
Epoch 1441 of 2000 took 0.106s
  training loss:		0.547320
  validation loss:		0.574912
  validation accuracy:		82.83 %
Epoch 1442 of 2000 took 0.105s
  training loss:		0.559619
  validation loss:		0.586829
  validation accuracy:		83.15 %
Epoch 1443 of 2000 took 0.106s
  training loss:		0.552472
  validation loss:		0.583901
  validation accuracy:		82.93 %
Epoch 1444 of 2000 took 0.105s
  training loss:		0.558860
  validation loss:		0.579561
  validation accuracy:		83.04 %
Epoch 1445 of 2000 took 0.106s
  training loss:		0.554563
  validation loss:		0.584318
  validation accuracy:		82.72 %
Epoch 1446 of 2000 took 0.106s
  training loss:		0.543515
  validation loss:		0.615901
  validation accuracy:		81.74 %
Epoch 1447 of 2000 took 0.106s
  training loss:		0.561794
  validation loss:		0.612368
  validation accuracy:		81.85 %
Epoch 1448 of 2000 took 0.106s
  training loss:		0.557187
  validation loss:		0.617565
  validation accuracy:		81.52 %
Epoch 1449 of 2000 took 0.105s
  training loss:		0.556976
  validation loss:		0.589419
  validation accuracy:		83.15 %
Epoch 1450 of 2000 took 0.106s
  training loss:		0.547793
  validation loss:		0.578071
  validation accuracy:		82.83 %
Epoch 1451 of 2000 took 0.106s
  training loss:		0.558392
  validation loss:		0.589000
  validation accuracy:		83.26 %
Epoch 1452 of 2000 took 0.106s
  training loss:		0.552506
  validation loss:		0.582135
  validation accuracy:		83.48 %
Epoch 1453 of 2000 took 0.105s
  training loss:		0.552069
  validation loss:		0.575102
  validation accuracy:		83.26 %
Epoch 1454 of 2000 took 0.106s
  training loss:		0.552684
  validation loss:		0.586303
  validation accuracy:		83.26 %
Epoch 1455 of 2000 took 0.106s
  training loss:		0.552826
  validation loss:		0.625709
  validation accuracy:		80.87 %
Epoch 1456 of 2000 took 0.106s
  training loss:		0.560110
  validation loss:		0.587430
  validation accuracy:		83.48 %
Epoch 1457 of 2000 took 0.106s
  training loss:		0.556396
  validation loss:		0.634428
  validation accuracy:		80.87 %
Epoch 1458 of 2000 took 0.106s
  training loss:		0.562064
  validation loss:		0.589636
  validation accuracy:		83.37 %
Epoch 1459 of 2000 took 0.105s
  training loss:		0.565893
  validation loss:		0.577475
  validation accuracy:		83.04 %
Epoch 1460 of 2000 took 0.106s
  training loss:		0.551183
  validation loss:		0.614015
  validation accuracy:		81.85 %
Epoch 1461 of 2000 took 0.105s
  training loss:		0.558761
  validation loss:		0.590923
  validation accuracy:		82.93 %
Epoch 1462 of 2000 took 0.106s
  training loss:		0.559039
  validation loss:		0.586544
  validation accuracy:		83.04 %
Epoch 1463 of 2000 took 0.105s
  training loss:		0.565737
  validation loss:		0.571145
  validation accuracy:		83.15 %
Epoch 1464 of 2000 took 0.106s
  training loss:		0.551003
  validation loss:		0.571567
  validation accuracy:		82.83 %
Epoch 1465 of 2000 took 0.106s
  training loss:		0.550321
  validation loss:		0.606083
  validation accuracy:		82.28 %
Epoch 1466 of 2000 took 0.106s
  training loss:		0.559394
  validation loss:		0.593983
  validation accuracy:		82.83 %
Epoch 1467 of 2000 took 0.106s
  training loss:		0.551932
  validation loss:		0.595371
  validation accuracy:		82.72 %
Epoch 1468 of 2000 took 0.106s
  training loss:		0.545203
  validation loss:		0.582872
  validation accuracy:		83.26 %
Epoch 1469 of 2000 took 0.106s
  training loss:		0.550714
  validation loss:		0.605117
  validation accuracy:		82.39 %
Epoch 1470 of 2000 took 0.106s
  training loss:		0.557967
  validation loss:		0.589340
  validation accuracy:		83.15 %
Epoch 1471 of 2000 took 0.106s
  training loss:		0.535932
  validation loss:		0.570959
  validation accuracy:		82.93 %
Epoch 1472 of 2000 took 0.106s
  training loss:		0.553393
  validation loss:		0.571724
  validation accuracy:		83.26 %
Epoch 1473 of 2000 took 0.106s
  training loss:		0.563046
  validation loss:		0.581046
  validation accuracy:		83.37 %
Epoch 1474 of 2000 took 0.105s
  training loss:		0.553312
  validation loss:		0.572513
  validation accuracy:		82.83 %
Epoch 1475 of 2000 took 0.106s
  training loss:		0.555737
  validation loss:		0.597691
  validation accuracy:		82.50 %
Epoch 1476 of 2000 took 0.106s
  training loss:		0.556151
  validation loss:		0.566930
  validation accuracy:		83.26 %
Epoch 1477 of 2000 took 0.106s
  training loss:		0.558795
  validation loss:		0.592663
  validation accuracy:		83.15 %
Epoch 1478 of 2000 took 0.105s
  training loss:		0.551248
  validation loss:		0.583207
  validation accuracy:		82.83 %
Epoch 1479 of 2000 took 0.106s
  training loss:		0.558347
  validation loss:		0.573164
  validation accuracy:		83.26 %
Epoch 1480 of 2000 took 0.105s
  training loss:		0.564958
  validation loss:		0.611198
  validation accuracy:		82.17 %
Epoch 1481 of 2000 took 0.106s
  training loss:		0.550169
  validation loss:		0.589299
  validation accuracy:		82.72 %
Epoch 1482 of 2000 took 0.106s
  training loss:		0.554868
  validation loss:		0.615309
  validation accuracy:		81.41 %
Epoch 1483 of 2000 took 0.106s
  training loss:		0.552995
  validation loss:		0.593757
  validation accuracy:		83.04 %
Epoch 1484 of 2000 took 0.106s
  training loss:		0.554774
  validation loss:		0.623083
  validation accuracy:		80.98 %
Epoch 1485 of 2000 took 0.106s
  training loss:		0.556278
  validation loss:		0.580867
  validation accuracy:		82.61 %
Epoch 1486 of 2000 took 0.106s
  training loss:		0.548262
  validation loss:		0.610632
  validation accuracy:		82.07 %
Epoch 1487 of 2000 took 0.105s
  training loss:		0.560133
  validation loss:		0.592383
  validation accuracy:		82.93 %
Epoch 1488 of 2000 took 0.108s
  training loss:		0.551038
  validation loss:		0.580601
  validation accuracy:		82.72 %
Epoch 1489 of 2000 took 0.106s
  training loss:		0.570233
  validation loss:		0.639954
  validation accuracy:		80.87 %
Epoch 1490 of 2000 took 0.106s
  training loss:		0.557889
  validation loss:		0.581391
  validation accuracy:		83.48 %
Epoch 1491 of 2000 took 0.106s
  training loss:		0.556352
  validation loss:		0.592979
  validation accuracy:		82.28 %
Epoch 1492 of 2000 took 0.106s
  training loss:		0.560902
  validation loss:		0.588155
  validation accuracy:		83.15 %
Epoch 1493 of 2000 took 0.105s
  training loss:		0.562740
  validation loss:		0.590367
  validation accuracy:		83.15 %
Epoch 1494 of 2000 took 0.106s
  training loss:		0.545715
  validation loss:		0.593543
  validation accuracy:		82.83 %
Epoch 1495 of 2000 took 0.106s
  training loss:		0.554213
  validation loss:		0.577275
  validation accuracy:		83.04 %
Epoch 1496 of 2000 took 0.106s
  training loss:		0.559630
  validation loss:		0.600350
  validation accuracy:		82.39 %
Epoch 1497 of 2000 took 0.106s
  training loss:		0.554014
  validation loss:		0.600336
  validation accuracy:		82.50 %
Epoch 1498 of 2000 took 0.106s
  training loss:		0.558902
  validation loss:		0.584303
  validation accuracy:		83.15 %
Epoch 1499 of 2000 took 0.105s
  training loss:		0.563198
  validation loss:		0.600606
  validation accuracy:		82.28 %
Epoch 1500 of 2000 took 0.106s
  training loss:		0.559238
  validation loss:		0.572791
  validation accuracy:		83.04 %
Epoch 1501 of 2000 took 0.106s
  training loss:		0.547833
  validation loss:		0.582630
  validation accuracy:		83.15 %
Epoch 1502 of 2000 took 0.106s
  training loss:		0.567879
  validation loss:		0.590590
  validation accuracy:		82.28 %
Epoch 1503 of 2000 took 0.106s
  training loss:		0.554316
  validation loss:		0.593516
  validation accuracy:		82.61 %
Epoch 1504 of 2000 took 0.105s
  training loss:		0.554038
  validation loss:		0.572392
  validation accuracy:		83.04 %
Epoch 1505 of 2000 took 0.106s
  training loss:		0.552052
  validation loss:		0.584082
  validation accuracy:		83.26 %
Epoch 1506 of 2000 took 0.105s
  training loss:		0.557316
  validation loss:		0.584267
  validation accuracy:		82.93 %
Epoch 1507 of 2000 took 0.106s
  training loss:		0.554638
  validation loss:		0.599419
  validation accuracy:		82.50 %
Epoch 1508 of 2000 took 0.106s
  training loss:		0.553634
  validation loss:		0.577905
  validation accuracy:		83.04 %
Epoch 1509 of 2000 took 0.106s
  training loss:		0.553035
  validation loss:		0.584567
  validation accuracy:		82.93 %
Epoch 1510 of 2000 took 0.105s
  training loss:		0.563725
  validation loss:		0.579996
  validation accuracy:		83.04 %
Epoch 1511 of 2000 took 0.106s
  training loss:		0.556640
  validation loss:		0.580269
  validation accuracy:		83.48 %
Epoch 1512 of 2000 took 0.106s
  training loss:		0.558527
  validation loss:		0.577205
  validation accuracy:		83.15 %
Epoch 1513 of 2000 took 0.106s
  training loss:		0.549390
  validation loss:		0.576599
  validation accuracy:		83.04 %
Epoch 1514 of 2000 took 0.105s
  training loss:		0.566008
  validation loss:		0.590593
  validation accuracy:		82.72 %
Epoch 1515 of 2000 took 0.105s
  training loss:		0.551540
  validation loss:		0.611660
  validation accuracy:		81.74 %
Epoch 1516 of 2000 took 0.105s
  training loss:		0.560874
  validation loss:		0.585785
  validation accuracy:		82.83 %
Epoch 1517 of 2000 took 0.106s
  training loss:		0.550651
  validation loss:		0.593491
  validation accuracy:		82.72 %
Epoch 1518 of 2000 took 0.106s
  training loss:		0.551469
  validation loss:		0.584290
  validation accuracy:		82.83 %
Epoch 1519 of 2000 took 0.106s
  training loss:		0.551079
  validation loss:		0.573344
  validation accuracy:		82.93 %
Epoch 1520 of 2000 took 0.106s
  training loss:		0.545682
  validation loss:		0.589591
  validation accuracy:		83.04 %
Epoch 1521 of 2000 took 0.105s
  training loss:		0.548626
  validation loss:		0.571731
  validation accuracy:		83.37 %
Epoch 1522 of 2000 took 0.106s
  training loss:		0.561311
  validation loss:		0.614027
  validation accuracy:		81.52 %
Epoch 1523 of 2000 took 0.106s
  training loss:		0.554288
  validation loss:		0.578370
  validation accuracy:		82.17 %
Epoch 1524 of 2000 took 0.106s
  training loss:		0.549099
  validation loss:		0.593011
  validation accuracy:		83.04 %
Epoch 1525 of 2000 took 0.105s
  training loss:		0.557626
  validation loss:		0.597274
  validation accuracy:		82.50 %
Epoch 1526 of 2000 took 0.106s
  training loss:		0.551690
  validation loss:		0.576270
  validation accuracy:		82.93 %
Epoch 1527 of 2000 took 0.103s
  training loss:		0.564667
  validation loss:		0.601764
  validation accuracy:		82.39 %
Epoch 1528 of 2000 took 0.102s
  training loss:		0.562716
  validation loss:		0.581060
  validation accuracy:		83.48 %
Epoch 1529 of 2000 took 0.102s
  training loss:		0.553196
  validation loss:		0.584047
  validation accuracy:		82.83 %
Epoch 1530 of 2000 took 0.102s
  training loss:		0.544641
  validation loss:		0.617875
  validation accuracy:		81.74 %
Epoch 1531 of 2000 took 0.102s
  training loss:		0.555442
  validation loss:		0.574306
  validation accuracy:		83.26 %
Epoch 1532 of 2000 took 0.102s
  training loss:		0.559985
  validation loss:		0.647502
  validation accuracy:		80.33 %
Epoch 1533 of 2000 took 0.102s
  training loss:		0.551915
  validation loss:		0.605372
  validation accuracy:		82.17 %
Epoch 1534 of 2000 took 0.104s
  training loss:		0.555958
  validation loss:		0.593628
  validation accuracy:		82.50 %
Epoch 1535 of 2000 took 0.104s
  training loss:		0.556777
  validation loss:		0.575189
  validation accuracy:		83.04 %
Epoch 1536 of 2000 took 0.102s
  training loss:		0.562671
  validation loss:		0.577867
  validation accuracy:		82.83 %
Epoch 1537 of 2000 took 0.103s
  training loss:		0.553876
  validation loss:		0.624487
  validation accuracy:		81.30 %
Epoch 1538 of 2000 took 0.103s
  training loss:		0.563295
  validation loss:		0.595337
  validation accuracy:		82.50 %
Epoch 1539 of 2000 took 0.102s
  training loss:		0.555584
  validation loss:		0.581156
  validation accuracy:		82.83 %
Epoch 1540 of 2000 took 0.103s
  training loss:		0.550041
  validation loss:		0.584358
  validation accuracy:		83.26 %
Epoch 1541 of 2000 took 0.102s
  training loss:		0.556539
  validation loss:		0.584881
  validation accuracy:		83.15 %
Epoch 1542 of 2000 took 0.102s
  training loss:		0.551996
  validation loss:		0.609670
  validation accuracy:		81.74 %
Epoch 1543 of 2000 took 0.102s
  training loss:		0.557147
  validation loss:		0.581818
  validation accuracy:		82.83 %
Epoch 1544 of 2000 took 0.102s
  training loss:		0.557506
  validation loss:		0.577550
  validation accuracy:		83.04 %
Epoch 1545 of 2000 took 0.102s
  training loss:		0.555240
  validation loss:		0.584093
  validation accuracy:		81.96 %
Epoch 1546 of 2000 took 0.103s
  training loss:		0.555478
  validation loss:		0.671079
  validation accuracy:		78.91 %
Epoch 1547 of 2000 took 0.102s
  training loss:		0.553749
  validation loss:		0.579753
  validation accuracy:		83.04 %
Epoch 1548 of 2000 took 0.102s
  training loss:		0.557663
  validation loss:		0.576674
  validation accuracy:		83.26 %
Epoch 1549 of 2000 took 0.102s
  training loss:		0.560630
  validation loss:		0.589302
  validation accuracy:		83.04 %
Epoch 1550 of 2000 took 0.102s
  training loss:		0.554794
  validation loss:		0.575447
  validation accuracy:		82.93 %
Epoch 1551 of 2000 took 0.102s
  training loss:		0.552420
  validation loss:		0.586641
  validation accuracy:		83.15 %
Epoch 1552 of 2000 took 0.102s
  training loss:		0.561254
  validation loss:		0.585900
  validation accuracy:		82.72 %
Epoch 1553 of 2000 took 0.102s
  training loss:		0.555314
  validation loss:		0.595870
  validation accuracy:		83.04 %
Epoch 1554 of 2000 took 0.102s
  training loss:		0.557138
  validation loss:		0.585017
  validation accuracy:		83.26 %
Epoch 1555 of 2000 took 0.102s
  training loss:		0.550157
  validation loss:		0.574047
  validation accuracy:		82.93 %
Epoch 1556 of 2000 took 0.102s
  training loss:		0.549066
  validation loss:		0.583595
  validation accuracy:		83.26 %
Epoch 1557 of 2000 took 0.102s
  training loss:		0.562087
  validation loss:		0.575644
  validation accuracy:		83.26 %
Epoch 1558 of 2000 took 0.102s
  training loss:		0.553172
  validation loss:		0.577836
  validation accuracy:		83.48 %
Epoch 1559 of 2000 took 0.102s
  training loss:		0.557095
  validation loss:		0.580332
  validation accuracy:		82.72 %
Epoch 1560 of 2000 took 0.102s
  training loss:		0.555639
  validation loss:		0.609579
  validation accuracy:		82.17 %
Epoch 1561 of 2000 took 0.102s
  training loss:		0.544849
  validation loss:		0.587346
  validation accuracy:		82.83 %
Epoch 1562 of 2000 took 0.102s
  training loss:		0.554824
  validation loss:		0.599459
  validation accuracy:		82.72 %
Epoch 1563 of 2000 took 0.102s
  training loss:		0.550213
  validation loss:		0.569662
  validation accuracy:		83.70 %
Epoch 1564 of 2000 took 0.102s
  training loss:		0.553486
  validation loss:		0.602884
  validation accuracy:		82.72 %
Epoch 1565 of 2000 took 0.102s
  training loss:		0.566267
  validation loss:		0.614555
  validation accuracy:		81.85 %
Epoch 1566 of 2000 took 0.102s
  training loss:		0.551675
  validation loss:		0.577738
  validation accuracy:		83.15 %
Epoch 1567 of 2000 took 0.102s
  training loss:		0.574850
  validation loss:		0.570349
  validation accuracy:		83.26 %
Epoch 1568 of 2000 took 0.102s
  training loss:		0.559631
  validation loss:		0.581842
  validation accuracy:		82.83 %
Epoch 1569 of 2000 took 0.103s
  training loss:		0.551994
  validation loss:		0.589043
  validation accuracy:		83.26 %
Epoch 1570 of 2000 took 0.102s
  training loss:		0.561014
  validation loss:		0.589850
  validation accuracy:		83.37 %
Epoch 1571 of 2000 took 0.102s
  training loss:		0.569336
  validation loss:		0.606494
  validation accuracy:		82.39 %
Epoch 1572 of 2000 took 0.102s
  training loss:		0.563980
  validation loss:		0.596990
  validation accuracy:		82.72 %
Epoch 1573 of 2000 took 0.102s
  training loss:		0.547454
  validation loss:		0.601964
  validation accuracy:		82.39 %
Epoch 1574 of 2000 took 0.102s
  training loss:		0.557605
  validation loss:		0.583322
  validation accuracy:		82.72 %
Epoch 1575 of 2000 took 0.102s
  training loss:		0.558542
  validation loss:		0.604705
  validation accuracy:		82.28 %
Epoch 1576 of 2000 took 0.102s
  training loss:		0.556002
  validation loss:		0.574530
  validation accuracy:		83.26 %
Epoch 1577 of 2000 took 0.102s
  training loss:		0.541167
  validation loss:		0.587189
  validation accuracy:		83.48 %
Epoch 1578 of 2000 took 0.102s
  training loss:		0.554137
  validation loss:		0.605802
  validation accuracy:		82.28 %
Epoch 1579 of 2000 took 0.102s
  training loss:		0.549121
  validation loss:		0.577361
  validation accuracy:		83.26 %
Epoch 1580 of 2000 took 0.102s
  training loss:		0.549734
  validation loss:		0.605751
  validation accuracy:		82.50 %
Epoch 1581 of 2000 took 0.102s
  training loss:		0.552750
  validation loss:		0.573016
  validation accuracy:		83.15 %
Epoch 1582 of 2000 took 0.102s
  training loss:		0.552044
  validation loss:		0.607921
  validation accuracy:		81.96 %
Epoch 1583 of 2000 took 0.102s
  training loss:		0.548566
  validation loss:		0.587265
  validation accuracy:		83.26 %
Epoch 1584 of 2000 took 0.102s
  training loss:		0.552639
  validation loss:		0.588915
  validation accuracy:		83.15 %
Epoch 1585 of 2000 took 0.102s
  training loss:		0.557989
  validation loss:		0.572701
  validation accuracy:		83.04 %
Epoch 1586 of 2000 took 0.102s
  training loss:		0.555157
  validation loss:		0.583770
  validation accuracy:		82.50 %
Epoch 1587 of 2000 took 0.102s
  training loss:		0.544571
  validation loss:		0.579570
  validation accuracy:		83.04 %
Epoch 1588 of 2000 took 0.102s
  training loss:		0.561368
  validation loss:		0.642424
  validation accuracy:		80.43 %
Epoch 1589 of 2000 took 0.102s
  training loss:		0.561014
  validation loss:		0.583087
  validation accuracy:		82.93 %
Epoch 1590 of 2000 took 0.102s
  training loss:		0.557921
  validation loss:		0.570894
  validation accuracy:		83.37 %
Epoch 1591 of 2000 took 0.102s
  training loss:		0.558558
  validation loss:		0.597397
  validation accuracy:		83.04 %
Epoch 1592 of 2000 took 0.102s
  training loss:		0.550743
  validation loss:		0.606203
  validation accuracy:		82.28 %
Epoch 1593 of 2000 took 0.102s
  training loss:		0.562873
  validation loss:		0.583752
  validation accuracy:		82.83 %
Epoch 1594 of 2000 took 0.102s
  training loss:		0.556949
  validation loss:		0.572284
  validation accuracy:		83.37 %
Epoch 1595 of 2000 took 0.102s
  training loss:		0.550014
  validation loss:		0.584523
  validation accuracy:		83.59 %
Epoch 1596 of 2000 took 0.102s
  training loss:		0.552060
  validation loss:		0.578594
  validation accuracy:		82.83 %
Epoch 1597 of 2000 took 0.102s
  training loss:		0.559370
  validation loss:		0.582324
  validation accuracy:		83.15 %
Epoch 1598 of 2000 took 0.102s
  training loss:		0.554217
  validation loss:		0.585739
  validation accuracy:		82.93 %
Epoch 1599 of 2000 took 0.103s
  training loss:		0.560329
  validation loss:		0.593330
  validation accuracy:		83.15 %
Epoch 1600 of 2000 took 0.103s
  training loss:		0.551606
  validation loss:		0.589641
  validation accuracy:		83.37 %
Epoch 1601 of 2000 took 0.102s
  training loss:		0.547812
  validation loss:		0.583334
  validation accuracy:		83.37 %
Epoch 1602 of 2000 took 0.102s
  training loss:		0.555033
  validation loss:		0.578786
  validation accuracy:		82.83 %
Epoch 1603 of 2000 took 0.102s
  training loss:		0.560062
  validation loss:		0.619609
  validation accuracy:		81.52 %
Epoch 1604 of 2000 took 0.102s
  training loss:		0.556517
  validation loss:		0.583055
  validation accuracy:		83.48 %
Epoch 1605 of 2000 took 0.103s
  training loss:		0.558501
  validation loss:		0.601786
  validation accuracy:		82.50 %
Epoch 1606 of 2000 took 0.102s
  training loss:		0.552829
  validation loss:		0.628738
  validation accuracy:		81.41 %
Epoch 1607 of 2000 took 0.102s
  training loss:		0.555673
  validation loss:		0.593086
  validation accuracy:		83.26 %
Epoch 1608 of 2000 took 0.102s
  training loss:		0.551265
  validation loss:		0.597581
  validation accuracy:		82.50 %
Epoch 1609 of 2000 took 0.102s
  training loss:		0.552683
  validation loss:		0.581523
  validation accuracy:		82.83 %
Epoch 1610 of 2000 took 0.102s
  training loss:		0.548306
  validation loss:		0.585191
  validation accuracy:		82.72 %
Epoch 1611 of 2000 took 0.102s
  training loss:		0.542736
  validation loss:		0.604023
  validation accuracy:		82.61 %
Epoch 1612 of 2000 took 0.102s
  training loss:		0.550878
  validation loss:		0.582160
  validation accuracy:		82.93 %
Epoch 1613 of 2000 took 0.102s
  training loss:		0.557786
  validation loss:		0.580407
  validation accuracy:		83.37 %
Epoch 1614 of 2000 took 0.102s
  training loss:		0.539116
  validation loss:		0.583375
  validation accuracy:		83.37 %
Epoch 1615 of 2000 took 0.102s
  training loss:		0.552969
  validation loss:		0.592734
  validation accuracy:		83.26 %
Epoch 1616 of 2000 took 0.102s
  training loss:		0.549522
  validation loss:		0.597468
  validation accuracy:		82.83 %
Epoch 1617 of 2000 took 0.102s
  training loss:		0.547360
  validation loss:		0.577783
  validation accuracy:		83.48 %
Epoch 1618 of 2000 took 0.102s
  training loss:		0.550716
  validation loss:		0.577133
  validation accuracy:		83.37 %
Epoch 1619 of 2000 took 0.102s
  training loss:		0.548321
  validation loss:		0.610073
  validation accuracy:		81.74 %
Epoch 1620 of 2000 took 0.102s
  training loss:		0.558817
  validation loss:		0.576939
  validation accuracy:		83.26 %
Epoch 1621 of 2000 took 0.102s
  training loss:		0.548588
  validation loss:		0.585367
  validation accuracy:		83.04 %
Epoch 1622 of 2000 took 0.102s
  training loss:		0.548813
  validation loss:		0.577734
  validation accuracy:		82.93 %
Epoch 1623 of 2000 took 0.102s
  training loss:		0.553643
  validation loss:		0.583582
  validation accuracy:		83.04 %
Epoch 1624 of 2000 took 0.102s
  training loss:		0.539251
  validation loss:		0.600289
  validation accuracy:		82.28 %
Epoch 1625 of 2000 took 0.102s
  training loss:		0.543984
  validation loss:		0.580969
  validation accuracy:		82.93 %
Epoch 1626 of 2000 took 0.102s
  training loss:		0.548179
  validation loss:		0.593896
  validation accuracy:		82.83 %
Epoch 1627 of 2000 took 0.102s
  training loss:		0.544154
  validation loss:		0.580628
  validation accuracy:		83.04 %
Epoch 1628 of 2000 took 0.103s
  training loss:		0.544652
  validation loss:		0.609000
  validation accuracy:		82.07 %
Epoch 1629 of 2000 took 0.102s
  training loss:		0.555418
  validation loss:		0.592950
  validation accuracy:		82.50 %
Epoch 1630 of 2000 took 0.102s
  training loss:		0.550043
  validation loss:		0.573891
  validation accuracy:		83.37 %
Epoch 1631 of 2000 took 0.102s
  training loss:		0.549607
  validation loss:		0.594944
  validation accuracy:		83.26 %
Epoch 1632 of 2000 took 0.102s
  training loss:		0.550707
  validation loss:		0.575169
  validation accuracy:		83.26 %
Epoch 1633 of 2000 took 0.102s
  training loss:		0.549307
  validation loss:		0.581528
  validation accuracy:		82.93 %
Epoch 1634 of 2000 took 0.102s
  training loss:		0.563929
  validation loss:		0.589293
  validation accuracy:		83.26 %
Epoch 1635 of 2000 took 0.102s
  training loss:		0.551218
  validation loss:		0.583010
  validation accuracy:		82.83 %
Epoch 1636 of 2000 took 0.102s
  training loss:		0.548970
  validation loss:		0.601627
  validation accuracy:		82.07 %
Epoch 1637 of 2000 took 0.102s
  training loss:		0.554227
  validation loss:		0.605303
  validation accuracy:		82.28 %
Epoch 1638 of 2000 took 0.102s
  training loss:		0.554677
  validation loss:		0.575364
  validation accuracy:		83.26 %
Epoch 1639 of 2000 took 0.102s
  training loss:		0.546828
  validation loss:		0.600560
  validation accuracy:		82.50 %
Epoch 1640 of 2000 took 0.103s
  training loss:		0.538195
  validation loss:		0.580361
  validation accuracy:		82.61 %
Epoch 1641 of 2000 took 0.103s
  training loss:		0.551705
  validation loss:		0.597281
  validation accuracy:		82.72 %
Epoch 1642 of 2000 took 0.102s
  training loss:		0.548792
  validation loss:		0.588532
  validation accuracy:		83.15 %
Epoch 1643 of 2000 took 0.103s
  training loss:		0.534778
  validation loss:		0.577641
  validation accuracy:		83.48 %
Epoch 1644 of 2000 took 0.103s
  training loss:		0.543959
  validation loss:		0.590543
  validation accuracy:		83.37 %
Epoch 1645 of 2000 took 0.102s
  training loss:		0.556657
  validation loss:		0.591089
  validation accuracy:		82.72 %
Epoch 1646 of 2000 took 0.102s
  training loss:		0.556091
  validation loss:		0.573304
  validation accuracy:		83.26 %
Epoch 1647 of 2000 took 0.102s
  training loss:		0.547397
  validation loss:		0.578550
  validation accuracy:		82.72 %
Epoch 1648 of 2000 took 0.102s
  training loss:		0.552618
  validation loss:		0.582114
  validation accuracy:		83.26 %
Epoch 1649 of 2000 took 0.102s
  training loss:		0.541783
  validation loss:		0.572746
  validation accuracy:		83.04 %
Epoch 1650 of 2000 took 0.102s
  training loss:		0.547705
  validation loss:		0.590361
  validation accuracy:		83.15 %
Epoch 1651 of 2000 took 0.102s
  training loss:		0.549831
  validation loss:		0.572978
  validation accuracy:		83.15 %
Epoch 1652 of 2000 took 0.102s
  training loss:		0.554768
  validation loss:		0.584185
  validation accuracy:		83.37 %
Epoch 1653 of 2000 took 0.102s
  training loss:		0.544995
  validation loss:		0.571425
  validation accuracy:		83.26 %
Epoch 1654 of 2000 took 0.102s
  training loss:		0.543613
  validation loss:		0.576449
  validation accuracy:		82.93 %
Epoch 1655 of 2000 took 0.103s
  training loss:		0.551389
  validation loss:		0.581665
  validation accuracy:		83.15 %
Epoch 1656 of 2000 took 0.102s
  training loss:		0.552557
  validation loss:		0.576220
  validation accuracy:		83.26 %
Epoch 1657 of 2000 took 0.102s
  training loss:		0.540399
  validation loss:		0.592741
  validation accuracy:		83.04 %
Epoch 1658 of 2000 took 0.103s
  training loss:		0.548540
  validation loss:		0.579518
  validation accuracy:		83.26 %
Epoch 1659 of 2000 took 0.102s
  training loss:		0.553341
  validation loss:		0.571175
  validation accuracy:		83.48 %
Epoch 1660 of 2000 took 0.102s
  training loss:		0.549491
  validation loss:		0.576895
  validation accuracy:		82.93 %
Epoch 1661 of 2000 took 0.102s
  training loss:		0.557010
  validation loss:		0.633414
  validation accuracy:		80.43 %
Epoch 1662 of 2000 took 0.102s
  training loss:		0.553039
  validation loss:		0.587710
  validation accuracy:		82.72 %
Epoch 1663 of 2000 took 0.102s
  training loss:		0.543113
  validation loss:		0.572166
  validation accuracy:		83.04 %
Epoch 1664 of 2000 took 0.102s
  training loss:		0.558560
  validation loss:		0.579583
  validation accuracy:		82.93 %
Epoch 1665 of 2000 took 0.102s
  training loss:		0.548990
  validation loss:		0.589319
  validation accuracy:		82.72 %
Epoch 1666 of 2000 took 0.102s
  training loss:		0.549126
  validation loss:		0.579808
  validation accuracy:		83.48 %
Epoch 1667 of 2000 took 0.102s
  training loss:		0.555776
  validation loss:		0.609360
  validation accuracy:		81.96 %
Epoch 1668 of 2000 took 0.102s
  training loss:		0.543715
  validation loss:		0.581539
  validation accuracy:		83.59 %
Epoch 1669 of 2000 took 0.102s
  training loss:		0.543903
  validation loss:		0.573566
  validation accuracy:		82.93 %
Epoch 1670 of 2000 took 0.102s
  training loss:		0.546322
  validation loss:		0.572101
  validation accuracy:		83.37 %
Epoch 1671 of 2000 took 0.102s
  training loss:		0.541603
  validation loss:		0.577136
  validation accuracy:		83.15 %
Epoch 1672 of 2000 took 0.102s
  training loss:		0.559280
  validation loss:		0.568158
  validation accuracy:		83.26 %
Epoch 1673 of 2000 took 0.102s
  training loss:		0.547781
  validation loss:		0.606439
  validation accuracy:		82.28 %
Epoch 1674 of 2000 took 0.102s
  training loss:		0.543919
  validation loss:		0.571609
  validation accuracy:		83.04 %
Epoch 1675 of 2000 took 0.102s
  training loss:		0.539052
  validation loss:		0.574265
  validation accuracy:		83.37 %
Epoch 1676 of 2000 took 0.102s
  training loss:		0.541168
  validation loss:		0.606677
  validation accuracy:		82.28 %
Epoch 1677 of 2000 took 0.102s
  training loss:		0.546637
  validation loss:		0.598293
  validation accuracy:		82.07 %
Epoch 1678 of 2000 took 0.102s
  training loss:		0.552747
  validation loss:		0.571953
  validation accuracy:		83.26 %
Epoch 1679 of 2000 took 0.102s
  training loss:		0.545156
  validation loss:		0.579046
  validation accuracy:		83.37 %
Epoch 1680 of 2000 took 0.103s
  training loss:		0.540521
  validation loss:		0.587988
  validation accuracy:		83.37 %
Epoch 1681 of 2000 took 0.102s
  training loss:		0.549108
  validation loss:		0.572571
  validation accuracy:		83.26 %
Epoch 1682 of 2000 took 0.102s
  training loss:		0.544741
  validation loss:		0.592888
  validation accuracy:		83.04 %
Epoch 1683 of 2000 took 0.103s
  training loss:		0.546727
  validation loss:		0.569925
  validation accuracy:		83.59 %
Epoch 1684 of 2000 took 0.102s
  training loss:		0.544501
  validation loss:		0.593238
  validation accuracy:		82.50 %
Epoch 1685 of 2000 took 0.102s
  training loss:		0.538909
  validation loss:		0.625174
  validation accuracy:		80.98 %
Epoch 1686 of 2000 took 0.102s
  training loss:		0.531327
  validation loss:		0.571713
  validation accuracy:		83.15 %
Epoch 1687 of 2000 took 0.103s
  training loss:		0.542197
  validation loss:		0.570874
  validation accuracy:		83.26 %
Epoch 1688 of 2000 took 0.102s
  training loss:		0.551747
  validation loss:		0.587769
  validation accuracy:		83.48 %
Epoch 1689 of 2000 took 0.102s
  training loss:		0.546059
  validation loss:		0.583858
  validation accuracy:		83.26 %
Epoch 1690 of 2000 took 0.102s
  training loss:		0.538132
  validation loss:		0.576462
  validation accuracy:		83.37 %
Epoch 1691 of 2000 took 0.102s
  training loss:		0.539747
  validation loss:		0.578100
  validation accuracy:		83.37 %
Epoch 1692 of 2000 took 0.102s
  training loss:		0.545682
  validation loss:		0.589260
  validation accuracy:		83.15 %
Epoch 1693 of 2000 took 0.102s
  training loss:		0.534904
  validation loss:		0.576157
  validation accuracy:		83.48 %
Epoch 1694 of 2000 took 0.102s
  training loss:		0.542890
  validation loss:		0.578376
  validation accuracy:		83.15 %
Epoch 1695 of 2000 took 0.102s
  training loss:		0.545214
  validation loss:		0.579823
  validation accuracy:		83.26 %
Epoch 1696 of 2000 took 0.102s
  training loss:		0.544738
  validation loss:		0.587772
  validation accuracy:		83.26 %
Epoch 1697 of 2000 took 0.102s
  training loss:		0.546298
  validation loss:		0.569127
  validation accuracy:		83.48 %
Epoch 1698 of 2000 took 0.102s
  training loss:		0.539590
  validation loss:		0.577609
  validation accuracy:		82.93 %
Epoch 1699 of 2000 took 0.102s
  training loss:		0.540167
  validation loss:		0.589496
  validation accuracy:		82.93 %
Epoch 1700 of 2000 took 0.102s
  training loss:		0.544880
  validation loss:		0.594560
  validation accuracy:		82.39 %
Epoch 1701 of 2000 took 0.102s
  training loss:		0.526997
  validation loss:		0.567649
  validation accuracy:		83.37 %
Epoch 1702 of 2000 took 0.103s
  training loss:		0.542665
  validation loss:		0.600895
  validation accuracy:		82.72 %
Epoch 1703 of 2000 took 0.103s
  training loss:		0.537185
  validation loss:		0.570761
  validation accuracy:		83.15 %
Epoch 1704 of 2000 took 0.103s
  training loss:		0.536029
  validation loss:		0.571980
  validation accuracy:		83.37 %
Epoch 1705 of 2000 took 0.102s
  training loss:		0.539976
  validation loss:		0.585287
  validation accuracy:		83.37 %
Epoch 1706 of 2000 took 0.102s
  training loss:		0.538030
  validation loss:		0.566455
  validation accuracy:		83.04 %
Epoch 1707 of 2000 took 0.103s
  training loss:		0.539125
  validation loss:		0.580965
  validation accuracy:		83.59 %
Epoch 1708 of 2000 took 0.102s
  training loss:		0.535355
  validation loss:		0.585836
  validation accuracy:		83.70 %
Epoch 1709 of 2000 took 0.102s
  training loss:		0.537662
  validation loss:		0.580798
  validation accuracy:		83.59 %
Epoch 1710 of 2000 took 0.102s
  training loss:		0.544515
  validation loss:		0.578695
  validation accuracy:		83.04 %
Epoch 1711 of 2000 took 0.102s
  training loss:		0.545657
  validation loss:		0.590350
  validation accuracy:		82.72 %
Epoch 1712 of 2000 took 0.102s
  training loss:		0.543434
  validation loss:		0.562981
  validation accuracy:		83.59 %
Epoch 1713 of 2000 took 0.102s
  training loss:		0.533316
  validation loss:		0.588347
  validation accuracy:		82.83 %
Epoch 1714 of 2000 took 0.102s
  training loss:		0.535660
  validation loss:		0.563640
  validation accuracy:		83.48 %
Epoch 1715 of 2000 took 0.102s
  training loss:		0.536985
  validation loss:		0.584124
  validation accuracy:		83.59 %
Epoch 1716 of 2000 took 0.102s
  training loss:		0.541665
  validation loss:		0.594299
  validation accuracy:		82.61 %
Epoch 1717 of 2000 took 0.104s
  training loss:		0.536605
  validation loss:		0.590368
  validation accuracy:		83.37 %
Epoch 1718 of 2000 took 0.102s
  training loss:		0.539241
  validation loss:		0.568621
  validation accuracy:		83.26 %
Epoch 1719 of 2000 took 0.103s
  training loss:		0.543679
  validation loss:		0.563542
  validation accuracy:		83.26 %
Epoch 1720 of 2000 took 0.102s
  training loss:		0.541133
  validation loss:		0.576803
  validation accuracy:		83.26 %
Epoch 1721 of 2000 took 0.102s
  training loss:		0.532080
  validation loss:		0.581030
  validation accuracy:		83.48 %
Epoch 1722 of 2000 took 0.103s
  training loss:		0.545276
  validation loss:		0.558241
  validation accuracy:		83.80 %
Epoch 1723 of 2000 took 0.102s
  training loss:		0.536070
  validation loss:		0.563553
  validation accuracy:		83.04 %
Epoch 1724 of 2000 took 0.102s
  training loss:		0.540039
  validation loss:		0.564490
  validation accuracy:		83.37 %
Epoch 1725 of 2000 took 0.102s
  training loss:		0.532940
  validation loss:		0.574297
  validation accuracy:		83.48 %
Epoch 1726 of 2000 took 0.102s
  training loss:		0.526115
  validation loss:		0.568571
  validation accuracy:		83.15 %
Epoch 1727 of 2000 took 0.102s
  training loss:		0.532299
  validation loss:		0.561819
  validation accuracy:		83.59 %
Epoch 1728 of 2000 took 0.102s
  training loss:		0.535358
  validation loss:		0.574460
  validation accuracy:		83.37 %
Epoch 1729 of 2000 took 0.102s
  training loss:		0.528333
  validation loss:		0.581554
  validation accuracy:		83.59 %
Epoch 1730 of 2000 took 0.102s
  training loss:		0.528188
  validation loss:		0.569223
  validation accuracy:		83.26 %
Epoch 1731 of 2000 took 0.102s
  training loss:		0.532990
  validation loss:		0.586907
  validation accuracy:		83.48 %
Epoch 1732 of 2000 took 0.102s
  training loss:		0.527077
  validation loss:		0.583255
  validation accuracy:		83.48 %
Epoch 1733 of 2000 took 0.102s
  training loss:		0.535551
  validation loss:		0.555031
  validation accuracy:		83.59 %
Epoch 1734 of 2000 took 0.102s
  training loss:		0.534242
  validation loss:		0.573163
  validation accuracy:		83.59 %
Epoch 1735 of 2000 took 0.102s
  training loss:		0.525440
  validation loss:		0.557598
  validation accuracy:		83.26 %
Epoch 1736 of 2000 took 0.102s
  training loss:		0.531140
  validation loss:		0.564852
  validation accuracy:		83.70 %
Epoch 1737 of 2000 took 0.102s
  training loss:		0.535368
  validation loss:		0.563463
  validation accuracy:		83.48 %
Epoch 1738 of 2000 took 0.102s
  training loss:		0.530806
  validation loss:		0.590050
  validation accuracy:		83.15 %
Epoch 1739 of 2000 took 0.103s
  training loss:		0.529784
  validation loss:		0.566326
  validation accuracy:		83.59 %
Epoch 1740 of 2000 took 0.102s
  training loss:		0.525797
  validation loss:		0.576510
  validation accuracy:		83.26 %
Epoch 1741 of 2000 took 0.102s
  training loss:		0.533033
  validation loss:		0.564947
  validation accuracy:		83.48 %
Epoch 1742 of 2000 took 0.102s
  training loss:		0.535750
  validation loss:		0.592129
  validation accuracy:		82.93 %
Epoch 1743 of 2000 took 0.103s
  training loss:		0.522906
  validation loss:		0.562148
  validation accuracy:		83.70 %
Epoch 1744 of 2000 took 0.102s
  training loss:		0.526675
  validation loss:		0.563973
  validation accuracy:		83.59 %
Epoch 1745 of 2000 took 0.102s
  training loss:		0.531032
  validation loss:		0.615658
  validation accuracy:		81.52 %
Epoch 1746 of 2000 took 0.103s
  training loss:		0.518101
  validation loss:		0.570771
  validation accuracy:		83.80 %
Epoch 1747 of 2000 took 0.102s
  training loss:		0.520589
  validation loss:		0.568854
  validation accuracy:		83.48 %
Epoch 1748 of 2000 took 0.102s
  training loss:		0.524078
  validation loss:		0.567410
  validation accuracy:		83.80 %
Epoch 1749 of 2000 took 0.103s
  training loss:		0.511347
  validation loss:		0.570645
  validation accuracy:		83.59 %
Epoch 1750 of 2000 took 0.102s
  training loss:		0.518639
  validation loss:		0.554922
  validation accuracy:		83.48 %
Epoch 1751 of 2000 took 0.103s
  training loss:		0.526162
  validation loss:		0.548878
  validation accuracy:		83.70 %
Epoch 1752 of 2000 took 0.102s
  training loss:		0.524964
  validation loss:		0.565691
  validation accuracy:		83.48 %
Epoch 1753 of 2000 took 0.102s
  training loss:		0.517262
  validation loss:		0.562724
  validation accuracy:		83.91 %
Epoch 1754 of 2000 took 0.102s
  training loss:		0.512348
  validation loss:		0.567624
  validation accuracy:		83.59 %
Epoch 1755 of 2000 took 0.102s
  training loss:		0.517808
  validation loss:		0.559879
  validation accuracy:		83.37 %
Epoch 1756 of 2000 took 0.102s
  training loss:		0.519357
  validation loss:		0.561612
  validation accuracy:		83.15 %
Epoch 1757 of 2000 took 0.102s
  training loss:		0.507577
  validation loss:		0.551907
  validation accuracy:		83.91 %
Epoch 1758 of 2000 took 0.103s
  training loss:		0.511697
  validation loss:		0.561003
  validation accuracy:		83.48 %
Epoch 1759 of 2000 took 0.102s
  training loss:		0.517612
  validation loss:		0.549681
  validation accuracy:		83.80 %
Epoch 1760 of 2000 took 0.102s
  training loss:		0.512105
  validation loss:		0.596936
  validation accuracy:		82.17 %
Epoch 1761 of 2000 took 0.103s
  training loss:		0.504120
  validation loss:		0.560145
  validation accuracy:		83.80 %
Epoch 1762 of 2000 took 0.103s
  training loss:		0.512340
  validation loss:		0.561715
  validation accuracy:		84.02 %
Epoch 1763 of 2000 took 0.102s
  training loss:		0.508994
  validation loss:		0.542514
  validation accuracy:		84.02 %
Epoch 1764 of 2000 took 0.102s
  training loss:		0.506808
  validation loss:		0.567916
  validation accuracy:		83.59 %
Epoch 1765 of 2000 took 0.102s
  training loss:		0.508565
  validation loss:		0.568054
  validation accuracy:		83.37 %
Epoch 1766 of 2000 took 0.102s
  training loss:		0.504009
  validation loss:		0.558905
  validation accuracy:		83.80 %
Epoch 1767 of 2000 took 0.102s
  training loss:		0.503044
  validation loss:		0.541265
  validation accuracy:		84.02 %
Epoch 1768 of 2000 took 0.102s
  training loss:		0.499658
  validation loss:		0.562705
  validation accuracy:		83.70 %
Epoch 1769 of 2000 took 0.102s
  training loss:		0.508028
  validation loss:		0.556296
  validation accuracy:		83.80 %
Epoch 1770 of 2000 took 0.102s
  training loss:		0.498639
  validation loss:		0.546981
  validation accuracy:		84.13 %
Epoch 1771 of 2000 took 0.102s
  training loss:		0.503909
  validation loss:		0.537925
  validation accuracy:		84.24 %
Epoch 1772 of 2000 took 0.102s
  training loss:		0.495670
  validation loss:		0.537523
  validation accuracy:		84.24 %
Epoch 1773 of 2000 took 0.102s
  training loss:		0.509715
  validation loss:		0.548545
  validation accuracy:		84.02 %
Epoch 1774 of 2000 took 0.102s
  training loss:		0.497986
  validation loss:		0.531328
  validation accuracy:		84.24 %
Epoch 1775 of 2000 took 0.103s
  training loss:		0.502249
  validation loss:		0.553981
  validation accuracy:		84.02 %
Epoch 1776 of 2000 took 0.102s
  training loss:		0.500651
  validation loss:		0.543927
  validation accuracy:		84.24 %
Epoch 1777 of 2000 took 0.102s
  training loss:		0.494366
  validation loss:		0.539051
  validation accuracy:		84.13 %
Epoch 1778 of 2000 took 0.102s
  training loss:		0.493243
  validation loss:		0.535677
  validation accuracy:		84.35 %
Epoch 1779 of 2000 took 0.102s
  training loss:		0.500971
  validation loss:		0.527519
  validation accuracy:		84.57 %
Epoch 1780 of 2000 took 0.102s
  training loss:		0.495216
  validation loss:		0.561656
  validation accuracy:		83.70 %
Epoch 1781 of 2000 took 0.102s
  training loss:		0.497214
  validation loss:		0.566377
  validation accuracy:		83.26 %
Epoch 1782 of 2000 took 0.102s
  training loss:		0.489619
  validation loss:		0.551249
  validation accuracy:		83.80 %
Epoch 1783 of 2000 took 0.102s
  training loss:		0.480067
  validation loss:		0.533250
  validation accuracy:		84.67 %
Epoch 1784 of 2000 took 0.102s
  training loss:		0.485157
  validation loss:		0.554618
  validation accuracy:		83.80 %
Epoch 1785 of 2000 took 0.102s
  training loss:		0.494835
  validation loss:		0.539395
  validation accuracy:		83.59 %
Epoch 1786 of 2000 took 0.102s
  training loss:		0.482720
  validation loss:		0.539555
  validation accuracy:		84.35 %
Epoch 1787 of 2000 took 0.102s
  training loss:		0.480066
  validation loss:		0.537714
  validation accuracy:		84.13 %
Epoch 1788 of 2000 took 0.102s
  training loss:		0.488450
  validation loss:		0.553090
  validation accuracy:		83.91 %
Epoch 1789 of 2000 took 0.103s
  training loss:		0.493411
  validation loss:		0.540662
  validation accuracy:		83.91 %
Epoch 1790 of 2000 took 0.102s
  training loss:		0.490030
  validation loss:		0.534865
  validation accuracy:		83.91 %
Epoch 1791 of 2000 took 0.102s
  training loss:		0.483012
  validation loss:		0.531898
  validation accuracy:		83.91 %
Epoch 1792 of 2000 took 0.103s
  training loss:		0.490254
  validation loss:		0.533599
  validation accuracy:		84.67 %
Epoch 1793 of 2000 took 0.102s
  training loss:		0.484076
  validation loss:		0.517954
  validation accuracy:		84.89 %
Epoch 1794 of 2000 took 0.102s
  training loss:		0.476460
  validation loss:		0.554787
  validation accuracy:		84.24 %
Epoch 1795 of 2000 took 0.102s
  training loss:		0.483618
  validation loss:		0.556924
  validation accuracy:		84.02 %
Epoch 1796 of 2000 took 0.102s
  training loss:		0.481139
  validation loss:		0.521676
  validation accuracy:		84.24 %
Epoch 1797 of 2000 took 0.102s
  training loss:		0.475870
  validation loss:		0.528953
  validation accuracy:		84.89 %
Epoch 1798 of 2000 took 0.102s
  training loss:		0.471038
  validation loss:		0.526778
  validation accuracy:		85.00 %
Epoch 1799 of 2000 took 0.102s
  training loss:		0.473876
  validation loss:		0.553287
  validation accuracy:		84.02 %
Epoch 1800 of 2000 took 0.103s
  training loss:		0.475381
  validation loss:		0.535898
  validation accuracy:		85.00 %
Epoch 1801 of 2000 took 0.102s
  training loss:		0.467549
  validation loss:		0.517313
  validation accuracy:		84.46 %
Epoch 1802 of 2000 took 0.102s
  training loss:		0.472014
  validation loss:		0.533722
  validation accuracy:		85.11 %
Epoch 1803 of 2000 took 0.102s
  training loss:		0.468023
  validation loss:		0.526539
  validation accuracy:		84.78 %
Epoch 1804 of 2000 took 0.102s
  training loss:		0.475210
  validation loss:		0.514477
  validation accuracy:		84.35 %
Epoch 1805 of 2000 took 0.103s
  training loss:		0.469440
  validation loss:		0.523222
  validation accuracy:		84.35 %
Epoch 1806 of 2000 took 0.102s
  training loss:		0.467668
  validation loss:		0.521028
  validation accuracy:		84.35 %
Epoch 1807 of 2000 took 0.102s
  training loss:		0.465474
  validation loss:		0.534526
  validation accuracy:		85.00 %
Epoch 1808 of 2000 took 0.102s
  training loss:		0.463837
  validation loss:		0.513516
  validation accuracy:		84.46 %
Epoch 1809 of 2000 took 0.102s
  training loss:		0.464542
  validation loss:		0.552205
  validation accuracy:		84.46 %
Epoch 1810 of 2000 took 0.102s
  training loss:		0.469148
  validation loss:		0.548848
  validation accuracy:		84.89 %
Epoch 1811 of 2000 took 0.102s
  training loss:		0.465639
  validation loss:		0.508938
  validation accuracy:		84.57 %
Epoch 1812 of 2000 took 0.102s
  training loss:		0.466623
  validation loss:		0.511094
  validation accuracy:		85.33 %
Epoch 1813 of 2000 took 0.102s
  training loss:		0.467244
  validation loss:		0.547116
  validation accuracy:		84.35 %
Epoch 1814 of 2000 took 0.102s
  training loss:		0.462813
  validation loss:		0.531200
  validation accuracy:		84.46 %
Epoch 1815 of 2000 took 0.102s
  training loss:		0.470065
  validation loss:		0.543439
  validation accuracy:		84.78 %
Epoch 1816 of 2000 took 0.102s
  training loss:		0.457228
  validation loss:		0.538268
  validation accuracy:		84.78 %
Epoch 1817 of 2000 took 0.102s
  training loss:		0.459022
  validation loss:		0.525399
  validation accuracy:		84.24 %
Epoch 1818 of 2000 took 0.102s
  training loss:		0.454377
  validation loss:		0.515731
  validation accuracy:		84.89 %
Epoch 1819 of 2000 took 0.102s
  training loss:		0.464761
  validation loss:		0.518350
  validation accuracy:		84.78 %
Epoch 1820 of 2000 took 0.102s
  training loss:		0.470504
  validation loss:		0.509253
  validation accuracy:		85.00 %
Epoch 1821 of 2000 took 0.102s
  training loss:		0.463555
  validation loss:		0.502223
  validation accuracy:		84.35 %
Epoch 1822 of 2000 took 0.102s
  training loss:		0.454011
  validation loss:		0.509854
  validation accuracy:		85.43 %
Epoch 1823 of 2000 took 0.102s
  training loss:		0.460570
  validation loss:		0.517500
  validation accuracy:		84.89 %
Epoch 1824 of 2000 took 0.102s
  training loss:		0.458923
  validation loss:		0.503760
  validation accuracy:		85.65 %
Epoch 1825 of 2000 took 0.102s
  training loss:		0.451338
  validation loss:		0.507648
  validation accuracy:		84.67 %
Epoch 1826 of 2000 took 0.102s
  training loss:		0.456438
  validation loss:		0.513550
  validation accuracy:		85.11 %
Epoch 1827 of 2000 took 0.102s
  training loss:		0.444762
  validation loss:		0.499125
  validation accuracy:		85.33 %
Epoch 1828 of 2000 took 0.102s
  training loss:		0.452287
  validation loss:		0.524565
  validation accuracy:		85.33 %
Epoch 1829 of 2000 took 0.102s
  training loss:		0.455904
  validation loss:		0.508516
  validation accuracy:		85.54 %
Epoch 1830 of 2000 took 0.102s
  training loss:		0.451268
  validation loss:		0.519491
  validation accuracy:		84.67 %
Epoch 1831 of 2000 took 0.102s
  training loss:		0.451438
  validation loss:		0.514241
  validation accuracy:		84.78 %
Epoch 1832 of 2000 took 0.102s
  training loss:		0.449992
  validation loss:		0.498295
  validation accuracy:		84.78 %
Epoch 1833 of 2000 took 0.102s
  training loss:		0.448455
  validation loss:		0.524270
  validation accuracy:		84.67 %
Epoch 1834 of 2000 took 0.103s
  training loss:		0.447746
  validation loss:		0.524723
  validation accuracy:		84.13 %
Epoch 1835 of 2000 took 0.102s
  training loss:		0.438598
  validation loss:		0.504779
  validation accuracy:		85.00 %
Epoch 1836 of 2000 took 0.103s
  training loss:		0.442297
  validation loss:		0.500079
  validation accuracy:		85.54 %
Epoch 1837 of 2000 took 0.103s
  training loss:		0.445769
  validation loss:		0.495641
  validation accuracy:		85.22 %
Epoch 1838 of 2000 took 0.102s
  training loss:		0.445184
  validation loss:		0.523369
  validation accuracy:		85.00 %
Epoch 1839 of 2000 took 0.103s
  training loss:		0.443240
  validation loss:		0.499854
  validation accuracy:		84.67 %
Epoch 1840 of 2000 took 0.102s
  training loss:		0.441280
  validation loss:		0.511841
  validation accuracy:		85.54 %
Epoch 1841 of 2000 took 0.103s
  training loss:		0.435934
  validation loss:		0.515743
  validation accuracy:		84.89 %
Epoch 1842 of 2000 took 0.102s
  training loss:		0.438642
  validation loss:		0.505759
  validation accuracy:		85.54 %
Epoch 1843 of 2000 took 0.102s
  training loss:		0.443754
  validation loss:		0.497536
  validation accuracy:		85.54 %
Epoch 1844 of 2000 took 0.102s
  training loss:		0.436107
  validation loss:		0.499031
  validation accuracy:		85.43 %
Epoch 1845 of 2000 took 0.103s
  training loss:		0.441607
  validation loss:		0.524845
  validation accuracy:		85.22 %
Epoch 1846 of 2000 took 0.103s
  training loss:		0.439458
  validation loss:		0.500385
  validation accuracy:		85.11 %
Epoch 1847 of 2000 took 0.102s
  training loss:		0.445073
  validation loss:		0.489702
  validation accuracy:		85.54 %
Epoch 1848 of 2000 took 0.102s
  training loss:		0.437680
  validation loss:		0.500834
  validation accuracy:		85.33 %
Epoch 1849 of 2000 took 0.102s
  training loss:		0.427499
  validation loss:		0.490677
  validation accuracy:		85.43 %
Epoch 1850 of 2000 took 0.102s
  training loss:		0.432058
  validation loss:		0.488775
  validation accuracy:		86.09 %
Epoch 1851 of 2000 took 0.103s
  training loss:		0.438666
  validation loss:		0.482945
  validation accuracy:		85.54 %
Epoch 1852 of 2000 took 0.102s
  training loss:		0.441895
  validation loss:		0.508067
  validation accuracy:		84.78 %
Epoch 1853 of 2000 took 0.103s
  training loss:		0.433671
  validation loss:		0.481736
  validation accuracy:		85.65 %
Epoch 1854 of 2000 took 0.102s
  training loss:		0.432378
  validation loss:		0.508612
  validation accuracy:		84.46 %
Epoch 1855 of 2000 took 0.102s
  training loss:		0.433450
  validation loss:		0.506291
  validation accuracy:		85.54 %
Epoch 1856 of 2000 took 0.105s
  training loss:		0.442801
  validation loss:		0.490144
  validation accuracy:		85.22 %
Epoch 1857 of 2000 took 0.102s
  training loss:		0.427256
  validation loss:		0.486877
  validation accuracy:		86.09 %
Epoch 1858 of 2000 took 0.102s
  training loss:		0.426959
  validation loss:		0.482674
  validation accuracy:		85.22 %
Epoch 1859 of 2000 took 0.103s
  training loss:		0.427468
  validation loss:		0.495664
  validation accuracy:		85.65 %
Epoch 1860 of 2000 took 0.103s
  training loss:		0.427389
  validation loss:		0.489411
  validation accuracy:		85.11 %
Epoch 1861 of 2000 took 0.102s
  training loss:		0.423052
  validation loss:		0.485075
  validation accuracy:		85.76 %
Epoch 1862 of 2000 took 0.102s
  training loss:		0.423918
  validation loss:		0.504489
  validation accuracy:		85.54 %
Epoch 1863 of 2000 took 0.102s
  training loss:		0.431958
  validation loss:		0.485573
  validation accuracy:		85.87 %
Epoch 1864 of 2000 took 0.103s
  training loss:		0.423035
  validation loss:		0.486969
  validation accuracy:		85.76 %
Epoch 1865 of 2000 took 0.102s
  training loss:		0.418952
  validation loss:		0.504381
  validation accuracy:		85.65 %
Epoch 1866 of 2000 took 0.102s
  training loss:		0.423228
  validation loss:		0.491489
  validation accuracy:		85.54 %
Epoch 1867 of 2000 took 0.102s
  training loss:		0.426257
  validation loss:		0.497773
  validation accuracy:		85.54 %
Epoch 1868 of 2000 took 0.102s
  training loss:		0.424128
  validation loss:		0.479688
  validation accuracy:		86.09 %
Epoch 1869 of 2000 took 0.102s
  training loss:		0.419745
  validation loss:		0.484307
  validation accuracy:		85.65 %
Epoch 1870 of 2000 took 0.102s
  training loss:		0.415727
  validation loss:		0.491635
  validation accuracy:		85.22 %
Epoch 1871 of 2000 took 0.102s
  training loss:		0.409294
  validation loss:		0.489437
  validation accuracy:		85.22 %
Epoch 1872 of 2000 took 0.101s
  training loss:		0.427190
  validation loss:		0.485218
  validation accuracy:		85.43 %
Epoch 1873 of 2000 took 0.106s
  training loss:		0.420300
  validation loss:		0.474678
  validation accuracy:		86.20 %
Epoch 1874 of 2000 took 0.109s
  training loss:		0.418828
  validation loss:		0.478025
  validation accuracy:		86.20 %
Epoch 1875 of 2000 took 0.169s
  training loss:		0.419516
  validation loss:		0.480369
  validation accuracy:		86.30 %
Epoch 1876 of 2000 took 0.110s
  training loss:		0.415845
  validation loss:		0.497408
  validation accuracy:		84.78 %
Epoch 1877 of 2000 took 0.101s
  training loss:		0.412605
  validation loss:		0.472965
  validation accuracy:		86.63 %
Epoch 1878 of 2000 took 0.104s
  training loss:		0.414779
  validation loss:		0.489541
  validation accuracy:		85.76 %
Epoch 1879 of 2000 took 0.106s
  training loss:		0.406588
  validation loss:		0.488929
  validation accuracy:		85.54 %
Epoch 1880 of 2000 took 0.102s
  training loss:		0.409275
  validation loss:		0.471748
  validation accuracy:		86.52 %
Epoch 1881 of 2000 took 0.104s
  training loss:		0.419382
  validation loss:		0.479306
  validation accuracy:		86.30 %
Epoch 1882 of 2000 took 0.103s
  training loss:		0.411775
  validation loss:		0.491287
  validation accuracy:		85.98 %
Epoch 1883 of 2000 took 0.101s
  training loss:		0.414535
  validation loss:		0.478326
  validation accuracy:		85.76 %
Epoch 1884 of 2000 took 0.100s
  training loss:		0.407889
  validation loss:		0.486663
  validation accuracy:		85.54 %
Epoch 1885 of 2000 took 0.100s
  training loss:		0.408293
  validation loss:		0.475447
  validation accuracy:		86.85 %
Epoch 1886 of 2000 took 0.100s
  training loss:		0.413985
  validation loss:		0.477874
  validation accuracy:		86.74 %
Epoch 1887 of 2000 took 0.100s
  training loss:		0.410722
  validation loss:		0.478957
  validation accuracy:		85.98 %
Epoch 1888 of 2000 took 0.101s
  training loss:		0.407418
  validation loss:		0.476092
  validation accuracy:		85.76 %
Epoch 1889 of 2000 took 0.100s
  training loss:		0.412894
  validation loss:		0.476400
  validation accuracy:		86.30 %
Epoch 1890 of 2000 took 0.100s
  training loss:		0.410515
  validation loss:		0.498506
  validation accuracy:		85.43 %
Epoch 1891 of 2000 took 0.100s
  training loss:		0.410531
  validation loss:		0.464069
  validation accuracy:		86.74 %
Epoch 1892 of 2000 took 0.101s
  training loss:		0.417116
  validation loss:		0.480677
  validation accuracy:		86.41 %
Epoch 1893 of 2000 took 0.100s
  training loss:		0.416372
  validation loss:		0.462147
  validation accuracy:		87.07 %
Epoch 1894 of 2000 took 0.100s
  training loss:		0.399057
  validation loss:		0.484195
  validation accuracy:		85.65 %
Epoch 1895 of 2000 took 0.100s
  training loss:		0.401552
  validation loss:		0.465703
  validation accuracy:		86.63 %
Epoch 1896 of 2000 took 0.100s
  training loss:		0.406167
  validation loss:		0.480533
  validation accuracy:		85.87 %
Epoch 1897 of 2000 took 0.100s
  training loss:		0.400547
  validation loss:		0.476997
  validation accuracy:		86.30 %
Epoch 1898 of 2000 took 0.101s
  training loss:		0.404872
  validation loss:		0.464181
  validation accuracy:		86.41 %
Epoch 1899 of 2000 took 0.101s
  training loss:		0.394243
  validation loss:		0.461409
  validation accuracy:		86.20 %
Epoch 1900 of 2000 took 0.100s
  training loss:		0.397129
  validation loss:		0.463950
  validation accuracy:		86.63 %
Epoch 1901 of 2000 took 0.100s
  training loss:		0.404525
  validation loss:		0.471511
  validation accuracy:		86.30 %
Epoch 1902 of 2000 took 0.101s
  training loss:		0.395510
  validation loss:		0.466116
  validation accuracy:		86.63 %
Epoch 1903 of 2000 took 0.100s
  training loss:		0.401240
  validation loss:		0.460163
  validation accuracy:		86.63 %
Epoch 1904 of 2000 took 0.100s
  training loss:		0.395854
  validation loss:		0.460670
  validation accuracy:		86.74 %
Epoch 1905 of 2000 took 0.100s
  training loss:		0.399095
  validation loss:		0.462358
  validation accuracy:		86.41 %
Epoch 1906 of 2000 took 0.100s
  training loss:		0.391844
  validation loss:		0.468021
  validation accuracy:		86.85 %
Epoch 1907 of 2000 took 0.100s
  training loss:		0.393064
  validation loss:		0.471229
  validation accuracy:		86.52 %
Epoch 1908 of 2000 took 0.100s
  training loss:		0.397819
  validation loss:		0.459972
  validation accuracy:		86.85 %
Epoch 1909 of 2000 took 0.101s
  training loss:		0.389467
  validation loss:		0.453621
  validation accuracy:		87.07 %
Epoch 1910 of 2000 took 0.100s
  training loss:		0.391598
  validation loss:		0.465769
  validation accuracy:		86.52 %
Epoch 1911 of 2000 took 0.100s
  training loss:		0.397660
  validation loss:		0.453378
  validation accuracy:		87.07 %
Epoch 1912 of 2000 took 0.101s
  training loss:		0.396091
  validation loss:		0.458217
  validation accuracy:		86.74 %
Epoch 1913 of 2000 took 0.101s
  training loss:		0.391095
  validation loss:		0.464088
  validation accuracy:		87.28 %
Epoch 1914 of 2000 took 0.103s
  training loss:		0.390166
  validation loss:		0.455325
  validation accuracy:		86.63 %
Epoch 1915 of 2000 took 0.115s
  training loss:		0.391733
  validation loss:		0.448509
  validation accuracy:		87.39 %
Epoch 1916 of 2000 took 0.167s
  training loss:		0.393165
  validation loss:		0.464270
  validation accuracy:		87.17 %
Epoch 1917 of 2000 took 0.166s
  training loss:		0.393451
  validation loss:		0.446470
  validation accuracy:		86.96 %
Epoch 1918 of 2000 took 0.167s
  training loss:		0.388124
  validation loss:		0.462964
  validation accuracy:		87.17 %
Epoch 1919 of 2000 took 0.167s
  training loss:		0.379684
  validation loss:		0.456505
  validation accuracy:		87.07 %
Epoch 1920 of 2000 took 0.166s
  training loss:		0.381688
  validation loss:		0.460835
  validation accuracy:		87.17 %
Epoch 1921 of 2000 took 0.166s
  training loss:		0.385392
  validation loss:		0.472758
  validation accuracy:		86.96 %
Epoch 1922 of 2000 took 0.167s
  training loss:		0.379330
  validation loss:		0.469275
  validation accuracy:		86.74 %
Epoch 1923 of 2000 took 0.167s
  training loss:		0.387524
  validation loss:		0.466888
  validation accuracy:		86.85 %
Epoch 1924 of 2000 took 0.166s
  training loss:		0.383116
  validation loss:		0.456606
  validation accuracy:		87.50 %
Epoch 1925 of 2000 took 0.120s
  training loss:		0.374052
  validation loss:		0.443710
  validation accuracy:		87.72 %
Epoch 1926 of 2000 took 0.100s
  training loss:		0.384214
  validation loss:		0.450090
  validation accuracy:		87.07 %
Epoch 1927 of 2000 took 0.101s
  training loss:		0.383803
  validation loss:		0.455032
  validation accuracy:		87.39 %
Epoch 1928 of 2000 took 0.103s
  training loss:		0.375239
  validation loss:		0.435425
  validation accuracy:		87.61 %
Epoch 1929 of 2000 took 0.109s
  training loss:		0.379270
  validation loss:		0.457689
  validation accuracy:		86.63 %
Epoch 1930 of 2000 took 0.105s
  training loss:		0.381717
  validation loss:		0.447745
  validation accuracy:		87.39 %
Epoch 1931 of 2000 took 0.100s
  training loss:		0.373872
  validation loss:		0.448532
  validation accuracy:		87.28 %
Epoch 1932 of 2000 took 0.100s
  training loss:		0.374409
  validation loss:		0.449728
  validation accuracy:		87.17 %
Epoch 1933 of 2000 took 0.100s
  training loss:		0.376419
  validation loss:		0.465994
  validation accuracy:		87.07 %
Epoch 1934 of 2000 took 0.100s
  training loss:		0.374873
  validation loss:		0.453275
  validation accuracy:		87.39 %
Epoch 1935 of 2000 took 0.100s
  training loss:		0.376353
  validation loss:		0.448297
  validation accuracy:		87.28 %
Epoch 1936 of 2000 took 0.098s
  training loss:		0.371690
  validation loss:		0.451772
  validation accuracy:		87.17 %
Epoch 1937 of 2000 took 0.095s
  training loss:		0.375439
  validation loss:		0.441481
  validation accuracy:		87.83 %
Epoch 1938 of 2000 took 0.095s
  training loss:		0.372013
  validation loss:		0.441357
  validation accuracy:		87.50 %
Epoch 1939 of 2000 took 0.095s
  training loss:		0.376635
  validation loss:		0.440819
  validation accuracy:		87.28 %
Epoch 1940 of 2000 took 0.097s
  training loss:		0.382125
  validation loss:		0.443012
  validation accuracy:		87.39 %
Epoch 1941 of 2000 took 0.103s
  training loss:		0.381793
  validation loss:		0.437980
  validation accuracy:		87.83 %
Epoch 1942 of 2000 took 0.097s
  training loss:		0.377843
  validation loss:		0.448945
  validation accuracy:		87.17 %
Epoch 1943 of 2000 took 0.096s
  training loss:		0.374532
  validation loss:		0.442775
  validation accuracy:		87.72 %
Epoch 1944 of 2000 took 0.100s
  training loss:		0.373186
  validation loss:		0.453217
  validation accuracy:		87.17 %
Epoch 1945 of 2000 took 0.099s
  training loss:		0.373319
  validation loss:		0.434002
  validation accuracy:		87.50 %
Epoch 1946 of 2000 took 0.100s
  training loss:		0.373614
  validation loss:		0.450732
  validation accuracy:		86.85 %
Epoch 1947 of 2000 took 0.096s
  training loss:		0.367806
  validation loss:		0.433831
  validation accuracy:		87.93 %
Epoch 1948 of 2000 took 0.096s
  training loss:		0.361173
  validation loss:		0.451080
  validation accuracy:		87.17 %
Epoch 1949 of 2000 took 0.096s
  training loss:		0.374157
  validation loss:		0.438549
  validation accuracy:		87.61 %
Epoch 1950 of 2000 took 0.096s
  training loss:		0.370307
  validation loss:		0.439956
  validation accuracy:		87.17 %
Epoch 1951 of 2000 took 0.097s
  training loss:		0.368329
  validation loss:		0.448803
  validation accuracy:		87.39 %
Epoch 1952 of 2000 took 0.097s
  training loss:		0.373445
  validation loss:		0.447418
  validation accuracy:		86.85 %
Epoch 1953 of 2000 took 0.097s
  training loss:		0.368059
  validation loss:		0.439532
  validation accuracy:		87.83 %
Epoch 1954 of 2000 took 0.096s
  training loss:		0.362597
  validation loss:		0.437525
  validation accuracy:		87.61 %
Epoch 1955 of 2000 took 0.096s
  training loss:		0.366905
  validation loss:		0.437695
  validation accuracy:		87.50 %
Epoch 1956 of 2000 took 0.096s
  training loss:		0.362732
  validation loss:		0.441587
  validation accuracy:		87.50 %
Epoch 1957 of 2000 took 0.096s
  training loss:		0.369483
  validation loss:		0.460296
  validation accuracy:		87.07 %
Epoch 1958 of 2000 took 0.096s
  training loss:		0.365912
  validation loss:		0.430823
  validation accuracy:		87.50 %
Epoch 1959 of 2000 took 0.096s
  training loss:		0.360400
  validation loss:		0.452713
  validation accuracy:		87.07 %
Epoch 1960 of 2000 took 0.096s
  training loss:		0.367735
  validation loss:		0.437494
  validation accuracy:		87.61 %
Epoch 1961 of 2000 took 0.096s
  training loss:		0.365675
  validation loss:		0.456141
  validation accuracy:		86.85 %
Epoch 1962 of 2000 took 0.096s
  training loss:		0.372454
  validation loss:		0.441657
  validation accuracy:		87.28 %
Epoch 1963 of 2000 took 0.096s
  training loss:		0.363658
  validation loss:		0.428863
  validation accuracy:		87.50 %
Epoch 1964 of 2000 took 0.096s
  training loss:		0.364991
  validation loss:		0.438269
  validation accuracy:		87.17 %
Epoch 1965 of 2000 took 0.096s
  training loss:		0.363303
  validation loss:		0.443073
  validation accuracy:		87.39 %
Epoch 1966 of 2000 took 0.096s
  training loss:		0.364422
  validation loss:		0.437933
  validation accuracy:		87.50 %
Epoch 1967 of 2000 took 0.096s
  training loss:		0.354891
  validation loss:		0.438066
  validation accuracy:		87.39 %
Epoch 1968 of 2000 took 0.096s
  training loss:		0.367143
  validation loss:		0.451136
  validation accuracy:		87.61 %
Epoch 1969 of 2000 took 0.096s
  training loss:		0.361246
  validation loss:		0.447976
  validation accuracy:		87.28 %
Epoch 1970 of 2000 took 0.096s
  training loss:		0.359223
  validation loss:		0.438535
  validation accuracy:		87.61 %
Epoch 1971 of 2000 took 0.096s
  training loss:		0.354758
  validation loss:		0.441202
  validation accuracy:		87.72 %
Epoch 1972 of 2000 took 0.096s
  training loss:		0.354952
  validation loss:		0.445283
  validation accuracy:		87.39 %
Epoch 1973 of 2000 took 0.097s
  training loss:		0.360913
  validation loss:		0.433848
  validation accuracy:		87.72 %
Epoch 1974 of 2000 took 0.096s
  training loss:		0.361882
  validation loss:		0.444389
  validation accuracy:		87.61 %
Epoch 1975 of 2000 took 0.096s
  training loss:		0.361978
  validation loss:		0.441825
  validation accuracy:		87.83 %
Epoch 1976 of 2000 took 0.096s
  training loss:		0.362425
  validation loss:		0.438603
  validation accuracy:		87.39 %
Epoch 1977 of 2000 took 0.096s
  training loss:		0.354479
  validation loss:		0.442324
  validation accuracy:		87.50 %
Epoch 1978 of 2000 took 0.097s
  training loss:		0.360897
  validation loss:		0.447139
  validation accuracy:		87.72 %
Epoch 1979 of 2000 took 0.096s
  training loss:		0.353887
  validation loss:		0.430637
  validation accuracy:		87.61 %
Epoch 1980 of 2000 took 0.096s
  training loss:		0.354985
  validation loss:		0.448300
  validation accuracy:		87.72 %
Epoch 1981 of 2000 took 0.096s
  training loss:		0.363457
  validation loss:		0.445128
  validation accuracy:		86.96 %
Epoch 1982 of 2000 took 0.096s
  training loss:		0.359657
  validation loss:		0.438325
  validation accuracy:		87.50 %
Epoch 1983 of 2000 took 0.096s
  training loss:		0.355496
  validation loss:		0.435431
  validation accuracy:		87.61 %
Epoch 1984 of 2000 took 0.096s
  training loss:		0.348435
  validation loss:		0.430102
  validation accuracy:		87.50 %
Epoch 1985 of 2000 took 0.096s
  training loss:		0.357010
  validation loss:		0.439956
  validation accuracy:		87.61 %
Epoch 1986 of 2000 took 0.096s
  training loss:		0.363416
  validation loss:		0.435511
  validation accuracy:		87.39 %
Epoch 1987 of 2000 took 0.096s
  training loss:		0.358726
  validation loss:		0.447274
  validation accuracy:		87.50 %
Epoch 1988 of 2000 took 0.096s
  training loss:		0.353768
  validation loss:		0.438292
  validation accuracy:		87.39 %
Epoch 1989 of 2000 took 0.096s
  training loss:		0.351232
  validation loss:		0.426476
  validation accuracy:		87.93 %
Epoch 1990 of 2000 took 0.096s
  training loss:		0.362357
  validation loss:		0.439835
  validation accuracy:		87.83 %
Epoch 1991 of 2000 took 0.096s
  training loss:		0.355576
  validation loss:		0.432715
  validation accuracy:		87.72 %
Epoch 1992 of 2000 took 0.097s
  training loss:		0.345738
  validation loss:		0.437043
  validation accuracy:		87.61 %
Epoch 1993 of 2000 took 0.097s
  training loss:		0.355164
  validation loss:		0.442019
  validation accuracy:		86.96 %
Epoch 1994 of 2000 took 0.096s
  training loss:		0.351538
  validation loss:		0.438297
  validation accuracy:		87.83 %
Epoch 1995 of 2000 took 0.096s
  training loss:		0.350440
  validation loss:		0.428093
  validation accuracy:		87.83 %
Epoch 1996 of 2000 took 0.096s
  training loss:		0.353084
  validation loss:		0.453548
  validation accuracy:		87.50 %
Epoch 1997 of 2000 took 0.096s
  training loss:		0.353614
  validation loss:		0.432050
  validation accuracy:		87.61 %
Epoch 1998 of 2000 took 0.096s
  training loss:		0.354767
  validation loss:		0.428419
  validation accuracy:		87.50 %
Epoch 1999 of 2000 took 0.096s
  training loss:		0.351650
  validation loss:		0.429743
  validation accuracy:		87.17 %
Epoch 2000 of 2000 took 0.096s
  training loss:		0.356328
  validation loss:		0.441302
  validation accuracy:		87.61 %
Final results:
  test loss:			0.748234
  test accuracy:		79.29 %
