Loading data...
#train = 4423, #test = 3677, #valid = 1101
Building model and compiling functions...
w=(200, 150),b=(150,)
w=(150, 100),b=(100,)
w=(100, 50),b=(50,)
decomposing tensor W of shape (3, 200, 150)...
decomposing tensor B of shape (3, 150)...
Starting training...
Epoch 1 of 2000 took 0.098s
  training loss:		2.988365
  validation loss:		2.972014
  validation accuracy:		11.85 %
Epoch 2 of 2000 took 0.095s
  training loss:		2.956306
  validation loss:		2.932904
  validation accuracy:		11.85 %
Epoch 3 of 2000 took 0.095s
  training loss:		2.914868
  validation loss:		2.890462
  validation accuracy:		11.85 %
Epoch 4 of 2000 took 0.097s
  training loss:		2.872110
  validation loss:		2.847094
  validation accuracy:		11.85 %
Epoch 5 of 2000 took 0.096s
  training loss:		2.829935
  validation loss:		2.802479
  validation accuracy:		11.85 %
Epoch 6 of 2000 took 0.096s
  training loss:		2.788457
  validation loss:		2.755970
  validation accuracy:		11.85 %
Epoch 7 of 2000 took 0.096s
  training loss:		2.744241
  validation loss:		2.706882
  validation accuracy:		11.85 %
Epoch 8 of 2000 took 0.096s
  training loss:		2.699784
  validation loss:		2.655398
  validation accuracy:		11.85 %
Epoch 9 of 2000 took 0.096s
  training loss:		2.654141
  validation loss:		2.602135
  validation accuracy:		11.85 %
Epoch 10 of 2000 took 0.096s
  training loss:		2.608432
  validation loss:		2.547417
  validation accuracy:		11.85 %
Epoch 11 of 2000 took 0.096s
  training loss:		2.565657
  validation loss:		2.493209
  validation accuracy:		13.04 %
Epoch 12 of 2000 took 0.096s
  training loss:		2.523232
  validation loss:		2.443055
  validation accuracy:		13.04 %
Epoch 13 of 2000 took 0.096s
  training loss:		2.484560
  validation loss:		2.396835
  validation accuracy:		13.04 %
Epoch 14 of 2000 took 0.096s
  training loss:		2.446970
  validation loss:		2.356728
  validation accuracy:		13.04 %
Epoch 15 of 2000 took 0.096s
  training loss:		2.414940
  validation loss:		2.324774
  validation accuracy:		13.04 %
Epoch 16 of 2000 took 0.096s
  training loss:		2.391771
  validation loss:		2.300884
  validation accuracy:		13.04 %
Epoch 17 of 2000 took 0.096s
  training loss:		2.368269
  validation loss:		2.284629
  validation accuracy:		13.04 %
Epoch 18 of 2000 took 0.096s
  training loss:		2.350945
  validation loss:		2.276166
  validation accuracy:		13.04 %
Epoch 19 of 2000 took 0.096s
  training loss:		2.339294
  validation loss:		2.268474
  validation accuracy:		13.04 %
Epoch 20 of 2000 took 0.097s
  training loss:		2.330891
  validation loss:		2.266707
  validation accuracy:		14.89 %
Epoch 21 of 2000 took 0.096s
  training loss:		2.323861
  validation loss:		2.266589
  validation accuracy:		14.24 %
Epoch 22 of 2000 took 0.097s
  training loss:		2.318549
  validation loss:		2.260896
  validation accuracy:		18.15 %
Epoch 23 of 2000 took 0.096s
  training loss:		2.314639
  validation loss:		2.259583
  validation accuracy:		14.67 %
Epoch 24 of 2000 took 0.096s
  training loss:		2.311235
  validation loss:		2.256420
  validation accuracy:		13.04 %
Epoch 25 of 2000 took 0.096s
  training loss:		2.308421
  validation loss:		2.253450
  validation accuracy:		14.13 %
Epoch 26 of 2000 took 0.096s
  training loss:		2.306233
  validation loss:		2.251594
  validation accuracy:		14.67 %
Epoch 27 of 2000 took 0.099s
  training loss:		2.305784
  validation loss:		2.249835
  validation accuracy:		13.04 %
Epoch 28 of 2000 took 0.096s
  training loss:		2.303425
  validation loss:		2.251831
  validation accuracy:		12.83 %
Epoch 29 of 2000 took 0.096s
  training loss:		2.303228
  validation loss:		2.250973
  validation accuracy:		13.04 %
Epoch 30 of 2000 took 0.096s
  training loss:		2.302114
  validation loss:		2.250953
  validation accuracy:		13.04 %
Epoch 31 of 2000 took 0.096s
  training loss:		2.300666
  validation loss:		2.247634
  validation accuracy:		13.04 %
Epoch 32 of 2000 took 0.096s
  training loss:		2.300881
  validation loss:		2.247957
  validation accuracy:		19.57 %
Epoch 33 of 2000 took 0.096s
  training loss:		2.299488
  validation loss:		2.248493
  validation accuracy:		13.15 %
Epoch 34 of 2000 took 0.096s
  training loss:		2.299739
  validation loss:		2.248505
  validation accuracy:		17.83 %
Epoch 35 of 2000 took 0.096s
  training loss:		2.298589
  validation loss:		2.247556
  validation accuracy:		13.15 %
Epoch 36 of 2000 took 0.096s
  training loss:		2.298182
  validation loss:		2.248222
  validation accuracy:		19.24 %
Epoch 37 of 2000 took 0.096s
  training loss:		2.297783
  validation loss:		2.248255
  validation accuracy:		13.48 %
Epoch 38 of 2000 took 0.096s
  training loss:		2.297177
  validation loss:		2.246996
  validation accuracy:		14.67 %
Epoch 39 of 2000 took 0.096s
  training loss:		2.297898
  validation loss:		2.246831
  validation accuracy:		18.59 %
Epoch 40 of 2000 took 0.096s
  training loss:		2.296941
  validation loss:		2.248154
  validation accuracy:		13.80 %
Epoch 41 of 2000 took 0.096s
  training loss:		2.296518
  validation loss:		2.243826
  validation accuracy:		20.87 %
Epoch 42 of 2000 took 0.096s
  training loss:		2.295896
  validation loss:		2.244088
  validation accuracy:		13.04 %
Epoch 43 of 2000 took 0.096s
  training loss:		2.295787
  validation loss:		2.243949
  validation accuracy:		17.93 %
Epoch 44 of 2000 took 0.096s
  training loss:		2.295934
  validation loss:		2.245238
  validation accuracy:		13.70 %
Epoch 45 of 2000 took 0.096s
  training loss:		2.295043
  validation loss:		2.243741
  validation accuracy:		21.85 %
Epoch 46 of 2000 took 0.096s
  training loss:		2.295723
  validation loss:		2.242965
  validation accuracy:		25.43 %
Epoch 47 of 2000 took 0.096s
  training loss:		2.294690
  validation loss:		2.246037
  validation accuracy:		12.93 %
Epoch 48 of 2000 took 0.096s
  training loss:		2.294422
  validation loss:		2.243568
  validation accuracy:		18.37 %
Epoch 49 of 2000 took 0.096s
  training loss:		2.294554
  validation loss:		2.241537
  validation accuracy:		14.02 %
Epoch 50 of 2000 took 0.096s
  training loss:		2.294890
  validation loss:		2.245887
  validation accuracy:		18.15 %
Epoch 51 of 2000 took 0.096s
  training loss:		2.294414
  validation loss:		2.246944
  validation accuracy:		25.76 %
Epoch 52 of 2000 took 0.096s
  training loss:		2.293963
  validation loss:		2.240086
  validation accuracy:		13.26 %
Epoch 53 of 2000 took 0.097s
  training loss:		2.293734
  validation loss:		2.242455
  validation accuracy:		16.85 %
Epoch 54 of 2000 took 0.096s
  training loss:		2.293198
  validation loss:		2.240101
  validation accuracy:		23.04 %
Epoch 55 of 2000 took 0.096s
  training loss:		2.293341
  validation loss:		2.240003
  validation accuracy:		15.87 %
Epoch 56 of 2000 took 0.096s
  training loss:		2.293558
  validation loss:		2.243117
  validation accuracy:		15.54 %
Epoch 57 of 2000 took 0.096s
  training loss:		2.293486
  validation loss:		2.241906
  validation accuracy:		18.26 %
Epoch 58 of 2000 took 0.096s
  training loss:		2.293339
  validation loss:		2.242873
  validation accuracy:		13.37 %
Epoch 59 of 2000 took 0.096s
  training loss:		2.291564
  validation loss:		2.240800
  validation accuracy:		18.04 %
Epoch 60 of 2000 took 0.096s
  training loss:		2.293860
  validation loss:		2.241184
  validation accuracy:		13.80 %
Epoch 61 of 2000 took 0.096s
  training loss:		2.293010
  validation loss:		2.241979
  validation accuracy:		13.59 %
Epoch 62 of 2000 took 0.097s
  training loss:		2.292490
  validation loss:		2.243286
  validation accuracy:		20.11 %
Epoch 63 of 2000 took 0.096s
  training loss:		2.291549
  validation loss:		2.239685
  validation accuracy:		17.72 %
Epoch 64 of 2000 took 0.096s
  training loss:		2.292046
  validation loss:		2.237176
  validation accuracy:		15.65 %
Epoch 65 of 2000 took 0.096s
  training loss:		2.290928
  validation loss:		2.240051
  validation accuracy:		23.04 %
Epoch 66 of 2000 took 0.096s
  training loss:		2.292127
  validation loss:		2.242663
  validation accuracy:		23.15 %
Epoch 67 of 2000 took 0.096s
  training loss:		2.290889
  validation loss:		2.240885
  validation accuracy:		20.87 %
Epoch 68 of 2000 took 0.096s
  training loss:		2.291142
  validation loss:		2.238446
  validation accuracy:		22.72 %
Epoch 69 of 2000 took 0.096s
  training loss:		2.292038
  validation loss:		2.237273
  validation accuracy:		23.48 %
Epoch 70 of 2000 took 0.096s
  training loss:		2.291507
  validation loss:		2.246128
  validation accuracy:		20.00 %
Epoch 71 of 2000 took 0.096s
  training loss:		2.291419
  validation loss:		2.241726
  validation accuracy:		18.80 %
Epoch 72 of 2000 took 0.096s
  training loss:		2.290525
  validation loss:		2.235152
  validation accuracy:		20.11 %
Epoch 73 of 2000 took 0.096s
  training loss:		2.289947
  validation loss:		2.234735
  validation accuracy:		18.80 %
Epoch 74 of 2000 took 0.099s
  training loss:		2.290879
  validation loss:		2.239393
  validation accuracy:		21.41 %
Epoch 75 of 2000 took 0.096s
  training loss:		2.290879
  validation loss:		2.237683
  validation accuracy:		14.02 %
Epoch 76 of 2000 took 0.096s
  training loss:		2.289729
  validation loss:		2.237748
  validation accuracy:		19.89 %
Epoch 77 of 2000 took 0.096s
  training loss:		2.289544
  validation loss:		2.239885
  validation accuracy:		14.57 %
Epoch 78 of 2000 took 0.096s
  training loss:		2.289704
  validation loss:		2.235341
  validation accuracy:		15.87 %
Epoch 79 of 2000 took 0.096s
  training loss:		2.289292
  validation loss:		2.236700
  validation accuracy:		15.65 %
Epoch 80 of 2000 took 0.096s
  training loss:		2.288543
  validation loss:		2.235031
  validation accuracy:		13.15 %
Epoch 81 of 2000 took 0.096s
  training loss:		2.288778
  validation loss:		2.236600
  validation accuracy:		26.96 %
Epoch 82 of 2000 took 0.096s
  training loss:		2.288967
  validation loss:		2.241608
  validation accuracy:		25.11 %
Epoch 83 of 2000 took 0.096s
  training loss:		2.288680
  validation loss:		2.236571
  validation accuracy:		20.22 %
Epoch 84 of 2000 took 0.097s
  training loss:		2.288152
  validation loss:		2.233370
  validation accuracy:		13.70 %
Epoch 85 of 2000 took 0.096s
  training loss:		2.288049
  validation loss:		2.236505
  validation accuracy:		18.80 %
Epoch 86 of 2000 took 0.096s
  training loss:		2.287210
  validation loss:		2.238442
  validation accuracy:		20.43 %
Epoch 87 of 2000 took 0.096s
  training loss:		2.287717
  validation loss:		2.235278
  validation accuracy:		23.80 %
Epoch 88 of 2000 took 0.096s
  training loss:		2.287201
  validation loss:		2.235742
  validation accuracy:		15.65 %
Epoch 89 of 2000 took 0.096s
  training loss:		2.287078
  validation loss:		2.235062
  validation accuracy:		17.39 %
Epoch 90 of 2000 took 0.096s
  training loss:		2.286668
  validation loss:		2.233963
  validation accuracy:		19.57 %
Epoch 91 of 2000 took 0.096s
  training loss:		2.285932
  validation loss:		2.234846
  validation accuracy:		22.93 %
Epoch 92 of 2000 took 0.096s
  training loss:		2.285770
  validation loss:		2.232836
  validation accuracy:		20.00 %
Epoch 93 of 2000 took 0.096s
  training loss:		2.285599
  validation loss:		2.234601
  validation accuracy:		23.26 %
Epoch 94 of 2000 took 0.096s
  training loss:		2.285974
  validation loss:		2.235123
  validation accuracy:		18.59 %
Epoch 95 of 2000 took 0.096s
  training loss:		2.284365
  validation loss:		2.231563
  validation accuracy:		22.50 %
Epoch 96 of 2000 took 0.096s
  training loss:		2.284291
  validation loss:		2.230533
  validation accuracy:		18.48 %
Epoch 97 of 2000 took 0.096s
  training loss:		2.284162
  validation loss:		2.233378
  validation accuracy:		22.07 %
Epoch 98 of 2000 took 0.096s
  training loss:		2.283990
  validation loss:		2.229501
  validation accuracy:		25.65 %
Epoch 99 of 2000 took 0.096s
  training loss:		2.283299
  validation loss:		2.227309
  validation accuracy:		19.02 %
Epoch 100 of 2000 took 0.096s
  training loss:		2.282502
  validation loss:		2.232104
  validation accuracy:		23.04 %
Epoch 101 of 2000 took 0.096s
  training loss:		2.282555
  validation loss:		2.230668
  validation accuracy:		20.98 %
Epoch 102 of 2000 took 0.096s
  training loss:		2.281400
  validation loss:		2.226965
  validation accuracy:		21.96 %
Epoch 103 of 2000 took 0.096s
  training loss:		2.280908
  validation loss:		2.227069
  validation accuracy:		22.93 %
Epoch 104 of 2000 took 0.096s
  training loss:		2.280869
  validation loss:		2.224964
  validation accuracy:		21.52 %
Epoch 105 of 2000 took 0.096s
  training loss:		2.280658
  validation loss:		2.229472
  validation accuracy:		19.78 %
Epoch 106 of 2000 took 0.096s
  training loss:		2.280361
  validation loss:		2.229336
  validation accuracy:		24.57 %
Epoch 107 of 2000 took 0.096s
  training loss:		2.279177
  validation loss:		2.223046
  validation accuracy:		26.63 %
Epoch 108 of 2000 took 0.096s
  training loss:		2.278580
  validation loss:		2.220651
  validation accuracy:		22.72 %
Epoch 109 of 2000 took 0.096s
  training loss:		2.277139
  validation loss:		2.223426
  validation accuracy:		22.17 %
Epoch 110 of 2000 took 0.096s
  training loss:		2.278168
  validation loss:		2.231885
  validation accuracy:		26.09 %
Epoch 111 of 2000 took 0.096s
  training loss:		2.275516
  validation loss:		2.220969
  validation accuracy:		28.59 %
Epoch 112 of 2000 took 0.096s
  training loss:		2.275829
  validation loss:		2.218002
  validation accuracy:		24.35 %
Epoch 113 of 2000 took 0.096s
  training loss:		2.274865
  validation loss:		2.224752
  validation accuracy:		24.35 %
Epoch 114 of 2000 took 0.096s
  training loss:		2.272429
  validation loss:		2.219620
  validation accuracy:		26.41 %
Epoch 115 of 2000 took 0.097s
  training loss:		2.272773
  validation loss:		2.214810
  validation accuracy:		24.13 %
Epoch 116 of 2000 took 0.097s
  training loss:		2.271743
  validation loss:		2.217721
  validation accuracy:		22.61 %
Epoch 117 of 2000 took 0.097s
  training loss:		2.270704
  validation loss:		2.218178
  validation accuracy:		25.54 %
Epoch 118 of 2000 took 0.096s
  training loss:		2.268633
  validation loss:		2.216020
  validation accuracy:		29.13 %
Epoch 119 of 2000 took 0.096s
  training loss:		2.266818
  validation loss:		2.211762
  validation accuracy:		28.37 %
Epoch 120 of 2000 took 0.096s
  training loss:		2.266453
  validation loss:		2.209024
  validation accuracy:		23.37 %
Epoch 121 of 2000 took 0.096s
  training loss:		2.264317
  validation loss:		2.209635
  validation accuracy:		25.33 %
Epoch 122 of 2000 took 0.096s
  training loss:		2.262257
  validation loss:		2.208755
  validation accuracy:		26.52 %
Epoch 123 of 2000 took 0.096s
  training loss:		2.260149
  validation loss:		2.200189
  validation accuracy:		25.54 %
Epoch 124 of 2000 took 0.096s
  training loss:		2.257209
  validation loss:		2.195889
  validation accuracy:		23.37 %
Epoch 125 of 2000 took 0.096s
  training loss:		2.256795
  validation loss:		2.201210
  validation accuracy:		25.98 %
Epoch 126 of 2000 took 0.096s
  training loss:		2.254461
  validation loss:		2.201946
  validation accuracy:		23.91 %
Epoch 127 of 2000 took 0.096s
  training loss:		2.251435
  validation loss:		2.193132
  validation accuracy:		27.50 %
Epoch 128 of 2000 took 0.096s
  training loss:		2.248679
  validation loss:		2.187841
  validation accuracy:		26.20 %
Epoch 129 of 2000 took 0.096s
  training loss:		2.244968
  validation loss:		2.186057
  validation accuracy:		25.33 %
Epoch 130 of 2000 took 0.096s
  training loss:		2.243164
  validation loss:		2.183128
  validation accuracy:		28.04 %
Epoch 131 of 2000 took 0.096s
  training loss:		2.238730
  validation loss:		2.180172
  validation accuracy:		25.54 %
Epoch 132 of 2000 took 0.096s
  training loss:		2.234200
  validation loss:		2.173045
  validation accuracy:		26.20 %
Epoch 133 of 2000 took 0.096s
  training loss:		2.229884
  validation loss:		2.167955
  validation accuracy:		24.35 %
Epoch 134 of 2000 took 0.098s
  training loss:		2.225579
  validation loss:		2.161111
  validation accuracy:		24.78 %
Epoch 135 of 2000 took 0.097s
  training loss:		2.220985
  validation loss:		2.159969
  validation accuracy:		26.41 %
Epoch 136 of 2000 took 0.096s
  training loss:		2.213778
  validation loss:		2.151951
  validation accuracy:		25.76 %
Epoch 137 of 2000 took 0.096s
  training loss:		2.206624
  validation loss:		2.137431
  validation accuracy:		26.20 %
Epoch 138 of 2000 took 0.096s
  training loss:		2.198979
  validation loss:		2.126957
  validation accuracy:		27.72 %
Epoch 139 of 2000 took 0.096s
  training loss:		2.190080
  validation loss:		2.121099
  validation accuracy:		27.83 %
Epoch 140 of 2000 took 0.096s
  training loss:		2.181209
  validation loss:		2.118977
  validation accuracy:		26.41 %
Epoch 141 of 2000 took 0.096s
  training loss:		2.169966
  validation loss:		2.096358
  validation accuracy:		26.20 %
Epoch 142 of 2000 took 0.096s
  training loss:		2.155046
  validation loss:		2.080018
  validation accuracy:		26.96 %
Epoch 143 of 2000 took 0.096s
  training loss:		2.142577
  validation loss:		2.063930
  validation accuracy:		27.61 %
Epoch 144 of 2000 took 0.096s
  training loss:		2.128618
  validation loss:		2.047695
  validation accuracy:		26.85 %
Epoch 145 of 2000 took 0.096s
  training loss:		2.110183
  validation loss:		2.032810
  validation accuracy:		29.35 %
Epoch 146 of 2000 took 0.096s
  training loss:		2.091296
  validation loss:		2.004633
  validation accuracy:		28.15 %
Epoch 147 of 2000 took 0.097s
  training loss:		2.073734
  validation loss:		1.986234
  validation accuracy:		28.48 %
Epoch 148 of 2000 took 0.096s
  training loss:		2.050659
  validation loss:		1.964372
  validation accuracy:		30.00 %
Epoch 149 of 2000 took 0.096s
  training loss:		2.026259
  validation loss:		1.932926
  validation accuracy:		29.67 %
Epoch 150 of 2000 took 0.096s
  training loss:		2.003785
  validation loss:		1.907674
  validation accuracy:		30.76 %
Epoch 151 of 2000 took 0.096s
  training loss:		1.979935
  validation loss:		1.879897
  validation accuracy:		31.85 %
Epoch 152 of 2000 took 0.096s
  training loss:		1.953774
  validation loss:		1.855458
  validation accuracy:		32.50 %
Epoch 153 of 2000 took 0.096s
  training loss:		1.928150
  validation loss:		1.826356
  validation accuracy:		32.39 %
Epoch 154 of 2000 took 0.096s
  training loss:		1.901973
  validation loss:		1.797541
  validation accuracy:		32.61 %
Epoch 155 of 2000 took 0.097s
  training loss:		1.877769
  validation loss:		1.775545
  validation accuracy:		34.89 %
Epoch 156 of 2000 took 0.096s
  training loss:		1.855698
  validation loss:		1.751159
  validation accuracy:		33.80 %
Epoch 157 of 2000 took 0.096s
  training loss:		1.833971
  validation loss:		1.726858
  validation accuracy:		36.41 %
Epoch 158 of 2000 took 0.096s
  training loss:		1.804858
  validation loss:		1.702883
  validation accuracy:		35.11 %
Epoch 159 of 2000 took 0.096s
  training loss:		1.789503
  validation loss:		1.683070
  validation accuracy:		37.72 %
Epoch 160 of 2000 took 0.096s
  training loss:		1.768086
  validation loss:		1.664141
  validation accuracy:		38.37 %
Epoch 161 of 2000 took 0.096s
  training loss:		1.755872
  validation loss:		1.644847
  validation accuracy:		37.93 %
Epoch 162 of 2000 took 0.096s
  training loss:		1.737302
  validation loss:		1.626365
  validation accuracy:		39.46 %
Epoch 163 of 2000 took 0.096s
  training loss:		1.716356
  validation loss:		1.606285
  validation accuracy:		39.24 %
Epoch 164 of 2000 took 0.096s
  training loss:		1.697733
  validation loss:		1.595648
  validation accuracy:		39.24 %
Epoch 165 of 2000 took 0.096s
  training loss:		1.685907
  validation loss:		1.579578
  validation accuracy:		40.11 %
Epoch 166 of 2000 took 0.097s
  training loss:		1.670019
  validation loss:		1.558723
  validation accuracy:		40.54 %
Epoch 167 of 2000 took 0.096s
  training loss:		1.656278
  validation loss:		1.551375
  validation accuracy:		41.96 %
Epoch 168 of 2000 took 0.096s
  training loss:		1.634951
  validation loss:		1.538077
  validation accuracy:		41.74 %
Epoch 169 of 2000 took 0.096s
  training loss:		1.629671
  validation loss:		1.519200
  validation accuracy:		41.96 %
Epoch 170 of 2000 took 0.096s
  training loss:		1.618195
  validation loss:		1.510128
  validation accuracy:		42.17 %
Epoch 171 of 2000 took 0.096s
  training loss:		1.609366
  validation loss:		1.497545
  validation accuracy:		41.85 %
Epoch 172 of 2000 took 0.096s
  training loss:		1.593879
  validation loss:		1.492487
  validation accuracy:		43.80 %
Epoch 173 of 2000 took 0.096s
  training loss:		1.579220
  validation loss:		1.480251
  validation accuracy:		42.72 %
Epoch 174 of 2000 took 0.096s
  training loss:		1.571391
  validation loss:		1.470882
  validation accuracy:		42.83 %
Epoch 175 of 2000 took 0.096s
  training loss:		1.559597
  validation loss:		1.454061
  validation accuracy:		43.80 %
Epoch 176 of 2000 took 0.096s
  training loss:		1.556004
  validation loss:		1.449221
  validation accuracy:		44.02 %
Epoch 177 of 2000 took 0.096s
  training loss:		1.539992
  validation loss:		1.446009
  validation accuracy:		43.91 %
Epoch 178 of 2000 took 0.097s
  training loss:		1.537686
  validation loss:		1.433976
  validation accuracy:		43.91 %
Epoch 179 of 2000 took 0.096s
  training loss:		1.526239
  validation loss:		1.423293
  validation accuracy:		45.76 %
Epoch 180 of 2000 took 0.096s
  training loss:		1.519952
  validation loss:		1.416147
  validation accuracy:		45.43 %
Epoch 181 of 2000 took 0.096s
  training loss:		1.508297
  validation loss:		1.411422
  validation accuracy:		44.57 %
Epoch 182 of 2000 took 0.096s
  training loss:		1.505785
  validation loss:		1.401857
  validation accuracy:		45.22 %
Epoch 183 of 2000 took 0.096s
  training loss:		1.498376
  validation loss:		1.395560
  validation accuracy:		45.43 %
Epoch 184 of 2000 took 0.096s
  training loss:		1.489430
  validation loss:		1.386008
  validation accuracy:		46.63 %
Epoch 185 of 2000 took 0.096s
  training loss:		1.490277
  validation loss:		1.391705
  validation accuracy:		46.41 %
Epoch 186 of 2000 took 0.096s
  training loss:		1.481057
  validation loss:		1.382489
  validation accuracy:		46.85 %
Epoch 187 of 2000 took 0.096s
  training loss:		1.476152
  validation loss:		1.372705
  validation accuracy:		47.28 %
Epoch 188 of 2000 took 0.096s
  training loss:		1.470529
  validation loss:		1.368900
  validation accuracy:		46.74 %
Epoch 189 of 2000 took 0.096s
  training loss:		1.466317
  validation loss:		1.372709
  validation accuracy:		46.74 %
Epoch 190 of 2000 took 0.096s
  training loss:		1.458889
  validation loss:		1.390521
  validation accuracy:		49.13 %
Epoch 191 of 2000 took 0.096s
  training loss:		1.470187
  validation loss:		1.355721
  validation accuracy:		48.26 %
Epoch 192 of 2000 took 0.096s
  training loss:		1.451226
  validation loss:		1.371612
  validation accuracy:		50.00 %
Epoch 193 of 2000 took 0.096s
  training loss:		1.453766
  validation loss:		1.352153
  validation accuracy:		49.24 %
Epoch 194 of 2000 took 0.096s
  training loss:		1.450031
  validation loss:		1.349939
  validation accuracy:		49.57 %
Epoch 195 of 2000 took 0.096s
  training loss:		1.438309
  validation loss:		1.343551
  validation accuracy:		49.89 %
Epoch 196 of 2000 took 0.097s
  training loss:		1.441241
  validation loss:		1.338145
  validation accuracy:		49.13 %
Epoch 197 of 2000 took 0.097s
  training loss:		1.434690
  validation loss:		1.337723
  validation accuracy:		49.89 %
Epoch 198 of 2000 took 0.096s
  training loss:		1.434208
  validation loss:		1.330421
  validation accuracy:		49.57 %
Epoch 199 of 2000 took 0.097s
  training loss:		1.433085
  validation loss:		1.329212
  validation accuracy:		49.57 %
Epoch 200 of 2000 took 0.096s
  training loss:		1.421761
  validation loss:		1.322269
  validation accuracy:		50.22 %
Epoch 201 of 2000 took 0.099s
  training loss:		1.423004
  validation loss:		1.319024
  validation accuracy:		50.54 %
Epoch 202 of 2000 took 0.096s
  training loss:		1.422373
  validation loss:		1.325298
  validation accuracy:		50.65 %
Epoch 203 of 2000 took 0.096s
  training loss:		1.421931
  validation loss:		1.311154
  validation accuracy:		50.11 %
Epoch 204 of 2000 took 0.096s
  training loss:		1.419003
  validation loss:		1.316802
  validation accuracy:		50.54 %
Epoch 205 of 2000 took 0.096s
  training loss:		1.414977
  validation loss:		1.313437
  validation accuracy:		50.54 %
Epoch 206 of 2000 took 0.096s
  training loss:		1.409249
  validation loss:		1.311637
  validation accuracy:		50.76 %
Epoch 207 of 2000 took 0.096s
  training loss:		1.416090
  validation loss:		1.319432
  validation accuracy:		51.30 %
Epoch 208 of 2000 took 0.096s
  training loss:		1.412773
  validation loss:		1.311106
  validation accuracy:		51.96 %
Epoch 209 of 2000 took 0.097s
  training loss:		1.407554
  validation loss:		1.307543
  validation accuracy:		51.20 %
Epoch 210 of 2000 took 0.096s
  training loss:		1.411541
  validation loss:		1.332483
  validation accuracy:		51.41 %
Epoch 211 of 2000 took 0.096s
  training loss:		1.415791
  validation loss:		1.303176
  validation accuracy:		50.22 %
Epoch 212 of 2000 took 0.096s
  training loss:		1.401276
  validation loss:		1.299675
  validation accuracy:		51.20 %
Epoch 213 of 2000 took 0.096s
  training loss:		1.398886
  validation loss:		1.302754
  validation accuracy:		52.07 %
Epoch 214 of 2000 took 0.096s
  training loss:		1.404054
  validation loss:		1.301097
  validation accuracy:		51.09 %
Epoch 215 of 2000 took 0.096s
  training loss:		1.397463
  validation loss:		1.321500
  validation accuracy:		52.28 %
Epoch 216 of 2000 took 0.096s
  training loss:		1.395207
  validation loss:		1.290035
  validation accuracy:		50.98 %
Epoch 217 of 2000 took 0.096s
  training loss:		1.388531
  validation loss:		1.292091
  validation accuracy:		52.17 %
Epoch 218 of 2000 took 0.096s
  training loss:		1.390638
  validation loss:		1.291542
  validation accuracy:		52.50 %
Epoch 219 of 2000 took 0.096s
  training loss:		1.384212
  validation loss:		1.292921
  validation accuracy:		51.85 %
Epoch 220 of 2000 took 0.097s
  training loss:		1.397455
  validation loss:		1.288985
  validation accuracy:		52.72 %
Epoch 221 of 2000 took 0.096s
  training loss:		1.391898
  validation loss:		1.285513
  validation accuracy:		51.09 %
Epoch 222 of 2000 took 0.097s
  training loss:		1.384718
  validation loss:		1.282992
  validation accuracy:		52.28 %
Epoch 223 of 2000 took 0.096s
  training loss:		1.387923
  validation loss:		1.310969
  validation accuracy:		52.72 %
Epoch 224 of 2000 took 0.096s
  training loss:		1.382034
  validation loss:		1.274753
  validation accuracy:		51.63 %
Epoch 225 of 2000 took 0.096s
  training loss:		1.391911
  validation loss:		1.291664
  validation accuracy:		53.04 %
Epoch 226 of 2000 took 0.096s
  training loss:		1.381866
  validation loss:		1.276418
  validation accuracy:		52.39 %
Epoch 227 of 2000 took 0.096s
  training loss:		1.381075
  validation loss:		1.290341
  validation accuracy:		49.46 %
Epoch 228 of 2000 took 0.096s
  training loss:		1.384563
  validation loss:		1.286703
  validation accuracy:		52.28 %
Epoch 229 of 2000 took 0.096s
  training loss:		1.387966
  validation loss:		1.281608
  validation accuracy:		50.43 %
Epoch 230 of 2000 took 0.096s
  training loss:		1.381574
  validation loss:		1.280225
  validation accuracy:		52.93 %
Epoch 231 of 2000 took 0.096s
  training loss:		1.383978
  validation loss:		1.277075
  validation accuracy:		52.61 %
Epoch 232 of 2000 took 0.096s
  training loss:		1.372883
  validation loss:		1.286693
  validation accuracy:		53.15 %
Epoch 233 of 2000 took 0.096s
  training loss:		1.361447
  validation loss:		1.288979
  validation accuracy:		52.28 %
Epoch 234 of 2000 took 0.096s
  training loss:		1.377494
  validation loss:		1.281709
  validation accuracy:		52.07 %
Epoch 235 of 2000 took 0.096s
  training loss:		1.375048
  validation loss:		1.272023
  validation accuracy:		51.74 %
Epoch 236 of 2000 took 0.097s
  training loss:		1.386352
  validation loss:		1.318069
  validation accuracy:		52.72 %
Epoch 237 of 2000 took 0.096s
  training loss:		1.379819
  validation loss:		1.302878
  validation accuracy:		51.96 %
Epoch 238 of 2000 took 0.097s
  training loss:		1.384847
  validation loss:		1.279518
  validation accuracy:		52.50 %
Epoch 239 of 2000 took 0.096s
  training loss:		1.371772
  validation loss:		1.271801
  validation accuracy:		51.52 %
Epoch 240 of 2000 took 0.097s
  training loss:		1.379392
  validation loss:		1.279787
  validation accuracy:		49.57 %
Epoch 241 of 2000 took 0.097s
  training loss:		1.372466
  validation loss:		1.267653
  validation accuracy:		51.52 %
Epoch 242 of 2000 took 0.096s
  training loss:		1.382745
  validation loss:		1.280521
  validation accuracy:		50.00 %
Epoch 243 of 2000 took 0.097s
  training loss:		1.371406
  validation loss:		1.263018
  validation accuracy:		51.96 %
Epoch 244 of 2000 took 0.096s
  training loss:		1.381263
  validation loss:		1.291583
  validation accuracy:		53.70 %
Epoch 245 of 2000 took 0.096s
  training loss:		1.368648
  validation loss:		1.271603
  validation accuracy:		51.52 %
Epoch 246 of 2000 took 0.096s
  training loss:		1.369388
  validation loss:		1.306163
  validation accuracy:		53.04 %
Epoch 247 of 2000 took 0.096s
  training loss:		1.379381
  validation loss:		1.356532
  validation accuracy:		50.22 %
Epoch 248 of 2000 took 0.096s
  training loss:		1.381454
  validation loss:		1.267567
  validation accuracy:		50.76 %
Epoch 249 of 2000 took 0.096s
  training loss:		1.369036
  validation loss:		1.269600
  validation accuracy:		53.04 %
Epoch 250 of 2000 took 0.096s
  training loss:		1.358899
  validation loss:		1.297467
  validation accuracy:		52.72 %
Epoch 251 of 2000 took 0.096s
  training loss:		1.365712
  validation loss:		1.289451
  validation accuracy:		52.07 %
Epoch 252 of 2000 took 0.096s
  training loss:		1.368849
  validation loss:		1.302402
  validation accuracy:		52.39 %
Epoch 253 of 2000 took 0.096s
  training loss:		1.403643
  validation loss:		1.257688
  validation accuracy:		50.43 %
Epoch 254 of 2000 took 0.096s
  training loss:		1.354323
  validation loss:		1.268100
  validation accuracy:		49.13 %
Epoch 255 of 2000 took 0.096s
  training loss:		1.356731
  validation loss:		1.256875
  validation accuracy:		51.85 %
Epoch 256 of 2000 took 0.096s
  training loss:		1.374767
  validation loss:		1.254461
  validation accuracy:		52.17 %
Epoch 257 of 2000 took 0.096s
  training loss:		1.356174
  validation loss:		1.317146
  validation accuracy:		51.63 %
Epoch 258 of 2000 took 0.096s
  training loss:		1.375013
  validation loss:		1.274211
  validation accuracy:		52.50 %
Epoch 259 of 2000 took 0.096s
  training loss:		1.354878
  validation loss:		1.255108
  validation accuracy:		51.96 %
Epoch 260 of 2000 took 0.096s
  training loss:		1.357330
  validation loss:		1.256455
  validation accuracy:		50.65 %
Epoch 261 of 2000 took 0.096s
  training loss:		1.362062
  validation loss:		1.256379
  validation accuracy:		49.57 %
Epoch 262 of 2000 took 0.096s
  training loss:		1.363244
  validation loss:		1.305721
  validation accuracy:		51.30 %
Epoch 263 of 2000 took 0.097s
  training loss:		1.372384
  validation loss:		1.259222
  validation accuracy:		49.13 %
Epoch 264 of 2000 took 0.096s
  training loss:		1.345765
  validation loss:		1.319663
  validation accuracy:		50.54 %
Epoch 265 of 2000 took 0.096s
  training loss:		1.379214
  validation loss:		1.290585
  validation accuracy:		47.61 %
Epoch 266 of 2000 took 0.100s
  training loss:		1.353553
  validation loss:		1.249288
  validation accuracy:		50.43 %
Epoch 267 of 2000 took 0.100s
  training loss:		1.348782
  validation loss:		1.259027
  validation accuracy:		51.41 %
Epoch 268 of 2000 took 0.099s
  training loss:		1.363543
  validation loss:		1.259184
  validation accuracy:		51.63 %
Epoch 269 of 2000 took 0.096s
  training loss:		1.352846
  validation loss:		1.259058
  validation accuracy:		51.30 %
Epoch 270 of 2000 took 0.096s
  training loss:		1.356527
  validation loss:		1.269172
  validation accuracy:		51.74 %
Epoch 271 of 2000 took 0.096s
  training loss:		1.359407
  validation loss:		1.252819
  validation accuracy:		51.20 %
Epoch 272 of 2000 took 0.097s
  training loss:		1.352488
  validation loss:		1.247414
  validation accuracy:		49.67 %
Epoch 273 of 2000 took 0.096s
  training loss:		1.349815
  validation loss:		1.252030
  validation accuracy:		49.89 %
Epoch 274 of 2000 took 0.099s
  training loss:		1.364292
  validation loss:		1.296587
  validation accuracy:		46.41 %
Epoch 275 of 2000 took 0.097s
  training loss:		1.374895
  validation loss:		1.344244
  validation accuracy:		49.57 %
Epoch 276 of 2000 took 0.096s
  training loss:		1.351816
  validation loss:		1.251129
  validation accuracy:		49.67 %
Epoch 277 of 2000 took 0.096s
  training loss:		1.387697
  validation loss:		1.313228
  validation accuracy:		50.22 %
Epoch 278 of 2000 took 0.096s
  training loss:		1.369498
  validation loss:		1.255937
  validation accuracy:		49.78 %
Epoch 279 of 2000 took 0.097s
  training loss:		1.350579
  validation loss:		1.294342
  validation accuracy:		51.74 %
Epoch 280 of 2000 took 0.097s
  training loss:		1.356496
  validation loss:		1.241974
  validation accuracy:		49.35 %
Epoch 281 of 2000 took 0.096s
  training loss:		1.351058
  validation loss:		1.276590
  validation accuracy:		51.85 %
Epoch 282 of 2000 took 0.097s
  training loss:		1.340669
  validation loss:		1.257331
  validation accuracy:		51.30 %
Epoch 283 of 2000 took 0.097s
  training loss:		1.341655
  validation loss:		1.250446
  validation accuracy:		50.11 %
Epoch 284 of 2000 took 0.097s
  training loss:		1.356716
  validation loss:		1.244017
  validation accuracy:		50.43 %
Epoch 285 of 2000 took 0.096s
  training loss:		1.363504
  validation loss:		1.251899
  validation accuracy:		48.48 %
Epoch 286 of 2000 took 0.096s
  training loss:		1.343972
  validation loss:		1.244141
  validation accuracy:		49.89 %
Epoch 287 of 2000 took 0.096s
  training loss:		1.353415
  validation loss:		1.262365
  validation accuracy:		51.30 %
Epoch 288 of 2000 took 0.097s
  training loss:		1.368248
  validation loss:		1.250744
  validation accuracy:		48.80 %
Epoch 289 of 2000 took 0.096s
  training loss:		1.339310
  validation loss:		1.247284
  validation accuracy:		50.76 %
Epoch 290 of 2000 took 0.096s
  training loss:		1.343537
  validation loss:		1.243163
  validation accuracy:		50.00 %
Epoch 291 of 2000 took 0.096s
  training loss:		1.358939
  validation loss:		1.253706
  validation accuracy:		50.87 %
Epoch 292 of 2000 took 0.097s
  training loss:		1.346957
  validation loss:		1.248330
  validation accuracy:		50.98 %
Epoch 293 of 2000 took 0.096s
  training loss:		1.342689
  validation loss:		1.316466
  validation accuracy:		50.43 %
Epoch 294 of 2000 took 0.096s
  training loss:		1.346961
  validation loss:		1.263092
  validation accuracy:		51.41 %
Epoch 295 of 2000 took 0.096s
  training loss:		1.357200
  validation loss:		1.244490
  validation accuracy:		51.41 %
Epoch 296 of 2000 took 0.096s
  training loss:		1.343072
  validation loss:		1.247341
  validation accuracy:		50.54 %
Epoch 297 of 2000 took 0.096s
  training loss:		1.341937
  validation loss:		1.251353
  validation accuracy:		51.20 %
Epoch 298 of 2000 took 0.096s
  training loss:		1.347252
  validation loss:		1.246524
  validation accuracy:		50.65 %
Epoch 299 of 2000 took 0.096s
  training loss:		1.344472
  validation loss:		1.257556
  validation accuracy:		50.76 %
Epoch 300 of 2000 took 0.096s
  training loss:		1.364097
  validation loss:		1.285619
  validation accuracy:		51.85 %
Epoch 301 of 2000 took 0.096s
  training loss:		1.365295
  validation loss:		1.248381
  validation accuracy:		51.09 %
Epoch 302 of 2000 took 0.096s
  training loss:		1.366696
  validation loss:		1.262825
  validation accuracy:		52.50 %
Epoch 303 of 2000 took 0.097s
  training loss:		1.341565
  validation loss:		1.254095
  validation accuracy:		52.39 %
Epoch 304 of 2000 took 0.096s
  training loss:		1.339269
  validation loss:		1.247141
  validation accuracy:		50.43 %
Epoch 305 of 2000 took 0.096s
  training loss:		1.358323
  validation loss:		1.258970
  validation accuracy:		49.89 %
Epoch 306 of 2000 took 0.096s
  training loss:		1.345376
  validation loss:		1.258535
  validation accuracy:		52.17 %
Epoch 307 of 2000 took 0.096s
  training loss:		1.348567
  validation loss:		1.293783
  validation accuracy:		52.17 %
Epoch 308 of 2000 took 0.096s
  training loss:		1.373646
  validation loss:		1.274525
  validation accuracy:		47.83 %
Epoch 309 of 2000 took 0.096s
  training loss:		1.356002
  validation loss:		1.266627
  validation accuracy:		52.83 %
Epoch 310 of 2000 took 0.096s
  training loss:		1.364976
  validation loss:		1.241027
  validation accuracy:		49.57 %
Epoch 311 of 2000 took 0.096s
  training loss:		1.336084
  validation loss:		1.243513
  validation accuracy:		49.57 %
Epoch 312 of 2000 took 0.096s
  training loss:		1.341673
  validation loss:		1.239617
  validation accuracy:		50.76 %
Epoch 313 of 2000 took 0.096s
  training loss:		1.362052
  validation loss:		1.249293
  validation accuracy:		48.91 %
Epoch 314 of 2000 took 0.096s
  training loss:		1.344963
  validation loss:		1.255552
  validation accuracy:		52.17 %
Epoch 315 of 2000 took 0.096s
  training loss:		1.342635
  validation loss:		1.261974
  validation accuracy:		52.17 %
Epoch 316 of 2000 took 0.096s
  training loss:		1.338738
  validation loss:		1.243813
  validation accuracy:		51.52 %
Epoch 317 of 2000 took 0.096s
  training loss:		1.340022
  validation loss:		1.244397
  validation accuracy:		52.17 %
Epoch 318 of 2000 took 0.096s
  training loss:		1.362875
  validation loss:		1.246104
  validation accuracy:		49.67 %
Epoch 319 of 2000 took 0.096s
  training loss:		1.346088
  validation loss:		1.367895
  validation accuracy:		48.48 %
Epoch 320 of 2000 took 0.096s
  training loss:		1.380759
  validation loss:		1.275636
  validation accuracy:		52.61 %
Epoch 321 of 2000 took 0.096s
  training loss:		1.344447
  validation loss:		1.258334
  validation accuracy:		52.17 %
Epoch 322 of 2000 took 0.096s
  training loss:		1.345672
  validation loss:		1.265103
  validation accuracy:		48.91 %
Epoch 323 of 2000 took 0.096s
  training loss:		1.400246
  validation loss:		1.280438
  validation accuracy:		52.72 %
Epoch 324 of 2000 took 0.097s
  training loss:		1.340617
  validation loss:		1.265687
  validation accuracy:		47.50 %
Epoch 325 of 2000 took 0.096s
  training loss:		1.350473
  validation loss:		1.256940
  validation accuracy:		52.50 %
Epoch 326 of 2000 took 0.096s
  training loss:		1.343428
  validation loss:		1.244856
  validation accuracy:		50.54 %
Epoch 327 of 2000 took 0.096s
  training loss:		1.337998
  validation loss:		1.237343
  validation accuracy:		52.07 %
Epoch 328 of 2000 took 0.096s
  training loss:		1.348397
  validation loss:		1.252687
  validation accuracy:		52.39 %
Epoch 329 of 2000 took 0.096s
  training loss:		1.332562
  validation loss:		1.240901
  validation accuracy:		51.52 %
Epoch 330 of 2000 took 0.096s
  training loss:		1.353683
  validation loss:		1.251532
  validation accuracy:		52.50 %
Epoch 331 of 2000 took 0.096s
  training loss:		1.352747
  validation loss:		1.239727
  validation accuracy:		51.41 %
Epoch 332 of 2000 took 0.096s
  training loss:		1.332821
  validation loss:		1.234758
  validation accuracy:		50.76 %
Epoch 333 of 2000 took 0.096s
  training loss:		1.334945
  validation loss:		1.229661
  validation accuracy:		52.50 %
Epoch 334 of 2000 took 0.097s
  training loss:		1.367286
  validation loss:		1.305733
  validation accuracy:		51.63 %
Epoch 335 of 2000 took 0.096s
  training loss:		1.344339
  validation loss:		1.239852
  validation accuracy:		52.07 %
Epoch 336 of 2000 took 0.096s
  training loss:		1.343362
  validation loss:		1.238384
  validation accuracy:		52.28 %
Epoch 337 of 2000 took 0.096s
  training loss:		1.356093
  validation loss:		1.232734
  validation accuracy:		51.63 %
Epoch 338 of 2000 took 0.096s
  training loss:		1.330275
  validation loss:		1.254749
  validation accuracy:		52.07 %
Epoch 339 of 2000 took 0.096s
  training loss:		1.358895
  validation loss:		1.311952
  validation accuracy:		51.63 %
Epoch 340 of 2000 took 0.096s
  training loss:		1.351561
  validation loss:		1.231203
  validation accuracy:		51.96 %
Epoch 341 of 2000 took 0.096s
  training loss:		1.361195
  validation loss:		1.273565
  validation accuracy:		52.61 %
Epoch 342 of 2000 took 0.096s
  training loss:		1.374267
  validation loss:		1.322944
  validation accuracy:		51.41 %
Epoch 343 of 2000 took 0.096s
  training loss:		1.354978
  validation loss:		1.244936
  validation accuracy:		51.85 %
Epoch 344 of 2000 took 0.096s
  training loss:		1.332831
  validation loss:		1.245164
  validation accuracy:		52.50 %
Epoch 345 of 2000 took 0.096s
  training loss:		1.336973
  validation loss:		1.248978
  validation accuracy:		51.96 %
Epoch 346 of 2000 took 0.096s
  training loss:		1.361813
  validation loss:		1.239998
  validation accuracy:		52.07 %
Epoch 347 of 2000 took 0.096s
  training loss:		1.337386
  validation loss:		1.265527
  validation accuracy:		52.93 %
Epoch 348 of 2000 took 0.096s
  training loss:		1.365595
  validation loss:		1.239579
  validation accuracy:		52.50 %
Epoch 349 of 2000 took 0.096s
  training loss:		1.349848
  validation loss:		1.236471
  validation accuracy:		52.07 %
Epoch 350 of 2000 took 0.096s
  training loss:		1.333584
  validation loss:		1.230953
  validation accuracy:		52.93 %
Epoch 351 of 2000 took 0.096s
  training loss:		1.333907
  validation loss:		1.238881
  validation accuracy:		52.72 %
Epoch 352 of 2000 took 0.096s
  training loss:		1.338103
  validation loss:		1.254270
  validation accuracy:		53.04 %
Epoch 353 of 2000 took 0.096s
  training loss:		1.356273
  validation loss:		1.278581
  validation accuracy:		52.07 %
Epoch 354 of 2000 took 0.096s
  training loss:		1.345847
  validation loss:		1.247500
  validation accuracy:		52.17 %
Epoch 355 of 2000 took 0.099s
  training loss:		1.365052
  validation loss:		1.243842
  validation accuracy:		50.98 %
Epoch 356 of 2000 took 0.096s
  training loss:		1.339750
  validation loss:		1.232490
  validation accuracy:		51.63 %
Epoch 357 of 2000 took 0.096s
  training loss:		1.326949
  validation loss:		1.251437
  validation accuracy:		52.93 %
Epoch 358 of 2000 took 0.096s
  training loss:		1.332936
  validation loss:		1.254599
  validation accuracy:		52.50 %
Epoch 359 of 2000 took 0.096s
  training loss:		1.371060
  validation loss:		1.262010
  validation accuracy:		52.61 %
Epoch 360 of 2000 took 0.096s
  training loss:		1.329479
  validation loss:		1.233017
  validation accuracy:		51.63 %
Epoch 361 of 2000 took 0.096s
  training loss:		1.329063
  validation loss:		1.231309
  validation accuracy:		51.52 %
Epoch 362 of 2000 took 0.096s
  training loss:		1.338476
  validation loss:		1.236962
  validation accuracy:		52.50 %
Epoch 363 of 2000 took 0.096s
  training loss:		1.333498
  validation loss:		1.257256
  validation accuracy:		52.07 %
Epoch 364 of 2000 took 0.096s
  training loss:		1.331847
  validation loss:		1.231978
  validation accuracy:		51.30 %
Epoch 365 of 2000 took 0.097s
  training loss:		1.340016
  validation loss:		1.262308
  validation accuracy:		52.72 %
Epoch 366 of 2000 took 0.097s
  training loss:		1.344879
  validation loss:		1.308988
  validation accuracy:		52.17 %
Epoch 367 of 2000 took 0.096s
  training loss:		1.348328
  validation loss:		1.230798
  validation accuracy:		52.61 %
Epoch 368 of 2000 took 0.096s
  training loss:		1.346360
  validation loss:		1.242352
  validation accuracy:		51.20 %
Epoch 369 of 2000 took 0.096s
  training loss:		1.342149
  validation loss:		1.247100
  validation accuracy:		50.98 %
Epoch 370 of 2000 took 0.096s
  training loss:		1.331623
  validation loss:		1.229894
  validation accuracy:		51.96 %
Epoch 371 of 2000 took 0.096s
  training loss:		1.329989
  validation loss:		1.233884
  validation accuracy:		53.04 %
Epoch 372 of 2000 took 0.096s
  training loss:		1.324377
  validation loss:		1.244923
  validation accuracy:		53.15 %
Epoch 373 of 2000 took 0.096s
  training loss:		1.342397
  validation loss:		1.231912
  validation accuracy:		51.74 %
Epoch 374 of 2000 took 0.096s
  training loss:		1.331966
  validation loss:		1.230346
  validation accuracy:		52.83 %
Epoch 375 of 2000 took 0.096s
  training loss:		1.327528
  validation loss:		1.231201
  validation accuracy:		52.39 %
Epoch 376 of 2000 took 0.096s
  training loss:		1.342699
  validation loss:		1.239819
  validation accuracy:		53.37 %
Epoch 377 of 2000 took 0.096s
  training loss:		1.328937
  validation loss:		1.232221
  validation accuracy:		53.26 %
Epoch 378 of 2000 took 0.096s
  training loss:		1.323539
  validation loss:		1.273812
  validation accuracy:		53.91 %
Epoch 379 of 2000 took 0.096s
  training loss:		1.336851
  validation loss:		1.267276
  validation accuracy:		52.50 %
Epoch 380 of 2000 took 0.096s
  training loss:		1.343040
  validation loss:		1.365100
  validation accuracy:		50.33 %
Epoch 381 of 2000 took 0.096s
  training loss:		1.343996
  validation loss:		1.231434
  validation accuracy:		51.63 %
Epoch 382 of 2000 took 0.096s
  training loss:		1.329002
  validation loss:		1.302818
  validation accuracy:		52.93 %
Epoch 383 of 2000 took 0.096s
  training loss:		1.354211
  validation loss:		1.250747
  validation accuracy:		53.59 %
Epoch 384 of 2000 took 0.097s
  training loss:		1.324335
  validation loss:		1.230927
  validation accuracy:		52.61 %
Epoch 385 of 2000 took 0.096s
  training loss:		1.337503
  validation loss:		1.250512
  validation accuracy:		52.28 %
Epoch 386 of 2000 took 0.096s
  training loss:		1.330130
  validation loss:		1.255645
  validation accuracy:		53.80 %
Epoch 387 of 2000 took 0.096s
  training loss:		1.339941
  validation loss:		1.233997
  validation accuracy:		53.15 %
Epoch 388 of 2000 took 0.096s
  training loss:		1.339577
  validation loss:		1.246291
  validation accuracy:		52.93 %
Epoch 389 of 2000 took 0.096s
  training loss:		1.330592
  validation loss:		1.264611
  validation accuracy:		53.37 %
Epoch 390 of 2000 took 0.096s
  training loss:		1.331950
  validation loss:		1.267572
  validation accuracy:		53.37 %
Epoch 391 of 2000 took 0.096s
  training loss:		1.352005
  validation loss:		1.230545
  validation accuracy:		53.70 %
Epoch 392 of 2000 took 0.096s
  training loss:		1.327083
  validation loss:		1.233535
  validation accuracy:		51.74 %
Epoch 393 of 2000 took 0.097s
  training loss:		1.336085
  validation loss:		1.244670
  validation accuracy:		53.70 %
Epoch 394 of 2000 took 0.096s
  training loss:		1.335790
  validation loss:		1.250819
  validation accuracy:		51.09 %
Epoch 395 of 2000 took 0.096s
  training loss:		1.345459
  validation loss:		1.230814
  validation accuracy:		52.72 %
Epoch 396 of 2000 took 0.097s
  training loss:		1.325143
  validation loss:		1.241903
  validation accuracy:		54.13 %
Epoch 397 of 2000 took 0.097s
  training loss:		1.335490
  validation loss:		1.238872
  validation accuracy:		53.91 %
Epoch 398 of 2000 took 0.096s
  training loss:		1.329870
  validation loss:		1.230331
  validation accuracy:		52.72 %
Epoch 399 of 2000 took 0.096s
  training loss:		1.325041
  validation loss:		1.226269
  validation accuracy:		53.91 %
Epoch 400 of 2000 took 0.096s
  training loss:		1.333555
  validation loss:		1.243415
  validation accuracy:		53.80 %
Epoch 401 of 2000 took 0.096s
  training loss:		1.324424
  validation loss:		1.232116
  validation accuracy:		54.57 %
Epoch 402 of 2000 took 0.096s
  training loss:		1.324805
  validation loss:		1.224571
  validation accuracy:		53.70 %
Epoch 403 of 2000 took 0.096s
  training loss:		1.353408
  validation loss:		1.251751
  validation accuracy:		53.04 %
Epoch 404 of 2000 took 0.096s
  training loss:		1.334307
  validation loss:		1.226593
  validation accuracy:		54.13 %
Epoch 405 of 2000 took 0.096s
  training loss:		1.329985
  validation loss:		1.239789
  validation accuracy:		52.07 %
Epoch 406 of 2000 took 0.096s
  training loss:		1.338361
  validation loss:		1.220635
  validation accuracy:		54.24 %
Epoch 407 of 2000 took 0.096s
  training loss:		1.324913
  validation loss:		1.257401
  validation accuracy:		52.83 %
Epoch 408 of 2000 took 0.096s
  training loss:		1.336597
  validation loss:		1.263339
  validation accuracy:		53.80 %
Epoch 409 of 2000 took 0.096s
  training loss:		1.340210
  validation loss:		1.259150
  validation accuracy:		53.70 %
Epoch 410 of 2000 took 0.096s
  training loss:		1.328561
  validation loss:		1.233869
  validation accuracy:		53.91 %
Epoch 411 of 2000 took 0.096s
  training loss:		1.322450
  validation loss:		1.220804
  validation accuracy:		53.15 %
Epoch 412 of 2000 took 0.096s
  training loss:		1.338211
  validation loss:		1.288355
  validation accuracy:		54.24 %
Epoch 413 of 2000 took 0.096s
  training loss:		1.334943
  validation loss:		1.234157
  validation accuracy:		54.35 %
Epoch 414 of 2000 took 0.096s
  training loss:		1.320426
  validation loss:		1.231672
  validation accuracy:		53.15 %
Epoch 415 of 2000 took 0.097s
  training loss:		1.334237
  validation loss:		1.223090
  validation accuracy:		54.67 %
Epoch 416 of 2000 took 0.096s
  training loss:		1.321474
  validation loss:		1.228008
  validation accuracy:		55.33 %
Epoch 417 of 2000 took 0.096s
  training loss:		1.326261
  validation loss:		1.216504
  validation accuracy:		55.76 %
Epoch 418 of 2000 took 0.096s
  training loss:		1.321130
  validation loss:		1.232163
  validation accuracy:		54.24 %
Epoch 419 of 2000 took 0.096s
  training loss:		1.322976
  validation loss:		1.252358
  validation accuracy:		53.37 %
Epoch 420 of 2000 took 0.096s
  training loss:		1.326407
  validation loss:		1.237468
  validation accuracy:		55.00 %
Epoch 421 of 2000 took 0.096s
  training loss:		1.324953
  validation loss:		1.225064
  validation accuracy:		55.65 %
Epoch 422 of 2000 took 0.097s
  training loss:		1.326444
  validation loss:		1.247812
  validation accuracy:		54.24 %
Epoch 423 of 2000 took 0.096s
  training loss:		1.338876
  validation loss:		1.236771
  validation accuracy:		55.11 %
Epoch 424 of 2000 took 0.096s
  training loss:		1.323047
  validation loss:		1.219200
  validation accuracy:		54.13 %
Epoch 425 of 2000 took 0.096s
  training loss:		1.322515
  validation loss:		1.241900
  validation accuracy:		53.48 %
Epoch 426 of 2000 took 0.096s
  training loss:		1.324142
  validation loss:		1.242829
  validation accuracy:		54.24 %
Epoch 427 of 2000 took 0.097s
  training loss:		1.334336
  validation loss:		1.259993
  validation accuracy:		54.46 %
Epoch 428 of 2000 took 0.097s
  training loss:		1.344520
  validation loss:		1.217155
  validation accuracy:		55.22 %
Epoch 429 of 2000 took 0.096s
  training loss:		1.321319
  validation loss:		1.221756
  validation accuracy:		55.22 %
Epoch 430 of 2000 took 0.096s
  training loss:		1.321411
  validation loss:		1.228206
  validation accuracy:		55.22 %
Epoch 431 of 2000 took 0.096s
  training loss:		1.315159
  validation loss:		1.237782
  validation accuracy:		55.43 %
Epoch 432 of 2000 took 0.096s
  training loss:		1.319716
  validation loss:		1.222120
  validation accuracy:		56.30 %
Epoch 433 of 2000 took 0.096s
  training loss:		1.313702
  validation loss:		1.211470
  validation accuracy:		55.54 %
Epoch 434 of 2000 took 0.096s
  training loss:		1.321704
  validation loss:		1.225059
  validation accuracy:		55.65 %
Epoch 435 of 2000 took 0.096s
  training loss:		1.314783
  validation loss:		1.222422
  validation accuracy:		55.76 %
Epoch 436 of 2000 took 0.097s
  training loss:		1.323426
  validation loss:		1.216759
  validation accuracy:		55.43 %
Epoch 437 of 2000 took 0.096s
  training loss:		1.311106
  validation loss:		1.214715
  validation accuracy:		55.98 %
Epoch 438 of 2000 took 0.096s
  training loss:		1.331183
  validation loss:		1.209559
  validation accuracy:		55.65 %
Epoch 439 of 2000 took 0.096s
  training loss:		1.321221
  validation loss:		1.215693
  validation accuracy:		55.76 %
Epoch 440 of 2000 took 0.096s
  training loss:		1.319120
  validation loss:		1.204946
  validation accuracy:		55.54 %
Epoch 441 of 2000 took 0.096s
  training loss:		1.314153
  validation loss:		1.202978
  validation accuracy:		55.98 %
Epoch 442 of 2000 took 0.097s
  training loss:		1.316061
  validation loss:		1.210668
  validation accuracy:		56.63 %
Epoch 443 of 2000 took 0.096s
  training loss:		1.316317
  validation loss:		1.215274
  validation accuracy:		56.96 %
Epoch 444 of 2000 took 0.099s
  training loss:		1.311611
  validation loss:		1.213416
  validation accuracy:		55.76 %
Epoch 445 of 2000 took 0.097s
  training loss:		1.312704
  validation loss:		1.201035
  validation accuracy:		56.85 %
Epoch 446 of 2000 took 0.096s
  training loss:		1.308643
  validation loss:		1.222115
  validation accuracy:		54.57 %
Epoch 447 of 2000 took 0.096s
  training loss:		1.332271
  validation loss:		1.213938
  validation accuracy:		57.61 %
Epoch 448 of 2000 took 0.096s
  training loss:		1.308901
  validation loss:		1.199110
  validation accuracy:		57.07 %
Epoch 449 of 2000 took 0.096s
  training loss:		1.302119
  validation loss:		1.204244
  validation accuracy:		55.33 %
Epoch 450 of 2000 took 0.096s
  training loss:		1.291616
  validation loss:		1.214091
  validation accuracy:		57.17 %
Epoch 451 of 2000 took 0.096s
  training loss:		1.299929
  validation loss:		1.225658
  validation accuracy:		57.50 %
Epoch 452 of 2000 took 0.096s
  training loss:		1.299934
  validation loss:		1.217029
  validation accuracy:		58.04 %
Epoch 453 of 2000 took 0.096s
  training loss:		1.296813
  validation loss:		1.209596
  validation accuracy:		57.72 %
Epoch 454 of 2000 took 0.096s
  training loss:		1.295861
  validation loss:		1.191891
  validation accuracy:		57.72 %
Epoch 455 of 2000 took 0.096s
  training loss:		1.296165
  validation loss:		1.199335
  validation accuracy:		57.28 %
Epoch 456 of 2000 took 0.097s
  training loss:		1.311937
  validation loss:		1.207927
  validation accuracy:		58.26 %
Epoch 457 of 2000 took 0.096s
  training loss:		1.292171
  validation loss:		1.201828
  validation accuracy:		58.59 %
Epoch 458 of 2000 took 0.096s
  training loss:		1.292318
  validation loss:		1.235020
  validation accuracy:		56.74 %
Epoch 459 of 2000 took 0.097s
  training loss:		1.297779
  validation loss:		1.186577
  validation accuracy:		58.37 %
Epoch 460 of 2000 took 0.096s
  training loss:		1.285313
  validation loss:		1.191504
  validation accuracy:		58.04 %
Epoch 461 of 2000 took 0.096s
  training loss:		1.282165
  validation loss:		1.180114
  validation accuracy:		59.35 %
Epoch 462 of 2000 took 0.096s
  training loss:		1.282495
  validation loss:		1.192656
  validation accuracy:		58.91 %
Epoch 463 of 2000 took 0.096s
  training loss:		1.281687
  validation loss:		1.179655
  validation accuracy:		60.11 %
Epoch 464 of 2000 took 0.096s
  training loss:		1.276530
  validation loss:		1.172836
  validation accuracy:		58.59 %
Epoch 465 of 2000 took 0.096s
  training loss:		1.269696
  validation loss:		1.168688
  validation accuracy:		59.89 %
Epoch 466 of 2000 took 0.096s
  training loss:		1.266234
  validation loss:		1.176817
  validation accuracy:		60.00 %
Epoch 467 of 2000 took 0.096s
  training loss:		1.256242
  validation loss:		1.173783
  validation accuracy:		60.33 %
Epoch 468 of 2000 took 0.096s
  training loss:		1.253882
  validation loss:		1.159734
  validation accuracy:		61.30 %
Epoch 469 of 2000 took 0.096s
  training loss:		1.271190
  validation loss:		1.163847
  validation accuracy:		59.24 %
Epoch 470 of 2000 took 0.096s
  training loss:		1.241818
  validation loss:		1.147454
  validation accuracy:		62.17 %
Epoch 471 of 2000 took 0.096s
  training loss:		1.237043
  validation loss:		1.146660
  validation accuracy:		62.28 %
Epoch 472 of 2000 took 0.096s
  training loss:		1.228970
  validation loss:		1.139446
  validation accuracy:		61.20 %
Epoch 473 of 2000 took 0.096s
  training loss:		1.224881
  validation loss:		1.146354
  validation accuracy:		62.39 %
Epoch 474 of 2000 took 0.096s
  training loss:		1.224826
  validation loss:		1.112627
  validation accuracy:		63.15 %
Epoch 475 of 2000 took 0.096s
  training loss:		1.211333
  validation loss:		1.117921
  validation accuracy:		63.37 %
Epoch 476 of 2000 took 0.097s
  training loss:		1.206003
  validation loss:		1.120592
  validation accuracy:		63.59 %
Epoch 477 of 2000 took 0.097s
  training loss:		1.200439
  validation loss:		1.107382
  validation accuracy:		61.85 %
Epoch 478 of 2000 took 0.097s
  training loss:		1.187735
  validation loss:		1.090894
  validation accuracy:		64.89 %
Epoch 479 of 2000 took 0.097s
  training loss:		1.175093
  validation loss:		1.074279
  validation accuracy:		63.91 %
Epoch 480 of 2000 took 0.096s
  training loss:		1.157968
  validation loss:		1.061559
  validation accuracy:		65.54 %
Epoch 481 of 2000 took 0.096s
  training loss:		1.157352
  validation loss:		1.048038
  validation accuracy:		66.09 %
Epoch 482 of 2000 took 0.096s
  training loss:		1.150602
  validation loss:		1.039653
  validation accuracy:		66.30 %
Epoch 483 of 2000 took 0.097s
  training loss:		1.129393
  validation loss:		1.027417
  validation accuracy:		66.85 %
Epoch 484 of 2000 took 0.096s
  training loss:		1.132723
  validation loss:		1.013865
  validation accuracy:		67.28 %
Epoch 485 of 2000 took 0.096s
  training loss:		1.103799
  validation loss:		1.011573
  validation accuracy:		66.85 %
Epoch 486 of 2000 took 0.096s
  training loss:		1.095136
  validation loss:		1.012859
  validation accuracy:		69.02 %
Epoch 487 of 2000 took 0.097s
  training loss:		1.074326
  validation loss:		1.016308
  validation accuracy:		68.15 %
Epoch 488 of 2000 took 0.097s
  training loss:		1.081254
  validation loss:		0.966710
  validation accuracy:		70.54 %
Epoch 489 of 2000 took 0.097s
  training loss:		1.063115
  validation loss:		0.962177
  validation accuracy:		70.22 %
Epoch 490 of 2000 took 0.097s
  training loss:		1.036259
  validation loss:		0.938720
  validation accuracy:		70.87 %
Epoch 491 of 2000 took 0.096s
  training loss:		1.026396
  validation loss:		0.964455
  validation accuracy:		69.02 %
Epoch 492 of 2000 took 0.097s
  training loss:		1.010460
  validation loss:		0.919392
  validation accuracy:		73.26 %
Epoch 493 of 2000 took 0.096s
  training loss:		1.001702
  validation loss:		0.895926
  validation accuracy:		74.13 %
Epoch 494 of 2000 took 0.096s
  training loss:		0.986204
  validation loss:		0.887737
  validation accuracy:		73.37 %
Epoch 495 of 2000 took 0.096s
  training loss:		0.976168
  validation loss:		0.892124
  validation accuracy:		73.80 %
Epoch 496 of 2000 took 0.096s
  training loss:		0.955303
  validation loss:		0.876783
  validation accuracy:		74.02 %
Epoch 497 of 2000 took 0.096s
  training loss:		0.955995
  validation loss:		0.867121
  validation accuracy:		74.35 %
Epoch 498 of 2000 took 0.096s
  training loss:		0.942901
  validation loss:		0.866197
  validation accuracy:		74.35 %
Epoch 499 of 2000 took 0.096s
  training loss:		0.919081
  validation loss:		0.858284
  validation accuracy:		74.89 %
Epoch 500 of 2000 took 0.096s
  training loss:		0.913599
  validation loss:		0.831877
  validation accuracy:		75.54 %
Epoch 501 of 2000 took 0.096s
  training loss:		0.892357
  validation loss:		0.818831
  validation accuracy:		76.20 %
Epoch 502 of 2000 took 0.097s
  training loss:		0.872263
  validation loss:		0.813657
  validation accuracy:		75.87 %
Epoch 503 of 2000 took 0.096s
  training loss:		0.856481
  validation loss:		0.782147
  validation accuracy:		76.96 %
Epoch 504 of 2000 took 0.096s
  training loss:		0.855186
  validation loss:		0.750902
  validation accuracy:		77.72 %
Epoch 505 of 2000 took 0.096s
  training loss:		0.842060
  validation loss:		0.740604
  validation accuracy:		77.72 %
Epoch 506 of 2000 took 0.096s
  training loss:		0.833236
  validation loss:		0.740446
  validation accuracy:		77.72 %
Epoch 507 of 2000 took 0.096s
  training loss:		0.810060
  validation loss:		0.762190
  validation accuracy:		77.50 %
Epoch 508 of 2000 took 0.098s
  training loss:		0.802241
  validation loss:		0.712318
  validation accuracy:		77.07 %
Epoch 509 of 2000 took 0.098s
  training loss:		0.808594
  validation loss:		0.710873
  validation accuracy:		77.39 %
Epoch 510 of 2000 took 0.097s
  training loss:		0.775131
  validation loss:		0.729982
  validation accuracy:		78.37 %
Epoch 511 of 2000 took 0.097s
  training loss:		0.774843
  validation loss:		0.725607
  validation accuracy:		77.72 %
Epoch 512 of 2000 took 0.096s
  training loss:		0.764033
  validation loss:		0.676040
  validation accuracy:		79.13 %
Epoch 513 of 2000 took 0.096s
  training loss:		0.754833
  validation loss:		0.664377
  validation accuracy:		79.02 %
Epoch 514 of 2000 took 0.096s
  training loss:		0.748554
  validation loss:		0.670838
  validation accuracy:		80.00 %
Epoch 515 of 2000 took 0.096s
  training loss:		0.736840
  validation loss:		0.649164
  validation accuracy:		78.70 %
Epoch 516 of 2000 took 0.096s
  training loss:		0.735387
  validation loss:		0.630464
  validation accuracy:		79.57 %
Epoch 517 of 2000 took 0.096s
  training loss:		0.724024
  validation loss:		0.630490
  validation accuracy:		80.11 %
Epoch 518 of 2000 took 0.096s
  training loss:		0.714752
  validation loss:		0.642653
  validation accuracy:		79.78 %
Epoch 519 of 2000 took 0.096s
  training loss:		0.717810
  validation loss:		0.622090
  validation accuracy:		79.13 %
Epoch 520 of 2000 took 0.096s
  training loss:		0.700612
  validation loss:		0.623865
  validation accuracy:		80.11 %
Epoch 521 of 2000 took 0.097s
  training loss:		0.706477
  validation loss:		0.611903
  validation accuracy:		80.87 %
Epoch 522 of 2000 took 0.097s
  training loss:		0.685105
  validation loss:		0.599992
  validation accuracy:		80.33 %
Epoch 523 of 2000 took 0.097s
  training loss:		0.676108
  validation loss:		0.605591
  validation accuracy:		80.22 %
Epoch 524 of 2000 took 0.096s
  training loss:		0.679922
  validation loss:		0.600822
  validation accuracy:		80.33 %
Epoch 525 of 2000 took 0.096s
  training loss:		0.677508
  validation loss:		0.611419
  validation accuracy:		80.87 %
Epoch 526 of 2000 took 0.096s
  training loss:		0.674455
  validation loss:		0.602918
  validation accuracy:		81.20 %
Epoch 527 of 2000 took 0.097s
  training loss:		0.672428
  validation loss:		0.578523
  validation accuracy:		81.20 %
Epoch 528 of 2000 took 0.097s
  training loss:		0.662781
  validation loss:		0.572330
  validation accuracy:		81.20 %
Epoch 529 of 2000 took 0.097s
  training loss:		0.664989
  validation loss:		0.584476
  validation accuracy:		81.41 %
Epoch 530 of 2000 took 0.096s
  training loss:		0.654912
  validation loss:		0.602594
  validation accuracy:		81.20 %
Epoch 531 of 2000 took 0.097s
  training loss:		0.660216
  validation loss:		0.572095
  validation accuracy:		81.85 %
Epoch 532 of 2000 took 0.098s
  training loss:		0.651299
  validation loss:		0.566701
  validation accuracy:		81.63 %
Epoch 533 of 2000 took 0.097s
  training loss:		0.651675
  validation loss:		0.555758
  validation accuracy:		81.09 %
Epoch 534 of 2000 took 0.098s
  training loss:		0.646444
  validation loss:		0.555821
  validation accuracy:		81.85 %
Epoch 535 of 2000 took 0.099s
  training loss:		0.641227
  validation loss:		0.550348
  validation accuracy:		81.41 %
Epoch 536 of 2000 took 0.099s
  training loss:		0.645172
  validation loss:		0.552204
  validation accuracy:		81.41 %
Epoch 537 of 2000 took 0.099s
  training loss:		0.636222
  validation loss:		0.605051
  validation accuracy:		80.11 %
Epoch 538 of 2000 took 0.099s
  training loss:		0.646954
  validation loss:		0.562014
  validation accuracy:		80.43 %
Epoch 539 of 2000 took 0.096s
  training loss:		0.646361
  validation loss:		0.550749
  validation accuracy:		81.74 %
Epoch 540 of 2000 took 0.096s
  training loss:		0.636265
  validation loss:		0.553591
  validation accuracy:		80.98 %
Epoch 541 of 2000 took 0.098s
  training loss:		0.640897
  validation loss:		0.581771
  validation accuracy:		80.87 %
Epoch 542 of 2000 took 0.098s
  training loss:		0.646333
  validation loss:		0.560364
  validation accuracy:		82.50 %
Epoch 543 of 2000 took 0.097s
  training loss:		0.623217
  validation loss:		0.547899
  validation accuracy:		81.30 %
Epoch 544 of 2000 took 0.097s
  training loss:		0.629567
  validation loss:		0.549241
  validation accuracy:		81.30 %
Epoch 545 of 2000 took 0.096s
  training loss:		0.633481
  validation loss:		0.544298
  validation accuracy:		82.17 %
Epoch 546 of 2000 took 0.097s
  training loss:		0.621740
  validation loss:		0.565527
  validation accuracy:		82.50 %
Epoch 547 of 2000 took 0.096s
  training loss:		0.693905
  validation loss:		0.537814
  validation accuracy:		81.74 %
Epoch 548 of 2000 took 0.096s
  training loss:		0.620294
  validation loss:		0.537525
  validation accuracy:		82.39 %
Epoch 549 of 2000 took 0.096s
  training loss:		0.618950
  validation loss:		0.553742
  validation accuracy:		82.93 %
Epoch 550 of 2000 took 0.097s
  training loss:		0.633539
  validation loss:		0.556686
  validation accuracy:		81.52 %
Epoch 551 of 2000 took 0.096s
  training loss:		0.628958
  validation loss:		0.539169
  validation accuracy:		82.61 %
Epoch 552 of 2000 took 0.096s
  training loss:		0.637330
  validation loss:		0.580236
  validation accuracy:		81.09 %
Epoch 553 of 2000 took 0.097s
  training loss:		0.641430
  validation loss:		0.536176
  validation accuracy:		81.96 %
Epoch 554 of 2000 took 0.097s
  training loss:		0.621204
  validation loss:		0.555146
  validation accuracy:		82.83 %
Epoch 555 of 2000 took 0.096s
  training loss:		0.612488
  validation loss:		0.554921
  validation accuracy:		82.39 %
Epoch 556 of 2000 took 0.096s
  training loss:		0.631813
  validation loss:		0.547864
  validation accuracy:		83.26 %
Epoch 557 of 2000 took 0.096s
  training loss:		0.611050
  validation loss:		0.534741
  validation accuracy:		82.17 %
Epoch 558 of 2000 took 0.097s
  training loss:		0.617033
  validation loss:		0.541207
  validation accuracy:		82.93 %
Epoch 559 of 2000 took 0.096s
  training loss:		0.606246
  validation loss:		0.535667
  validation accuracy:		82.39 %
Epoch 560 of 2000 took 0.096s
  training loss:		0.617891
  validation loss:		0.524782
  validation accuracy:		82.83 %
Epoch 561 of 2000 took 0.097s
  training loss:		0.607930
  validation loss:		0.537750
  validation accuracy:		82.28 %
Epoch 562 of 2000 took 0.097s
  training loss:		0.615176
  validation loss:		0.618339
  validation accuracy:		79.35 %
Epoch 563 of 2000 took 0.096s
  training loss:		0.627564
  validation loss:		0.522608
  validation accuracy:		82.50 %
Epoch 564 of 2000 took 0.097s
  training loss:		0.607905
  validation loss:		0.524467
  validation accuracy:		82.93 %
Epoch 565 of 2000 took 0.096s
  training loss:		0.612661
  validation loss:		0.524278
  validation accuracy:		83.04 %
Epoch 566 of 2000 took 0.096s
  training loss:		0.628021
  validation loss:		0.606782
  validation accuracy:		80.22 %
Epoch 567 of 2000 took 0.096s
  training loss:		0.620572
  validation loss:		0.538247
  validation accuracy:		82.28 %
Epoch 568 of 2000 took 0.096s
  training loss:		0.599817
  validation loss:		0.519688
  validation accuracy:		83.04 %
Epoch 569 of 2000 took 0.097s
  training loss:		0.617712
  validation loss:		0.544979
  validation accuracy:		82.83 %
Epoch 570 of 2000 took 0.097s
  training loss:		0.663065
  validation loss:		0.604264
  validation accuracy:		80.22 %
Epoch 571 of 2000 took 0.096s
  training loss:		0.611596
  validation loss:		0.535548
  validation accuracy:		83.15 %
Epoch 572 of 2000 took 0.096s
  training loss:		0.606486
  validation loss:		0.538254
  validation accuracy:		81.96 %
Epoch 573 of 2000 took 0.097s
  training loss:		0.594386
  validation loss:		0.517435
  validation accuracy:		83.26 %
Epoch 574 of 2000 took 0.096s
  training loss:		0.598479
  validation loss:		0.535270
  validation accuracy:		83.26 %
Epoch 575 of 2000 took 0.096s
  training loss:		0.593090
  validation loss:		0.530123
  validation accuracy:		82.83 %
Epoch 576 of 2000 took 0.096s
  training loss:		0.597665
  validation loss:		0.543903
  validation accuracy:		81.41 %
Epoch 577 of 2000 took 0.096s
  training loss:		0.600404
  validation loss:		0.556789
  validation accuracy:		81.20 %
Epoch 578 of 2000 took 0.099s
  training loss:		0.617517
  validation loss:		0.522576
  validation accuracy:		82.72 %
Epoch 579 of 2000 took 0.103s
  training loss:		0.588447
  validation loss:		0.535553
  validation accuracy:		83.04 %
Epoch 580 of 2000 took 0.098s
  training loss:		0.606722
  validation loss:		0.537162
  validation accuracy:		81.41 %
Epoch 581 of 2000 took 0.096s
  training loss:		0.648159
  validation loss:		0.579470
  validation accuracy:		81.30 %
Epoch 582 of 2000 took 0.096s
  training loss:		0.620089
  validation loss:		0.529411
  validation accuracy:		82.17 %
Epoch 583 of 2000 took 0.096s
  training loss:		0.603002
  validation loss:		0.541780
  validation accuracy:		81.63 %
Epoch 584 of 2000 took 0.097s
  training loss:		0.589333
  validation loss:		0.549290
  validation accuracy:		83.15 %
Epoch 585 of 2000 took 0.097s
  training loss:		0.592541
  validation loss:		0.533654
  validation accuracy:		81.85 %
Epoch 586 of 2000 took 0.096s
  training loss:		0.624199
  validation loss:		0.569320
  validation accuracy:		80.87 %
Epoch 587 of 2000 took 0.096s
  training loss:		0.601570
  validation loss:		0.538458
  validation accuracy:		82.72 %
Epoch 588 of 2000 took 0.096s
  training loss:		0.599276
  validation loss:		0.525488
  validation accuracy:		82.39 %
Epoch 589 of 2000 took 0.096s
  training loss:		0.599023
  validation loss:		0.580319
  validation accuracy:		82.50 %
Epoch 590 of 2000 took 0.096s
  training loss:		0.590259
  validation loss:		0.524528
  validation accuracy:		82.93 %
Epoch 591 of 2000 took 0.096s
  training loss:		0.610453
  validation loss:		0.593900
  validation accuracy:		81.52 %
Epoch 592 of 2000 took 0.096s
  training loss:		0.594241
  validation loss:		0.536576
  validation accuracy:		82.93 %
Epoch 593 of 2000 took 0.096s
  training loss:		0.590701
  validation loss:		0.524992
  validation accuracy:		82.28 %
Epoch 594 of 2000 took 0.096s
  training loss:		0.607605
  validation loss:		0.521724
  validation accuracy:		82.07 %
Epoch 595 of 2000 took 0.097s
  training loss:		0.593215
  validation loss:		0.526640
  validation accuracy:		83.15 %
Epoch 596 of 2000 took 0.096s
  training loss:		0.588235
  validation loss:		0.543821
  validation accuracy:		82.39 %
Epoch 597 of 2000 took 0.096s
  training loss:		0.588097
  validation loss:		0.549025
  validation accuracy:		82.93 %
Epoch 598 of 2000 took 0.096s
  training loss:		0.575365
  validation loss:		0.527691
  validation accuracy:		83.15 %
Epoch 599 of 2000 took 0.096s
  training loss:		0.593178
  validation loss:		0.548077
  validation accuracy:		83.26 %
Epoch 600 of 2000 took 0.096s
  training loss:		0.587557
  validation loss:		0.522170
  validation accuracy:		83.04 %
Epoch 601 of 2000 took 0.096s
  training loss:		0.580010
  validation loss:		0.517936
  validation accuracy:		82.93 %
Epoch 602 of 2000 took 0.096s
  training loss:		0.593366
  validation loss:		0.518870
  validation accuracy:		83.15 %
Epoch 603 of 2000 took 0.097s
  training loss:		0.590187
  validation loss:		0.536625
  validation accuracy:		81.30 %
Epoch 604 of 2000 took 0.097s
  training loss:		0.604921
  validation loss:		0.525539
  validation accuracy:		82.72 %
Epoch 605 of 2000 took 0.097s
  training loss:		0.600686
  validation loss:		0.509540
  validation accuracy:		82.72 %
Epoch 606 of 2000 took 0.097s
  training loss:		0.584196
  validation loss:		0.543494
  validation accuracy:		81.52 %
Epoch 607 of 2000 took 0.096s
  training loss:		0.599012
  validation loss:		0.546308
  validation accuracy:		82.93 %
Epoch 608 of 2000 took 0.096s
  training loss:		0.581038
  validation loss:		0.502393
  validation accuracy:		83.37 %
Epoch 609 of 2000 took 0.096s
  training loss:		0.657712
  validation loss:		0.637147
  validation accuracy:		78.59 %
Epoch 610 of 2000 took 0.097s
  training loss:		0.606920
  validation loss:		0.594880
  validation accuracy:		80.11 %
Epoch 611 of 2000 took 0.097s
  training loss:		0.602356
  validation loss:		0.508778
  validation accuracy:		83.15 %
Epoch 612 of 2000 took 0.096s
  training loss:		0.590451
  validation loss:		0.545187
  validation accuracy:		81.74 %
Epoch 613 of 2000 took 0.096s
  training loss:		0.585336
  validation loss:		0.568156
  validation accuracy:		83.15 %
Epoch 614 of 2000 took 0.097s
  training loss:		0.607957
  validation loss:		0.506700
  validation accuracy:		82.83 %
Epoch 615 of 2000 took 0.097s
  training loss:		0.601280
  validation loss:		0.510572
  validation accuracy:		82.50 %
Epoch 616 of 2000 took 0.097s
  training loss:		0.588338
  validation loss:		0.526099
  validation accuracy:		83.48 %
Epoch 617 of 2000 took 0.096s
  training loss:		0.589125
  validation loss:		0.509976
  validation accuracy:		82.72 %
Epoch 618 of 2000 took 0.097s
  training loss:		0.599580
  validation loss:		0.510323
  validation accuracy:		83.15 %
Epoch 619 of 2000 took 0.096s
  training loss:		0.579613
  validation loss:		0.520144
  validation accuracy:		83.15 %
Epoch 620 of 2000 took 0.097s
  training loss:		0.576037
  validation loss:		0.595021
  validation accuracy:		80.33 %
Epoch 621 of 2000 took 0.096s
  training loss:		0.591608
  validation loss:		0.507861
  validation accuracy:		83.04 %
Epoch 622 of 2000 took 0.096s
  training loss:		0.581186
  validation loss:		0.520626
  validation accuracy:		83.48 %
Epoch 623 of 2000 took 0.097s
  training loss:		0.581408
  validation loss:		0.550291
  validation accuracy:		81.74 %
Epoch 624 of 2000 took 0.096s
  training loss:		0.600804
  validation loss:		0.525240
  validation accuracy:		83.37 %
Epoch 625 of 2000 took 0.097s
  training loss:		0.582497
  validation loss:		0.575434
  validation accuracy:		82.93 %
Epoch 626 of 2000 took 0.097s
  training loss:		0.585461
  validation loss:		0.550382
  validation accuracy:		83.59 %
Epoch 627 of 2000 took 0.096s
  training loss:		0.584076
  validation loss:		0.502466
  validation accuracy:		83.04 %
Epoch 628 of 2000 took 0.097s
  training loss:		0.584105
  validation loss:		0.516669
  validation accuracy:		83.04 %
Epoch 629 of 2000 took 0.096s
  training loss:		0.592298
  validation loss:		0.502480
  validation accuracy:		83.15 %
Epoch 630 of 2000 took 0.097s
  training loss:		0.582477
  validation loss:		0.523677
  validation accuracy:		82.72 %
Epoch 631 of 2000 took 0.096s
  training loss:		0.588664
  validation loss:		0.636009
  validation accuracy:		78.59 %
Epoch 632 of 2000 took 0.097s
  training loss:		0.627640
  validation loss:		0.508186
  validation accuracy:		83.80 %
Epoch 633 of 2000 took 0.096s
  training loss:		0.584704
  validation loss:		0.522702
  validation accuracy:		82.72 %
Epoch 634 of 2000 took 0.097s
  training loss:		0.573177
  validation loss:		0.512447
  validation accuracy:		83.04 %
Epoch 635 of 2000 took 0.097s
  training loss:		0.580104
  validation loss:		0.540394
  validation accuracy:		81.74 %
Epoch 636 of 2000 took 0.097s
  training loss:		0.596413
  validation loss:		0.550922
  validation accuracy:		82.72 %
Epoch 637 of 2000 took 0.097s
  training loss:		0.605813
  validation loss:		0.519449
  validation accuracy:		82.93 %
Epoch 638 of 2000 took 0.097s
  training loss:		0.608828
  validation loss:		0.531032
  validation accuracy:		83.70 %
Epoch 639 of 2000 took 0.096s
  training loss:		0.582497
  validation loss:		0.510322
  validation accuracy:		83.48 %
Epoch 640 of 2000 took 0.097s
  training loss:		0.583256
  validation loss:		0.553653
  validation accuracy:		83.70 %
Epoch 641 of 2000 took 0.098s
  training loss:		0.586804
  validation loss:		0.503046
  validation accuracy:		82.72 %
Epoch 642 of 2000 took 0.096s
  training loss:		0.594553
  validation loss:		0.518308
  validation accuracy:		82.93 %
Epoch 643 of 2000 took 0.096s
  training loss:		0.578063
  validation loss:		0.526342
  validation accuracy:		81.96 %
Epoch 644 of 2000 took 0.096s
  training loss:		0.588209
  validation loss:		0.531476
  validation accuracy:		82.17 %
Epoch 645 of 2000 took 0.096s
  training loss:		0.584464
  validation loss:		0.506276
  validation accuracy:		83.15 %
Epoch 646 of 2000 took 0.097s
  training loss:		0.590249
  validation loss:		0.527772
  validation accuracy:		81.63 %
Epoch 647 of 2000 took 0.097s
  training loss:		0.584171
  validation loss:		0.541805
  validation accuracy:		83.91 %
Epoch 648 of 2000 took 0.096s
  training loss:		0.580396
  validation loss:		0.539005
  validation accuracy:		83.59 %
Epoch 649 of 2000 took 0.099s
  training loss:		0.578511
  validation loss:		0.533480
  validation accuracy:		83.70 %
Epoch 650 of 2000 took 0.097s
  training loss:		0.576075
  validation loss:		0.539994
  validation accuracy:		81.74 %
Epoch 651 of 2000 took 0.097s
  training loss:		0.586688
  validation loss:		0.536340
  validation accuracy:		82.39 %
Epoch 652 of 2000 took 0.097s
  training loss:		0.579995
  validation loss:		0.507256
  validation accuracy:		83.26 %
Epoch 653 of 2000 took 0.097s
  training loss:		0.584785
  validation loss:		0.507863
  validation accuracy:		83.15 %
Epoch 654 of 2000 took 0.096s
  training loss:		0.581298
  validation loss:		0.514397
  validation accuracy:		83.26 %
Epoch 655 of 2000 took 0.097s
  training loss:		0.570929
  validation loss:		0.509800
  validation accuracy:		83.59 %
Epoch 656 of 2000 took 0.097s
  training loss:		0.582252
  validation loss:		0.511663
  validation accuracy:		82.83 %
Epoch 657 of 2000 took 0.096s
  training loss:		0.592423
  validation loss:		0.544686
  validation accuracy:		83.59 %
Epoch 658 of 2000 took 0.097s
  training loss:		0.584421
  validation loss:		0.508998
  validation accuracy:		83.26 %
Epoch 659 of 2000 took 0.096s
  training loss:		0.572554
  validation loss:		0.596025
  validation accuracy:		79.78 %
Epoch 660 of 2000 took 0.097s
  training loss:		0.591154
  validation loss:		0.542155
  validation accuracy:		82.07 %
Epoch 661 of 2000 took 0.099s
  training loss:		0.587581
  validation loss:		0.524782
  validation accuracy:		82.50 %
Epoch 662 of 2000 took 0.099s
  training loss:		0.585279
  validation loss:		0.505986
  validation accuracy:		82.61 %
Epoch 663 of 2000 took 0.099s
  training loss:		0.571495
  validation loss:		0.504129
  validation accuracy:		83.04 %
Epoch 664 of 2000 took 0.099s
  training loss:		0.587239
  validation loss:		0.514662
  validation accuracy:		83.26 %
Epoch 665 of 2000 took 0.100s
  training loss:		0.580589
  validation loss:		0.551582
  validation accuracy:		81.85 %
Epoch 666 of 2000 took 0.099s
  training loss:		0.590461
  validation loss:		0.527808
  validation accuracy:		83.48 %
Epoch 667 of 2000 took 0.098s
  training loss:		0.575579
  validation loss:		0.514304
  validation accuracy:		82.72 %
Epoch 668 of 2000 took 0.097s
  training loss:		0.581840
  validation loss:		0.510599
  validation accuracy:		83.04 %
Epoch 669 of 2000 took 0.097s
  training loss:		0.566603
  validation loss:		0.521371
  validation accuracy:		83.26 %
Epoch 670 of 2000 took 0.097s
  training loss:		0.596649
  validation loss:		0.529453
  validation accuracy:		82.39 %
Epoch 671 of 2000 took 0.096s
  training loss:		0.572982
  validation loss:		0.513723
  validation accuracy:		83.15 %
Epoch 672 of 2000 took 0.099s
  training loss:		0.575225
  validation loss:		0.500449
  validation accuracy:		83.15 %
Epoch 673 of 2000 took 0.099s
  training loss:		0.594123
  validation loss:		0.511086
  validation accuracy:		83.04 %
Epoch 674 of 2000 took 0.099s
  training loss:		0.586268
  validation loss:		0.523951
  validation accuracy:		83.26 %
Epoch 675 of 2000 took 0.099s
  training loss:		0.579067
  validation loss:		0.517925
  validation accuracy:		83.59 %
Epoch 676 of 2000 took 0.099s
  training loss:		0.574990
  validation loss:		0.505749
  validation accuracy:		83.37 %
Epoch 677 of 2000 took 0.100s
  training loss:		0.575779
  validation loss:		0.508939
  validation accuracy:		83.48 %
Epoch 678 of 2000 took 0.100s
  training loss:		0.580381
  validation loss:		0.520095
  validation accuracy:		82.93 %
Epoch 679 of 2000 took 0.099s
  training loss:		0.589314
  validation loss:		0.512527
  validation accuracy:		83.04 %
Epoch 680 of 2000 took 0.099s
  training loss:		0.568989
  validation loss:		0.528114
  validation accuracy:		82.72 %
Epoch 681 of 2000 took 0.099s
  training loss:		0.571983
  validation loss:		0.519342
  validation accuracy:		82.72 %
Epoch 682 of 2000 took 0.100s
  training loss:		0.574655
  validation loss:		0.526309
  validation accuracy:		82.39 %
Epoch 683 of 2000 took 0.099s
  training loss:		0.571719
  validation loss:		0.510758
  validation accuracy:		82.50 %
Epoch 684 of 2000 took 0.098s
  training loss:		0.608169
  validation loss:		0.511162
  validation accuracy:		83.04 %
Epoch 685 of 2000 took 0.096s
  training loss:		0.568354
  validation loss:		0.524335
  validation accuracy:		82.17 %
Epoch 686 of 2000 took 0.096s
  training loss:		0.581181
  validation loss:		0.500909
  validation accuracy:		83.15 %
Epoch 687 of 2000 took 0.096s
  training loss:		0.570148
  validation loss:		0.513624
  validation accuracy:		83.48 %
Epoch 688 of 2000 took 0.099s
  training loss:		0.580520
  validation loss:		0.559461
  validation accuracy:		83.48 %
Epoch 689 of 2000 took 0.096s
  training loss:		0.583692
  validation loss:		0.522500
  validation accuracy:		83.37 %
Epoch 690 of 2000 took 0.096s
  training loss:		0.564416
  validation loss:		0.531944
  validation accuracy:		81.96 %
Epoch 691 of 2000 took 0.096s
  training loss:		0.573553
  validation loss:		0.515525
  validation accuracy:		83.15 %
Epoch 692 of 2000 took 0.097s
  training loss:		0.588252
  validation loss:		0.520338
  validation accuracy:		83.70 %
Epoch 693 of 2000 took 0.097s
  training loss:		0.567848
  validation loss:		0.528117
  validation accuracy:		83.37 %
Epoch 694 of 2000 took 0.096s
  training loss:		0.561642
  validation loss:		0.504245
  validation accuracy:		83.26 %
Epoch 695 of 2000 took 0.096s
  training loss:		0.583799
  validation loss:		0.507019
  validation accuracy:		83.37 %
Epoch 696 of 2000 took 0.097s
  training loss:		0.574371
  validation loss:		0.548452
  validation accuracy:		82.50 %
Epoch 697 of 2000 took 0.097s
  training loss:		0.579618
  validation loss:		0.530488
  validation accuracy:		82.17 %
Epoch 698 of 2000 took 0.097s
  training loss:		0.571348
  validation loss:		0.508818
  validation accuracy:		83.04 %
Epoch 699 of 2000 took 0.096s
  training loss:		0.561939
  validation loss:		0.512256
  validation accuracy:		83.26 %
Epoch 700 of 2000 took 0.096s
  training loss:		0.575211
  validation loss:		0.505987
  validation accuracy:		83.37 %
Epoch 701 of 2000 took 0.096s
  training loss:		0.573042
  validation loss:		0.525018
  validation accuracy:		82.39 %
Epoch 702 of 2000 took 0.096s
  training loss:		0.575171
  validation loss:		0.505570
  validation accuracy:		83.04 %
Epoch 703 of 2000 took 0.096s
  training loss:		0.576248
  validation loss:		0.506457
  validation accuracy:		82.61 %
Epoch 704 of 2000 took 0.097s
  training loss:		0.569374
  validation loss:		0.550193
  validation accuracy:		82.07 %
Epoch 705 of 2000 took 0.096s
  training loss:		0.573825
  validation loss:		0.522026
  validation accuracy:		82.50 %
Epoch 706 of 2000 took 0.098s
  training loss:		0.575549
  validation loss:		0.532605
  validation accuracy:		83.59 %
Epoch 707 of 2000 took 0.099s
  training loss:		0.585522
  validation loss:		0.506113
  validation accuracy:		83.15 %
Epoch 708 of 2000 took 0.099s
  training loss:		0.586206
  validation loss:		0.530832
  validation accuracy:		82.17 %
Epoch 709 of 2000 took 0.096s
  training loss:		0.563643
  validation loss:		0.517228
  validation accuracy:		83.37 %
Epoch 710 of 2000 took 0.096s
  training loss:		0.563721
  validation loss:		0.517980
  validation accuracy:		82.72 %
Epoch 711 of 2000 took 0.096s
  training loss:		0.577408
  validation loss:		0.534346
  validation accuracy:		82.72 %
Epoch 712 of 2000 took 0.096s
  training loss:		0.573520
  validation loss:		0.542051
  validation accuracy:		82.07 %
Epoch 713 of 2000 took 0.096s
  training loss:		0.591446
  validation loss:		0.517228
  validation accuracy:		82.39 %
Epoch 714 of 2000 took 0.096s
  training loss:		0.563599
  validation loss:		0.503292
  validation accuracy:		82.93 %
Epoch 715 of 2000 took 0.097s
  training loss:		0.574783
  validation loss:		0.530274
  validation accuracy:		82.28 %
Epoch 716 of 2000 took 0.096s
  training loss:		0.578437
  validation loss:		0.539115
  validation accuracy:		83.26 %
Epoch 717 of 2000 took 0.096s
  training loss:		0.622996
  validation loss:		0.518231
  validation accuracy:		82.50 %
Epoch 718 of 2000 took 0.096s
  training loss:		0.574947
  validation loss:		0.510901
  validation accuracy:		82.83 %
Epoch 719 of 2000 took 0.096s
  training loss:		0.598127
  validation loss:		0.507253
  validation accuracy:		83.26 %
Epoch 720 of 2000 took 0.096s
  training loss:		0.576823
  validation loss:		0.551169
  validation accuracy:		82.72 %
Epoch 721 of 2000 took 0.096s
  training loss:		0.569861
  validation loss:		0.506799
  validation accuracy:		82.72 %
Epoch 722 of 2000 took 0.096s
  training loss:		0.576791
  validation loss:		0.511698
  validation accuracy:		82.83 %
Epoch 723 of 2000 took 0.096s
  training loss:		0.561804
  validation loss:		0.505264
  validation accuracy:		83.15 %
Epoch 724 of 2000 took 0.096s
  training loss:		0.570378
  validation loss:		0.547996
  validation accuracy:		83.59 %
Epoch 725 of 2000 took 0.097s
  training loss:		0.578663
  validation loss:		0.501922
  validation accuracy:		83.26 %
Epoch 726 of 2000 took 0.097s
  training loss:		0.574065
  validation loss:		0.509303
  validation accuracy:		83.37 %
Epoch 727 of 2000 took 0.096s
  training loss:		0.567488
  validation loss:		0.524726
  validation accuracy:		83.04 %
Epoch 728 of 2000 took 0.096s
  training loss:		0.572945
  validation loss:		0.517029
  validation accuracy:		82.50 %
Epoch 729 of 2000 took 0.097s
  training loss:		0.560597
  validation loss:		0.529671
  validation accuracy:		83.37 %
Epoch 730 of 2000 took 0.096s
  training loss:		0.582950
  validation loss:		0.524616
  validation accuracy:		82.50 %
Epoch 731 of 2000 took 0.097s
  training loss:		0.577361
  validation loss:		0.514173
  validation accuracy:		82.72 %
Epoch 732 of 2000 took 0.100s
  training loss:		0.572658
  validation loss:		0.531239
  validation accuracy:		83.04 %
Epoch 733 of 2000 took 0.099s
  training loss:		0.583784
  validation loss:		0.551652
  validation accuracy:		83.04 %
Epoch 734 of 2000 took 0.098s
  training loss:		0.567572
  validation loss:		0.501646
  validation accuracy:		83.70 %
Epoch 735 of 2000 took 0.096s
  training loss:		0.575272
  validation loss:		0.522410
  validation accuracy:		82.72 %
Epoch 736 of 2000 took 0.096s
  training loss:		0.575693
  validation loss:		0.516076
  validation accuracy:		82.50 %
Epoch 737 of 2000 took 0.097s
  training loss:		0.570182
  validation loss:		0.517639
  validation accuracy:		82.61 %
Epoch 738 of 2000 took 0.097s
  training loss:		0.568111
  validation loss:		0.515798
  validation accuracy:		82.83 %
Epoch 739 of 2000 took 0.097s
  training loss:		0.566224
  validation loss:		0.519483
  validation accuracy:		83.04 %
Epoch 740 of 2000 took 0.096s
  training loss:		0.577495
  validation loss:		0.510000
  validation accuracy:		83.48 %
Epoch 741 of 2000 took 0.096s
  training loss:		0.566435
  validation loss:		0.531341
  validation accuracy:		82.83 %
Epoch 742 of 2000 took 0.096s
  training loss:		0.567131
  validation loss:		0.499217
  validation accuracy:		83.15 %
Epoch 743 of 2000 took 0.096s
  training loss:		0.584340
  validation loss:		0.505337
  validation accuracy:		82.93 %
Epoch 744 of 2000 took 0.096s
  training loss:		0.576724
  validation loss:		0.501495
  validation accuracy:		83.48 %
Epoch 745 of 2000 took 0.096s
  training loss:		0.570789
  validation loss:		0.517959
  validation accuracy:		83.15 %
Epoch 746 of 2000 took 0.096s
  training loss:		0.582308
  validation loss:		0.512398
  validation accuracy:		83.04 %
Epoch 747 of 2000 took 0.096s
  training loss:		0.578765
  validation loss:		0.509130
  validation accuracy:		82.61 %
Epoch 748 of 2000 took 0.096s
  training loss:		0.581351
  validation loss:		0.509559
  validation accuracy:		83.04 %
Epoch 749 of 2000 took 0.096s
  training loss:		0.570699
  validation loss:		0.524462
  validation accuracy:		81.52 %
Epoch 750 of 2000 took 0.096s
  training loss:		0.569447
  validation loss:		0.556828
  validation accuracy:		83.26 %
Epoch 751 of 2000 took 0.096s
  training loss:		0.576797
  validation loss:		0.519541
  validation accuracy:		82.93 %
Epoch 752 of 2000 took 0.097s
  training loss:		0.575413
  validation loss:		0.516138
  validation accuracy:		83.15 %
Epoch 753 of 2000 took 0.097s
  training loss:		0.568814
  validation loss:		0.510340
  validation accuracy:		83.15 %
Epoch 754 of 2000 took 0.096s
  training loss:		0.569545
  validation loss:		0.506920
  validation accuracy:		83.04 %
Epoch 755 of 2000 took 0.096s
  training loss:		0.571651
  validation loss:		0.551483
  validation accuracy:		81.74 %
Epoch 756 of 2000 took 0.097s
  training loss:		0.572249
  validation loss:		0.507627
  validation accuracy:		83.04 %
Epoch 757 of 2000 took 0.096s
  training loss:		0.567368
  validation loss:		0.516253
  validation accuracy:		82.61 %
Epoch 758 of 2000 took 0.096s
  training loss:		0.574564
  validation loss:		0.504843
  validation accuracy:		83.48 %
Epoch 759 of 2000 took 0.096s
  training loss:		0.572957
  validation loss:		0.524003
  validation accuracy:		83.04 %
Epoch 760 of 2000 took 0.096s
  training loss:		0.572551
  validation loss:		0.535570
  validation accuracy:		82.07 %
Epoch 761 of 2000 took 0.096s
  training loss:		0.568993
  validation loss:		0.526668
  validation accuracy:		82.61 %
Epoch 762 of 2000 took 0.096s
  training loss:		0.567388
  validation loss:		0.509907
  validation accuracy:		83.37 %
Epoch 763 of 2000 took 0.096s
  training loss:		0.572388
  validation loss:		0.516227
  validation accuracy:		83.37 %
Epoch 764 of 2000 took 0.096s
  training loss:		0.564553
  validation loss:		0.508100
  validation accuracy:		82.83 %
Epoch 765 of 2000 took 0.096s
  training loss:		0.562386
  validation loss:		0.513453
  validation accuracy:		83.04 %
Epoch 766 of 2000 took 0.099s
  training loss:		0.579907
  validation loss:		0.521028
  validation accuracy:		82.61 %
Epoch 767 of 2000 took 0.096s
  training loss:		0.566632
  validation loss:		0.511947
  validation accuracy:		83.15 %
Epoch 768 of 2000 took 0.096s
  training loss:		0.575107
  validation loss:		0.524344
  validation accuracy:		82.93 %
Epoch 769 of 2000 took 0.096s
  training loss:		0.563213
  validation loss:		0.524590
  validation accuracy:		82.50 %
Epoch 770 of 2000 took 0.097s
  training loss:		0.578175
  validation loss:		0.551995
  validation accuracy:		82.28 %
Epoch 771 of 2000 took 0.096s
  training loss:		0.574645
  validation loss:		0.523074
  validation accuracy:		83.15 %
Epoch 772 of 2000 took 0.096s
  training loss:		0.566717
  validation loss:		0.527275
  validation accuracy:		82.83 %
Epoch 773 of 2000 took 0.096s
  training loss:		0.577722
  validation loss:		0.531632
  validation accuracy:		82.61 %
Epoch 774 of 2000 took 0.096s
  training loss:		0.562700
  validation loss:		0.535058
  validation accuracy:		83.37 %
Epoch 775 of 2000 took 0.096s
  training loss:		0.593537
  validation loss:		0.527050
  validation accuracy:		82.50 %
Epoch 776 of 2000 took 0.097s
  training loss:		0.571991
  validation loss:		0.527794
  validation accuracy:		83.59 %
Epoch 777 of 2000 took 0.097s
  training loss:		0.583715
  validation loss:		0.503406
  validation accuracy:		83.26 %
Epoch 778 of 2000 took 0.096s
  training loss:		0.577060
  validation loss:		0.506369
  validation accuracy:		82.39 %
Epoch 779 of 2000 took 0.097s
  training loss:		0.566256
  validation loss:		0.514330
  validation accuracy:		83.04 %
Epoch 780 of 2000 took 0.096s
  training loss:		0.569555
  validation loss:		0.513818
  validation accuracy:		83.48 %
Epoch 781 of 2000 took 0.097s
  training loss:		0.576384
  validation loss:		0.512359
  validation accuracy:		82.39 %
Epoch 782 of 2000 took 0.096s
  training loss:		0.587623
  validation loss:		0.527641
  validation accuracy:		82.83 %
Epoch 783 of 2000 took 0.096s
  training loss:		0.570747
  validation loss:		0.542666
  validation accuracy:		82.28 %
Epoch 784 of 2000 took 0.096s
  training loss:		0.575898
  validation loss:		0.518007
  validation accuracy:		83.70 %
Epoch 785 of 2000 took 0.096s
  training loss:		0.561140
  validation loss:		0.517987
  validation accuracy:		82.93 %
Epoch 786 of 2000 took 0.096s
  training loss:		0.577360
  validation loss:		0.523607
  validation accuracy:		82.83 %
Epoch 787 of 2000 took 0.096s
  training loss:		0.566877
  validation loss:		0.514847
  validation accuracy:		83.26 %
Epoch 788 of 2000 took 0.096s
  training loss:		0.560624
  validation loss:		0.515908
  validation accuracy:		82.93 %
Epoch 789 of 2000 took 0.096s
  training loss:		0.564445
  validation loss:		0.548748
  validation accuracy:		81.74 %
Epoch 790 of 2000 took 0.096s
  training loss:		0.567193
  validation loss:		0.505289
  validation accuracy:		83.37 %
Epoch 791 of 2000 took 0.097s
  training loss:		0.576437
  validation loss:		0.516462
  validation accuracy:		83.37 %
Epoch 792 of 2000 took 0.096s
  training loss:		0.560266
  validation loss:		0.520249
  validation accuracy:		83.04 %
Epoch 793 of 2000 took 0.096s
  training loss:		0.567978
  validation loss:		0.520799
  validation accuracy:		83.80 %
Epoch 794 of 2000 took 0.096s
  training loss:		0.584330
  validation loss:		0.510526
  validation accuracy:		82.83 %
Epoch 795 of 2000 took 0.096s
  training loss:		0.563613
  validation loss:		0.516325
  validation accuracy:		83.15 %
Epoch 796 of 2000 took 0.096s
  training loss:		0.558994
  validation loss:		0.522807
  validation accuracy:		83.59 %
Epoch 797 of 2000 took 0.096s
  training loss:		0.561891
  validation loss:		0.510314
  validation accuracy:		83.26 %
Epoch 798 of 2000 took 0.096s
  training loss:		0.573917
  validation loss:		0.506120
  validation accuracy:		83.59 %
Epoch 799 of 2000 took 0.096s
  training loss:		0.568147
  validation loss:		0.507892
  validation accuracy:		83.48 %
Epoch 800 of 2000 took 0.096s
  training loss:		0.561202
  validation loss:		0.555799
  validation accuracy:		81.63 %
Epoch 801 of 2000 took 0.097s
  training loss:		0.567124
  validation loss:		0.521910
  validation accuracy:		83.48 %
Epoch 802 of 2000 took 0.097s
  training loss:		0.570440
  validation loss:		0.516877
  validation accuracy:		83.37 %
Epoch 803 of 2000 took 0.097s
  training loss:		0.585918
  validation loss:		0.523387
  validation accuracy:		83.04 %
Epoch 804 of 2000 took 0.096s
  training loss:		0.563608
  validation loss:		0.515914
  validation accuracy:		82.72 %
Epoch 805 of 2000 took 0.096s
  training loss:		0.570234
  validation loss:		0.517074
  validation accuracy:		83.59 %
Epoch 806 of 2000 took 0.096s
  training loss:		0.569211
  validation loss:		0.518482
  validation accuracy:		82.93 %
Epoch 807 of 2000 took 0.096s
  training loss:		0.558016
  validation loss:		0.508000
  validation accuracy:		83.04 %
Epoch 808 of 2000 took 0.097s
  training loss:		0.565105
  validation loss:		0.509121
  validation accuracy:		82.93 %
Epoch 809 of 2000 took 0.097s
  training loss:		0.557682
  validation loss:		0.525407
  validation accuracy:		83.48 %
Epoch 810 of 2000 took 0.096s
  training loss:		0.572180
  validation loss:		0.513378
  validation accuracy:		82.50 %
Epoch 811 of 2000 took 0.096s
  training loss:		0.568339
  validation loss:		0.524144
  validation accuracy:		83.80 %
Epoch 812 of 2000 took 0.097s
  training loss:		0.563756
  validation loss:		0.514055
  validation accuracy:		83.37 %
Epoch 813 of 2000 took 0.096s
  training loss:		0.561654
  validation loss:		0.524819
  validation accuracy:		83.48 %
Epoch 814 of 2000 took 0.097s
  training loss:		0.568622
  validation loss:		0.524008
  validation accuracy:		82.93 %
Epoch 815 of 2000 took 0.097s
  training loss:		0.555583
  validation loss:		0.505665
  validation accuracy:		83.04 %
Epoch 816 of 2000 took 0.097s
  training loss:		0.579838
  validation loss:		0.522022
  validation accuracy:		82.83 %
Epoch 817 of 2000 took 0.097s
  training loss:		0.571353
  validation loss:		0.560790
  validation accuracy:		81.74 %
Epoch 818 of 2000 took 0.097s
  training loss:		0.567286
  validation loss:		0.529761
  validation accuracy:		82.39 %
Epoch 819 of 2000 took 0.097s
  training loss:		0.576347
  validation loss:		0.501320
  validation accuracy:		83.04 %
Epoch 820 of 2000 took 0.097s
  training loss:		0.562313
  validation loss:		0.500563
  validation accuracy:		83.37 %
Epoch 821 of 2000 took 0.097s
  training loss:		0.564549
  validation loss:		0.509201
  validation accuracy:		83.37 %
Epoch 822 of 2000 took 0.097s
  training loss:		0.563733
  validation loss:		0.513943
  validation accuracy:		82.50 %
Epoch 823 of 2000 took 0.096s
  training loss:		0.560089
  validation loss:		0.522899
  validation accuracy:		83.37 %
Epoch 824 of 2000 took 0.096s
  training loss:		0.559789
  validation loss:		0.508323
  validation accuracy:		83.26 %
Epoch 825 of 2000 took 0.096s
  training loss:		0.562683
  validation loss:		0.507110
  validation accuracy:		83.48 %
Epoch 826 of 2000 took 0.096s
  training loss:		0.570532
  validation loss:		0.502410
  validation accuracy:		83.15 %
Epoch 827 of 2000 took 0.096s
  training loss:		0.563345
  validation loss:		0.519599
  validation accuracy:		83.04 %
Epoch 828 of 2000 took 0.096s
  training loss:		0.566596
  validation loss:		0.509350
  validation accuracy:		82.50 %
Epoch 829 of 2000 took 0.096s
  training loss:		0.565880
  validation loss:		0.525920
  validation accuracy:		83.04 %
Epoch 830 of 2000 took 0.096s
  training loss:		0.559551
  validation loss:		0.505689
  validation accuracy:		83.26 %
Epoch 831 of 2000 took 0.096s
  training loss:		0.559164
  validation loss:		0.513790
  validation accuracy:		83.04 %
Epoch 832 of 2000 took 0.097s
  training loss:		0.560478
  validation loss:		0.538217
  validation accuracy:		83.37 %
Epoch 833 of 2000 took 0.097s
  training loss:		0.589935
  validation loss:		0.552324
  validation accuracy:		81.74 %
Epoch 834 of 2000 took 0.096s
  training loss:		0.588521
  validation loss:		0.578541
  validation accuracy:		82.83 %
Epoch 835 of 2000 took 0.096s
  training loss:		0.569328
  validation loss:		0.516232
  validation accuracy:		83.48 %
Epoch 836 of 2000 took 0.096s
  training loss:		0.560587
  validation loss:		0.504628
  validation accuracy:		83.37 %
Epoch 837 of 2000 took 0.096s
  training loss:		0.568955
  validation loss:		0.545193
  validation accuracy:		83.15 %
Epoch 838 of 2000 took 0.097s
  training loss:		0.561099
  validation loss:		0.506207
  validation accuracy:		83.15 %
Epoch 839 of 2000 took 0.097s
  training loss:		0.562529
  validation loss:		0.512668
  validation accuracy:		82.72 %
Epoch 840 of 2000 took 0.096s
  training loss:		0.562049
  validation loss:		0.506480
  validation accuracy:		82.93 %
Epoch 841 of 2000 took 0.097s
  training loss:		0.574480
  validation loss:		0.525094
  validation accuracy:		83.37 %
Epoch 842 of 2000 took 0.096s
  training loss:		0.558951
  validation loss:		0.529102
  validation accuracy:		83.04 %
Epoch 843 of 2000 took 0.096s
  training loss:		0.569217
  validation loss:		0.517177
  validation accuracy:		83.15 %
Epoch 844 of 2000 took 0.096s
  training loss:		0.567434
  validation loss:		0.505876
  validation accuracy:		82.28 %
Epoch 845 of 2000 took 0.096s
  training loss:		0.573725
  validation loss:		0.530360
  validation accuracy:		83.26 %
Epoch 846 of 2000 took 0.096s
  training loss:		0.564951
  validation loss:		0.523127
  validation accuracy:		83.37 %
Epoch 847 of 2000 took 0.096s
  training loss:		0.567023
  validation loss:		0.504486
  validation accuracy:		83.26 %
Epoch 848 of 2000 took 0.096s
  training loss:		0.558177
  validation loss:		0.525218
  validation accuracy:		82.39 %
Epoch 849 of 2000 took 0.097s
  training loss:		0.559441
  validation loss:		0.541445
  validation accuracy:		81.63 %
Epoch 850 of 2000 took 0.096s
  training loss:		0.572011
  validation loss:		0.500322
  validation accuracy:		83.04 %
Epoch 851 of 2000 took 0.096s
  training loss:		0.562052
  validation loss:		0.533363
  validation accuracy:		83.37 %
Epoch 852 of 2000 took 0.097s
  training loss:		0.575182
  validation loss:		0.525492
  validation accuracy:		83.26 %
Epoch 853 of 2000 took 0.097s
  training loss:		0.570042
  validation loss:		0.509133
  validation accuracy:		82.72 %
Epoch 854 of 2000 took 0.096s
  training loss:		0.574328
  validation loss:		0.512876
  validation accuracy:		83.26 %
Epoch 855 of 2000 took 0.097s
  training loss:		0.557119
  validation loss:		0.521624
  validation accuracy:		83.15 %
Epoch 856 of 2000 took 0.097s
  training loss:		0.551367
  validation loss:		0.511483
  validation accuracy:		82.50 %
Epoch 857 of 2000 took 0.096s
  training loss:		0.568364
  validation loss:		0.508981
  validation accuracy:		83.04 %
Epoch 858 of 2000 took 0.097s
  training loss:		0.560006
  validation loss:		0.519450
  validation accuracy:		83.37 %
Epoch 859 of 2000 took 0.096s
  training loss:		0.555614
  validation loss:		0.506548
  validation accuracy:		83.15 %
Epoch 860 of 2000 took 0.096s
  training loss:		0.564077
  validation loss:		0.500700
  validation accuracy:		83.48 %
Epoch 861 of 2000 took 0.096s
  training loss:		0.557874
  validation loss:		0.505233
  validation accuracy:		83.37 %
Epoch 862 of 2000 took 0.097s
  training loss:		0.560056
  validation loss:		0.511999
  validation accuracy:		83.26 %
Epoch 863 of 2000 took 0.096s
  training loss:		0.571403
  validation loss:		0.502419
  validation accuracy:		83.37 %
Epoch 864 of 2000 took 0.097s
  training loss:		0.561585
  validation loss:		0.503473
  validation accuracy:		82.93 %
Epoch 865 of 2000 took 0.097s
  training loss:		0.553857
  validation loss:		0.500360
  validation accuracy:		83.04 %
Epoch 866 of 2000 took 0.096s
  training loss:		0.563560
  validation loss:		0.571988
  validation accuracy:		82.17 %
Epoch 867 of 2000 took 0.096s
  training loss:		0.588358
  validation loss:		0.523969
  validation accuracy:		82.83 %
Epoch 868 of 2000 took 0.096s
  training loss:		0.556125
  validation loss:		0.514411
  validation accuracy:		83.26 %
Epoch 869 of 2000 took 0.096s
  training loss:		0.565458
  validation loss:		0.516678
  validation accuracy:		82.72 %
Epoch 870 of 2000 took 0.097s
  training loss:		0.563541
  validation loss:		0.511701
  validation accuracy:		82.39 %
Epoch 871 of 2000 took 0.104s
  training loss:		0.559564
  validation loss:		0.500100
  validation accuracy:		83.04 %
Epoch 872 of 2000 took 0.112s
  training loss:		0.554529
  validation loss:		0.517716
  validation accuracy:		83.48 %
Epoch 873 of 2000 took 0.133s
  training loss:		0.586371
  validation loss:		0.520059
  validation accuracy:		82.93 %
Epoch 874 of 2000 took 0.093s
  training loss:		0.561072
  validation loss:		0.529001
  validation accuracy:		82.61 %
Epoch 875 of 2000 took 0.095s
  training loss:		0.581258
  validation loss:		0.534472
  validation accuracy:		83.26 %
Epoch 876 of 2000 took 0.100s
  training loss:		0.548939
  validation loss:		0.514860
  validation accuracy:		83.48 %
Epoch 877 of 2000 took 0.103s
  training loss:		0.558449
  validation loss:		0.506198
  validation accuracy:		83.15 %
Epoch 878 of 2000 took 0.102s
  training loss:		0.566758
  validation loss:		0.506863
  validation accuracy:		82.83 %
Epoch 879 of 2000 took 0.099s
  training loss:		0.575473
  validation loss:		0.522422
  validation accuracy:		82.50 %
Epoch 880 of 2000 took 0.098s
  training loss:		0.552795
  validation loss:		0.510254
  validation accuracy:		83.15 %
Epoch 881 of 2000 took 0.096s
  training loss:		0.559530
  validation loss:		0.513794
  validation accuracy:		83.48 %
Epoch 882 of 2000 took 0.096s
  training loss:		0.560544
  validation loss:		0.504210
  validation accuracy:		83.15 %
Epoch 883 of 2000 took 0.096s
  training loss:		0.563060
  validation loss:		0.520353
  validation accuracy:		83.48 %
Epoch 884 of 2000 took 0.097s
  training loss:		0.565860
  validation loss:		0.512033
  validation accuracy:		83.70 %
Epoch 885 of 2000 took 0.096s
  training loss:		0.565952
  validation loss:		0.501985
  validation accuracy:		83.48 %
Epoch 886 of 2000 took 0.097s
  training loss:		0.555010
  validation loss:		0.501307
  validation accuracy:		83.04 %
Epoch 887 of 2000 took 0.100s
  training loss:		0.564840
  validation loss:		0.509980
  validation accuracy:		82.93 %
Epoch 888 of 2000 took 0.096s
  training loss:		0.569563
  validation loss:		0.572407
  validation accuracy:		81.85 %
Epoch 889 of 2000 took 0.097s
  training loss:		0.567763
  validation loss:		0.497559
  validation accuracy:		83.15 %
Epoch 890 of 2000 took 0.096s
  training loss:		0.568159
  validation loss:		0.521772
  validation accuracy:		83.48 %
Epoch 891 of 2000 took 0.096s
  training loss:		0.562007
  validation loss:		0.523140
  validation accuracy:		82.61 %
Epoch 892 of 2000 took 0.097s
  training loss:		0.558549
  validation loss:		0.505315
  validation accuracy:		83.04 %
Epoch 893 of 2000 took 0.096s
  training loss:		0.546583
  validation loss:		0.519395
  validation accuracy:		82.50 %
Epoch 894 of 2000 took 0.097s
  training loss:		0.549580
  validation loss:		0.536993
  validation accuracy:		83.15 %
Epoch 895 of 2000 took 0.098s
  training loss:		0.552784
  validation loss:		0.519290
  validation accuracy:		83.26 %
Epoch 896 of 2000 took 0.097s
  training loss:		0.558302
  validation loss:		0.513173
  validation accuracy:		82.83 %
Epoch 897 of 2000 took 0.096s
  training loss:		0.555319
  validation loss:		0.523639
  validation accuracy:		83.04 %
Epoch 898 of 2000 took 0.096s
  training loss:		0.561048
  validation loss:		0.507295
  validation accuracy:		82.72 %
Epoch 899 of 2000 took 0.097s
  training loss:		0.558338
  validation loss:		0.525128
  validation accuracy:		83.26 %
Epoch 900 of 2000 took 0.096s
  training loss:		0.560088
  validation loss:		0.515954
  validation accuracy:		83.37 %
Epoch 901 of 2000 took 0.096s
  training loss:		0.553680
  validation loss:		0.521291
  validation accuracy:		83.37 %
Epoch 902 of 2000 took 0.096s
  training loss:		0.562561
  validation loss:		0.511832
  validation accuracy:		83.48 %
Epoch 903 of 2000 took 0.097s
  training loss:		0.560914
  validation loss:		0.507638
  validation accuracy:		82.93 %
Epoch 904 of 2000 took 0.097s
  training loss:		0.553177
  validation loss:		0.533046
  validation accuracy:		82.72 %
Epoch 905 of 2000 took 0.096s
  training loss:		0.564574
  validation loss:		0.506194
  validation accuracy:		83.26 %
Epoch 906 of 2000 took 0.096s
  training loss:		0.557866
  validation loss:		0.545550
  validation accuracy:		82.83 %
Epoch 907 of 2000 took 0.096s
  training loss:		0.567840
  validation loss:		0.522198
  validation accuracy:		83.37 %
Epoch 908 of 2000 took 0.096s
  training loss:		0.556171
  validation loss:		0.500981
  validation accuracy:		83.26 %
Epoch 909 of 2000 took 0.096s
  training loss:		0.559551
  validation loss:		0.510452
  validation accuracy:		83.26 %
Epoch 910 of 2000 took 0.098s
  training loss:		0.549509
  validation loss:		0.504741
  validation accuracy:		82.93 %
Epoch 911 of 2000 took 0.098s
  training loss:		0.552192
  validation loss:		0.505630
  validation accuracy:		82.61 %
Epoch 912 of 2000 took 0.096s
  training loss:		0.563582
  validation loss:		0.536814
  validation accuracy:		82.28 %
Epoch 913 of 2000 took 0.096s
  training loss:		0.571584
  validation loss:		0.511914
  validation accuracy:		83.59 %
Epoch 914 of 2000 took 0.095s
  training loss:		0.556116
  validation loss:		0.510933
  validation accuracy:		82.72 %
Epoch 915 of 2000 took 0.095s
  training loss:		0.538880
  validation loss:		0.502605
  validation accuracy:		83.26 %
Epoch 916 of 2000 took 0.095s
  training loss:		0.561477
  validation loss:		0.514927
  validation accuracy:		82.83 %
Epoch 917 of 2000 took 0.095s
  training loss:		0.568799
  validation loss:		0.531258
  validation accuracy:		83.04 %
Epoch 918 of 2000 took 0.095s
  training loss:		0.552555
  validation loss:		0.504810
  validation accuracy:		83.15 %
Epoch 919 of 2000 took 0.095s
  training loss:		0.556867
  validation loss:		0.529729
  validation accuracy:		83.37 %
Epoch 920 of 2000 took 0.095s
  training loss:		0.551892
  validation loss:		0.516996
  validation accuracy:		82.83 %
Epoch 921 of 2000 took 0.095s
  training loss:		0.550075
  validation loss:		0.502705
  validation accuracy:		82.39 %
Epoch 922 of 2000 took 0.095s
  training loss:		0.563749
  validation loss:		0.516342
  validation accuracy:		83.26 %
Epoch 923 of 2000 took 0.097s
  training loss:		0.558116
  validation loss:		0.514552
  validation accuracy:		83.04 %
Epoch 924 of 2000 took 0.096s
  training loss:		0.557892
  validation loss:		0.508979
  validation accuracy:		83.15 %
Epoch 925 of 2000 took 0.097s
  training loss:		0.548310
  validation loss:		0.509480
  validation accuracy:		83.26 %
Epoch 926 of 2000 took 0.103s
  training loss:		0.562615
  validation loss:		0.509332
  validation accuracy:		83.04 %
Epoch 927 of 2000 took 0.106s
  training loss:		0.557701
  validation loss:		0.504000
  validation accuracy:		83.04 %
Epoch 928 of 2000 took 0.101s
  training loss:		0.556562
  validation loss:		0.509905
  validation accuracy:		82.93 %
Epoch 929 of 2000 took 0.096s
  training loss:		0.556251
  validation loss:		0.512862
  validation accuracy:		83.59 %
Epoch 930 of 2000 took 0.096s
  training loss:		0.554340
  validation loss:		0.507582
  validation accuracy:		83.15 %
Epoch 931 of 2000 took 0.096s
  training loss:		0.556796
  validation loss:		0.519187
  validation accuracy:		83.26 %
Epoch 932 of 2000 took 0.097s
  training loss:		0.551331
  validation loss:		0.537781
  validation accuracy:		81.74 %
Epoch 933 of 2000 took 0.099s
  training loss:		0.562083
  validation loss:		0.535202
  validation accuracy:		82.83 %
Epoch 934 of 2000 took 0.099s
  training loss:		0.551638
  validation loss:		0.506764
  validation accuracy:		83.15 %
Epoch 935 of 2000 took 0.097s
  training loss:		0.544723
  validation loss:		0.504880
  validation accuracy:		83.26 %
Epoch 936 of 2000 took 0.095s
  training loss:		0.550877
  validation loss:		0.502161
  validation accuracy:		83.26 %
Epoch 937 of 2000 took 0.096s
  training loss:		0.553863
  validation loss:		0.500607
  validation accuracy:		83.26 %
Epoch 938 of 2000 took 0.097s
  training loss:		0.552863
  validation loss:		0.507106
  validation accuracy:		83.04 %
Epoch 939 of 2000 took 0.100s
  training loss:		0.551396
  validation loss:		0.512816
  validation accuracy:		83.70 %
Epoch 940 of 2000 took 0.096s
  training loss:		0.563768
  validation loss:		0.506344
  validation accuracy:		83.48 %
Epoch 941 of 2000 took 0.097s
  training loss:		0.560710
  validation loss:		0.530624
  validation accuracy:		83.04 %
Epoch 942 of 2000 took 0.096s
  training loss:		0.552129
  validation loss:		0.523734
  validation accuracy:		83.26 %
Epoch 943 of 2000 took 0.097s
  training loss:		0.549946
  validation loss:		0.511746
  validation accuracy:		83.59 %
Epoch 944 of 2000 took 0.098s
  training loss:		0.555913
  validation loss:		0.532458
  validation accuracy:		82.17 %
Epoch 945 of 2000 took 0.097s
  training loss:		0.556273
  validation loss:		0.533845
  validation accuracy:		83.04 %
Epoch 946 of 2000 took 0.096s
  training loss:		0.549944
  validation loss:		0.518459
  validation accuracy:		83.15 %
Epoch 947 of 2000 took 0.097s
  training loss:		0.555039
  validation loss:		0.504862
  validation accuracy:		83.15 %
Epoch 948 of 2000 took 0.096s
  training loss:		0.559922
  validation loss:		0.540957
  validation accuracy:		81.63 %
Epoch 949 of 2000 took 0.096s
  training loss:		0.561105
  validation loss:		0.505166
  validation accuracy:		82.93 %
Epoch 950 of 2000 took 0.096s
  training loss:		0.557977
  validation loss:		0.516363
  validation accuracy:		83.15 %
Epoch 951 of 2000 took 0.096s
  training loss:		0.547126
  validation loss:		0.518199
  validation accuracy:		83.37 %
Epoch 952 of 2000 took 0.096s
  training loss:		0.555190
  validation loss:		0.500827
  validation accuracy:		83.15 %
Epoch 953 of 2000 took 0.096s
  training loss:		0.552487
  validation loss:		0.539113
  validation accuracy:		82.72 %
Epoch 954 of 2000 took 0.096s
  training loss:		0.546472
  validation loss:		0.501297
  validation accuracy:		82.28 %
Epoch 955 of 2000 took 0.096s
  training loss:		0.565786
  validation loss:		0.502614
  validation accuracy:		83.04 %
Epoch 956 of 2000 took 0.098s
  training loss:		0.554886
  validation loss:		0.513803
  validation accuracy:		83.37 %
Epoch 957 of 2000 took 0.096s
  training loss:		0.553959
  validation loss:		0.509056
  validation accuracy:		82.72 %
Epoch 958 of 2000 took 0.096s
  training loss:		0.551118
  validation loss:		0.507022
  validation accuracy:		82.83 %
Epoch 959 of 2000 took 0.097s
  training loss:		0.561458
  validation loss:		0.521044
  validation accuracy:		82.61 %
Epoch 960 of 2000 took 0.096s
  training loss:		0.565154
  validation loss:		0.507132
  validation accuracy:		82.93 %
Epoch 961 of 2000 took 0.096s
  training loss:		0.548218
  validation loss:		0.515142
  validation accuracy:		82.72 %
Epoch 962 of 2000 took 0.096s
  training loss:		0.547849
  validation loss:		0.504563
  validation accuracy:		82.93 %
Epoch 963 of 2000 took 0.096s
  training loss:		0.561465
  validation loss:		0.502214
  validation accuracy:		82.93 %
Epoch 964 of 2000 took 0.096s
  training loss:		0.553175
  validation loss:		0.538138
  validation accuracy:		82.07 %
Epoch 965 of 2000 took 0.096s
  training loss:		0.549530
  validation loss:		0.506640
  validation accuracy:		83.37 %
Epoch 966 of 2000 took 0.100s
  training loss:		0.551922
  validation loss:		0.505313
  validation accuracy:		83.48 %
Epoch 967 of 2000 took 0.100s
  training loss:		0.542952
  validation loss:		0.514647
  validation accuracy:		83.15 %
Epoch 968 of 2000 took 0.099s
  training loss:		0.556667
  validation loss:		0.550530
  validation accuracy:		82.39 %
Epoch 969 of 2000 took 0.099s
  training loss:		0.579501
  validation loss:		0.523617
  validation accuracy:		82.72 %
Epoch 970 of 2000 took 0.099s
  training loss:		0.553622
  validation loss:		0.525884
  validation accuracy:		83.37 %
Epoch 971 of 2000 took 0.099s
  training loss:		0.554179
  validation loss:		0.501966
  validation accuracy:		83.26 %
Epoch 972 of 2000 took 0.099s
  training loss:		0.556047
  validation loss:		0.529677
  validation accuracy:		82.39 %
Epoch 973 of 2000 took 0.097s
  training loss:		0.553291
  validation loss:		0.515759
  validation accuracy:		83.04 %
Epoch 974 of 2000 took 0.096s
  training loss:		0.555009
  validation loss:		0.515790
  validation accuracy:		82.50 %
Epoch 975 of 2000 took 0.096s
  training loss:		0.553329
  validation loss:		0.507158
  validation accuracy:		83.26 %
Epoch 976 of 2000 took 0.097s
  training loss:		0.552908
  validation loss:		0.511906
  validation accuracy:		83.04 %
Epoch 977 of 2000 took 0.096s
  training loss:		0.561339
  validation loss:		0.512651
  validation accuracy:		83.48 %
Epoch 978 of 2000 took 0.096s
  training loss:		0.554362
  validation loss:		0.509654
  validation accuracy:		83.48 %
Epoch 979 of 2000 took 0.096s
  training loss:		0.562532
  validation loss:		0.519093
  validation accuracy:		82.07 %
Epoch 980 of 2000 took 0.096s
  training loss:		0.562856
  validation loss:		0.522159
  validation accuracy:		82.61 %
Epoch 981 of 2000 took 0.097s
  training loss:		0.556690
  validation loss:		0.505124
  validation accuracy:		83.04 %
Epoch 982 of 2000 took 0.096s
  training loss:		0.555044
  validation loss:		0.552581
  validation accuracy:		81.96 %
Epoch 983 of 2000 took 0.096s
  training loss:		0.546205
  validation loss:		0.511057
  validation accuracy:		83.59 %
Epoch 984 of 2000 took 0.096s
  training loss:		0.552791
  validation loss:		0.521787
  validation accuracy:		82.72 %
Epoch 985 of 2000 took 0.097s
  training loss:		0.545952
  validation loss:		0.512663
  validation accuracy:		83.59 %
Epoch 986 of 2000 took 0.096s
  training loss:		0.559180
  validation loss:		0.511488
  validation accuracy:		82.93 %
Epoch 987 of 2000 took 0.097s
  training loss:		0.549092
  validation loss:		0.520054
  validation accuracy:		82.72 %
Epoch 988 of 2000 took 0.096s
  training loss:		0.565557
  validation loss:		0.517566
  validation accuracy:		83.37 %
Epoch 989 of 2000 took 0.096s
  training loss:		0.549461
  validation loss:		0.514050
  validation accuracy:		82.83 %
Epoch 990 of 2000 took 0.096s
  training loss:		0.555062
  validation loss:		0.504584
  validation accuracy:		83.04 %
Epoch 991 of 2000 took 0.096s
  training loss:		0.548611
  validation loss:		0.519724
  validation accuracy:		83.48 %
Epoch 992 of 2000 took 0.097s
  training loss:		0.546279
  validation loss:		0.528092
  validation accuracy:		83.04 %
Epoch 993 of 2000 took 0.099s
  training loss:		0.550046
  validation loss:		0.515890
  validation accuracy:		82.83 %
Epoch 994 of 2000 took 0.098s
  training loss:		0.545936
  validation loss:		0.501384
  validation accuracy:		82.93 %
Epoch 995 of 2000 took 0.096s
  training loss:		0.561034
  validation loss:		0.511596
  validation accuracy:		83.26 %
Epoch 996 of 2000 took 0.096s
  training loss:		0.541251
  validation loss:		0.538195
  validation accuracy:		81.74 %
Epoch 997 of 2000 took 0.096s
  training loss:		0.559840
  validation loss:		0.507405
  validation accuracy:		83.37 %
Epoch 998 of 2000 took 0.096s
  training loss:		0.558395
  validation loss:		0.511899
  validation accuracy:		82.83 %
Epoch 999 of 2000 took 0.096s
  training loss:		0.568985
  validation loss:		0.504527
  validation accuracy:		82.83 %
Epoch 1000 of 2000 took 0.096s
  training loss:		0.555227
  validation loss:		0.514817
  validation accuracy:		83.15 %
Epoch 1001 of 2000 took 0.096s
  training loss:		0.554478
  validation loss:		0.517109
  validation accuracy:		82.07 %
Epoch 1002 of 2000 took 0.096s
  training loss:		0.542979
  validation loss:		0.513913
  validation accuracy:		83.04 %
Epoch 1003 of 2000 took 0.096s
  training loss:		0.542860
  validation loss:		0.509388
  validation accuracy:		82.61 %
Epoch 1004 of 2000 took 0.096s
  training loss:		0.547282
  validation loss:		0.513162
  validation accuracy:		83.48 %
Epoch 1005 of 2000 took 0.097s
  training loss:		0.552908
  validation loss:		0.521651
  validation accuracy:		82.93 %
Epoch 1006 of 2000 took 0.096s
  training loss:		0.564857
  validation loss:		0.511273
  validation accuracy:		83.15 %
Epoch 1007 of 2000 took 0.097s
  training loss:		0.547991
  validation loss:		0.502762
  validation accuracy:		83.26 %
Epoch 1008 of 2000 took 0.096s
  training loss:		0.553079
  validation loss:		0.510950
  validation accuracy:		83.37 %
Epoch 1009 of 2000 took 0.096s
  training loss:		0.544811
  validation loss:		0.512649
  validation accuracy:		83.04 %
Epoch 1010 of 2000 took 0.096s
  training loss:		0.556782
  validation loss:		0.503945
  validation accuracy:		83.26 %
Epoch 1011 of 2000 took 0.096s
  training loss:		0.546268
  validation loss:		0.515275
  validation accuracy:		83.04 %
Epoch 1012 of 2000 took 0.096s
  training loss:		0.553524
  validation loss:		0.512598
  validation accuracy:		82.50 %
Epoch 1013 of 2000 took 0.097s
  training loss:		0.549777
  validation loss:		0.507977
  validation accuracy:		82.93 %
Epoch 1014 of 2000 took 0.096s
  training loss:		0.549777
  validation loss:		0.503598
  validation accuracy:		83.26 %
Epoch 1015 of 2000 took 0.096s
  training loss:		0.566267
  validation loss:		0.555004
  validation accuracy:		82.61 %
Epoch 1016 of 2000 took 0.097s
  training loss:		0.583182
  validation loss:		0.508709
  validation accuracy:		82.93 %
Epoch 1017 of 2000 took 0.097s
  training loss:		0.545987
  validation loss:		0.521398
  validation accuracy:		83.04 %
Epoch 1018 of 2000 took 0.097s
  training loss:		0.561524
  validation loss:		0.504240
  validation accuracy:		83.04 %
Epoch 1019 of 2000 took 0.097s
  training loss:		0.549069
  validation loss:		0.565136
  validation accuracy:		82.07 %
Epoch 1020 of 2000 took 0.097s
  training loss:		0.551727
  validation loss:		0.515694
  validation accuracy:		82.39 %
Epoch 1021 of 2000 took 0.096s
  training loss:		0.549280
  validation loss:		0.535806
  validation accuracy:		82.72 %
Epoch 1022 of 2000 took 0.097s
  training loss:		0.553362
  validation loss:		0.502596
  validation accuracy:		83.48 %
Epoch 1023 of 2000 took 0.097s
  training loss:		0.552889
  validation loss:		0.513171
  validation accuracy:		83.04 %
Epoch 1024 of 2000 took 0.097s
  training loss:		0.550138
  validation loss:		0.518170
  validation accuracy:		83.70 %
Epoch 1025 of 2000 took 0.096s
  training loss:		0.543814
  validation loss:		0.524120
  validation accuracy:		83.15 %
Epoch 1026 of 2000 took 0.097s
  training loss:		0.543312
  validation loss:		0.506398
  validation accuracy:		83.15 %
Epoch 1027 of 2000 took 0.096s
  training loss:		0.548779
  validation loss:		0.507899
  validation accuracy:		83.04 %
Epoch 1028 of 2000 took 0.096s
  training loss:		0.549419
  validation loss:		0.517352
  validation accuracy:		82.93 %
Epoch 1029 of 2000 took 0.096s
  training loss:		0.549302
  validation loss:		0.505838
  validation accuracy:		83.15 %
Epoch 1030 of 2000 took 0.096s
  training loss:		0.550078
  validation loss:		0.513376
  validation accuracy:		82.93 %
Epoch 1031 of 2000 took 0.096s
  training loss:		0.550756
  validation loss:		0.514943
  validation accuracy:		83.48 %
Epoch 1032 of 2000 took 0.096s
  training loss:		0.556907
  validation loss:		0.525444
  validation accuracy:		82.61 %
Epoch 1033 of 2000 took 0.096s
  training loss:		0.548184
  validation loss:		0.508598
  validation accuracy:		83.26 %
Epoch 1034 of 2000 took 0.096s
  training loss:		0.544229
  validation loss:		0.506678
  validation accuracy:		83.15 %
Epoch 1035 of 2000 took 0.096s
  training loss:		0.552234
  validation loss:		0.512145
  validation accuracy:		83.04 %
Epoch 1036 of 2000 took 0.096s
  training loss:		0.545502
  validation loss:		0.508031
  validation accuracy:		83.26 %
Epoch 1037 of 2000 took 0.096s
  training loss:		0.553096
  validation loss:		0.521454
  validation accuracy:		82.72 %
Epoch 1038 of 2000 took 0.099s
  training loss:		0.548435
  validation loss:		0.517125
  validation accuracy:		83.15 %
Epoch 1039 of 2000 took 0.096s
  training loss:		0.542576
  validation loss:		0.519599
  validation accuracy:		83.70 %
Epoch 1040 of 2000 took 0.096s
  training loss:		0.542438
  validation loss:		0.512947
  validation accuracy:		83.15 %
Epoch 1041 of 2000 took 0.096s
  training loss:		0.546336
  validation loss:		0.501947
  validation accuracy:		83.48 %
Epoch 1042 of 2000 took 0.096s
  training loss:		0.539424
  validation loss:		0.517407
  validation accuracy:		83.48 %
Epoch 1043 of 2000 took 0.096s
  training loss:		0.546038
  validation loss:		0.515657
  validation accuracy:		83.37 %
Epoch 1044 of 2000 took 0.098s
  training loss:		0.556816
  validation loss:		0.562589
  validation accuracy:		82.39 %
Epoch 1045 of 2000 took 0.096s
  training loss:		0.552927
  validation loss:		0.517820
  validation accuracy:		83.48 %
Epoch 1046 of 2000 took 0.096s
  training loss:		0.552645
  validation loss:		0.526298
  validation accuracy:		82.72 %
Epoch 1047 of 2000 took 0.097s
  training loss:		0.558857
  validation loss:		0.538807
  validation accuracy:		82.07 %
Epoch 1048 of 2000 took 0.096s
  training loss:		0.549159
  validation loss:		0.521327
  validation accuracy:		82.72 %
Epoch 1049 of 2000 took 0.097s
  training loss:		0.541349
  validation loss:		0.503884
  validation accuracy:		83.15 %
Epoch 1050 of 2000 took 0.097s
  training loss:		0.551742
  validation loss:		0.500928
  validation accuracy:		83.26 %
Epoch 1051 of 2000 took 0.097s
  training loss:		0.546706
  validation loss:		0.525833
  validation accuracy:		82.93 %
Epoch 1052 of 2000 took 0.096s
  training loss:		0.548217
  validation loss:		0.524153
  validation accuracy:		82.50 %
Epoch 1053 of 2000 took 0.096s
  training loss:		0.550615
  validation loss:		0.503803
  validation accuracy:		83.04 %
Epoch 1054 of 2000 took 0.096s
  training loss:		0.545784
  validation loss:		0.510600
  validation accuracy:		82.93 %
Epoch 1055 of 2000 took 0.096s
  training loss:		0.545986
  validation loss:		0.506377
  validation accuracy:		83.15 %
Epoch 1056 of 2000 took 0.096s
  training loss:		0.549757
  validation loss:		0.510139
  validation accuracy:		83.26 %
Epoch 1057 of 2000 took 0.096s
  training loss:		0.547316
  validation loss:		0.513286
  validation accuracy:		83.80 %
Epoch 1058 of 2000 took 0.096s
  training loss:		0.548420
  validation loss:		0.531914
  validation accuracy:		83.04 %
Epoch 1059 of 2000 took 0.097s
  training loss:		0.552530
  validation loss:		0.509834
  validation accuracy:		83.04 %
Epoch 1060 of 2000 took 0.096s
  training loss:		0.549810
  validation loss:		0.506848
  validation accuracy:		83.37 %
Epoch 1061 of 2000 took 0.096s
  training loss:		0.548101
  validation loss:		0.508994
  validation accuracy:		83.26 %
Epoch 1062 of 2000 took 0.097s
  training loss:		0.543745
  validation loss:		0.507342
  validation accuracy:		83.48 %
Epoch 1063 of 2000 took 0.096s
  training loss:		0.545668
  validation loss:		0.503851
  validation accuracy:		83.15 %
Epoch 1064 of 2000 took 0.097s
  training loss:		0.544794
  validation loss:		0.532136
  validation accuracy:		82.83 %
Epoch 1065 of 2000 took 0.096s
  training loss:		0.554896
  validation loss:		0.518997
  validation accuracy:		82.07 %
Epoch 1066 of 2000 took 0.096s
  training loss:		0.546713
  validation loss:		0.513812
  validation accuracy:		83.04 %
Epoch 1067 of 2000 took 0.096s
  training loss:		0.551820
  validation loss:		0.512570
  validation accuracy:		83.26 %
Epoch 1068 of 2000 took 0.097s
  training loss:		0.550099
  validation loss:		0.531896
  validation accuracy:		82.83 %
Epoch 1069 of 2000 took 0.096s
  training loss:		0.550310
  validation loss:		0.508998
  validation accuracy:		83.15 %
Epoch 1070 of 2000 took 0.096s
  training loss:		0.547916
  validation loss:		0.512486
  validation accuracy:		83.59 %
Epoch 1071 of 2000 took 0.096s
  training loss:		0.561028
  validation loss:		0.509586
  validation accuracy:		83.37 %
Epoch 1072 of 2000 took 0.097s
  training loss:		0.551389
  validation loss:		0.528124
  validation accuracy:		82.72 %
Epoch 1073 of 2000 took 0.096s
  training loss:		0.544921
  validation loss:		0.513970
  validation accuracy:		83.48 %
Epoch 1074 of 2000 took 0.096s
  training loss:		0.549291
  validation loss:		0.520893
  validation accuracy:		82.28 %
Epoch 1075 of 2000 took 0.096s
  training loss:		0.529896
  validation loss:		0.506195
  validation accuracy:		83.15 %
Epoch 1076 of 2000 took 0.096s
  training loss:		0.551856
  validation loss:		0.508774
  validation accuracy:		83.15 %
Epoch 1077 of 2000 took 0.096s
  training loss:		0.535866
  validation loss:		0.503328
  validation accuracy:		83.37 %
Epoch 1078 of 2000 took 0.096s
  training loss:		0.551926
  validation loss:		0.525915
  validation accuracy:		82.83 %
Epoch 1079 of 2000 took 0.096s
  training loss:		0.548184
  validation loss:		0.524229
  validation accuracy:		82.93 %
Epoch 1080 of 2000 took 0.096s
  training loss:		0.542569
  validation loss:		0.528216
  validation accuracy:		82.83 %
Epoch 1081 of 2000 took 0.097s
  training loss:		0.547141
  validation loss:		0.520896
  validation accuracy:		82.83 %
Epoch 1082 of 2000 took 0.097s
  training loss:		0.540821
  validation loss:		0.506492
  validation accuracy:		83.37 %
Epoch 1083 of 2000 took 0.096s
  training loss:		0.548794
  validation loss:		0.503343
  validation accuracy:		83.48 %
Epoch 1084 of 2000 took 0.097s
  training loss:		0.538161
  validation loss:		0.529331
  validation accuracy:		82.83 %
Epoch 1085 of 2000 took 0.096s
  training loss:		0.549390
  validation loss:		0.505552
  validation accuracy:		83.70 %
Epoch 1086 of 2000 took 0.096s
  training loss:		0.543249
  validation loss:		0.531017
  validation accuracy:		82.28 %
Epoch 1087 of 2000 took 0.096s
  training loss:		0.547592
  validation loss:		0.507281
  validation accuracy:		82.93 %
Epoch 1088 of 2000 took 0.096s
  training loss:		0.541663
  validation loss:		0.514047
  validation accuracy:		82.83 %
Epoch 1089 of 2000 took 0.096s
  training loss:		0.551011
  validation loss:		0.531727
  validation accuracy:		82.72 %
Epoch 1090 of 2000 took 0.096s
  training loss:		0.551284
  validation loss:		0.521553
  validation accuracy:		82.93 %
Epoch 1091 of 2000 took 0.096s
  training loss:		0.561446
  validation loss:		0.519739
  validation accuracy:		83.37 %
Epoch 1092 of 2000 took 0.096s
  training loss:		0.548487
  validation loss:		0.503787
  validation accuracy:		83.37 %
Epoch 1093 of 2000 took 0.096s
  training loss:		0.551108
  validation loss:		0.546379
  validation accuracy:		82.72 %
Epoch 1094 of 2000 took 0.096s
  training loss:		0.540758
  validation loss:		0.516338
  validation accuracy:		82.93 %
Epoch 1095 of 2000 took 0.096s
  training loss:		0.551406
  validation loss:		0.515199
  validation accuracy:		83.37 %
Epoch 1096 of 2000 took 0.096s
  training loss:		0.536057
  validation loss:		0.511488
  validation accuracy:		82.93 %
Epoch 1097 of 2000 took 0.096s
  training loss:		0.543744
  validation loss:		0.523383
  validation accuracy:		82.83 %
Epoch 1098 of 2000 took 0.096s
  training loss:		0.551516
  validation loss:		0.522253
  validation accuracy:		83.15 %
Epoch 1099 of 2000 took 0.096s
  training loss:		0.555032
  validation loss:		0.508548
  validation accuracy:		83.15 %
Epoch 1100 of 2000 took 0.096s
  training loss:		0.542109
  validation loss:		0.519039
  validation accuracy:		83.26 %
Epoch 1101 of 2000 took 0.096s
  training loss:		0.539736
  validation loss:		0.516750
  validation accuracy:		82.72 %
Epoch 1102 of 2000 took 0.096s
  training loss:		0.541461
  validation loss:		0.525848
  validation accuracy:		82.28 %
Epoch 1103 of 2000 took 0.097s
  training loss:		0.539567
  validation loss:		0.533601
  validation accuracy:		82.28 %
Epoch 1104 of 2000 took 0.096s
  training loss:		0.541612
  validation loss:		0.505630
  validation accuracy:		82.93 %
Epoch 1105 of 2000 took 0.096s
  training loss:		0.538250
  validation loss:		0.513832
  validation accuracy:		83.26 %
Epoch 1106 of 2000 took 0.096s
  training loss:		0.542145
  validation loss:		0.546611
  validation accuracy:		82.39 %
Epoch 1107 of 2000 took 0.096s
  training loss:		0.548592
  validation loss:		0.502444
  validation accuracy:		83.15 %
Epoch 1108 of 2000 took 0.096s
  training loss:		0.564612
  validation loss:		0.516663
  validation accuracy:		82.72 %
Epoch 1109 of 2000 took 0.097s
  training loss:		0.541146
  validation loss:		0.537730
  validation accuracy:		82.39 %
Epoch 1110 of 2000 took 0.096s
  training loss:		0.544451
  validation loss:		0.510777
  validation accuracy:		83.15 %
Epoch 1111 of 2000 took 0.096s
  training loss:		0.540482
  validation loss:		0.512538
  validation accuracy:		83.15 %
Epoch 1112 of 2000 took 0.097s
  training loss:		0.554291
  validation loss:		0.525459
  validation accuracy:		82.83 %
Epoch 1113 of 2000 took 0.096s
  training loss:		0.556322
  validation loss:		0.532558
  validation accuracy:		82.17 %
Epoch 1114 of 2000 took 0.096s
  training loss:		0.543298
  validation loss:		0.523968
  validation accuracy:		83.15 %
Epoch 1115 of 2000 took 0.096s
  training loss:		0.554144
  validation loss:		0.514881
  validation accuracy:		83.15 %
Epoch 1116 of 2000 took 0.096s
  training loss:		0.548707
  validation loss:		0.520464
  validation accuracy:		82.83 %
Epoch 1117 of 2000 took 0.096s
  training loss:		0.544146
  validation loss:		0.506738
  validation accuracy:		83.15 %
Epoch 1118 of 2000 took 0.096s
  training loss:		0.556885
  validation loss:		0.526504
  validation accuracy:		82.39 %
Epoch 1119 of 2000 took 0.096s
  training loss:		0.559167
  validation loss:		0.510369
  validation accuracy:		83.48 %
Epoch 1120 of 2000 took 0.096s
  training loss:		0.540786
  validation loss:		0.516829
  validation accuracy:		82.93 %
Epoch 1121 of 2000 took 0.096s
  training loss:		0.539656
  validation loss:		0.510116
  validation accuracy:		82.93 %
Epoch 1122 of 2000 took 0.096s
  training loss:		0.545752
  validation loss:		0.530726
  validation accuracy:		82.50 %
Epoch 1123 of 2000 took 0.097s
  training loss:		0.534256
  validation loss:		0.525682
  validation accuracy:		82.17 %
Epoch 1124 of 2000 took 0.096s
  training loss:		0.539108
  validation loss:		0.514724
  validation accuracy:		83.04 %
Epoch 1125 of 2000 took 0.096s
  training loss:		0.545221
  validation loss:		0.535009
  validation accuracy:		82.07 %
Epoch 1126 of 2000 took 0.096s
  training loss:		0.548713
  validation loss:		0.571093
  validation accuracy:		82.07 %
Epoch 1127 of 2000 took 0.096s
  training loss:		0.552103
  validation loss:		0.520204
  validation accuracy:		82.39 %
Epoch 1128 of 2000 took 0.096s
  training loss:		0.539316
  validation loss:		0.508324
  validation accuracy:		82.83 %
Epoch 1129 of 2000 took 0.096s
  training loss:		0.543788
  validation loss:		0.526357
  validation accuracy:		82.28 %
Epoch 1130 of 2000 took 0.096s
  training loss:		0.550932
  validation loss:		0.528962
  validation accuracy:		82.28 %
Epoch 1131 of 2000 took 0.097s
  training loss:		0.550442
  validation loss:		0.539225
  validation accuracy:		82.61 %
Epoch 1132 of 2000 took 0.103s
  training loss:		0.545469
  validation loss:		0.507923
  validation accuracy:		83.37 %
Epoch 1133 of 2000 took 0.103s
  training loss:		0.548072
  validation loss:		0.509823
  validation accuracy:		83.04 %
Epoch 1134 of 2000 took 0.103s
  training loss:		0.546008
  validation loss:		0.537038
  validation accuracy:		81.85 %
Epoch 1135 of 2000 took 0.103s
  training loss:		0.543391
  validation loss:		0.508962
  validation accuracy:		83.26 %
Epoch 1136 of 2000 took 0.103s
  training loss:		0.543241
  validation loss:		0.510453
  validation accuracy:		83.26 %
Epoch 1137 of 2000 took 0.103s
  training loss:		0.541632
  validation loss:		0.508955
  validation accuracy:		83.04 %
Epoch 1138 of 2000 took 0.103s
  training loss:		0.548203
  validation loss:		0.504843
  validation accuracy:		83.15 %
Epoch 1139 of 2000 took 0.102s
  training loss:		0.535422
  validation loss:		0.522591
  validation accuracy:		82.83 %
Epoch 1140 of 2000 took 0.099s
  training loss:		0.542827
  validation loss:		0.533602
  validation accuracy:		82.17 %
Epoch 1141 of 2000 took 0.099s
  training loss:		0.549017
  validation loss:		0.515992
  validation accuracy:		82.93 %
Epoch 1142 of 2000 took 0.100s
  training loss:		0.543066
  validation loss:		0.513064
  validation accuracy:		82.83 %
Epoch 1143 of 2000 took 0.098s
  training loss:		0.541117
  validation loss:		0.518271
  validation accuracy:		83.04 %
Epoch 1144 of 2000 took 0.097s
  training loss:		0.546081
  validation loss:		0.508102
  validation accuracy:		83.04 %
Epoch 1145 of 2000 took 0.096s
  training loss:		0.544957
  validation loss:		0.555025
  validation accuracy:		80.87 %
Epoch 1146 of 2000 took 0.097s
  training loss:		0.547458
  validation loss:		0.514199
  validation accuracy:		82.50 %
Epoch 1147 of 2000 took 0.096s
  training loss:		0.536656
  validation loss:		0.512486
  validation accuracy:		82.93 %
Epoch 1148 of 2000 took 0.096s
  training loss:		0.534984
  validation loss:		0.514502
  validation accuracy:		82.61 %
Epoch 1149 of 2000 took 0.096s
  training loss:		0.553531
  validation loss:		0.520320
  validation accuracy:		83.04 %
Epoch 1150 of 2000 took 0.097s
  training loss:		0.548838
  validation loss:		0.514980
  validation accuracy:		82.72 %
Epoch 1151 of 2000 took 0.096s
  training loss:		0.534097
  validation loss:		0.538538
  validation accuracy:		82.50 %
Epoch 1152 of 2000 took 0.096s
  training loss:		0.545989
  validation loss:		0.518637
  validation accuracy:		82.93 %
Epoch 1153 of 2000 took 0.096s
  training loss:		0.548429
  validation loss:		0.513588
  validation accuracy:		82.72 %
Epoch 1154 of 2000 took 0.096s
  training loss:		0.541716
  validation loss:		0.511592
  validation accuracy:		83.04 %
Epoch 1155 of 2000 took 0.096s
  training loss:		0.538573
  validation loss:		0.506915
  validation accuracy:		83.04 %
Epoch 1156 of 2000 took 0.096s
  training loss:		0.542546
  validation loss:		0.517416
  validation accuracy:		82.61 %
Epoch 1157 of 2000 took 0.096s
  training loss:		0.544111
  validation loss:		0.523562
  validation accuracy:		82.28 %
Epoch 1158 of 2000 took 0.096s
  training loss:		0.545745
  validation loss:		0.543861
  validation accuracy:		81.41 %
Epoch 1159 of 2000 took 0.096s
  training loss:		0.545915
  validation loss:		0.526511
  validation accuracy:		83.26 %
Epoch 1160 of 2000 took 0.096s
  training loss:		0.546262
  validation loss:		0.522340
  validation accuracy:		82.07 %
Epoch 1161 of 2000 took 0.096s
  training loss:		0.564083
  validation loss:		0.512535
  validation accuracy:		83.37 %
Epoch 1162 of 2000 took 0.097s
  training loss:		0.542990
  validation loss:		0.514756
  validation accuracy:		83.26 %
Epoch 1163 of 2000 took 0.096s
  training loss:		0.540412
  validation loss:		0.504535
  validation accuracy:		83.15 %
Epoch 1164 of 2000 took 0.097s
  training loss:		0.541984
  validation loss:		0.515988
  validation accuracy:		83.26 %
Epoch 1165 of 2000 took 0.097s
  training loss:		0.544127
  validation loss:		0.513829
  validation accuracy:		82.61 %
Epoch 1166 of 2000 took 0.096s
  training loss:		0.534968
  validation loss:		0.517356
  validation accuracy:		83.15 %
Epoch 1167 of 2000 took 0.096s
  training loss:		0.539446
  validation loss:		0.514024
  validation accuracy:		82.93 %
Epoch 1168 of 2000 took 0.096s
  training loss:		0.544238
  validation loss:		0.531657
  validation accuracy:		83.04 %
Epoch 1169 of 2000 took 0.096s
  training loss:		0.544307
  validation loss:		0.520569
  validation accuracy:		82.50 %
Epoch 1170 of 2000 took 0.096s
  training loss:		0.542834
  validation loss:		0.544553
  validation accuracy:		81.74 %
Epoch 1171 of 2000 took 0.096s
  training loss:		0.541920
  validation loss:		0.525375
  validation accuracy:		81.96 %
Epoch 1172 of 2000 took 0.096s
  training loss:		0.544503
  validation loss:		0.511167
  validation accuracy:		83.26 %
Epoch 1173 of 2000 took 0.096s
  training loss:		0.529374
  validation loss:		0.509481
  validation accuracy:		82.72 %
Epoch 1174 of 2000 took 0.097s
  training loss:		0.542801
  validation loss:		0.523728
  validation accuracy:		82.39 %
Epoch 1175 of 2000 took 0.096s
  training loss:		0.538891
  validation loss:		0.508766
  validation accuracy:		83.37 %
Epoch 1176 of 2000 took 0.096s
  training loss:		0.544789
  validation loss:		0.510693
  validation accuracy:		83.59 %
Epoch 1177 of 2000 took 0.096s
  training loss:		0.542168
  validation loss:		0.527445
  validation accuracy:		82.72 %
Epoch 1178 of 2000 took 0.096s
  training loss:		0.545992
  validation loss:		0.507451
  validation accuracy:		83.26 %
Epoch 1179 of 2000 took 0.096s
  training loss:		0.534574
  validation loss:		0.512819
  validation accuracy:		82.72 %
Epoch 1180 of 2000 took 0.096s
  training loss:		0.539361
  validation loss:		0.520018
  validation accuracy:		82.39 %
Epoch 1181 of 2000 took 0.096s
  training loss:		0.538494
  validation loss:		0.507757
  validation accuracy:		82.93 %
Epoch 1182 of 2000 took 0.096s
  training loss:		0.552005
  validation loss:		0.513222
  validation accuracy:		82.93 %
Epoch 1183 of 2000 took 0.097s
  training loss:		0.538892
  validation loss:		0.530014
  validation accuracy:		82.39 %
Epoch 1184 of 2000 took 0.096s
  training loss:		0.549450
  validation loss:		0.513368
  validation accuracy:		83.15 %
Epoch 1185 of 2000 took 0.096s
  training loss:		0.541074
  validation loss:		0.509385
  validation accuracy:		82.93 %
Epoch 1186 of 2000 took 0.096s
  training loss:		0.531814
  validation loss:		0.516854
  validation accuracy:		82.93 %
Epoch 1187 of 2000 took 0.096s
  training loss:		0.545617
  validation loss:		0.508331
  validation accuracy:		82.72 %
Epoch 1188 of 2000 took 0.097s
  training loss:		0.539912
  validation loss:		0.514057
  validation accuracy:		83.26 %
Epoch 1189 of 2000 took 0.096s
  training loss:		0.540543
  validation loss:		0.578208
  validation accuracy:		80.43 %
Epoch 1190 of 2000 took 0.096s
  training loss:		0.557400
  validation loss:		0.530657
  validation accuracy:		81.52 %
Epoch 1191 of 2000 took 0.097s
  training loss:		0.545047
  validation loss:		0.518980
  validation accuracy:		82.93 %
Epoch 1192 of 2000 took 0.096s
  training loss:		0.533323
  validation loss:		0.514782
  validation accuracy:		83.26 %
Epoch 1193 of 2000 took 0.096s
  training loss:		0.542223
  validation loss:		0.518152
  validation accuracy:		82.28 %
Epoch 1194 of 2000 took 0.098s
  training loss:		0.539158
  validation loss:		0.534555
  validation accuracy:		82.50 %
Epoch 1195 of 2000 took 0.098s
  training loss:		0.546655
  validation loss:		0.515652
  validation accuracy:		82.50 %
Epoch 1196 of 2000 took 0.096s
  training loss:		0.549729
  validation loss:		0.513245
  validation accuracy:		82.93 %
Epoch 1197 of 2000 took 0.096s
  training loss:		0.541407
  validation loss:		0.536421
  validation accuracy:		82.72 %
Epoch 1198 of 2000 took 0.096s
  training loss:		0.532964
  validation loss:		0.509571
  validation accuracy:		83.15 %
Epoch 1199 of 2000 took 0.097s
  training loss:		0.538109
  validation loss:		0.520600
  validation accuracy:		82.93 %
Epoch 1200 of 2000 took 0.096s
  training loss:		0.544644
  validation loss:		0.527080
  validation accuracy:		82.28 %
Epoch 1201 of 2000 took 0.096s
  training loss:		0.532543
  validation loss:		0.503270
  validation accuracy:		82.93 %
Epoch 1202 of 2000 took 0.096s
  training loss:		0.537629
  validation loss:		0.544794
  validation accuracy:		81.85 %
Epoch 1203 of 2000 took 0.096s
  training loss:		0.545489
  validation loss:		0.521628
  validation accuracy:		82.28 %
Epoch 1204 of 2000 took 0.096s
  training loss:		0.543371
  validation loss:		0.518899
  validation accuracy:		83.04 %
Epoch 1205 of 2000 took 0.097s
  training loss:		0.538401
  validation loss:		0.518924
  validation accuracy:		82.83 %
Epoch 1206 of 2000 took 0.096s
  training loss:		0.536752
  validation loss:		0.544230
  validation accuracy:		81.74 %
Epoch 1207 of 2000 took 0.096s
  training loss:		0.537911
  validation loss:		0.515329
  validation accuracy:		82.83 %
Epoch 1208 of 2000 took 0.096s
  training loss:		0.540729
  validation loss:		0.517188
  validation accuracy:		82.93 %
Epoch 1209 of 2000 took 0.096s
  training loss:		0.539705
  validation loss:		0.528656
  validation accuracy:		82.50 %
Epoch 1210 of 2000 took 0.096s
  training loss:		0.547318
  validation loss:		0.522363
  validation accuracy:		82.39 %
Epoch 1211 of 2000 took 0.096s
  training loss:		0.549654
  validation loss:		0.504321
  validation accuracy:		83.04 %
Epoch 1212 of 2000 took 0.096s
  training loss:		0.538603
  validation loss:		0.560538
  validation accuracy:		80.54 %
Epoch 1213 of 2000 took 0.096s
  training loss:		0.547015
  validation loss:		0.506454
  validation accuracy:		83.15 %
Epoch 1214 of 2000 took 0.096s
  training loss:		0.547184
  validation loss:		0.533194
  validation accuracy:		81.85 %
Epoch 1215 of 2000 took 0.096s
  training loss:		0.538840
  validation loss:		0.511824
  validation accuracy:		82.93 %
Epoch 1216 of 2000 took 0.097s
  training loss:		0.547673
  validation loss:		0.514550
  validation accuracy:		83.04 %
Epoch 1217 of 2000 took 0.096s
  training loss:		0.544776
  validation loss:		0.511814
  validation accuracy:		83.48 %
Epoch 1218 of 2000 took 0.096s
  training loss:		0.536496
  validation loss:		0.529864
  validation accuracy:		82.61 %
Epoch 1219 of 2000 took 0.096s
  training loss:		0.536354
  validation loss:		0.503984
  validation accuracy:		83.59 %
Epoch 1220 of 2000 took 0.096s
  training loss:		0.538495
  validation loss:		0.508888
  validation accuracy:		83.26 %
Epoch 1221 of 2000 took 0.096s
  training loss:		0.536729
  validation loss:		0.505422
  validation accuracy:		83.26 %
Epoch 1222 of 2000 took 0.097s
  training loss:		0.546622
  validation loss:		0.517446
  validation accuracy:		82.50 %
Epoch 1223 of 2000 took 0.096s
  training loss:		0.543540
  validation loss:		0.508677
  validation accuracy:		83.59 %
Epoch 1224 of 2000 took 0.096s
  training loss:		0.549825
  validation loss:		0.529916
  validation accuracy:		81.96 %
Epoch 1225 of 2000 took 0.096s
  training loss:		0.538599
  validation loss:		0.523292
  validation accuracy:		82.17 %
Epoch 1226 of 2000 took 0.096s
  training loss:		0.539890
  validation loss:		0.515268
  validation accuracy:		82.17 %
Epoch 1227 of 2000 took 0.096s
  training loss:		0.546938
  validation loss:		0.509830
  validation accuracy:		82.72 %
Epoch 1228 of 2000 took 0.096s
  training loss:		0.541325
  validation loss:		0.508153
  validation accuracy:		83.15 %
Epoch 1229 of 2000 took 0.097s
  training loss:		0.537092
  validation loss:		0.511231
  validation accuracy:		82.61 %
Epoch 1230 of 2000 took 0.096s
  training loss:		0.544290
  validation loss:		0.509372
  validation accuracy:		83.04 %
Epoch 1231 of 2000 took 0.096s
  training loss:		0.542947
  validation loss:		0.516161
  validation accuracy:		83.37 %
Epoch 1232 of 2000 took 0.096s
  training loss:		0.537078
  validation loss:		0.515579
  validation accuracy:		82.39 %
Epoch 1233 of 2000 took 0.099s
  training loss:		0.541716
  validation loss:		0.513778
  validation accuracy:		82.39 %
Epoch 1234 of 2000 took 0.099s
  training loss:		0.539674
  validation loss:		0.513197
  validation accuracy:		82.61 %
Epoch 1235 of 2000 took 0.099s
  training loss:		0.534773
  validation loss:		0.516863
  validation accuracy:		83.04 %
Epoch 1236 of 2000 took 0.100s
  training loss:		0.544146
  validation loss:		0.517700
  validation accuracy:		83.04 %
Epoch 1237 of 2000 took 0.100s
  training loss:		0.549993
  validation loss:		0.513750
  validation accuracy:		82.93 %
Epoch 1238 of 2000 took 0.099s
  training loss:		0.535678
  validation loss:		0.514333
  validation accuracy:		82.83 %
Epoch 1239 of 2000 took 0.100s
  training loss:		0.530472
  validation loss:		0.510477
  validation accuracy:		82.93 %
Epoch 1240 of 2000 took 0.099s
  training loss:		0.535503
  validation loss:		0.512874
  validation accuracy:		82.83 %
Epoch 1241 of 2000 took 0.100s
  training loss:		0.532297
  validation loss:		0.526490
  validation accuracy:		82.72 %
Epoch 1242 of 2000 took 0.099s
  training loss:		0.536416
  validation loss:		0.511377
  validation accuracy:		82.72 %
Epoch 1243 of 2000 took 0.099s
  training loss:		0.541713
  validation loss:		0.508863
  validation accuracy:		82.93 %
Epoch 1244 of 2000 took 0.099s
  training loss:		0.541392
  validation loss:		0.507564
  validation accuracy:		82.28 %
Epoch 1245 of 2000 took 0.100s
  training loss:		0.542095
  validation loss:		0.543261
  validation accuracy:		81.30 %
Epoch 1246 of 2000 took 0.099s
  training loss:		0.542800
  validation loss:		0.516683
  validation accuracy:		82.83 %
Epoch 1247 of 2000 took 0.099s
  training loss:		0.534985
  validation loss:		0.515397
  validation accuracy:		82.50 %
Epoch 1248 of 2000 took 0.099s
  training loss:		0.542449
  validation loss:		0.510615
  validation accuracy:		82.72 %
Epoch 1249 of 2000 took 0.099s
  training loss:		0.534687
  validation loss:		0.508021
  validation accuracy:		82.17 %
Epoch 1250 of 2000 took 0.099s
  training loss:		0.539412
  validation loss:		0.522709
  validation accuracy:		82.07 %
Epoch 1251 of 2000 took 0.099s
  training loss:		0.542256
  validation loss:		0.533764
  validation accuracy:		81.85 %
Epoch 1252 of 2000 took 0.099s
  training loss:		0.531344
  validation loss:		0.512247
  validation accuracy:		82.39 %
Epoch 1253 of 2000 took 0.100s
  training loss:		0.534371
  validation loss:		0.515963
  validation accuracy:		83.04 %
Epoch 1254 of 2000 took 0.099s
  training loss:		0.538168
  validation loss:		0.525502
  validation accuracy:		82.50 %
Epoch 1255 of 2000 took 0.100s
  training loss:		0.546012
  validation loss:		0.510284
  validation accuracy:		82.39 %
Epoch 1256 of 2000 took 0.099s
  training loss:		0.546357
  validation loss:		0.529162
  validation accuracy:		82.50 %
Epoch 1257 of 2000 took 0.099s
  training loss:		0.543029
  validation loss:		0.510524
  validation accuracy:		82.93 %
Epoch 1258 of 2000 took 0.099s
  training loss:		0.545903
  validation loss:		0.533283
  validation accuracy:		81.74 %
Epoch 1259 of 2000 took 0.099s
  training loss:		0.543114
  validation loss:		0.520197
  validation accuracy:		81.96 %
Epoch 1260 of 2000 took 0.099s
  training loss:		0.540765
  validation loss:		0.516807
  validation accuracy:		82.72 %
Epoch 1261 of 2000 took 0.099s
  training loss:		0.540455
  validation loss:		0.536057
  validation accuracy:		82.50 %
Epoch 1262 of 2000 took 0.099s
  training loss:		0.543580
  validation loss:		0.510445
  validation accuracy:		82.61 %
Epoch 1263 of 2000 took 0.099s
  training loss:		0.524865
  validation loss:		0.547043
  validation accuracy:		81.09 %
Epoch 1264 of 2000 took 0.099s
  training loss:		0.551711
  validation loss:		0.517711
  validation accuracy:		82.39 %
Epoch 1265 of 2000 took 0.100s
  training loss:		0.535492
  validation loss:		0.510584
  validation accuracy:		82.83 %
Epoch 1266 of 2000 took 0.099s
  training loss:		0.539983
  validation loss:		0.503362
  validation accuracy:		82.61 %
Epoch 1267 of 2000 took 0.101s
  training loss:		0.541495
  validation loss:		0.525683
  validation accuracy:		82.28 %
Epoch 1268 of 2000 took 0.100s
  training loss:		0.538938
  validation loss:		0.546617
  validation accuracy:		82.50 %
Epoch 1269 of 2000 took 0.100s
  training loss:		0.551671
  validation loss:		0.521270
  validation accuracy:		82.17 %
Epoch 1270 of 2000 took 0.100s
  training loss:		0.538726
  validation loss:		0.512394
  validation accuracy:		82.50 %
Epoch 1271 of 2000 took 0.099s
  training loss:		0.539029
  validation loss:		0.526003
  validation accuracy:		82.61 %
Epoch 1272 of 2000 took 0.099s
  training loss:		0.537552
  validation loss:		0.517457
  validation accuracy:		82.61 %
Epoch 1273 of 2000 took 0.099s
  training loss:		0.544690
  validation loss:		0.532404
  validation accuracy:		82.28 %
Epoch 1274 of 2000 took 0.097s
  training loss:		0.535844
  validation loss:		0.519600
  validation accuracy:		82.93 %
Epoch 1275 of 2000 took 0.098s
  training loss:		0.539044
  validation loss:		0.511162
  validation accuracy:		82.28 %
Epoch 1276 of 2000 took 0.099s
  training loss:		0.526248
  validation loss:		0.512177
  validation accuracy:		82.28 %
Epoch 1277 of 2000 took 0.099s
  training loss:		0.541690
  validation loss:		0.511363
  validation accuracy:		83.04 %
Epoch 1278 of 2000 took 0.096s
  training loss:		0.539028
  validation loss:		0.510198
  validation accuracy:		82.72 %
Epoch 1279 of 2000 took 0.096s
  training loss:		0.543821
  validation loss:		0.535965
  validation accuracy:		82.39 %
Epoch 1280 of 2000 took 0.096s
  training loss:		0.548078
  validation loss:		0.520209
  validation accuracy:		82.50 %
Epoch 1281 of 2000 took 0.097s
  training loss:		0.534150
  validation loss:		0.509916
  validation accuracy:		82.83 %
Epoch 1282 of 2000 took 0.096s
  training loss:		0.537879
  validation loss:		0.516347
  validation accuracy:		82.93 %
Epoch 1283 of 2000 took 0.096s
  training loss:		0.540123
  validation loss:		0.509230
  validation accuracy:		82.61 %
Epoch 1284 of 2000 took 0.096s
  training loss:		0.542343
  validation loss:		0.521892
  validation accuracy:		82.83 %
Epoch 1285 of 2000 took 0.096s
  training loss:		0.539082
  validation loss:		0.513321
  validation accuracy:		82.72 %
Epoch 1286 of 2000 took 0.096s
  training loss:		0.535009
  validation loss:		0.522002
  validation accuracy:		82.83 %
Epoch 1287 of 2000 took 0.096s
  training loss:		0.536361
  validation loss:		0.521389
  validation accuracy:		82.72 %
Epoch 1288 of 2000 took 0.096s
  training loss:		0.531834
  validation loss:		0.532119
  validation accuracy:		82.39 %
Epoch 1289 of 2000 took 0.096s
  training loss:		0.545102
  validation loss:		0.509540
  validation accuracy:		82.07 %
Epoch 1290 of 2000 took 0.096s
  training loss:		0.543761
  validation loss:		0.510019
  validation accuracy:		83.04 %
Epoch 1291 of 2000 took 0.096s
  training loss:		0.532848
  validation loss:		0.508748
  validation accuracy:		82.93 %
Epoch 1292 of 2000 took 0.096s
  training loss:		0.540965
  validation loss:		0.511067
  validation accuracy:		82.28 %
Epoch 1293 of 2000 took 0.096s
  training loss:		0.533666
  validation loss:		0.527331
  validation accuracy:		82.17 %
Epoch 1294 of 2000 took 0.096s
  training loss:		0.534901
  validation loss:		0.517768
  validation accuracy:		82.61 %
Epoch 1295 of 2000 took 0.096s
  training loss:		0.535490
  validation loss:		0.514561
  validation accuracy:		82.83 %
Epoch 1296 of 2000 took 0.096s
  training loss:		0.541773
  validation loss:		0.515851
  validation accuracy:		82.83 %
Epoch 1297 of 2000 took 0.096s
  training loss:		0.535335
  validation loss:		0.523778
  validation accuracy:		82.61 %
Epoch 1298 of 2000 took 0.097s
  training loss:		0.530938
  validation loss:		0.534903
  validation accuracy:		82.07 %
Epoch 1299 of 2000 took 0.096s
  training loss:		0.542068
  validation loss:		0.521780
  validation accuracy:		82.39 %
Epoch 1300 of 2000 took 0.096s
  training loss:		0.535153
  validation loss:		0.510929
  validation accuracy:		82.17 %
Epoch 1301 of 2000 took 0.096s
  training loss:		0.546005
  validation loss:		0.514572
  validation accuracy:		82.72 %
Epoch 1302 of 2000 took 0.097s
  training loss:		0.534142
  validation loss:		0.512266
  validation accuracy:		82.61 %
Epoch 1303 of 2000 took 0.096s
  training loss:		0.535906
  validation loss:		0.513328
  validation accuracy:		82.17 %
Epoch 1304 of 2000 took 0.097s
  training loss:		0.548957
  validation loss:		0.533590
  validation accuracy:		82.39 %
Epoch 1305 of 2000 took 0.096s
  training loss:		0.538901
  validation loss:		0.519909
  validation accuracy:		82.72 %
Epoch 1306 of 2000 took 0.096s
  training loss:		0.541618
  validation loss:		0.509649
  validation accuracy:		82.72 %
Epoch 1307 of 2000 took 0.096s
  training loss:		0.534428
  validation loss:		0.511601
  validation accuracy:		82.72 %
Epoch 1308 of 2000 took 0.096s
  training loss:		0.536823
  validation loss:		0.528171
  validation accuracy:		82.93 %
Epoch 1309 of 2000 took 0.096s
  training loss:		0.539211
  validation loss:		0.526144
  validation accuracy:		82.39 %
Epoch 1310 of 2000 took 0.096s
  training loss:		0.534773
  validation loss:		0.534180
  validation accuracy:		82.07 %
Epoch 1311 of 2000 took 0.097s
  training loss:		0.548692
  validation loss:		0.517758
  validation accuracy:		83.04 %
Epoch 1312 of 2000 took 0.096s
  training loss:		0.539532
  validation loss:		0.525901
  validation accuracy:		82.28 %
Epoch 1313 of 2000 took 0.096s
  training loss:		0.541769
  validation loss:		0.521556
  validation accuracy:		82.50 %
Epoch 1314 of 2000 took 0.096s
  training loss:		0.529871
  validation loss:		0.514150
  validation accuracy:		82.50 %
Epoch 1315 of 2000 took 0.097s
  training loss:		0.541481
  validation loss:		0.509779
  validation accuracy:		82.72 %
Epoch 1316 of 2000 took 0.096s
  training loss:		0.540801
  validation loss:		0.528634
  validation accuracy:		82.83 %
Epoch 1317 of 2000 took 0.096s
  training loss:		0.535617
  validation loss:		0.528493
  validation accuracy:		82.93 %
Epoch 1318 of 2000 took 0.096s
  training loss:		0.535305
  validation loss:		0.535482
  validation accuracy:		82.17 %
Epoch 1319 of 2000 took 0.096s
  training loss:		0.534026
  validation loss:		0.512848
  validation accuracy:		82.72 %
Epoch 1320 of 2000 took 0.096s
  training loss:		0.540491
  validation loss:		0.509122
  validation accuracy:		82.61 %
Epoch 1321 of 2000 took 0.096s
  training loss:		0.537014
  validation loss:		0.510070
  validation accuracy:		82.28 %
Epoch 1322 of 2000 took 0.096s
  training loss:		0.528293
  validation loss:		0.518539
  validation accuracy:		82.61 %
Epoch 1323 of 2000 took 0.096s
  training loss:		0.538612
  validation loss:		0.509723
  validation accuracy:		82.50 %
Epoch 1324 of 2000 took 0.096s
  training loss:		0.544357
  validation loss:		0.539577
  validation accuracy:		81.74 %
Epoch 1325 of 2000 took 0.096s
  training loss:		0.543197
  validation loss:		0.510635
  validation accuracy:		82.17 %
Epoch 1326 of 2000 took 0.096s
  training loss:		0.532484
  validation loss:		0.515111
  validation accuracy:		82.72 %
Epoch 1327 of 2000 took 0.096s
  training loss:		0.541691
  validation loss:		0.542214
  validation accuracy:		81.30 %
Epoch 1328 of 2000 took 0.096s
  training loss:		0.543236
  validation loss:		0.540394
  validation accuracy:		81.74 %
Epoch 1329 of 2000 took 0.097s
  training loss:		0.553733
  validation loss:		0.528095
  validation accuracy:		82.72 %
Epoch 1330 of 2000 took 0.096s
  training loss:		0.537005
  validation loss:		0.546965
  validation accuracy:		81.85 %
Epoch 1331 of 2000 took 0.096s
  training loss:		0.541100
  validation loss:		0.548763
  validation accuracy:		83.15 %
Epoch 1332 of 2000 took 0.096s
  training loss:		0.538126
  validation loss:		0.520719
  validation accuracy:		82.93 %
Epoch 1333 of 2000 took 0.096s
  training loss:		0.534546
  validation loss:		0.531467
  validation accuracy:		82.17 %
Epoch 1334 of 2000 took 0.096s
  training loss:		0.533208
  validation loss:		0.513483
  validation accuracy:		82.50 %
Epoch 1335 of 2000 took 0.096s
  training loss:		0.538875
  validation loss:		0.522436
  validation accuracy:		82.93 %
Epoch 1336 of 2000 took 0.096s
  training loss:		0.539515
  validation loss:		0.529019
  validation accuracy:		82.28 %
Epoch 1337 of 2000 took 0.096s
  training loss:		0.543798
  validation loss:		0.523312
  validation accuracy:		82.61 %
Epoch 1338 of 2000 took 0.096s
  training loss:		0.535129
  validation loss:		0.515444
  validation accuracy:		82.83 %
Epoch 1339 of 2000 took 0.096s
  training loss:		0.534757
  validation loss:		0.529292
  validation accuracy:		82.39 %
Epoch 1340 of 2000 took 0.096s
  training loss:		0.543743
  validation loss:		0.519145
  validation accuracy:		82.72 %
Epoch 1341 of 2000 took 0.096s
  training loss:		0.542858
  validation loss:		0.512954
  validation accuracy:		82.83 %
Epoch 1342 of 2000 took 0.096s
  training loss:		0.536414
  validation loss:		0.521234
  validation accuracy:		82.83 %
Epoch 1343 of 2000 took 0.097s
  training loss:		0.537734
  validation loss:		0.515320
  validation accuracy:		83.04 %
Epoch 1344 of 2000 took 0.096s
  training loss:		0.527749
  validation loss:		0.522730
  validation accuracy:		82.83 %
Epoch 1345 of 2000 took 0.096s
  training loss:		0.544561
  validation loss:		0.510223
  validation accuracy:		82.83 %
Epoch 1346 of 2000 took 0.096s
  training loss:		0.524590
  validation loss:		0.521100
  validation accuracy:		82.83 %
Epoch 1347 of 2000 took 0.096s
  training loss:		0.535557
  validation loss:		0.516462
  validation accuracy:		82.07 %
Epoch 1348 of 2000 took 0.097s
  training loss:		0.534445
  validation loss:		0.513055
  validation accuracy:		82.72 %
Epoch 1349 of 2000 took 0.096s
  training loss:		0.536535
  validation loss:		0.521977
  validation accuracy:		81.96 %
Epoch 1350 of 2000 took 0.096s
  training loss:		0.544909
  validation loss:		0.508510
  validation accuracy:		82.39 %
Epoch 1351 of 2000 took 0.096s
  training loss:		0.538008
  validation loss:		0.509290
  validation accuracy:		82.83 %
Epoch 1352 of 2000 took 0.097s
  training loss:		0.534781
  validation loss:		0.510700
  validation accuracy:		82.83 %
Epoch 1353 of 2000 took 0.097s
  training loss:		0.538435
  validation loss:		0.518536
  validation accuracy:		82.28 %
Epoch 1354 of 2000 took 0.096s
  training loss:		0.541823
  validation loss:		0.510146
  validation accuracy:		82.83 %
Epoch 1355 of 2000 took 0.096s
  training loss:		0.541613
  validation loss:		0.533177
  validation accuracy:		83.04 %
Epoch 1356 of 2000 took 0.097s
  training loss:		0.553650
  validation loss:		0.519554
  validation accuracy:		82.93 %
Epoch 1357 of 2000 took 0.097s
  training loss:		0.528991
  validation loss:		0.509978
  validation accuracy:		82.50 %
Epoch 1358 of 2000 took 0.096s
  training loss:		0.534067
  validation loss:		0.521337
  validation accuracy:		82.93 %
Epoch 1359 of 2000 took 0.096s
  training loss:		0.538206
  validation loss:		0.518611
  validation accuracy:		83.15 %
Epoch 1360 of 2000 took 0.097s
  training loss:		0.531076
  validation loss:		0.510761
  validation accuracy:		82.83 %
Epoch 1361 of 2000 took 0.096s
  training loss:		0.539200
  validation loss:		0.534336
  validation accuracy:		82.61 %
Epoch 1362 of 2000 took 0.096s
  training loss:		0.539563
  validation loss:		0.514328
  validation accuracy:		82.28 %
Epoch 1363 of 2000 took 0.096s
  training loss:		0.537294
  validation loss:		0.535149
  validation accuracy:		82.93 %
Epoch 1364 of 2000 took 0.097s
  training loss:		0.532318
  validation loss:		0.522131
  validation accuracy:		82.61 %
Epoch 1365 of 2000 took 0.097s
  training loss:		0.531924
  validation loss:		0.512757
  validation accuracy:		82.72 %
Epoch 1366 of 2000 took 0.106s
  training loss:		0.543354
  validation loss:		0.514779
  validation accuracy:		82.50 %
Epoch 1367 of 2000 took 0.104s
  training loss:		0.531839
  validation loss:		0.520543
  validation accuracy:		82.83 %
Epoch 1368 of 2000 took 0.138s
  training loss:		0.533528
  validation loss:		0.522566
  validation accuracy:		82.83 %
Epoch 1369 of 2000 took 0.111s
  training loss:		0.532837
  validation loss:		0.509115
  validation accuracy:		82.72 %
Epoch 1370 of 2000 took 0.103s
  training loss:		0.539522
  validation loss:		0.511806
  validation accuracy:		83.15 %
Epoch 1371 of 2000 took 0.104s
  training loss:		0.540688
  validation loss:		0.520263
  validation accuracy:		82.72 %
Epoch 1372 of 2000 took 0.098s
  training loss:		0.547073
  validation loss:		0.539306
  validation accuracy:		81.52 %
Epoch 1373 of 2000 took 0.103s
  training loss:		0.549631
  validation loss:		0.525901
  validation accuracy:		82.83 %
Epoch 1374 of 2000 took 0.104s
  training loss:		0.533638
  validation loss:		0.520556
  validation accuracy:		82.93 %
Epoch 1375 of 2000 took 0.103s
  training loss:		0.530524
  validation loss:		0.534130
  validation accuracy:		83.04 %
Epoch 1376 of 2000 took 0.100s
  training loss:		0.539843
  validation loss:		0.519442
  validation accuracy:		82.39 %
Epoch 1377 of 2000 took 0.101s
  training loss:		0.528472
  validation loss:		0.515009
  validation accuracy:		82.93 %
Epoch 1378 of 2000 took 0.100s
  training loss:		0.535862
  validation loss:		0.521002
  validation accuracy:		82.93 %
Epoch 1379 of 2000 took 0.100s
  training loss:		0.534988
  validation loss:		0.521641
  validation accuracy:		83.04 %
Epoch 1380 of 2000 took 0.100s
  training loss:		0.540816
  validation loss:		0.546466
  validation accuracy:		82.50 %
Epoch 1381 of 2000 took 0.100s
  training loss:		0.537650
  validation loss:		0.524683
  validation accuracy:		83.04 %
Epoch 1382 of 2000 took 0.100s
  training loss:		0.531675
  validation loss:		0.516482
  validation accuracy:		82.72 %
Epoch 1383 of 2000 took 0.101s
  training loss:		0.557324
  validation loss:		0.527377
  validation accuracy:		83.37 %
Epoch 1384 of 2000 took 0.100s
  training loss:		0.542756
  validation loss:		0.528664
  validation accuracy:		82.72 %
Epoch 1385 of 2000 took 0.100s
  training loss:		0.532328
  validation loss:		0.510451
  validation accuracy:		81.74 %
Epoch 1386 of 2000 took 0.100s
  training loss:		0.537901
  validation loss:		0.514186
  validation accuracy:		82.61 %
Epoch 1387 of 2000 took 0.101s
  training loss:		0.531587
  validation loss:		0.508815
  validation accuracy:		82.50 %
Epoch 1388 of 2000 took 0.101s
  training loss:		0.539568
  validation loss:		0.525480
  validation accuracy:		82.17 %
Epoch 1389 of 2000 took 0.100s
  training loss:		0.534100
  validation loss:		0.512795
  validation accuracy:		82.83 %
Epoch 1390 of 2000 took 0.101s
  training loss:		0.535931
  validation loss:		0.537583
  validation accuracy:		83.48 %
Epoch 1391 of 2000 took 0.100s
  training loss:		0.538615
  validation loss:		0.518497
  validation accuracy:		82.93 %
Epoch 1392 of 2000 took 0.101s
  training loss:		0.529818
  validation loss:		0.529437
  validation accuracy:		82.72 %
Epoch 1393 of 2000 took 0.100s
  training loss:		0.527235
  validation loss:		0.529162
  validation accuracy:		82.72 %
Epoch 1394 of 2000 took 0.100s
  training loss:		0.543440
  validation loss:		0.532608
  validation accuracy:		83.04 %
Epoch 1395 of 2000 took 0.101s
  training loss:		0.538607
  validation loss:		0.511965
  validation accuracy:		82.83 %
Epoch 1396 of 2000 took 0.100s
  training loss:		0.532726
  validation loss:		0.531442
  validation accuracy:		82.39 %
Epoch 1397 of 2000 took 0.101s
  training loss:		0.529696
  validation loss:		0.518122
  validation accuracy:		82.28 %
Epoch 1398 of 2000 took 0.101s
  training loss:		0.532108
  validation loss:		0.525967
  validation accuracy:		82.61 %
Epoch 1399 of 2000 took 0.100s
  training loss:		0.534080
  validation loss:		0.515360
  validation accuracy:		82.28 %
Epoch 1400 of 2000 took 0.100s
  training loss:		0.526007
  validation loss:		0.517492
  validation accuracy:		83.04 %
Epoch 1401 of 2000 took 0.100s
  training loss:		0.533961
  validation loss:		0.535205
  validation accuracy:		81.74 %
Epoch 1402 of 2000 took 0.101s
  training loss:		0.536578
  validation loss:		0.530041
  validation accuracy:		82.39 %
Epoch 1403 of 2000 took 0.100s
  training loss:		0.541517
  validation loss:		0.517947
  validation accuracy:		82.83 %
Epoch 1404 of 2000 took 0.100s
  training loss:		0.540399
  validation loss:		0.540986
  validation accuracy:		82.17 %
Epoch 1405 of 2000 took 0.101s
  training loss:		0.528588
  validation loss:		0.509680
  validation accuracy:		83.04 %
Epoch 1406 of 2000 took 0.100s
  training loss:		0.537838
  validation loss:		0.525882
  validation accuracy:		82.83 %
Epoch 1407 of 2000 took 0.102s
  training loss:		0.536187
  validation loss:		0.512237
  validation accuracy:		82.83 %
Epoch 1408 of 2000 took 0.106s
  training loss:		0.532442
  validation loss:		0.520862
  validation accuracy:		82.72 %
Epoch 1409 of 2000 took 0.165s
  training loss:		0.531486
  validation loss:		0.530614
  validation accuracy:		82.50 %
Epoch 1410 of 2000 took 0.165s
  training loss:		0.539059
  validation loss:		0.528062
  validation accuracy:		82.72 %
Epoch 1411 of 2000 took 0.165s
  training loss:		0.538517
  validation loss:		0.523857
  validation accuracy:		82.83 %
Epoch 1412 of 2000 took 0.165s
  training loss:		0.540464
  validation loss:		0.531998
  validation accuracy:		82.39 %
Epoch 1413 of 2000 took 0.165s
  training loss:		0.530425
  validation loss:		0.528306
  validation accuracy:		82.72 %
Epoch 1414 of 2000 took 0.165s
  training loss:		0.536844
  validation loss:		0.524062
  validation accuracy:		82.93 %
Epoch 1415 of 2000 took 0.165s
  training loss:		0.536552
  validation loss:		0.539920
  validation accuracy:		81.52 %
Epoch 1416 of 2000 took 0.165s
  training loss:		0.537480
  validation loss:		0.515828
  validation accuracy:		82.39 %
Epoch 1417 of 2000 took 0.165s
  training loss:		0.542766
  validation loss:		0.539057
  validation accuracy:		81.63 %
Epoch 1418 of 2000 took 0.165s
  training loss:		0.535939
  validation loss:		0.522045
  validation accuracy:		82.28 %
Epoch 1419 of 2000 took 0.165s
  training loss:		0.528964
  validation loss:		0.542187
  validation accuracy:		82.50 %
Epoch 1420 of 2000 took 0.165s
  training loss:		0.539237
  validation loss:		0.516061
  validation accuracy:		82.61 %
Epoch 1421 of 2000 took 0.101s
  training loss:		0.535978
  validation loss:		0.522206
  validation accuracy:		82.72 %
Epoch 1422 of 2000 took 0.100s
  training loss:		0.538289
  validation loss:		0.515932
  validation accuracy:		83.04 %
Epoch 1423 of 2000 took 0.100s
  training loss:		0.533779
  validation loss:		0.516378
  validation accuracy:		82.39 %
Epoch 1424 of 2000 took 0.104s
  training loss:		0.536753
  validation loss:		0.527147
  validation accuracy:		83.04 %
Epoch 1425 of 2000 took 0.110s
  training loss:		0.535326
  validation loss:		0.524912
  validation accuracy:		83.04 %
Epoch 1426 of 2000 took 0.101s
  training loss:		0.547930
  validation loss:		0.528150
  validation accuracy:		81.41 %
Epoch 1427 of 2000 took 0.100s
  training loss:		0.533650
  validation loss:		0.535340
  validation accuracy:		81.63 %
Epoch 1428 of 2000 took 0.100s
  training loss:		0.528804
  validation loss:		0.512341
  validation accuracy:		82.07 %
Epoch 1429 of 2000 took 0.116s
  training loss:		0.539887
  validation loss:		0.527653
  validation accuracy:		82.72 %
Epoch 1430 of 2000 took 0.107s
  training loss:		0.538622
  validation loss:		0.519905
  validation accuracy:		82.93 %
Epoch 1431 of 2000 took 0.096s
  training loss:		0.536307
  validation loss:		0.526486
  validation accuracy:		83.04 %
Epoch 1432 of 2000 took 0.096s
  training loss:		0.535614
  validation loss:		0.529215
  validation accuracy:		82.61 %
Epoch 1433 of 2000 took 0.096s
  training loss:		0.539976
  validation loss:		0.510507
  validation accuracy:		82.28 %
Epoch 1434 of 2000 took 0.096s
  training loss:		0.536647
  validation loss:		0.519989
  validation accuracy:		82.61 %
Epoch 1435 of 2000 took 0.096s
  training loss:		0.539181
  validation loss:		0.518350
  validation accuracy:		82.61 %
Epoch 1436 of 2000 took 0.098s
  training loss:		0.534719
  validation loss:		0.539609
  validation accuracy:		82.93 %
Epoch 1437 of 2000 took 0.096s
  training loss:		0.529866
  validation loss:		0.540631
  validation accuracy:		82.17 %
Epoch 1438 of 2000 took 0.096s
  training loss:		0.539162
  validation loss:		0.532423
  validation accuracy:		82.93 %
Epoch 1439 of 2000 took 0.098s
  training loss:		0.542148
  validation loss:		0.525786
  validation accuracy:		82.61 %
Epoch 1440 of 2000 took 0.096s
  training loss:		0.526345
  validation loss:		0.517604
  validation accuracy:		82.50 %
Epoch 1441 of 2000 took 0.096s
  training loss:		0.523472
  validation loss:		0.523451
  validation accuracy:		82.39 %
Epoch 1442 of 2000 took 0.097s
  training loss:		0.536137
  validation loss:		0.513659
  validation accuracy:		81.74 %
Epoch 1443 of 2000 took 0.096s
  training loss:		0.542509
  validation loss:		0.532036
  validation accuracy:		82.72 %
Epoch 1444 of 2000 took 0.096s
  training loss:		0.541653
  validation loss:		0.522060
  validation accuracy:		82.61 %
Epoch 1445 of 2000 took 0.096s
  training loss:		0.538050
  validation loss:		0.515281
  validation accuracy:		82.50 %
Epoch 1446 of 2000 took 0.096s
  training loss:		0.542450
  validation loss:		0.538441
  validation accuracy:		82.28 %
Epoch 1447 of 2000 took 0.096s
  training loss:		0.534605
  validation loss:		0.521563
  validation accuracy:		83.04 %
Epoch 1448 of 2000 took 0.096s
  training loss:		0.535746
  validation loss:		0.528737
  validation accuracy:		82.93 %
Epoch 1449 of 2000 took 0.096s
  training loss:		0.540716
  validation loss:		0.526475
  validation accuracy:		82.61 %
Epoch 1450 of 2000 took 0.096s
  training loss:		0.537734
  validation loss:		0.513783
  validation accuracy:		82.39 %
Epoch 1451 of 2000 took 0.096s
  training loss:		0.532815
  validation loss:		0.524365
  validation accuracy:		82.28 %
Epoch 1452 of 2000 took 0.096s
  training loss:		0.534883
  validation loss:		0.515808
  validation accuracy:		82.39 %
Epoch 1453 of 2000 took 0.096s
  training loss:		0.537567
  validation loss:		0.518822
  validation accuracy:		82.93 %
Epoch 1454 of 2000 took 0.096s
  training loss:		0.536512
  validation loss:		0.514781
  validation accuracy:		82.39 %
Epoch 1455 of 2000 took 0.096s
  training loss:		0.532771
  validation loss:		0.513450
  validation accuracy:		82.50 %
Epoch 1456 of 2000 took 0.096s
  training loss:		0.538411
  validation loss:		0.528511
  validation accuracy:		81.41 %
Epoch 1457 of 2000 took 0.096s
  training loss:		0.543492
  validation loss:		0.523915
  validation accuracy:		83.26 %
Epoch 1458 of 2000 took 0.096s
  training loss:		0.533947
  validation loss:		0.519753
  validation accuracy:		82.39 %
Epoch 1459 of 2000 took 0.096s
  training loss:		0.533773
  validation loss:		0.520661
  validation accuracy:		82.61 %
Epoch 1460 of 2000 took 0.097s
  training loss:		0.536221
  validation loss:		0.529515
  validation accuracy:		82.50 %
Epoch 1461 of 2000 took 0.096s
  training loss:		0.533684
  validation loss:		0.541802
  validation accuracy:		82.39 %
Epoch 1462 of 2000 took 0.096s
  training loss:		0.534822
  validation loss:		0.524424
  validation accuracy:		82.50 %
Epoch 1463 of 2000 took 0.097s
  training loss:		0.535079
  validation loss:		0.524778
  validation accuracy:		82.07 %
Epoch 1464 of 2000 took 0.096s
  training loss:		0.534039
  validation loss:		0.514004
  validation accuracy:		82.17 %
Epoch 1465 of 2000 took 0.097s
  training loss:		0.537495
  validation loss:		0.525762
  validation accuracy:		82.61 %
Epoch 1466 of 2000 took 0.097s
  training loss:		0.539755
  validation loss:		0.521485
  validation accuracy:		82.07 %
Epoch 1467 of 2000 took 0.096s
  training loss:		0.531057
  validation loss:		0.508863
  validation accuracy:		82.28 %
Epoch 1468 of 2000 took 0.097s
  training loss:		0.535926
  validation loss:		0.539319
  validation accuracy:		82.39 %
Epoch 1469 of 2000 took 0.097s
  training loss:		0.536785
  validation loss:		0.526763
  validation accuracy:		82.28 %
Epoch 1470 of 2000 took 0.097s
  training loss:		0.528882
  validation loss:		0.513699
  validation accuracy:		82.39 %
Epoch 1471 of 2000 took 0.096s
  training loss:		0.534909
  validation loss:		0.529179
  validation accuracy:		82.50 %
Epoch 1472 of 2000 took 0.096s
  training loss:		0.532185
  validation loss:		0.512914
  validation accuracy:		83.15 %
Epoch 1473 of 2000 took 0.097s
  training loss:		0.523101
  validation loss:		0.536412
  validation accuracy:		82.17 %
Epoch 1474 of 2000 took 0.096s
  training loss:		0.535778
  validation loss:		0.526826
  validation accuracy:		82.72 %
Epoch 1475 of 2000 took 0.097s
  training loss:		0.539898
  validation loss:		0.521021
  validation accuracy:		82.61 %
Epoch 1476 of 2000 took 0.096s
  training loss:		0.534534
  validation loss:		0.519303
  validation accuracy:		82.39 %
Epoch 1477 of 2000 took 0.096s
  training loss:		0.528554
  validation loss:		0.526809
  validation accuracy:		82.61 %
Epoch 1478 of 2000 took 0.096s
  training loss:		0.537884
  validation loss:		0.525307
  validation accuracy:		82.72 %
Epoch 1479 of 2000 took 0.096s
  training loss:		0.537797
  validation loss:		0.528905
  validation accuracy:		83.04 %
Epoch 1480 of 2000 took 0.096s
  training loss:		0.536078
  validation loss:		0.526079
  validation accuracy:		82.17 %
Epoch 1481 of 2000 took 0.097s
  training loss:		0.537940
  validation loss:		0.515285
  validation accuracy:		82.83 %
Epoch 1482 of 2000 took 0.096s
  training loss:		0.534310
  validation loss:		0.521759
  validation accuracy:		82.83 %
Epoch 1483 of 2000 took 0.096s
  training loss:		0.542506
  validation loss:		0.520012
  validation accuracy:		82.50 %
Epoch 1484 of 2000 took 0.096s
  training loss:		0.530427
  validation loss:		0.513203
  validation accuracy:		82.17 %
Epoch 1485 of 2000 took 0.096s
  training loss:		0.535569
  validation loss:		0.535853
  validation accuracy:		82.93 %
Epoch 1486 of 2000 took 0.096s
  training loss:		0.541048
  validation loss:		0.519399
  validation accuracy:		82.07 %
Epoch 1487 of 2000 took 0.097s
  training loss:		0.529041
  validation loss:		0.545716
  validation accuracy:		82.61 %
Epoch 1488 of 2000 took 0.096s
  training loss:		0.532580
  validation loss:		0.536804
  validation accuracy:		82.17 %
Epoch 1489 of 2000 took 0.096s
  training loss:		0.543226
  validation loss:		0.518512
  validation accuracy:		82.17 %
Epoch 1490 of 2000 took 0.096s
  training loss:		0.536643
  validation loss:		0.524839
  validation accuracy:		82.50 %
Epoch 1491 of 2000 took 0.096s
  training loss:		0.531310
  validation loss:		0.509667
  validation accuracy:		82.28 %
Epoch 1492 of 2000 took 0.096s
  training loss:		0.537372
  validation loss:		0.522685
  validation accuracy:		82.39 %
Epoch 1493 of 2000 took 0.096s
  training loss:		0.535019
  validation loss:		0.539360
  validation accuracy:		82.07 %
Epoch 1494 of 2000 took 0.096s
  training loss:		0.530405
  validation loss:		0.518601
  validation accuracy:		82.28 %
Epoch 1495 of 2000 took 0.096s
  training loss:		0.530437
  validation loss:		0.523204
  validation accuracy:		82.39 %
Epoch 1496 of 2000 took 0.096s
  training loss:		0.529062
  validation loss:		0.516695
  validation accuracy:		82.39 %
Epoch 1497 of 2000 took 0.097s
  training loss:		0.534386
  validation loss:		0.523726
  validation accuracy:		82.39 %
Epoch 1498 of 2000 took 0.096s
  training loss:		0.541946
  validation loss:		0.516356
  validation accuracy:		82.72 %
Epoch 1499 of 2000 took 0.096s
  training loss:		0.536466
  validation loss:		0.519160
  validation accuracy:		82.07 %
Epoch 1500 of 2000 took 0.096s
  training loss:		0.535369
  validation loss:		0.514360
  validation accuracy:		82.39 %
Epoch 1501 of 2000 took 0.096s
  training loss:		0.532648
  validation loss:		0.519282
  validation accuracy:		82.93 %
Epoch 1502 of 2000 took 0.096s
  training loss:		0.536800
  validation loss:		0.521253
  validation accuracy:		82.50 %
Epoch 1503 of 2000 took 0.096s
  training loss:		0.532920
  validation loss:		0.538445
  validation accuracy:		82.83 %
Epoch 1504 of 2000 took 0.097s
  training loss:		0.532922
  validation loss:		0.520131
  validation accuracy:		81.74 %
Epoch 1505 of 2000 took 0.097s
  training loss:		0.530165
  validation loss:		0.529339
  validation accuracy:		82.61 %
Epoch 1506 of 2000 took 0.097s
  training loss:		0.527490
  validation loss:		0.522841
  validation accuracy:		82.61 %
Epoch 1507 of 2000 took 0.096s
  training loss:		0.533702
  validation loss:		0.517199
  validation accuracy:		82.50 %
Epoch 1508 of 2000 took 0.096s
  training loss:		0.544973
  validation loss:		0.516359
  validation accuracy:		82.83 %
Epoch 1509 of 2000 took 0.096s
  training loss:		0.528385
  validation loss:		0.527821
  validation accuracy:		81.52 %
Epoch 1510 of 2000 took 0.097s
  training loss:		0.543099
  validation loss:		0.525551
  validation accuracy:		81.96 %
Epoch 1511 of 2000 took 0.096s
  training loss:		0.533389
  validation loss:		0.516285
  validation accuracy:		82.61 %
Epoch 1512 of 2000 took 0.096s
  training loss:		0.536579
  validation loss:		0.533379
  validation accuracy:		82.17 %
Epoch 1513 of 2000 took 0.096s
  training loss:		0.531037
  validation loss:		0.512464
  validation accuracy:		82.07 %
Epoch 1514 of 2000 took 0.096s
  training loss:		0.537055
  validation loss:		0.554827
  validation accuracy:		82.72 %
Epoch 1515 of 2000 took 0.096s
  training loss:		0.535796
  validation loss:		0.527700
  validation accuracy:		82.28 %
Epoch 1516 of 2000 took 0.096s
  training loss:		0.535924
  validation loss:		0.536386
  validation accuracy:		82.39 %
Epoch 1517 of 2000 took 0.096s
  training loss:		0.540583
  validation loss:		0.518636
  validation accuracy:		82.61 %
Epoch 1518 of 2000 took 0.096s
  training loss:		0.539903
  validation loss:		0.525785
  validation accuracy:		82.39 %
Epoch 1519 of 2000 took 0.096s
  training loss:		0.534149
  validation loss:		0.531487
  validation accuracy:		82.61 %
Epoch 1520 of 2000 took 0.096s
  training loss:		0.525220
  validation loss:		0.514884
  validation accuracy:		83.37 %
Epoch 1521 of 2000 took 0.096s
  training loss:		0.523223
  validation loss:		0.517924
  validation accuracy:		82.07 %
Epoch 1522 of 2000 took 0.096s
  training loss:		0.541814
  validation loss:		0.517232
  validation accuracy:		82.61 %
Epoch 1523 of 2000 took 0.096s
  training loss:		0.535231
  validation loss:		0.535036
  validation accuracy:		83.15 %
Epoch 1524 of 2000 took 0.096s
  training loss:		0.531513
  validation loss:		0.514401
  validation accuracy:		81.85 %
Epoch 1525 of 2000 took 0.096s
  training loss:		0.542054
  validation loss:		0.524049
  validation accuracy:		82.83 %
Epoch 1526 of 2000 took 0.096s
  training loss:		0.544347
  validation loss:		0.530066
  validation accuracy:		82.07 %
Epoch 1527 of 2000 took 0.096s
  training loss:		0.529487
  validation loss:		0.522671
  validation accuracy:		82.72 %
Epoch 1528 of 2000 took 0.096s
  training loss:		0.541817
  validation loss:		0.543453
  validation accuracy:		81.74 %
Epoch 1529 of 2000 took 0.096s
  training loss:		0.538310
  validation loss:		0.526823
  validation accuracy:		82.17 %
Epoch 1530 of 2000 took 0.096s
  training loss:		0.536130
  validation loss:		0.521650
  validation accuracy:		82.50 %
Epoch 1531 of 2000 took 0.096s
  training loss:		0.538310
  validation loss:		0.517066
  validation accuracy:		82.61 %
Epoch 1532 of 2000 took 0.096s
  training loss:		0.543879
  validation loss:		0.551462
  validation accuracy:		82.61 %
Epoch 1533 of 2000 took 0.096s
  training loss:		0.539026
  validation loss:		0.511026
  validation accuracy:		82.39 %
Epoch 1534 of 2000 took 0.096s
  training loss:		0.525773
  validation loss:		0.521893
  validation accuracy:		82.28 %
Epoch 1535 of 2000 took 0.096s
  training loss:		0.523886
  validation loss:		0.504251
  validation accuracy:		83.04 %
Epoch 1536 of 2000 took 0.097s
  training loss:		0.542584
  validation loss:		0.543347
  validation accuracy:		82.28 %
Epoch 1537 of 2000 took 0.097s
  training loss:		0.536581
  validation loss:		0.518743
  validation accuracy:		82.39 %
Epoch 1538 of 2000 took 0.096s
  training loss:		0.544339
  validation loss:		0.518619
  validation accuracy:		82.50 %
Epoch 1539 of 2000 took 0.096s
  training loss:		0.534558
  validation loss:		0.516892
  validation accuracy:		81.85 %
Epoch 1540 of 2000 took 0.096s
  training loss:		0.532354
  validation loss:		0.537668
  validation accuracy:		82.83 %
Epoch 1541 of 2000 took 0.096s
  training loss:		0.533109
  validation loss:		0.521595
  validation accuracy:		82.39 %
Epoch 1542 of 2000 took 0.096s
  training loss:		0.534354
  validation loss:		0.562052
  validation accuracy:		81.96 %
Epoch 1543 of 2000 took 0.097s
  training loss:		0.545966
  validation loss:		0.509522
  validation accuracy:		82.61 %
Epoch 1544 of 2000 took 0.096s
  training loss:		0.532400
  validation loss:		0.520106
  validation accuracy:		82.50 %
Epoch 1545 of 2000 took 0.099s
  training loss:		0.532453
  validation loss:		0.524154
  validation accuracy:		82.39 %
Epoch 1546 of 2000 took 0.096s
  training loss:		0.532679
  validation loss:		0.517619
  validation accuracy:		82.61 %
Epoch 1547 of 2000 took 0.096s
  training loss:		0.530242
  validation loss:		0.522755
  validation accuracy:		82.50 %
Epoch 1548 of 2000 took 0.097s
  training loss:		0.532340
  validation loss:		0.528363
  validation accuracy:		82.61 %
Epoch 1549 of 2000 took 0.096s
  training loss:		0.528464
  validation loss:		0.523186
  validation accuracy:		82.72 %
Epoch 1550 of 2000 took 0.096s
  training loss:		0.530988
  validation loss:		0.547976
  validation accuracy:		82.61 %
Epoch 1551 of 2000 took 0.097s
  training loss:		0.541366
  validation loss:		0.538318
  validation accuracy:		81.09 %
Epoch 1552 of 2000 took 0.097s
  training loss:		0.533970
  validation loss:		0.526643
  validation accuracy:		82.61 %
Epoch 1553 of 2000 took 0.096s
  training loss:		0.537538
  validation loss:		0.520769
  validation accuracy:		82.50 %
Epoch 1554 of 2000 took 0.096s
  training loss:		0.541266
  validation loss:		0.518208
  validation accuracy:		82.07 %
Epoch 1555 of 2000 took 0.096s
  training loss:		0.527600
  validation loss:		0.550468
  validation accuracy:		81.52 %
Epoch 1556 of 2000 took 0.096s
  training loss:		0.539682
  validation loss:		0.536763
  validation accuracy:		82.61 %
Epoch 1557 of 2000 took 0.096s
  training loss:		0.540297
  validation loss:		0.535419
  validation accuracy:		82.93 %
Epoch 1558 of 2000 took 0.096s
  training loss:		0.534787
  validation loss:		0.530374
  validation accuracy:		82.61 %
Epoch 1559 of 2000 took 0.096s
  training loss:		0.537704
  validation loss:		0.530321
  validation accuracy:		82.28 %
Epoch 1560 of 2000 took 0.097s
  training loss:		0.540974
  validation loss:		0.519015
  validation accuracy:		82.39 %
Epoch 1561 of 2000 took 0.096s
  training loss:		0.534403
  validation loss:		0.534273
  validation accuracy:		82.83 %
Epoch 1562 of 2000 took 0.097s
  training loss:		0.533783
  validation loss:		0.520738
  validation accuracy:		82.17 %
Epoch 1563 of 2000 took 0.096s
  training loss:		0.528683
  validation loss:		0.516485
  validation accuracy:		82.72 %
Epoch 1564 of 2000 took 0.096s
  training loss:		0.519434
  validation loss:		0.517611
  validation accuracy:		82.39 %
Epoch 1565 of 2000 took 0.096s
  training loss:		0.526682
  validation loss:		0.517976
  validation accuracy:		81.85 %
Epoch 1566 of 2000 took 0.096s
  training loss:		0.542938
  validation loss:		0.525840
  validation accuracy:		83.15 %
Epoch 1567 of 2000 took 0.098s
  training loss:		0.531545
  validation loss:		0.528283
  validation accuracy:		82.83 %
Epoch 1568 of 2000 took 0.096s
  training loss:		0.542795
  validation loss:		0.524622
  validation accuracy:		82.28 %
Epoch 1569 of 2000 took 0.096s
  training loss:		0.534263
  validation loss:		0.521348
  validation accuracy:		82.72 %
Epoch 1570 of 2000 took 0.096s
  training loss:		0.540434
  validation loss:		0.515377
  validation accuracy:		82.50 %
Epoch 1571 of 2000 took 0.096s
  training loss:		0.533037
  validation loss:		0.524486
  validation accuracy:		82.39 %
Epoch 1572 of 2000 took 0.096s
  training loss:		0.536517
  validation loss:		0.515564
  validation accuracy:		82.50 %
Epoch 1573 of 2000 took 0.096s
  training loss:		0.529916
  validation loss:		0.519999
  validation accuracy:		82.39 %
Epoch 1574 of 2000 took 0.096s
  training loss:		0.537003
  validation loss:		0.516350
  validation accuracy:		82.93 %
Epoch 1575 of 2000 took 0.096s
  training loss:		0.536971
  validation loss:		0.531565
  validation accuracy:		82.39 %
Epoch 1576 of 2000 took 0.096s
  training loss:		0.525416
  validation loss:		0.532594
  validation accuracy:		82.28 %
Epoch 1577 of 2000 took 0.096s
  training loss:		0.532698
  validation loss:		0.513461
  validation accuracy:		82.28 %
Epoch 1578 of 2000 took 0.096s
  training loss:		0.539258
  validation loss:		0.523777
  validation accuracy:		82.39 %
Epoch 1579 of 2000 took 0.096s
  training loss:		0.533723
  validation loss:		0.536407
  validation accuracy:		82.17 %
Epoch 1580 of 2000 took 0.096s
  training loss:		0.538065
  validation loss:		0.523113
  validation accuracy:		82.61 %
Epoch 1581 of 2000 took 0.096s
  training loss:		0.532562
  validation loss:		0.525613
  validation accuracy:		82.61 %
Epoch 1582 of 2000 took 0.097s
  training loss:		0.538495
  validation loss:		0.515430
  validation accuracy:		82.17 %
Epoch 1583 of 2000 took 0.096s
  training loss:		0.532595
  validation loss:		0.521179
  validation accuracy:		82.50 %
Epoch 1584 of 2000 took 0.096s
  training loss:		0.536756
  validation loss:		0.543041
  validation accuracy:		82.39 %
Epoch 1585 of 2000 took 0.096s
  training loss:		0.545209
  validation loss:		0.513591
  validation accuracy:		82.50 %
Epoch 1586 of 2000 took 0.096s
  training loss:		0.537792
  validation loss:		0.521289
  validation accuracy:		82.93 %
Epoch 1587 of 2000 took 0.096s
  training loss:		0.536232
  validation loss:		0.528754
  validation accuracy:		82.93 %
Epoch 1588 of 2000 took 0.096s
  training loss:		0.538546
  validation loss:		0.521348
  validation accuracy:		82.17 %
Epoch 1589 of 2000 took 0.097s
  training loss:		0.538827
  validation loss:		0.515964
  validation accuracy:		81.85 %
Epoch 1590 of 2000 took 0.096s
  training loss:		0.531465
  validation loss:		0.527196
  validation accuracy:		82.72 %
Epoch 1591 of 2000 took 0.096s
  training loss:		0.534053
  validation loss:		0.533631
  validation accuracy:		82.17 %
Epoch 1592 of 2000 took 0.096s
  training loss:		0.544847
  validation loss:		0.527253
  validation accuracy:		82.61 %
Epoch 1593 of 2000 took 0.097s
  training loss:		0.539717
  validation loss:		0.518532
  validation accuracy:		82.28 %
Epoch 1594 of 2000 took 0.096s
  training loss:		0.537229
  validation loss:		0.516668
  validation accuracy:		82.72 %
Epoch 1595 of 2000 took 0.096s
  training loss:		0.532713
  validation loss:		0.521120
  validation accuracy:		81.74 %
Epoch 1596 of 2000 took 0.096s
  training loss:		0.527672
  validation loss:		0.519314
  validation accuracy:		82.39 %
Epoch 1597 of 2000 took 0.096s
  training loss:		0.528339
  validation loss:		0.517565
  validation accuracy:		82.61 %
Epoch 1598 of 2000 took 0.097s
  training loss:		0.529271
  validation loss:		0.544531
  validation accuracy:		82.39 %
Epoch 1599 of 2000 took 0.096s
  training loss:		0.540976
  validation loss:		0.518413
  validation accuracy:		82.50 %
Epoch 1600 of 2000 took 0.096s
  training loss:		0.541719
  validation loss:		0.527350
  validation accuracy:		82.17 %
Epoch 1601 of 2000 took 0.096s
  training loss:		0.528953
  validation loss:		0.528078
  validation accuracy:		82.93 %
Epoch 1602 of 2000 took 0.096s
  training loss:		0.537191
  validation loss:		0.542387
  validation accuracy:		82.50 %
Epoch 1603 of 2000 took 0.096s
  training loss:		0.533195
  validation loss:		0.522742
  validation accuracy:		82.72 %
Epoch 1604 of 2000 took 0.096s
  training loss:		0.530416
  validation loss:		0.533609
  validation accuracy:		82.39 %
Epoch 1605 of 2000 took 0.096s
  training loss:		0.535412
  validation loss:		0.543627
  validation accuracy:		82.39 %
Epoch 1606 of 2000 took 0.096s
  training loss:		0.534522
  validation loss:		0.524864
  validation accuracy:		82.61 %
Epoch 1607 of 2000 took 0.096s
  training loss:		0.532788
  validation loss:		0.522692
  validation accuracy:		83.15 %
Epoch 1608 of 2000 took 0.096s
  training loss:		0.537664
  validation loss:		0.525703
  validation accuracy:		81.96 %
Epoch 1609 of 2000 took 0.097s
  training loss:		0.528195
  validation loss:		0.520517
  validation accuracy:		82.72 %
Epoch 1610 of 2000 took 0.096s
  training loss:		0.533809
  validation loss:		0.524484
  validation accuracy:		82.50 %
Epoch 1611 of 2000 took 0.096s
  training loss:		0.532103
  validation loss:		0.557145
  validation accuracy:		81.52 %
Epoch 1612 of 2000 took 0.096s
  training loss:		0.527886
  validation loss:		0.557124
  validation accuracy:		81.85 %
Epoch 1613 of 2000 took 0.096s
  training loss:		0.535742
  validation loss:		0.543054
  validation accuracy:		82.39 %
Epoch 1614 of 2000 took 0.096s
  training loss:		0.531995
  validation loss:		0.551065
  validation accuracy:		82.17 %
Epoch 1615 of 2000 took 0.096s
  training loss:		0.534240
  validation loss:		0.513348
  validation accuracy:		82.07 %
Epoch 1616 of 2000 took 0.096s
  training loss:		0.533870
  validation loss:		0.532709
  validation accuracy:		83.15 %
Epoch 1617 of 2000 took 0.096s
  training loss:		0.530510
  validation loss:		0.520231
  validation accuracy:		82.50 %
Epoch 1618 of 2000 took 0.096s
  training loss:		0.534075
  validation loss:		0.527747
  validation accuracy:		82.28 %
Epoch 1619 of 2000 took 0.096s
  training loss:		0.542252
  validation loss:		0.521042
  validation accuracy:		82.17 %
Epoch 1620 of 2000 took 0.096s
  training loss:		0.537022
  validation loss:		0.528965
  validation accuracy:		82.83 %
Epoch 1621 of 2000 took 0.096s
  training loss:		0.531135
  validation loss:		0.515606
  validation accuracy:		82.28 %
Epoch 1622 of 2000 took 0.097s
  training loss:		0.556801
  validation loss:		0.522822
  validation accuracy:		82.28 %
Epoch 1623 of 2000 took 0.096s
  training loss:		0.527049
  validation loss:		0.573276
  validation accuracy:		81.85 %
Epoch 1624 of 2000 took 0.096s
  training loss:		0.531227
  validation loss:		0.515610
  validation accuracy:		82.72 %
Epoch 1625 of 2000 took 0.096s
  training loss:		0.541599
  validation loss:		0.525065
  validation accuracy:		82.50 %
Epoch 1626 of 2000 took 0.096s
  training loss:		0.531918
  validation loss:		0.520845
  validation accuracy:		82.50 %
Epoch 1627 of 2000 took 0.096s
  training loss:		0.531991
  validation loss:		0.516224
  validation accuracy:		82.17 %
Epoch 1628 of 2000 took 0.096s
  training loss:		0.530827
  validation loss:		0.535143
  validation accuracy:		82.83 %
Epoch 1629 of 2000 took 0.097s
  training loss:		0.532239
  validation loss:		0.527545
  validation accuracy:		82.61 %
Epoch 1630 of 2000 took 0.097s
  training loss:		0.537799
  validation loss:		0.521966
  validation accuracy:		82.50 %
Epoch 1631 of 2000 took 0.097s
  training loss:		0.534876
  validation loss:		0.522688
  validation accuracy:		82.07 %
Epoch 1632 of 2000 took 0.096s
  training loss:		0.538209
  validation loss:		0.533533
  validation accuracy:		83.04 %
Epoch 1633 of 2000 took 0.097s
  training loss:		0.528708
  validation loss:		0.537890
  validation accuracy:		82.39 %
Epoch 1634 of 2000 took 0.096s
  training loss:		0.525570
  validation loss:		0.540867
  validation accuracy:		82.61 %
Epoch 1635 of 2000 took 0.097s
  training loss:		0.532746
  validation loss:		0.519059
  validation accuracy:		83.15 %
Epoch 1636 of 2000 took 0.100s
  training loss:		0.537860
  validation loss:		0.526873
  validation accuracy:		82.07 %
Epoch 1637 of 2000 took 0.099s
  training loss:		0.539175
  validation loss:		0.524105
  validation accuracy:		82.93 %
Epoch 1638 of 2000 took 0.099s
  training loss:		0.534625
  validation loss:		0.514524
  validation accuracy:		82.39 %
Epoch 1639 of 2000 took 0.099s
  training loss:		0.530658
  validation loss:		0.528801
  validation accuracy:		82.28 %
Epoch 1640 of 2000 took 0.099s
  training loss:		0.532676
  validation loss:		0.514060
  validation accuracy:		82.39 %
Epoch 1641 of 2000 took 0.099s
  training loss:		0.533623
  validation loss:		0.537752
  validation accuracy:		82.39 %
Epoch 1642 of 2000 took 0.099s
  training loss:		0.535750
  validation loss:		0.516040
  validation accuracy:		82.39 %
Epoch 1643 of 2000 took 0.099s
  training loss:		0.524213
  validation loss:		0.538339
  validation accuracy:		82.72 %
Epoch 1644 of 2000 took 0.099s
  training loss:		0.531290
  validation loss:		0.518534
  validation accuracy:		82.07 %
Epoch 1645 of 2000 took 0.099s
  training loss:		0.527987
  validation loss:		0.535693
  validation accuracy:		82.72 %
Epoch 1646 of 2000 took 0.099s
  training loss:		0.543553
  validation loss:		0.533250
  validation accuracy:		82.61 %
Epoch 1647 of 2000 took 0.099s
  training loss:		0.530450
  validation loss:		0.549866
  validation accuracy:		82.61 %
Epoch 1648 of 2000 took 0.099s
  training loss:		0.535550
  validation loss:		0.512390
  validation accuracy:		82.50 %
Epoch 1649 of 2000 took 0.099s
  training loss:		0.550069
  validation loss:		0.546426
  validation accuracy:		82.17 %
Epoch 1650 of 2000 took 0.099s
  training loss:		0.540354
  validation loss:		0.546486
  validation accuracy:		81.63 %
Epoch 1651 of 2000 took 0.099s
  training loss:		0.537526
  validation loss:		0.544906
  validation accuracy:		82.61 %
Epoch 1652 of 2000 took 0.099s
  training loss:		0.538447
  validation loss:		0.528724
  validation accuracy:		82.83 %
Epoch 1653 of 2000 took 0.099s
  training loss:		0.537604
  validation loss:		0.520646
  validation accuracy:		82.61 %
Epoch 1654 of 2000 took 0.099s
  training loss:		0.537703
  validation loss:		0.513267
  validation accuracy:		82.61 %
Epoch 1655 of 2000 took 0.100s
  training loss:		0.542565
  validation loss:		0.524009
  validation accuracy:		82.39 %
Epoch 1656 of 2000 took 0.097s
  training loss:		0.532473
  validation loss:		0.520796
  validation accuracy:		83.04 %
Epoch 1657 of 2000 took 0.096s
  training loss:		0.529457
  validation loss:		0.521163
  validation accuracy:		82.17 %
Epoch 1658 of 2000 took 0.096s
  training loss:		0.535087
  validation loss:		0.517178
  validation accuracy:		82.07 %
Epoch 1659 of 2000 took 0.096s
  training loss:		0.526098
  validation loss:		0.524920
  validation accuracy:		82.17 %
Epoch 1660 of 2000 took 0.097s
  training loss:		0.526153
  validation loss:		0.535400
  validation accuracy:		82.72 %
Epoch 1661 of 2000 took 0.096s
  training loss:		0.534727
  validation loss:		0.536459
  validation accuracy:		82.61 %
Epoch 1662 of 2000 took 0.096s
  training loss:		0.535765
  validation loss:		0.519020
  validation accuracy:		82.61 %
Epoch 1663 of 2000 took 0.096s
  training loss:		0.537731
  validation loss:		0.533752
  validation accuracy:		82.83 %
Epoch 1664 of 2000 took 0.096s
  training loss:		0.541447
  validation loss:		0.517753
  validation accuracy:		82.17 %
Epoch 1665 of 2000 took 0.096s
  training loss:		0.539500
  validation loss:		0.521392
  validation accuracy:		82.39 %
Epoch 1666 of 2000 took 0.096s
  training loss:		0.523853
  validation loss:		0.530706
  validation accuracy:		82.07 %
Epoch 1667 of 2000 took 0.096s
  training loss:		0.533350
  validation loss:		0.523326
  validation accuracy:		82.83 %
Epoch 1668 of 2000 took 0.096s
  training loss:		0.535661
  validation loss:		0.530537
  validation accuracy:		82.50 %
Epoch 1669 of 2000 took 0.096s
  training loss:		0.536945
  validation loss:		0.528580
  validation accuracy:		81.74 %
Epoch 1670 of 2000 took 0.097s
  training loss:		0.535111
  validation loss:		0.516862
  validation accuracy:		82.17 %
Epoch 1671 of 2000 took 0.096s
  training loss:		0.538284
  validation loss:		0.517618
  validation accuracy:		82.28 %
Epoch 1672 of 2000 took 0.097s
  training loss:		0.533921
  validation loss:		0.560899
  validation accuracy:		81.52 %
Epoch 1673 of 2000 took 0.096s
  training loss:		0.537362
  validation loss:		0.523453
  validation accuracy:		82.07 %
Epoch 1674 of 2000 took 0.096s
  training loss:		0.537456
  validation loss:		0.540012
  validation accuracy:		82.72 %
Epoch 1675 of 2000 took 0.097s
  training loss:		0.519183
  validation loss:		0.519580
  validation accuracy:		82.72 %
Epoch 1676 of 2000 took 0.097s
  training loss:		0.535861
  validation loss:		0.522389
  validation accuracy:		82.07 %
Epoch 1677 of 2000 took 0.096s
  training loss:		0.544317
  validation loss:		0.519043
  validation accuracy:		81.74 %
Epoch 1678 of 2000 took 0.096s
  training loss:		0.533917
  validation loss:		0.521382
  validation accuracy:		82.50 %
Epoch 1679 of 2000 took 0.096s
  training loss:		0.531784
  validation loss:		0.525619
  validation accuracy:		82.72 %
Epoch 1680 of 2000 took 0.096s
  training loss:		0.530036
  validation loss:		0.522186
  validation accuracy:		82.50 %
Epoch 1681 of 2000 took 0.096s
  training loss:		0.534617
  validation loss:		0.526752
  validation accuracy:		82.61 %
Epoch 1682 of 2000 took 0.096s
  training loss:		0.522361
  validation loss:		0.515337
  validation accuracy:		82.39 %
Epoch 1683 of 2000 took 0.096s
  training loss:		0.531205
  validation loss:		0.530281
  validation accuracy:		82.61 %
Epoch 1684 of 2000 took 0.096s
  training loss:		0.525586
  validation loss:		0.519929
  validation accuracy:		81.41 %
Epoch 1685 of 2000 took 0.096s
  training loss:		0.538140
  validation loss:		0.517638
  validation accuracy:		83.26 %
Epoch 1686 of 2000 took 0.096s
  training loss:		0.530019
  validation loss:		0.534937
  validation accuracy:		82.07 %
Epoch 1687 of 2000 took 0.096s
  training loss:		0.539819
  validation loss:		0.523104
  validation accuracy:		82.61 %
Epoch 1688 of 2000 took 0.096s
  training loss:		0.528836
  validation loss:		0.519457
  validation accuracy:		82.28 %
Epoch 1689 of 2000 took 0.096s
  training loss:		0.541960
  validation loss:		0.535534
  validation accuracy:		82.39 %
Epoch 1690 of 2000 took 0.096s
  training loss:		0.533154
  validation loss:		0.522829
  validation accuracy:		81.74 %
Epoch 1691 of 2000 took 0.097s
  training loss:		0.528075
  validation loss:		0.529835
  validation accuracy:		82.50 %
Epoch 1692 of 2000 took 0.097s
  training loss:		0.526460
  validation loss:		0.526213
  validation accuracy:		82.50 %
Epoch 1693 of 2000 took 0.096s
  training loss:		0.534688
  validation loss:		0.537423
  validation accuracy:		82.17 %
Epoch 1694 of 2000 took 0.096s
  training loss:		0.530527
  validation loss:		0.536843
  validation accuracy:		82.83 %
Epoch 1695 of 2000 took 0.096s
  training loss:		0.534371
  validation loss:		0.552780
  validation accuracy:		82.17 %
Epoch 1696 of 2000 took 0.097s
  training loss:		0.537542
  validation loss:		0.535443
  validation accuracy:		82.07 %
Epoch 1697 of 2000 took 0.096s
  training loss:		0.534204
  validation loss:		0.533026
  validation accuracy:		82.83 %
Epoch 1698 of 2000 took 0.096s
  training loss:		0.541865
  validation loss:		0.516137
  validation accuracy:		82.83 %
Epoch 1699 of 2000 took 0.096s
  training loss:		0.533277
  validation loss:		0.528542
  validation accuracy:		81.85 %
Epoch 1700 of 2000 took 0.096s
  training loss:		0.543066
  validation loss:		0.535174
  validation accuracy:		81.20 %
Epoch 1701 of 2000 took 0.096s
  training loss:		0.531428
  validation loss:		0.525263
  validation accuracy:		82.72 %
Epoch 1702 of 2000 took 0.096s
  training loss:		0.533822
  validation loss:		0.531205
  validation accuracy:		82.07 %
Epoch 1703 of 2000 took 0.096s
  training loss:		0.533808
  validation loss:		0.519121
  validation accuracy:		82.72 %
Epoch 1704 of 2000 took 0.097s
  training loss:		0.533117
  validation loss:		0.519450
  validation accuracy:		82.72 %
Epoch 1705 of 2000 took 0.096s
  training loss:		0.536619
  validation loss:		0.523275
  validation accuracy:		82.39 %
Epoch 1706 of 2000 took 0.096s
  training loss:		0.523346
  validation loss:		0.524252
  validation accuracy:		82.83 %
Epoch 1707 of 2000 took 0.096s
  training loss:		0.538165
  validation loss:		0.551765
  validation accuracy:		80.22 %
Epoch 1708 of 2000 took 0.096s
  training loss:		0.535588
  validation loss:		0.538591
  validation accuracy:		82.83 %
Epoch 1709 of 2000 took 0.096s
  training loss:		0.530220
  validation loss:		0.526499
  validation accuracy:		81.96 %
Epoch 1710 of 2000 took 0.096s
  training loss:		0.533722
  validation loss:		0.527528
  validation accuracy:		82.72 %
Epoch 1711 of 2000 took 0.096s
  training loss:		0.537222
  validation loss:		0.527675
  validation accuracy:		82.61 %
Epoch 1712 of 2000 took 0.096s
  training loss:		0.533970
  validation loss:		0.518158
  validation accuracy:		82.72 %
Epoch 1713 of 2000 took 0.097s
  training loss:		0.529282
  validation loss:		0.522946
  validation accuracy:		82.83 %
Epoch 1714 of 2000 took 0.096s
  training loss:		0.532130
  validation loss:		0.517881
  validation accuracy:		82.50 %
Epoch 1715 of 2000 took 0.096s
  training loss:		0.542011
  validation loss:		0.523631
  validation accuracy:		82.50 %
Epoch 1716 of 2000 took 0.096s
  training loss:		0.544510
  validation loss:		0.549258
  validation accuracy:		82.39 %
Epoch 1717 of 2000 took 0.097s
  training loss:		0.535993
  validation loss:		0.522099
  validation accuracy:		82.28 %
Epoch 1718 of 2000 took 0.096s
  training loss:		0.540140
  validation loss:		0.525948
  validation accuracy:		82.61 %
Epoch 1719 of 2000 took 0.096s
  training loss:		0.542162
  validation loss:		0.522817
  validation accuracy:		81.96 %
Epoch 1720 of 2000 took 0.096s
  training loss:		0.539244
  validation loss:		0.523390
  validation accuracy:		82.07 %
Epoch 1721 of 2000 took 0.096s
  training loss:		0.530285
  validation loss:		0.557316
  validation accuracy:		82.39 %
Epoch 1722 of 2000 took 0.097s
  training loss:		0.533250
  validation loss:		0.518690
  validation accuracy:		82.61 %
Epoch 1723 of 2000 took 0.097s
  training loss:		0.534084
  validation loss:		0.514229
  validation accuracy:		82.07 %
Epoch 1724 of 2000 took 0.096s
  training loss:		0.545774
  validation loss:		0.520391
  validation accuracy:		82.72 %
Epoch 1725 of 2000 took 0.096s
  training loss:		0.540953
  validation loss:		0.539006
  validation accuracy:		82.50 %
Epoch 1726 of 2000 took 0.100s
  training loss:		0.533838
  validation loss:		0.554165
  validation accuracy:		81.20 %
Epoch 1727 of 2000 took 0.095s
  training loss:		0.532654
  validation loss:		0.516252
  validation accuracy:		82.17 %
Epoch 1728 of 2000 took 0.095s
  training loss:		0.527177
  validation loss:		0.542334
  validation accuracy:		82.61 %
Epoch 1729 of 2000 took 0.095s
  training loss:		0.532732
  validation loss:		0.525030
  validation accuracy:		82.72 %
Epoch 1730 of 2000 took 0.095s
  training loss:		0.531991
  validation loss:		0.536669
  validation accuracy:		82.17 %
Epoch 1731 of 2000 took 0.095s
  training loss:		0.541254
  validation loss:		0.529980
  validation accuracy:		82.50 %
Epoch 1732 of 2000 took 0.095s
  training loss:		0.536760
  validation loss:		0.532333
  validation accuracy:		82.61 %
Epoch 1733 of 2000 took 0.095s
  training loss:		0.526890
  validation loss:		0.560429
  validation accuracy:		81.41 %
Epoch 1734 of 2000 took 0.095s
  training loss:		0.527490
  validation loss:		0.518442
  validation accuracy:		81.74 %
Epoch 1735 of 2000 took 0.095s
  training loss:		0.536748
  validation loss:		0.517421
  validation accuracy:		81.85 %
Epoch 1736 of 2000 took 0.095s
  training loss:		0.535007
  validation loss:		0.537228
  validation accuracy:		82.50 %
Epoch 1737 of 2000 took 0.095s
  training loss:		0.529497
  validation loss:		0.520449
  validation accuracy:		82.07 %
Epoch 1738 of 2000 took 0.095s
  training loss:		0.527252
  validation loss:		0.514223
  validation accuracy:		82.50 %
Epoch 1739 of 2000 took 0.096s
  training loss:		0.525011
  validation loss:		0.533810
  validation accuracy:		82.39 %
Epoch 1740 of 2000 took 0.096s
  training loss:		0.534460
  validation loss:		0.527077
  validation accuracy:		82.39 %
Epoch 1741 of 2000 took 0.096s
  training loss:		0.533277
  validation loss:		0.527658
  validation accuracy:		81.63 %
Epoch 1742 of 2000 took 0.096s
  training loss:		0.538342
  validation loss:		0.537260
  validation accuracy:		82.50 %
Epoch 1743 of 2000 took 0.096s
  training loss:		0.524119
  validation loss:		0.536272
  validation accuracy:		82.39 %
Epoch 1744 of 2000 took 0.096s
  training loss:		0.533702
  validation loss:		0.535915
  validation accuracy:		82.72 %
Epoch 1745 of 2000 took 0.096s
  training loss:		0.548267
  validation loss:		0.534420
  validation accuracy:		82.39 %
Epoch 1746 of 2000 took 0.096s
  training loss:		0.538989
  validation loss:		0.525428
  validation accuracy:		81.85 %
Epoch 1747 of 2000 took 0.096s
  training loss:		0.536670
  validation loss:		0.516000
  validation accuracy:		82.07 %
Epoch 1748 of 2000 took 0.096s
  training loss:		0.524880
  validation loss:		0.523171
  validation accuracy:		82.72 %
Epoch 1749 of 2000 took 0.096s
  training loss:		0.531535
  validation loss:		0.523409
  validation accuracy:		81.96 %
Epoch 1750 of 2000 took 0.096s
  training loss:		0.526291
  validation loss:		0.526844
  validation accuracy:		82.39 %
Epoch 1751 of 2000 took 0.096s
  training loss:		0.526748
  validation loss:		0.528092
  validation accuracy:		82.39 %
Epoch 1752 of 2000 took 0.096s
  training loss:		0.525463
  validation loss:		0.538050
  validation accuracy:		82.61 %
Epoch 1753 of 2000 took 0.096s
  training loss:		0.539529
  validation loss:		0.528833
  validation accuracy:		82.39 %
Epoch 1754 of 2000 took 0.099s
  training loss:		0.536315
  validation loss:		0.520144
  validation accuracy:		82.17 %
Epoch 1755 of 2000 took 0.096s
  training loss:		0.536591
  validation loss:		0.528418
  validation accuracy:		81.85 %
Epoch 1756 of 2000 took 0.096s
  training loss:		0.532603
  validation loss:		0.517947
  validation accuracy:		82.61 %
Epoch 1757 of 2000 took 0.096s
  training loss:		0.530619
  validation loss:		0.539335
  validation accuracy:		82.72 %
Epoch 1758 of 2000 took 0.096s
  training loss:		0.536559
  validation loss:		0.527837
  validation accuracy:		81.85 %
Epoch 1759 of 2000 took 0.096s
  training loss:		0.535263
  validation loss:		0.520638
  validation accuracy:		82.39 %
Epoch 1760 of 2000 took 0.096s
  training loss:		0.540266
  validation loss:		0.519632
  validation accuracy:		82.83 %
Epoch 1761 of 2000 took 0.096s
  training loss:		0.526282
  validation loss:		0.520788
  validation accuracy:		82.61 %
Epoch 1762 of 2000 took 0.096s
  training loss:		0.535408
  validation loss:		0.526546
  validation accuracy:		81.96 %
Epoch 1763 of 2000 took 0.096s
  training loss:		0.539297
  validation loss:		0.514797
  validation accuracy:		82.61 %
Epoch 1764 of 2000 took 0.096s
  training loss:		0.534285
  validation loss:		0.516105
  validation accuracy:		82.07 %
Epoch 1765 of 2000 took 0.096s
  training loss:		0.547201
  validation loss:		0.534587
  validation accuracy:		82.39 %
Epoch 1766 of 2000 took 0.096s
  training loss:		0.532909
  validation loss:		0.535640
  validation accuracy:		82.28 %
Epoch 1767 of 2000 took 0.096s
  training loss:		0.525402
  validation loss:		0.530413
  validation accuracy:		81.63 %
Epoch 1768 of 2000 took 0.096s
  training loss:		0.537189
  validation loss:		0.524817
  validation accuracy:		82.28 %
Epoch 1769 of 2000 took 0.096s
  training loss:		0.531999
  validation loss:		0.547165
  validation accuracy:		82.39 %
Epoch 1770 of 2000 took 0.096s
  training loss:		0.533841
  validation loss:		0.523732
  validation accuracy:		82.83 %
Epoch 1771 of 2000 took 0.096s
  training loss:		0.524094
  validation loss:		0.533697
  validation accuracy:		82.50 %
Epoch 1772 of 2000 took 0.096s
  training loss:		0.537929
  validation loss:		0.521330
  validation accuracy:		82.61 %
Epoch 1773 of 2000 took 0.096s
  training loss:		0.532775
  validation loss:		0.524642
  validation accuracy:		82.17 %
Epoch 1774 of 2000 took 0.096s
  training loss:		0.539715
  validation loss:		0.522620
  validation accuracy:		82.07 %
Epoch 1775 of 2000 took 0.096s
  training loss:		0.534462
  validation loss:		0.527126
  validation accuracy:		82.83 %
Epoch 1776 of 2000 took 0.096s
  training loss:		0.524382
  validation loss:		0.542836
  validation accuracy:		82.50 %
Epoch 1777 of 2000 took 0.096s
  training loss:		0.531602
  validation loss:		0.521256
  validation accuracy:		82.17 %
Epoch 1778 of 2000 took 0.096s
  training loss:		0.534366
  validation loss:		0.534879
  validation accuracy:		82.17 %
Epoch 1779 of 2000 took 0.096s
  training loss:		0.531666
  validation loss:		0.514586
  validation accuracy:		82.50 %
Epoch 1780 of 2000 took 0.096s
  training loss:		0.525249
  validation loss:		0.523561
  validation accuracy:		82.61 %
Epoch 1781 of 2000 took 0.096s
  training loss:		0.528327
  validation loss:		0.527831
  validation accuracy:		82.61 %
Epoch 1782 of 2000 took 0.096s
  training loss:		0.531532
  validation loss:		0.524515
  validation accuracy:		82.50 %
Epoch 1783 of 2000 took 0.096s
  training loss:		0.522492
  validation loss:		0.523759
  validation accuracy:		82.72 %
Epoch 1784 of 2000 took 0.096s
  training loss:		0.533918
  validation loss:		0.536683
  validation accuracy:		81.85 %
Epoch 1785 of 2000 took 0.097s
  training loss:		0.538270
  validation loss:		0.525047
  validation accuracy:		82.83 %
Epoch 1786 of 2000 took 0.096s
  training loss:		0.542209
  validation loss:		0.516316
  validation accuracy:		82.07 %
Epoch 1787 of 2000 took 0.096s
  training loss:		0.538885
  validation loss:		0.531993
  validation accuracy:		82.61 %
Epoch 1788 of 2000 took 0.096s
  training loss:		0.526483
  validation loss:		0.529076
  validation accuracy:		82.61 %
Epoch 1789 of 2000 took 0.096s
  training loss:		0.524528
  validation loss:		0.518230
  validation accuracy:		82.17 %
Epoch 1790 of 2000 took 0.096s
  training loss:		0.529200
  validation loss:		0.530919
  validation accuracy:		82.07 %
Epoch 1791 of 2000 took 0.096s
  training loss:		0.543340
  validation loss:		0.535984
  validation accuracy:		81.63 %
Epoch 1792 of 2000 took 0.096s
  training loss:		0.533426
  validation loss:		0.538377
  validation accuracy:		81.96 %
Epoch 1793 of 2000 took 0.096s
  training loss:		0.531445
  validation loss:		0.526806
  validation accuracy:		82.50 %
Epoch 1794 of 2000 took 0.096s
  training loss:		0.529686
  validation loss:		0.522245
  validation accuracy:		81.20 %
Epoch 1795 of 2000 took 0.096s
  training loss:		0.539228
  validation loss:		0.516583
  validation accuracy:		82.17 %
Epoch 1796 of 2000 took 0.096s
  training loss:		0.534457
  validation loss:		0.564189
  validation accuracy:		81.20 %
Epoch 1797 of 2000 took 0.096s
  training loss:		0.541291
  validation loss:		0.543364
  validation accuracy:		82.50 %
Epoch 1798 of 2000 took 0.096s
  training loss:		0.536005
  validation loss:		0.537840
  validation accuracy:		82.93 %
Epoch 1799 of 2000 took 0.096s
  training loss:		0.532482
  validation loss:		0.520614
  validation accuracy:		81.63 %
Epoch 1800 of 2000 took 0.096s
  training loss:		0.536240
  validation loss:		0.531335
  validation accuracy:		82.17 %
Epoch 1801 of 2000 took 0.096s
  training loss:		0.535213
  validation loss:		0.542022
  validation accuracy:		82.17 %
Epoch 1802 of 2000 took 0.096s
  training loss:		0.533457
  validation loss:		0.526160
  validation accuracy:		82.83 %
Epoch 1803 of 2000 took 0.096s
  training loss:		0.530266
  validation loss:		0.520214
  validation accuracy:		82.61 %
Epoch 1804 of 2000 took 0.096s
  training loss:		0.533847
  validation loss:		0.518082
  validation accuracy:		82.61 %
Epoch 1805 of 2000 took 0.096s
  training loss:		0.538496
  validation loss:		0.534697
  validation accuracy:		82.72 %
Epoch 1806 of 2000 took 0.096s
  training loss:		0.537549
  validation loss:		0.518265
  validation accuracy:		81.85 %
Epoch 1807 of 2000 took 0.096s
  training loss:		0.538912
  validation loss:		0.524298
  validation accuracy:		82.50 %
Epoch 1808 of 2000 took 0.097s
  training loss:		0.530977
  validation loss:		0.520727
  validation accuracy:		82.61 %
Epoch 1809 of 2000 took 0.096s
  training loss:		0.536528
  validation loss:		0.527246
  validation accuracy:		82.50 %
Epoch 1810 of 2000 took 0.096s
  training loss:		0.529114
  validation loss:		0.537208
  validation accuracy:		82.39 %
Epoch 1811 of 2000 took 0.096s
  training loss:		0.527924
  validation loss:		0.515317
  validation accuracy:		82.39 %
Epoch 1812 of 2000 took 0.096s
  training loss:		0.531281
  validation loss:		0.524961
  validation accuracy:		82.50 %
Epoch 1813 of 2000 took 0.096s
  training loss:		0.528271
  validation loss:		0.527407
  validation accuracy:		82.72 %
Epoch 1814 of 2000 took 0.096s
  training loss:		0.536614
  validation loss:		0.520973
  validation accuracy:		82.39 %
Epoch 1815 of 2000 took 0.096s
  training loss:		0.536134
  validation loss:		0.541627
  validation accuracy:		82.61 %
Epoch 1816 of 2000 took 0.096s
  training loss:		0.536474
  validation loss:		0.530308
  validation accuracy:		81.96 %
Epoch 1817 of 2000 took 0.097s
  training loss:		0.527399
  validation loss:		0.523031
  validation accuracy:		82.61 %
Epoch 1818 of 2000 took 0.096s
  training loss:		0.538355
  validation loss:		0.534068
  validation accuracy:		82.17 %
Epoch 1819 of 2000 took 0.096s
  training loss:		0.528364
  validation loss:		0.523507
  validation accuracy:		82.17 %
Epoch 1820 of 2000 took 0.096s
  training loss:		0.541676
  validation loss:		0.529029
  validation accuracy:		82.72 %
Epoch 1821 of 2000 took 0.096s
  training loss:		0.530199
  validation loss:		0.535018
  validation accuracy:		82.39 %
Epoch 1822 of 2000 took 0.096s
  training loss:		0.531785
  validation loss:		0.525217
  validation accuracy:		82.50 %
Epoch 1823 of 2000 took 0.096s
  training loss:		0.531842
  validation loss:		0.525031
  validation accuracy:		82.07 %
Epoch 1824 of 2000 took 0.096s
  training loss:		0.535500
  validation loss:		0.521285
  validation accuracy:		82.72 %
Epoch 1825 of 2000 took 0.096s
  training loss:		0.538657
  validation loss:		0.529530
  validation accuracy:		82.39 %
Epoch 1826 of 2000 took 0.096s
  training loss:		0.532997
  validation loss:		0.528816
  validation accuracy:		82.39 %
Epoch 1827 of 2000 took 0.096s
  training loss:		0.534943
  validation loss:		0.548401
  validation accuracy:		81.96 %
Epoch 1828 of 2000 took 0.096s
  training loss:		0.547798
  validation loss:		0.531272
  validation accuracy:		82.28 %
Epoch 1829 of 2000 took 0.096s
  training loss:		0.528229
  validation loss:		0.512423
  validation accuracy:		82.83 %
Epoch 1830 of 2000 took 0.097s
  training loss:		0.530276
  validation loss:		0.528695
  validation accuracy:		81.52 %
Epoch 1831 of 2000 took 0.095s
  training loss:		0.534986
  validation loss:		0.565984
  validation accuracy:		81.85 %
Epoch 1832 of 2000 took 0.095s
  training loss:		0.537217
  validation loss:		0.529999
  validation accuracy:		82.28 %
Epoch 1833 of 2000 took 0.095s
  training loss:		0.542281
  validation loss:		0.541950
  validation accuracy:		81.85 %
Epoch 1834 of 2000 took 0.095s
  training loss:		0.541515
  validation loss:		0.538006
  validation accuracy:		82.17 %
Epoch 1835 of 2000 took 0.095s
  training loss:		0.535811
  validation loss:		0.532086
  validation accuracy:		82.61 %
Epoch 1836 of 2000 took 0.095s
  training loss:		0.534860
  validation loss:		0.524907
  validation accuracy:		82.17 %
Epoch 1837 of 2000 took 0.095s
  training loss:		0.536417
  validation loss:		0.526501
  validation accuracy:		82.61 %
Epoch 1838 of 2000 took 0.095s
  training loss:		0.524326
  validation loss:		0.525492
  validation accuracy:		82.50 %
Epoch 1839 of 2000 took 0.095s
  training loss:		0.528678
  validation loss:		0.541420
  validation accuracy:		82.39 %
Epoch 1840 of 2000 took 0.095s
  training loss:		0.532353
  validation loss:		0.525471
  validation accuracy:		82.61 %
Epoch 1841 of 2000 took 0.096s
  training loss:		0.532829
  validation loss:		0.541320
  validation accuracy:		82.61 %
Epoch 1842 of 2000 took 0.099s
  training loss:		0.533439
  validation loss:		0.523632
  validation accuracy:		82.28 %
Epoch 1843 of 2000 took 0.099s
  training loss:		0.531714
  validation loss:		0.533533
  validation accuracy:		82.39 %
Epoch 1844 of 2000 took 0.099s
  training loss:		0.533205
  validation loss:		0.520362
  validation accuracy:		82.61 %
Epoch 1845 of 2000 took 0.099s
  training loss:		0.530766
  validation loss:		0.534336
  validation accuracy:		82.83 %
Epoch 1846 of 2000 took 0.099s
  training loss:		0.538919
  validation loss:		0.535580
  validation accuracy:		81.09 %
Epoch 1847 of 2000 took 0.099s
  training loss:		0.534012
  validation loss:		0.524531
  validation accuracy:		82.93 %
Epoch 1848 of 2000 took 0.100s
  training loss:		0.538863
  validation loss:		0.531950
  validation accuracy:		82.39 %
Epoch 1849 of 2000 took 0.099s
  training loss:		0.535046
  validation loss:		0.525494
  validation accuracy:		82.39 %
Epoch 1850 of 2000 took 0.099s
  training loss:		0.533570
  validation loss:		0.528855
  validation accuracy:		81.85 %
Epoch 1851 of 2000 took 0.099s
  training loss:		0.545850
  validation loss:		0.531648
  validation accuracy:		82.83 %
Epoch 1852 of 2000 took 0.099s
  training loss:		0.531190
  validation loss:		0.525490
  validation accuracy:		82.61 %
Epoch 1853 of 2000 took 0.099s
  training loss:		0.525737
  validation loss:		0.533762
  validation accuracy:		82.17 %
Epoch 1854 of 2000 took 0.099s
  training loss:		0.533465
  validation loss:		0.529973
  validation accuracy:		83.15 %
Epoch 1855 of 2000 took 0.099s
  training loss:		0.537978
  validation loss:		0.528597
  validation accuracy:		82.39 %
Epoch 1856 of 2000 took 0.099s
  training loss:		0.534192
  validation loss:		0.528417
  validation accuracy:		82.50 %
Epoch 1857 of 2000 took 0.099s
  training loss:		0.538907
  validation loss:		0.542687
  validation accuracy:		81.74 %
Epoch 1858 of 2000 took 0.099s
  training loss:		0.542440
  validation loss:		0.529866
  validation accuracy:		82.17 %
Epoch 1859 of 2000 took 0.099s
  training loss:		0.536642
  validation loss:		0.535060
  validation accuracy:		82.83 %
Epoch 1860 of 2000 took 0.099s
  training loss:		0.529781
  validation loss:		0.524018
  validation accuracy:		82.28 %
Epoch 1861 of 2000 took 0.099s
  training loss:		0.538768
  validation loss:		0.535611
  validation accuracy:		82.72 %
Epoch 1862 of 2000 took 0.099s
  training loss:		0.539491
  validation loss:		0.540539
  validation accuracy:		82.61 %
Epoch 1863 of 2000 took 0.099s
  training loss:		0.536300
  validation loss:		0.518721
  validation accuracy:		82.28 %
Epoch 1864 of 2000 took 0.099s
  training loss:		0.529980
  validation loss:		0.522782
  validation accuracy:		82.61 %
Epoch 1865 of 2000 took 0.099s
  training loss:		0.538775
  validation loss:		0.522607
  validation accuracy:		83.04 %
Epoch 1866 of 2000 took 0.099s
  training loss:		0.545774
  validation loss:		0.526221
  validation accuracy:		82.50 %
Epoch 1867 of 2000 took 0.099s
  training loss:		0.535463
  validation loss:		0.532222
  validation accuracy:		82.28 %
Epoch 1868 of 2000 took 0.099s
  training loss:		0.535719
  validation loss:		0.522884
  validation accuracy:		82.07 %
Epoch 1869 of 2000 took 0.099s
  training loss:		0.529905
  validation loss:		0.540719
  validation accuracy:		82.72 %
Epoch 1870 of 2000 took 0.099s
  training loss:		0.526718
  validation loss:		0.529848
  validation accuracy:		82.39 %
Epoch 1871 of 2000 took 0.099s
  training loss:		0.532832
  validation loss:		0.522590
  validation accuracy:		82.61 %
Epoch 1872 of 2000 took 0.099s
  training loss:		0.531037
  validation loss:		0.532897
  validation accuracy:		82.50 %
Epoch 1873 of 2000 took 0.099s
  training loss:		0.543072
  validation loss:		0.520906
  validation accuracy:		82.72 %
Epoch 1874 of 2000 took 0.099s
  training loss:		0.535549
  validation loss:		0.520356
  validation accuracy:		81.74 %
Epoch 1875 of 2000 took 0.099s
  training loss:		0.544669
  validation loss:		0.525971
  validation accuracy:		82.61 %
Epoch 1876 of 2000 took 0.099s
  training loss:		0.530785
  validation loss:		0.522428
  validation accuracy:		81.74 %
Epoch 1877 of 2000 took 0.099s
  training loss:		0.529792
  validation loss:		0.521403
  validation accuracy:		82.61 %
Epoch 1878 of 2000 took 0.100s
  training loss:		0.542475
  validation loss:		0.548111
  validation accuracy:		82.28 %
Epoch 1879 of 2000 took 0.099s
  training loss:		0.526446
  validation loss:		0.523655
  validation accuracy:		82.61 %
Epoch 1880 of 2000 took 0.099s
  training loss:		0.524934
  validation loss:		0.525544
  validation accuracy:		82.39 %
Epoch 1881 of 2000 took 0.099s
  training loss:		0.536718
  validation loss:		0.519516
  validation accuracy:		82.72 %
Epoch 1882 of 2000 took 0.096s
  training loss:		0.531994
  validation loss:		0.523068
  validation accuracy:		81.85 %
Epoch 1883 of 2000 took 0.096s
  training loss:		0.528792
  validation loss:		0.517902
  validation accuracy:		82.17 %
Epoch 1884 of 2000 took 0.096s
  training loss:		0.541770
  validation loss:		0.518716
  validation accuracy:		82.07 %
Epoch 1885 of 2000 took 0.096s
  training loss:		0.534350
  validation loss:		0.525448
  validation accuracy:		82.50 %
Epoch 1886 of 2000 took 0.096s
  training loss:		0.534519
  validation loss:		0.533100
  validation accuracy:		82.50 %
Epoch 1887 of 2000 took 0.096s
  training loss:		0.540207
  validation loss:		0.522082
  validation accuracy:		82.39 %
Epoch 1888 of 2000 took 0.096s
  training loss:		0.539525
  validation loss:		0.518563
  validation accuracy:		82.17 %
Epoch 1889 of 2000 took 0.096s
  training loss:		0.535429
  validation loss:		0.527937
  validation accuracy:		82.50 %
Epoch 1890 of 2000 took 0.096s
  training loss:		0.531114
  validation loss:		0.525437
  validation accuracy:		82.39 %
Epoch 1891 of 2000 took 0.096s
  training loss:		0.545480
  validation loss:		0.523441
  validation accuracy:		82.50 %
Epoch 1892 of 2000 took 0.096s
  training loss:		0.524822
  validation loss:		0.522363
  validation accuracy:		81.63 %
Epoch 1893 of 2000 took 0.096s
  training loss:		0.532570
  validation loss:		0.522736
  validation accuracy:		82.07 %
Epoch 1894 of 2000 took 0.096s
  training loss:		0.531009
  validation loss:		0.528428
  validation accuracy:		82.83 %
Epoch 1895 of 2000 took 0.096s
  training loss:		0.542366
  validation loss:		0.528610
  validation accuracy:		82.17 %
Epoch 1896 of 2000 took 0.096s
  training loss:		0.529999
  validation loss:		0.527718
  validation accuracy:		82.61 %
Epoch 1897 of 2000 took 0.096s
  training loss:		0.530841
  validation loss:		0.522264
  validation accuracy:		82.72 %
Epoch 1898 of 2000 took 0.096s
  training loss:		0.542667
  validation loss:		0.525223
  validation accuracy:		83.04 %
Epoch 1899 of 2000 took 0.096s
  training loss:		0.529445
  validation loss:		0.519038
  validation accuracy:		81.85 %
Epoch 1900 of 2000 took 0.096s
  training loss:		0.536378
  validation loss:		0.530680
  validation accuracy:		82.07 %
Epoch 1901 of 2000 took 0.096s
  training loss:		0.538739
  validation loss:		0.532483
  validation accuracy:		82.61 %
Epoch 1902 of 2000 took 0.096s
  training loss:		0.532007
  validation loss:		0.518727
  validation accuracy:		82.28 %
Epoch 1903 of 2000 took 0.096s
  training loss:		0.526554
  validation loss:		0.528219
  validation accuracy:		82.17 %
Epoch 1904 of 2000 took 0.096s
  training loss:		0.526888
  validation loss:		0.512047
  validation accuracy:		83.04 %
Epoch 1905 of 2000 took 0.096s
  training loss:		0.531326
  validation loss:		0.537790
  validation accuracy:		82.07 %
Epoch 1906 of 2000 took 0.096s
  training loss:		0.531578
  validation loss:		0.526180
  validation accuracy:		82.61 %
Epoch 1907 of 2000 took 0.096s
  training loss:		0.537848
  validation loss:		0.530963
  validation accuracy:		82.07 %
Epoch 1908 of 2000 took 0.096s
  training loss:		0.532988
  validation loss:		0.518286
  validation accuracy:		82.61 %
Epoch 1909 of 2000 took 0.097s
  training loss:		0.534103
  validation loss:		0.539590
  validation accuracy:		82.72 %
Epoch 1910 of 2000 took 0.096s
  training loss:		0.535070
  validation loss:		0.538370
  validation accuracy:		82.93 %
Epoch 1911 of 2000 took 0.097s
  training loss:		0.550228
  validation loss:		0.542316
  validation accuracy:		81.30 %
Epoch 1912 of 2000 took 0.096s
  training loss:		0.542033
  validation loss:		0.535337
  validation accuracy:		82.83 %
Epoch 1913 of 2000 took 0.096s
  training loss:		0.532681
  validation loss:		0.527205
  validation accuracy:		82.39 %
Epoch 1914 of 2000 took 0.096s
  training loss:		0.541032
  validation loss:		0.515343
  validation accuracy:		83.04 %
Epoch 1915 of 2000 took 0.096s
  training loss:		0.535346
  validation loss:		0.538023
  validation accuracy:		82.28 %
Epoch 1916 of 2000 took 0.096s
  training loss:		0.530832
  validation loss:		0.521617
  validation accuracy:		82.50 %
Epoch 1917 of 2000 took 0.096s
  training loss:		0.524511
  validation loss:		0.533554
  validation accuracy:		82.50 %
Epoch 1918 of 2000 took 0.096s
  training loss:		0.540515
  validation loss:		0.523698
  validation accuracy:		82.61 %
Epoch 1919 of 2000 took 0.096s
  training loss:		0.532053
  validation loss:		0.524285
  validation accuracy:		82.39 %
Epoch 1920 of 2000 took 0.096s
  training loss:		0.524561
  validation loss:		0.514939
  validation accuracy:		82.28 %
Epoch 1921 of 2000 took 0.096s
  training loss:		0.541969
  validation loss:		0.558305
  validation accuracy:		81.96 %
Epoch 1922 of 2000 took 0.096s
  training loss:		0.544213
  validation loss:		0.532559
  validation accuracy:		82.39 %
Epoch 1923 of 2000 took 0.096s
  training loss:		0.525464
  validation loss:		0.522521
  validation accuracy:		82.61 %
Epoch 1924 of 2000 took 0.099s
  training loss:		0.535313
  validation loss:		0.521960
  validation accuracy:		82.39 %
Epoch 1925 of 2000 took 0.099s
  training loss:		0.536153
  validation loss:		0.519869
  validation accuracy:		82.83 %
Epoch 1926 of 2000 took 0.099s
  training loss:		0.529564
  validation loss:		0.546942
  validation accuracy:		82.39 %
Epoch 1927 of 2000 took 0.099s
  training loss:		0.530347
  validation loss:		0.522665
  validation accuracy:		81.63 %
Epoch 1928 of 2000 took 0.099s
  training loss:		0.540200
  validation loss:		0.532904
  validation accuracy:		82.50 %
Epoch 1929 of 2000 took 0.099s
  training loss:		0.531492
  validation loss:		0.529559
  validation accuracy:		82.39 %
Epoch 1930 of 2000 took 0.099s
  training loss:		0.533735
  validation loss:		0.523067
  validation accuracy:		82.17 %
Epoch 1931 of 2000 took 0.099s
  training loss:		0.527902
  validation loss:		0.526501
  validation accuracy:		82.83 %
Epoch 1932 of 2000 took 0.099s
  training loss:		0.532178
  validation loss:		0.519210
  validation accuracy:		81.96 %
Epoch 1933 of 2000 took 0.099s
  training loss:		0.533355
  validation loss:		0.529391
  validation accuracy:		82.61 %
Epoch 1934 of 2000 took 0.099s
  training loss:		0.528475
  validation loss:		0.523913
  validation accuracy:		82.83 %
Epoch 1935 of 2000 took 0.099s
  training loss:		0.525126
  validation loss:		0.551029
  validation accuracy:		82.07 %
Epoch 1936 of 2000 took 0.100s
  training loss:		0.536639
  validation loss:		0.515715
  validation accuracy:		82.17 %
Epoch 1937 of 2000 took 0.099s
  training loss:		0.535154
  validation loss:		0.522527
  validation accuracy:		83.04 %
Epoch 1938 of 2000 took 0.099s
  training loss:		0.535458
  validation loss:		0.523375
  validation accuracy:		81.96 %
Epoch 1939 of 2000 took 0.099s
  training loss:		0.535150
  validation loss:		0.528760
  validation accuracy:		82.61 %
Epoch 1940 of 2000 took 0.100s
  training loss:		0.527728
  validation loss:		0.536454
  validation accuracy:		82.83 %
Epoch 1941 of 2000 took 0.099s
  training loss:		0.532888
  validation loss:		0.533625
  validation accuracy:		82.17 %
Epoch 1942 of 2000 took 0.099s
  training loss:		0.542079
  validation loss:		0.529393
  validation accuracy:		83.26 %
Epoch 1943 of 2000 took 0.099s
  training loss:		0.535298
  validation loss:		0.537939
  validation accuracy:		82.28 %
Epoch 1944 of 2000 took 0.099s
  training loss:		0.521707
  validation loss:		0.518426
  validation accuracy:		82.28 %
Epoch 1945 of 2000 took 0.099s
  training loss:		0.536088
  validation loss:		0.523744
  validation accuracy:		82.93 %
Epoch 1946 of 2000 took 0.099s
  training loss:		0.526663
  validation loss:		0.520143
  validation accuracy:		82.39 %
Epoch 1947 of 2000 took 0.099s
  training loss:		0.534454
  validation loss:		0.526682
  validation accuracy:		82.50 %
Epoch 1948 of 2000 took 0.099s
  training loss:		0.532317
  validation loss:		0.531702
  validation accuracy:		82.39 %
Epoch 1949 of 2000 took 0.099s
  training loss:		0.539336
  validation loss:		0.522215
  validation accuracy:		82.83 %
Epoch 1950 of 2000 took 0.099s
  training loss:		0.524027
  validation loss:		0.527571
  validation accuracy:		82.07 %
Epoch 1951 of 2000 took 0.099s
  training loss:		0.545413
  validation loss:		0.527168
  validation accuracy:		82.61 %
Epoch 1952 of 2000 took 0.099s
  training loss:		0.529985
  validation loss:		0.536831
  validation accuracy:		82.39 %
Epoch 1953 of 2000 took 0.099s
  training loss:		0.531957
  validation loss:		0.514697
  validation accuracy:		82.39 %
Epoch 1954 of 2000 took 0.099s
  training loss:		0.527842
  validation loss:		0.524164
  validation accuracy:		82.17 %
Epoch 1955 of 2000 took 0.099s
  training loss:		0.525923
  validation loss:		0.540987
  validation accuracy:		82.61 %
Epoch 1956 of 2000 took 0.099s
  training loss:		0.534024
  validation loss:		0.518033
  validation accuracy:		82.50 %
Epoch 1957 of 2000 took 0.099s
  training loss:		0.529156
  validation loss:		0.523142
  validation accuracy:		82.61 %
Epoch 1958 of 2000 took 0.099s
  training loss:		0.540119
  validation loss:		0.531042
  validation accuracy:		82.39 %
Epoch 1959 of 2000 took 0.099s
  training loss:		0.547243
  validation loss:		0.523491
  validation accuracy:		82.39 %
Epoch 1960 of 2000 took 0.099s
  training loss:		0.531065
  validation loss:		0.523688
  validation accuracy:		82.17 %
Epoch 1961 of 2000 took 0.099s
  training loss:		0.532023
  validation loss:		0.527971
  validation accuracy:		82.50 %
Epoch 1962 of 2000 took 0.099s
  training loss:		0.542348
  validation loss:		0.521594
  validation accuracy:		82.50 %
Epoch 1963 of 2000 took 0.099s
  training loss:		0.531355
  validation loss:		0.538847
  validation accuracy:		82.93 %
Epoch 1964 of 2000 took 0.098s
  training loss:		0.530001
  validation loss:		0.521671
  validation accuracy:		81.85 %
Epoch 1965 of 2000 took 0.096s
  training loss:		0.528088
  validation loss:		0.527268
  validation accuracy:		82.72 %
Epoch 1966 of 2000 took 0.096s
  training loss:		0.537068
  validation loss:		0.542939
  validation accuracy:		82.50 %
Epoch 1967 of 2000 took 0.096s
  training loss:		0.539425
  validation loss:		0.526282
  validation accuracy:		82.50 %
Epoch 1968 of 2000 took 0.096s
  training loss:		0.533352
  validation loss:		0.535284
  validation accuracy:		82.17 %
Epoch 1969 of 2000 took 0.096s
  training loss:		0.528311
  validation loss:		0.527985
  validation accuracy:		82.39 %
Epoch 1970 of 2000 took 0.096s
  training loss:		0.527210
  validation loss:		0.532971
  validation accuracy:		82.17 %
Epoch 1971 of 2000 took 0.097s
  training loss:		0.532301
  validation loss:		0.546023
  validation accuracy:		82.50 %
Epoch 1972 of 2000 took 0.096s
  training loss:		0.530611
  validation loss:		0.533571
  validation accuracy:		82.17 %
Epoch 1973 of 2000 took 0.096s
  training loss:		0.537244
  validation loss:		0.520122
  validation accuracy:		82.61 %
Epoch 1974 of 2000 took 0.096s
  training loss:		0.532438
  validation loss:		0.538484
  validation accuracy:		82.28 %
Epoch 1975 of 2000 took 0.096s
  training loss:		0.532106
  validation loss:		0.526194
  validation accuracy:		82.72 %
Epoch 1976 of 2000 took 0.096s
  training loss:		0.528719
  validation loss:		0.529863
  validation accuracy:		81.74 %
Epoch 1977 of 2000 took 0.096s
  training loss:		0.525267
  validation loss:		0.522984
  validation accuracy:		82.93 %
Epoch 1978 of 2000 took 0.096s
  training loss:		0.540226
  validation loss:		0.562439
  validation accuracy:		81.41 %
Epoch 1979 of 2000 took 0.096s
  training loss:		0.544443
  validation loss:		0.521604
  validation accuracy:		82.07 %
Epoch 1980 of 2000 took 0.096s
  training loss:		0.531058
  validation loss:		0.524452
  validation accuracy:		82.50 %
Epoch 1981 of 2000 took 0.096s
  training loss:		0.536734
  validation loss:		0.525010
  validation accuracy:		82.28 %
Epoch 1982 of 2000 took 0.096s
  training loss:		0.528368
  validation loss:		0.522176
  validation accuracy:		81.96 %
Epoch 1983 of 2000 took 0.098s
  training loss:		0.534507
  validation loss:		0.518340
  validation accuracy:		81.85 %
Epoch 1984 of 2000 took 0.096s
  training loss:		0.533266
  validation loss:		0.526633
  validation accuracy:		82.61 %
Epoch 1985 of 2000 took 0.096s
  training loss:		0.530475
  validation loss:		0.530235
  validation accuracy:		83.04 %
Epoch 1986 of 2000 took 0.096s
  training loss:		0.529241
  validation loss:		0.528552
  validation accuracy:		82.28 %
Epoch 1987 of 2000 took 0.096s
  training loss:		0.529697
  validation loss:		0.517385
  validation accuracy:		82.28 %
Epoch 1988 of 2000 took 0.096s
  training loss:		0.528724
  validation loss:		0.519710
  validation accuracy:		82.50 %
Epoch 1989 of 2000 took 0.096s
  training loss:		0.531451
  validation loss:		0.522003
  validation accuracy:		81.85 %
Epoch 1990 of 2000 took 0.096s
  training loss:		0.537174
  validation loss:		0.523701
  validation accuracy:		82.28 %
Epoch 1991 of 2000 took 0.096s
  training loss:		0.527920
  validation loss:		0.529973
  validation accuracy:		82.72 %
Epoch 1992 of 2000 took 0.096s
  training loss:		0.528683
  validation loss:		0.523383
  validation accuracy:		82.17 %
Epoch 1993 of 2000 took 0.096s
  training loss:		0.533401
  validation loss:		0.514914
  validation accuracy:		82.72 %
Epoch 1994 of 2000 took 0.096s
  training loss:		0.533104
  validation loss:		0.534347
  validation accuracy:		82.39 %
Epoch 1995 of 2000 took 0.096s
  training loss:		0.534023
  validation loss:		0.532448
  validation accuracy:		82.72 %
Epoch 1996 of 2000 took 0.096s
  training loss:		0.533885
  validation loss:		0.543688
  validation accuracy:		82.28 %
Epoch 1997 of 2000 took 0.096s
  training loss:		0.537988
  validation loss:		0.521931
  validation accuracy:		83.15 %
Epoch 1998 of 2000 took 0.096s
  training loss:		0.527044
  validation loss:		0.519872
  validation accuracy:		82.61 %
Epoch 1999 of 2000 took 0.096s
  training loss:		0.529981
  validation loss:		0.543431
  validation accuracy:		82.39 %
Epoch 2000 of 2000 took 0.096s
  training loss:		0.527039
  validation loss:		0.537318
  validation accuracy:		82.72 %
Final results:
  test loss:			0.880945
  test accuracy:		73.88 %
